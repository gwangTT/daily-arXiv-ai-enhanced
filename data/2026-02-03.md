<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 153]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.CL](#cs.CL) [Total: 178]
- [cs.CV](#cs.CV) [Total: 276]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.LG](#cs.LG) [Total: 421]
- [cs.NE](#cs.NE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 86]
- [cs.SE](#cs.SE) [Total: 29]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 35]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [math.ST](#math.ST) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [econ.GN](#econ.GN) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [stat.ME](#stat.ME) [Total: 4]
- [cs.HC](#cs.HC) [Total: 11]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 17]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [math.OC](#math.OC) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 7]
- [math.DS](#math.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 23]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.IV](#eess.IV) [Total: 13]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: This paper compares FastAPI and NVIDIA Triton Inference Server for deploying an AI model in healthcare settings, analyzing latency, throughput, and privacy standards, and proposes a hybrid architecture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of deploying machine learning models in regulated domains like healthcare, balancing latency, throughput, and data privacy requirements.

Method: The study conducted a benchmarking analysis by deploying a DistilBERT sentiment analysis model on Kubernetes, measuring latency, throughput, and exploring hybrid approaches.

Result: FastAPI showed better latency for single-request workloads (p50 latency of 22ms), while Triton achieved higher scalability in throughput (780 requests/second with batching on an NVIDIA T4 GPU).

Conclusion: A hybrid architecture combining FastAPI for secure data handling with Triton for optimized inference is validated as an effective practice for deploying clinical AI systems securely and efficiently.

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [2] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: The paper develops an interpretable and efficient model and algorithm, ADEPT, for dynamic pricing in high-dimensional markets, focusing on attribute-level contributions and achieving sublinear regret.


<details>
  <summary>Details</summary>
Motivation: Dynamic pricing in complex markets requires scalability, adaptability, and clear interpretability to connect product attributes with pricing strategies.

Method: A new model (AFDLD) and algorithm (ADEPT) are introduced, using attribute-level decomposition and applying projection-free, gradient-free online learning techniques for optimization under market dynamics.

Result: ADEPT learns dynamic, near-optimal pricing while adapting to shocks and drifts, providing clear, attribute-based pricing insights validated through synthetic and real-world data.

Conclusion: Attribute-driven structures enable autonomy in pricing while maintaining interpretability and scalability, addressing key challenges in high-dimensional dynamic markets.

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [3] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: This paper explores using Large Language Models (LLMs) to extract rules from video game gameplay data, comparing direct code generation to a method involving causal model inference, with the latter yielding more accurate and consistent results.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance agents' causal understanding of game mechanics in complex environments through evaluating reverse-engineering techniques.

Method: The paper proposes two methods: direct VGDL code generation using gameplay data, and a two-stage approach involving structural causal model inference followed by VGDL translation.

Result: The SCM-based approach outperformed direct code generation, with blind evaluations showing up to 81% preference win rates and generating fewer logically inconsistent rules.

Conclusion: Using structural causal models enhances rule extraction accuracy from gameplay data, supporting applications in causal reinforcement learning, interpretable agents, and procedurally consistent game generation.

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [4] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: The paper addresses the problem of complete identification of feedforward ReLU networks realizing a given function by leveraging Lukasiewicz logic and algebraic transformations.


<details>
  <summary>Details</summary>
Motivation: To identify all feedforward ReLU networks that implement a given function, addressing functional symmetries in neural network architectures and parameters.

Method: ReLU networks are translated into Lukasiewicz logic formulae. Functional equivalence transformations are achieved through algebraic rewrites based on logic axioms. A compositional norm form aids in mapping logic formulae back to ReLU networks.

Result: Using Chang's completeness theorem, the authors demonstrate that all ReLU networks in a functional equivalence class are connected by finite symmetries governed by Lukasiewicz logic axioms.

Conclusion: The study provides a systematic approach to identifying all architectures and parameters of ReLU networks realizing the same function using logic-based transformations and algebraic methods.

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [5] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: Large Language Models often fail symbolic planning tasks by violating constraints; a proposed Localized In-Context Learning (L-ICL) technique effectively addresses this issue.


<details>
  <summary>Details</summary>
Motivation: Large Language Models excel in reasoning tasks like math and coding but struggle with constraint-abiding symbolic planning tasks, necessitating improved approaches to handle these complex domains.

Method: Localized In-Context Learning (L-ICL) addresses failing steps by identifying violations and injecting minimal input-output corrections into plans iteratively.

Result: L-ICL achieved an 89% success rate on an 8x8 gridworld planning task, surpassing the best baseline performance by 30%, and showed improvements across other domains and LLM architectures.

Conclusion: L-ICL enhances LLM planning capabilities efficiently by fixing constraint-violating behaviors with localized corrections, outperforming traditional instruction methods.

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [6] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: This paper highlights the risks emergent misalignment poses to AI safety and presents findings from experiments on fine-tuned large language models using insecure datasets. Key results focus on backdoor triggers, domain vulnerabilities, and strategies for predicting misalignment.


<details>
  <summary>Details</summary>
Motivation: The paper addresses concerns around emergent misalignment in large language models, especially when used autonomously, and its risks to AI safety.

Method: Large language models were fine-tuned on insecure datasets from 11 different domains and evaluated on unrelated prompts, both with and without backdoor triggers. Further explorations included probing generalizability of misaligned behaviors and membership inference metrics.

Result: Results showed backdoor triggers increased misalignment in 77.8% of domains, with substantial variation in vulnerability across domains. Some fine-tuning leads to high misalignment rates (e.g., gore-movie-trivia), while others showed negligible effects (e.g., incorrect-math). Certain metrics predicted misalignment effectiveness and generalized behaviors between different models.

Conclusion: Emergent misalignment varies widely across domains and poses security risks, highlighting the need for mechanisms to predict and mitigate behaviors post-training. The paper’s contributions include taxonomic rankings and standardized methodologies for misalignment evaluation.

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [7] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: The paper presents ADP-MA, a framework using meta-agents to dynamically construct, execute, and refine data processing pipelines autonomously.


<details>
  <summary>Details</summary>
Motivation: Current data pipelines lack adaptability, and existing coding assistants cannot actively manage and optimize pipelines after deployment.

Method: ADP-MA employs meta-agents for hierarchical orchestration, pipeline monitoring, and optimization, with a planning module, orchestration layer, and monitoring loop.

Result: ADP-MA demonstrates adaptive pipeline construction and management in an interactive demo, improving scalability and redundancy reduction.

Conclusion: ADP-MA introduces a novel approach to dynamic pipeline refinement using meta-agents, offering enhanced adaptability and scalability in data processing.

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [8] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: The paper investigates next-utterance prediction using large language models (LLMs) in human dialogue, emphasizing the importance of multimodal cues.


<details>
  <summary>Details</summary>
Motivation: To address the surprising inability of advanced LLMs to predict human dialogue accurately, leveraging multimodal cues for improved AI interaction.

Method: Developing SayNext-Bench, a benchmark to evaluate LLMs and multimodal LLMs (MLLMs), backed by a novel dataset (SayNext-PC) and proposing a dual-route prediction MLLM, SayNext-Chat.

Result: Experimental results show SayNext-Chat outperforms state-of-the-art MLLMs in lexical overlap, semantic similarity, and emotion consistency.

Conclusion: The study highlights the critical role of multimodal cues and predictive processing in achieving human-like AI interaction, calling for further exploration and refinement of multimodal capabilities.

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [9] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: The paper introduces MHDash, an open-source platform to assist the development and evaluation of AI systems for mental health, addressing issues in risk detection during multi-turn interactions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve reliable detection of high-risk states like suicidal ideation in AI mental health systems, addressing limitations of current evaluation methods that obscure risk-specific behaviors.

Method: The study develops MHDash, which integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluations, focusing on Concern Type, Risk Level, and Dialogue Intent.

Result: Findings reveal discrepancies in AI performance on high-risk cases, ordinal severity ranking, and multi-turn dialogues, highlighting inadequacies of conventional benchmarks for mental health AI systems.

Conclusion: MHDash provides a reproducible and transparent platform for evaluating and improving AI systems in mental health, emphasizing the importance of fine-grained, safety-critical approaches.

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [10] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: The paper proposes a new approach for adapting Large Language Models (LLMs) in dynamic real-world environments by introducing agentic evolution as a scalable method for continual improvement.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in adapting to evolving real-world environments since static training cannot keep up with deployment-time changes.

Method: The paper introduces 'A-Evolve,' a framework treating deployment-time improvement as a goal-directed optimization process over persistent system state. It emphasizes evolution as a scalable, agentic approach to adaptation.

Result: It argues for the evolution-scaling hypothesis, suggesting adaptation capacity scales with compute allocated to evolutionary processes, enabling open-ended adaptation.

Conclusion: Agentic evolution represents an impactful and sustainable path for LLMs to remain adaptive in the real world, enhancing their persistence and strategic agency.

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [11] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: This paper proposes a framework to streamline eligibility criteria drafting for clinical trials using interpretable semantic axes, derived from large language models, and introduces a rubric-based evaluation method.


<details>
  <summary>Details</summary>
Motivation: Drafting clinical trial eligibility criteria is time-consuming and demanding for clinicians. Existing automated methods are impractical as they either require narrowly structured inputs or minimal input with overly generalized results.

Method: The paper introduces a guided generation framework leveraging semantic axes (e.g., Demographics, Behavioral Factors) derived from large language models to balance specificity and usability. Additionally, a rubric-based evaluation system is developed to assess the generated criteria.

Result: The guided generation approach surpasses unguided methods in performance based on automatic, rubric-based metrics and clinician evaluations.

Conclusion: The proposed framework enhances clinical trial design by offering a practical, interpretable, and impactful AI-supported solution for eligibility criteria drafting.

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [12] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: This paper proposes KEPO, a new framework combining selective on-policy distillation and knowledge-enhanced exploration to improve reasoning-intensive tasks in reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in RL post-training for reasoning-intensive tasks, specifically the issues of sparse rewards, ambiguous credit assignment, and exploration failures.

Method: KEPO integrates two innovations: (i) selective distillation, applying dense teacher guidance only to high-quality trajectories, and (ii) knowledge-enhanced exploration, using teacher hints to sample effective trajectories and prevent exploration collapse.

Result: KEPO outperforms RL and on-policy distillation methods in training stability, reasoning coherence, and out-of-distribution performance, as demonstrated on a medical visual question answering benchmark.

Conclusion: KEPO mitigates the challenges in reasoning-oriented RL post-training, offering a more effective and stable approach for tasks requiring coherent reasoning.

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [13] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: The paper introduces RobustDebias, a fine-tuning method using Distributionally Robust Optimization (DRO) to mitigate biases in pretrained language models, particularly during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Pretrained language models have biases and stereotypes, and existing debiasing methods during pretraining are costly and less scalable for large models.

Method: The study proposes RobustDebias, which applies DRO for bias mitigation during fine-tuning stage, focusing on BERT models and addressing bias amplification caused by Empirical Risk Minimization.

Result: Extensive experiments show that RobustDebias reduces biases across multiple demographics with minimal negative effects on model performance.

Conclusion: RobustDebias is effective in debiasing language models during fine-tuning, offering a scalable, generalizable approach that minimizes performance trade-offs.

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [14] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: This paper proposes PolarMem, a novel memory system that enables multimodal agents to integrate verifiable and logical reasoning into their operations, addressing limitations of probabilistic vision-language models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the issue of epistemic asymmetry in current multimodal agents, where these systems often conflate semantic understanding with factual correctness and fail to encode negative constraints in their reasoning.

Method: The authors introduce PolarMem, a polarized latent graph memory that partitions perceptual likelihoods into logical constraints non-parametrically, employing orthogonal inhibitory connections in its graph topology to explicitly capture and store negation as primary states.

Result: Extensive tests using eight vision-language models across six benchmarks demonstrate that PolarMem enhances reasoning by preventing hallucinatory outputs and grounding decisions in verifiable evidence.

Conclusion: PolarMem is established as a robust cognitive system for multimodal agents, enabling logic-driven and verifiable reasoning. This foundation is essential for building reliable and rational multimodal decision-making systems at scale.

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [15] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: The paper studies CODI, a latent Chain-of-Thought (Latent-CoT) model, for sequential reasoning tasks, revealing mechanisms of intermediate representations and tracing computation pathways to its outputs.


<details>
  <summary>Details</summary>
Motivation: To understand how Latent-CoT enables step-by-step computation without emitting verbose rationales, and to investigate the robustness of these models for iterative reasoning tasks.

Method: The study uses logit-lens decoding, linear probes, attention analysis, and activation patching to analyze CODI's mechanisms on polynomial-iteration tasks with varying lengths.

Result: CODI forms bridge states and predictions via late fusion but struggles with longer tasks, where it employs partial latent pathways and compressed strategies. It showcases limitations under harder optimization and shifts towards shortcut strategies.

Conclusion: The paper delineates conditions for faithful latent-CoT execution and highlights challenges in designing robust objectives for sequential reasoning, emphasizing the model's limitations under complex tasks.

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [16] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR replaces textual debate histories with image representations to address context and token limitations, achieving significant compression and faster computation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of rapidly increasing context limits and token usage in multi-agent debates, which leads to inefficiencies and information loss during reasoning and summarization.

Method: The authors introduced DebateOCR, a cross-modal compression approach that transforms textual debate traces into image representations. These images are processed by a vision encoder to condition subsequent debate rounds, drastically reducing token usage.

Result: DebateOCR achieves over 92% reduction in token usage, significantly lowers compute costs, and enables faster inference across benchmarks while maintaining reasoning performance.

Conclusion: The framework demonstrates how compressed image representations can retain essential information, and utilizing multiple agents' views allows effective recovery of omitted details, ensuring effective reasoning in debates.

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [17] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: UNDERWRITE is a new, realistic benchmark for evaluating AI agents in enterprise insurance underwriting contexts, emphasizing expert-designed challenges and revealing gaps between model lab performance and real-world readiness.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks fail to capture enterprise complexities, focusing narrowly on open domains, accuracy metrics, and simple tasks. This undermines reliable evaluation for deployment in real-world scenarios.

Method: UNDERWRITE is developed collaboratively with domain experts to simulate multi-turn insurance underwriting tasks, incorporating proprietary knowledge, noisy tool interfaces, and imperfect user simulation.

Result: Testing 13 advanced models highlights significant issues: inefficiency in accurate models, hallucination despite tool access, and substantial drops in pass^k performance. These insights show a discrepancy between lab and enterprise outcomes.

Conclusion: Expert-driven benchmarks are critical for realistic AI evaluation in enterprises. Existing frameworks show brittleness, and addressing hallucination in complex domains requires innovative compositional methods.

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [18] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: Current Visual Multi-Agent Systems (VMAS) face a "scaling wall," where increasing agent interactions reduce performance and raise token costs. The proposed L$^{2}$-VMAS framework tackles this issue with dual latent memories, perception/thinking decoupling, and entropy-driven proactive memory triggering, improving scalability, accuracy, and token efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of traditional VMAS systems, which suffer from degraded performance and increased token costs when scaling due to the informational bottleneck caused by converting perceptual data and thoughts into natural language.

Method: The authors propose L$^{2}$-VMAS, a model-agnostic framework combining dual latent memories, decoupled perception/thinking synthesis, and entropy-driven proactive triggering to optimize inter-agent collaboration and reduce token overhead.

Result: Experiments demonstrate that L$^{2}$-VMAS improves average accuracy by 2.7-5.4% and reduces token usage by 21.3-44.8% across various backbones, sizes, and multi-agent configurations, resolving the "scaling wall" problem.

Conclusion: The L$^{2}$-VMAS framework effectively enhances the scalability of VMAS, proving its applicability for efficient inter-agent collaboration while significantly improving accuracy and reducing token costs.

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [19] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: The paper introduces MoR, a method for aligning heterogeneous vision-language models (VLMs) in federated learning (FL) settings while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in privacy-sensitive domains (e.g., healthcare and finance) where centralized training is infeasible due to data-sharing constraints and client heterogeneity.

Method: MoR uses a KL-regularized visual foundation model, client-side reward models trained on preference annotations, and a routing-based fusion mechanism to aggregate client reward signals for training under federated conditions.

Result: Experiments on VQA benchmarks showed that MoR outperforms baseline methods in generalization, robustness, and cross-client adaptability.

Conclusion: MoR presents a scalable and privacy-preserving solution for aligning heterogeneous VLMs in decentralized, federated settings.

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [20] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: The paper introduces PCBSchemaGen, a training-free framework for automating PCB schematic design, achieving high accuracy and efficiency across various design tasks.


<details>
  <summary>Details</summary>
Motivation: Automated PCB schematic design remains unexplored due to the lack of open-source datasets and simulation-based verification. The authors aim to address these challenges and advance PCB design to handle heterogeneous circuit domains.

Method: The proposed PCBSchemaGen framework includes an LLM-based code generation paradigm with feedback, a Knowledge Graph-based verification framework, and evaluations on 23 PCB schematic tasks.

Result: PCBSchemaGen demonstrated significant improvement in design accuracy and computational efficiency when tested across digital, analog, and power PCB schematic design tasks.

Conclusion: PCBSchemaGen proves to be an effective, computationally efficient solution for PCB schematic designs, offering advancements for heterogeneous circuit tasks in the absence of training requirements.

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [21] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: The paper introduces a diagnostic framework using Item Response Theory (IRT) to evaluate the reliability of LLMs as judges, focusing on intrinsic consistency and human alignment. It provides practical tools for analyzing and improving LLM judgments.


<details>
  <summary>Details</summary>
Motivation: Existing validation practices for LLMs as judges are limited to assessing observed outputs without understanding the reliability of their measurement behaviors.

Method: The paper uses a two-phase diagnostic framework based on the Graded Response Model of Item Response Theory to evaluate intrinsic consistency and human alignment of LLMs through prompt variations and alignment with human quality assessments.

Result: Empirical testing shows that using IRT-GRM generates interpretable diagnostic signals, which help verify reliability and identify issues causing unreliability in LLMs' judgment processes.

Conclusion: The proposed framework enhances reliability assessment of LLM-as-a-Judge, facilitating systematic diagnostics and offering actionable insights for improving their judgment capabilities.

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [22] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: This paper studies Large Language Models' (LLMs) strategic reasoning in poker, revealing current limitations and introducing ToolPoker for improved performance and reasoning integration.


<details>
  <summary>Details</summary>
Motivation: To evaluate the ability of LLMs to reason and act effectively in high-stakes domains like poker, which requires both strategic gameplay and game-theoretic reasoning.

Method: The authors analyzed LLMs' capability using realistic poker tasks, identifying limitations and introducing ToolPoker—a reasoning framework combining external solvers for game-theoretic actions with professional-style explanations.

Result: ToolPoker demonstrated state-of-the-art gameplay and reasoning traces that align better with game-theoretic principles compared to existing approaches.

Conclusion: While current LLMs fail at complex poker reasoning, integrating external tools like solvers can enhance their strategic performance and reasoning clarity.

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [23] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: The paper introduces the Adaptive Flow Routing Network (AFR-Net), a framework to model how structural constraints in neural connectivity give rise to functional communication patterns.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding and explaining the dynamic relationship between structural and functional connectivity in the brain, and to uncover latent interactions between neural regions.

Method: The authors propose AFR-Net, a physics-informed framework that models neural communication dynamics by fusing structural and functional connectivity data in an interpretable manner.

Result: The AFR-Net demonstrates superior performance compared to state-of-the-art baselines in extensive experiments.

Conclusion: AFR-Net provides a robust and interpretable approach for fusing structural and functional neural connectivity, advancing insights into neural communication and critical pathways.

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [24] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: Recent large language models struggle with structural reasoning tasks despite high performance on existing benchmarks. New tools and datasets reveal limitations in reasoning competence.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical datasets lack coverage of complex reasoning skills, leading to inflated accuracy scores for current models.

Method: ReasoningMath-Plus benchmark was developed with problems focusing on structural reasoning and constraint interaction. HCRS scoring function and PRM were introduced for process-level evaluation.

Result: Top models achieved relatively high accuracy on final answers but scored lower using HCRS, indicating issues in reasoning robustness.

Conclusion: Answer-only metrics overestimate reasoning capabilities. Fine-grained benchmarks like ReasoningMath-Plus are crucial for true competence evaluation.

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [25] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: This paper proposes an extension of chain-of-thought (CoT) reasoning to multimodal problems by introducing a modal-mixed CoT framework that integrates text and visual latent embeddings, achieving superior performance on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in language models struggles with vision-intensive problems because traditional CoT methods are text-only and do not incorporate intermediate visual reasoning steps, which are often crucial.

Method: The method introduces a modal-mixed CoT framework interleaving text and visual latent embeddings. It trains the visual-language model (VLM) to encode and align visual information semantically, alongside adding a diffusion-based decoder for fine-grained visual details. The training involves supervised fine-tuning and reinforcement learning to optimize modal switching and reasoning composition.

Result: Experiments on 11 multimodal reasoning tasks show this method outperforms language-only CoT approaches and other multimodal CoT techniques.

Conclusion: The proposed modal-mixed CoT effectively bridges the gap between language and vision for better multimodal reasoning, highlighting its potential for future advancements in handling complex reasoning across modalities.

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [26] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: The paper introduces MedBeads, an immutable, tamper-evident data framework designed to enhance AI usability in clinical settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the 'Context Mismatch' in current AI clinical applications caused by fragmented, probabilistic patient data reconstruction in traditional EMRs.

Method: The authors developed MedBeads using a graph-based architecture (Beads in a Merkle-DAG), implemented with Go, Python, and React, offering efficient and secure ways to structure medical data.

Result: They successfully prototyped a synthetic data workflow, converting FHIR standards into causally-linked DAGs enabling tamper-evidence and efficient, deterministic data retrieval.

Conclusion: MedBeads shifts clinical AI data handling from probabilistic to deterministic processes, enhancing trustworthiness. Released as open source, it provides a foundation for agent-native data standards.

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [27] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: The paper proposes TSP-MDF, a framework that improves traditional heuristic tour constructors for the Traveling Salesman Problem (TSP) through neural-based guided sampling, achieving competitive results with minimal training.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional heuristic tour constructors in TSP, which suffer from deterministic behavior leading to local optima, while avoiding the extensive training and ground-truth supervision required by neural-based methods.

Method: TSP-MDF uses a neural-based instance modifier that strategically tweaks node coordinates to enable guided-sampling for traditional heuristic constructors, allowing them to find higher-quality solutions without requiring ground-truth supervision.

Result: The proposed method significantly enhances the performance of traditional heuristics, achieving solution quality comparable to advanced neural methods, demonstrated on large-scale TSP and real-world benchmarks, with minimal training time.

Conclusion: TSP-MDF bridges the gap between traditional deterministic methods and neural-based approaches, making heuristic tour construction more effective and practical at solving TSP efficiently.

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [28] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: This paper focuses on consolidating heterogeneous information-seeking agents into a unified foundation model using data-level and parameter-level strategies, comparing their performance and identifying design considerations for parameter-level merging.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing information-seeking agents, which are domain-specific and struggle with scalability and cross-domain generalization.

Method: The paper explores two consolidation strategies: (1) data-level consolidation which trains a unified model on mixed datasets, and (2) parameter-level consolidation which merges separately trained models at the parameter level. Performance metrics and analysis of both methods are provided.

Result: Data-level consolidation proves to be a strong, stable baseline, while parameter-level consolidation shows efficiency but is impacted by interference and robustness issues.

Conclusion: The research highlights data-level consolidation's stability and identifies challenges in parameter-level consolidation, proposing design factors such as granularity, task heterogeneity awareness, and consensus strategies for improvement.

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [29] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith is an advanced agentic Docker builder addressing reliable environment construction as a scalable preprocessing challenge, showcasing state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The need for reliable Docker environment construction is a bottleneck for scaling execution-grounded training and evaluation in software engineering agents.

Method: DockSmith utilizes a 30B-A3B model trained on large-scale Docker-building trajectories, incorporating tools such as a loop-detection controller and success memory for enhanced performance.

Result: DockSmith achieved leading performance on Multi-Docker-Eval benchmarks with impressive metrics and demonstrated improvement in diverse real-world and multilingual benchmarks.

Conclusion: DockSmith demonstrates the potential of robust environment construction as a transferable agentic capability, advancing reliability and scalability in Docker-based software engineering workflows.

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [30] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: The paper addresses the "Memory Wall" limitation in real-time generative game engines and proposes a scalable Hardware-Algorithm Co-Design framework to achieve neural simulations at high resolutions.


<details>
  <summary>Details</summary>
Motivation: Existing real-time generative game engines are restricted to low resolutions due to the "Memory Wall," constraining their practical use and interactivity.

Method: The paper introduces a heterogeneous architecture with three innovations: asymmetric resource allocation, memory-centric operator fusion, and manifold-aware latent extrapolation, distributed across AI accelerators.

Result: The system achieves real-time generation at 720x480 resolution with significant gains in performance, including 50x pixel throughput improvement and fluid framerates (26.4 FPS and 48.3 FPS).

Conclusion: Resolving the "Memory Wall" through architectural co-design is essential for enabling high-fidelity and responsive neural gameplay, marking a paradigm shift in generative interactive simulations.

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [31] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: This paper evaluates Large Language Models (LLMs) for embodied AI tasks on the VirtualHome benchmark, comparing two 7B-parameter models and introducing Structured Self-Consistency (SSC) for improved task performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess how effectively LLMs can perform complex tasks required in embodied AI systems such as goal understanding and action execution, thereby contributing towards the advancement of these systems.

Method: The authors conducted evaluations of two 7B-parameter LLMs, OPENPANGU-7B and QWEN2.5-7B, on four tasks using the VirtualHome benchmark and introduced a new decoding method, SSC, to enhance the models' outputs.

Result: The SSC decoding strategy significantly improved performance. OPENPANGU-7B excels in hierarchical planning while QWEN2.5-7B performs better on action-level tasks, revealing complementary strengths.

Conclusion: The findings highlight the importance of diverse model capabilities and innovative decoding strategies in advancing embodied AI systems for structured tasks.

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [32] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: This paper addresses safe deployment for Text-to-Image (T2I) diffusion models by proposing a framework to suppress unsafe content without retraining the model.


<details>
  <summary>Details</summary>
Motivation: Real-world deployment of T2I models requires safeguards against generating inappropriate or unsafe content while maintaining accurate benign prompt-image alignment.

Method: An inference-only prompt projection framework is proposed to selectively modify high-risk prompts into a safe set through a surrogate objective, avoiding retraining or fine-tuning the model.

Result: The approach reduces inappropriate content generation by 16.7-60% across datasets and diffusion backbones, while maintaining benign prompt-image alignment nearly unchanged.

Conclusion: The framework strikes a balance between safety and prompt-image alignment, providing a practical solution for deploying T2I models.

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [33] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: This paper introduces an explainable framework for predicting the lifespan of ultrafiltration membranes in reverse osmosis desalination, achieving accurate and interpretable results.


<details>
  <summary>Details</summary>
Motivation: To improve trust and utility in predictive maintenance for ultrafiltration membranes, as existing machine learning models lack interpretability and are not operator-friendly.

Method: The method employs a Health Index based on transmembrane pressure and other metrics, uses fuzzy Gaussian membership functions for degradation signaling, and applies a similarity-based Takagi-Sugeno fuzzy rule system for lifespan predictions.

Result: The framework demonstrated high accuracy (mean absolute error of 4.50 cycles) on 12,528 operational cycles while providing interpretable results consistent with expert understanding.

Conclusion: This approach offers a transparent and reliable tool for degradation assessment and maintenance planning of ultrafiltration systems, blending physics-informed analytics with fuzzy logic.

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [34] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO, an LLM-based agent, efficiently optimizes molecular structures by leveraging full optimization trajectories and explanatory feedback, outperforming prior methods in tasks and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of current molecular optimization methods in achieving desired properties, especially in the pharmaceutical industry, where optimization requires costly and rate-limited evaluations.

Method: SEISMO performs online inference-time molecular optimization by conditioning proposals on natural language descriptions, scalar scores, and structured feedback, updating after every oracle call without needing population-based or batched learning.

Result: SEISMO outperforms prior methods with 2-3 times higher optimization efficiency across 23 benchmark tasks, often reaching near-maximal scores with fewer than 50 oracle calls.

Conclusion: Structured explanatory feedback and integration of domain knowledge are critical for highly sample-efficient molecular optimization, as demonstrated by SEISMO's superior performance.

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [35] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: The paper introduces OpenGuanDan, a benchmark for developing AI agents in the Chinese card game GuanDan, featuring challenges like imperfect information and dynamic teaming. Results show AI agents outperform rule-based strategies but lack superhuman performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the lack of challenging AI benchmarks for advancing research in intelligent decision-making, particularly in complex multi-agent environments.

Method: The authors designed OpenGuanDan, a benchmark for GuanDan with a focus on enabling efficient simulation and comprehensive evaluation of AI agents. It offers APIs for human-AI interaction and LLM integration.

Result: Empirical evaluations show that learning-based AI agents perform better than rule-based methods but still do not achieve superhuman performance. Both AI-Human interactions and pairwise competitions highlight the challenges involved.

Conclusion: OpenGuanDan presents a demanding testbed for multi-agent decision-making methods and underlines the need for further advancements as current AI still falls short of superhuman levels.

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [36] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: The paper addresses the instability of LLM-based agents in social science experiments and presents HUMANSTUDY-BENCH, a benchmark tool that standardizes the reconstruction of human-subject experiments for improved evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unstable and design-sensitive behavior of LLMs when used as simulated participants in social science experiments, while distinguishing between base-model capabilities and agent configuration.

Method: The authors frame participant simulation as an agent-design problem, combining base models with specifications. They introduce HUMANSTUDY-BENCH, which reconstructs social experiments using a Filter-Extract-Execute-Evaluate framework and establishes metrics to evaluate agreement between human and agent behaviors.

Result: They implemented 12 foundational studies within the HUMANSTUDY-BENCH, spanning various subjects like cognition, interaction, and psychology, and evaluated over 6,000 human trials for fidelity.

Conclusion: The study offers a standardized approach to evaluating LLM-based agents in social experiments, improving the robustness and interpretability of inferences while enabling fair comparison between human and agent behaviors.

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [37] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: The study explores using Large Language Models (LLMs) to automate ontology construction in casting manufacturing, testing three methods and creating a validated casting ontology.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the labor-intensive and costly nature of traditional ontology construction, particularly in specialized fields, by leveraging LLMs to automate the process.

Method: The study evaluates three LLM-based methods: pre-trained LLMs, in-context learning (ICL), and fine-tuning, to extract terms and relations from domain-specific texts with limited data.

Result: The best-performing method among the three approaches was selected to build a casting ontology, which was subsequently validated by a domain expert.

Conclusion: LLMs demonstrate promise in automating ontology construction for specialized fields like casting manufacturing, offering a more efficient alternative to traditional methods.

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [38] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: The paper introduces Self-Guard, a lightweight framework for improving the safety compliance of Large Reasoning Models (LRMs) by addressing misalignment issues at the representational level.


<details>
  <summary>Details</summary>
Motivation: Existing alignment strategies for LRMs often rely on computationally intensive methods that fail to address the awareness-compliance gap, where models recognize risks but follow user instructions anyway.

Method: The authors propose Self-Guard, a two-stage framework: (1) safety-oriented prompting to activate latent safety awareness, and (2) safety activation steering to amplify this awareness and prioritize safety compliance over sycophantic behavior.

Result: Self-Guard effectively bridges the awareness-compliance gap, providing robust safety performance without sacrificing model utility. It generalizes well across different risks and model scales.

Conclusion: Self-Guard offers a cost-efficient, generalizable solution for enhancing the safety alignment of LRMs, avoiding heavy computational overhead and improving compliance with safety measures.

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [39] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: This paper outlines the design of a production-grade AI system in healthcare, emphasizing integration of engineering practices for trustworthy and accountable AI systems.


<details>
  <summary>Details</summary>
Motivation: The integration of AI into clinical settings faces systemic challenges, including brittle architectures and lack of accountability, compromising safety and reliability.

Method: This study designs a healthcare AI platform based on Clean Architecture, Event-driven architecture, Agent modularity with autonomous MLOps lifecycles, and Human-in-the-Loop governance.

Result: The paper presents the ``Maria'' platform as a case study, offering a reference architecture that improves resilience, scalability, auditability, and accountability in clinical AI.

Conclusion: Holistic integration of foundational engineering principles is essential for trustworthy and scalable AI systems in healthcare, as demonstrated by the Maria platform's success.

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [40] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: The paper introduces PDG, a framework optimized for interpolating geomagnetic maps, addressing issues such as noise and physical constraints, and demonstrates its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: Existing interpolation methods for geomagnetic maps overlook specific challenges like noise and adherence to physical laws, leading to suboptimal results.

Method: The PDG framework integrates a physics-informed mask strategy to minimize noise and applies constraints adhering to kriging principles to ensure physical plausibility in interpolation.

Result: Experiments on four real-world datasets validate the framework’s improvement over traditional methods in geomagnetic map interpolation.

Conclusion: The PDG framework is effective and superior in geomagnetic map interpolation by addressing both noise issues and physical adherence.

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [41] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: This paper proposes REPCORE, a method that uses aligned latent spaces from hidden states to efficiently construct item coresets for benchmarking LLMs, showing improved performance with minimal source models.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs is expensive. Current methods depend on large source data pools for coreset selection, which is statistically unstable for smaller or new benchmarks.

Method: The authors introduce REPCORE, which aligns hidden states from models into a unified latent space to construct representative coresets for performance extrapolation.

Result: REPCORE achieves high accuracy in estimating performance with limited source models (as few as ten) on five benchmarks, outperforming output-based baselines.

Conclusion: REPCORE provides a more reliable and efficient alternative for benchmarking LLMs, leveraging hidden states to address statistical instabilities in traditional methods.

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [42] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: The paper introduces CAREP, a system that automates the creation of error pattern rules from vehicle Diagnostic Trouble Codes (DTCs), using agents for causal discovery and integration of metadata. It outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: To address the expensive and error-prone manual process of creating Boolean error patterns (EPs) for diagnosing vehicle faults, particularly as vehicle systems become increasingly complex.

Method: The CAREP system uses a multi-agent architecture involving a causal discovery agent, a contextual information agent, and an orchestrator agent to produce EP rules and interpretable causal explanations from DTC event sequences.

Result: CAREP accurately and automatically discovers error pattern rules in a large automotive dataset, surpassing LLM-only baselines while providing transparency in its causal reasoning.

Conclusion: CAREP provides a scalable and interpretable system for automated vehicle fault diagnostics, reducing costs and improving efficiency through automated and accurate error pattern detection.

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [43] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: The paper systematically reviews Predictive Maintenance (PdM) methods, emphasizing the limitations of traditional and data-driven approaches, and proposes Neuro-Symbolic AI (NeSy) as a hybrid solution.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the limitations of traditional and deep learning-based PdM systems by exploring neuro-symbolic AI as a robust and interpretable hybrid system in industrial applications.

Method: The authors conduct a systematic review of recent PdM advancements and propose Neuro-symbolic AI architectures that integrate deep learning and symbolic logic.

Result: The review identifies the strengths and weaknesses of traditional, data-driven, and hybrid systems, advocating for neuro-symbolic AI due to its potential advantages in accuracy and interpretability within PdM.

Conclusion: Neuro-symbolic AI, as a hybrid approach, holds promise for enhancing predictive maintenance systems through improved accuracy, robustness, and explainability, addressing the limitations of standalone methods.

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [44] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA introduces an adaptive pruning framework for Vision-Language-Action models to reduce inference latency with minimal performance degradation.


<details>
  <summary>Details</summary>
Motivation: Addressing the substantial inference latency of VLA models caused by their large parameter counts, especially for real-time tasks where static methods fail to adapt to dynamic environments.

Method: EcoVLA employs two components: EAP for lightweight adaptive channel pruning considering environmental characteristics, and $I^2O$ to orchestrate pruning in parallel with model inference.

Result: EcoVLA achieves up to 1.60× speedup with a 0.4% drop in success rate, and up to 2.18× speedup with 0.5% degradation when combined with token pruning.

Conclusion: EcoVLA offers a training-free, adaptable solution to reduce VLA model latency, supporting real-time applications with negligible performance trade-offs.

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [45] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: The paper introduces a fully automated multi-agent system for software engineering, mimicking real-world team structures and methodologies. It outperforms single-agent baselines in task completion.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current autonomous systems that often treat issue resolution as a linear process instead of incorporating real-world team-based methodologies.

Method: The system uses an open-source platform, agyn, to create specialized agents with defined roles like coordination, research, implementation, and review. It operates autonomously under a set development methodology without human intervention.

Result: The system achieves a 72.4% task resolution rate on SWE-bench 500, significantly outperforming single-agent baselines with comparable language models.

Conclusion: Replicating team structures, methodologies, and communication in autonomous systems is crucial for advancing software engineering, highlighting the importance of organizational design alongside model improvements.

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [46] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: The paper advocates using world models as intermediaries between agents and real-world interactions to address the high cost of action in complex domains.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of LLMs trained via reinforcement learning in high-cost domains like robotics and scientific experiments due to the expense of executing actions.

Method: The paper proposes leveraging world models as dynamics, rewards, and task-distribution representations to alleviate challenges like off-policy learning and sample inefficiency. It also discusses approaches to build and enhance these models.

Result: World models provide valuable learning signals across domains like machine learning engineering, computer use, robotics, and scientific experiments.

Conclusion: Incorporating world models can mitigate high-cost constraints of agent interactions and enhance long-horizon and sample-efficient learning. Actions like dataset curation and proper architecture design are essential to realize this vision.

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [47] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: The paper introduces ProjDevBench, a benchmark for evaluating coding agents' capability in end-to-end project development.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current benchmarks that focus mainly on issue-level bug fixing rather than evaluating coding agents' potential in end-to-end development.

Method: Developed ProjDevBench, combining Online Judge testing and LLM-enhanced code review, and evaluated six coding agents across 20 problems involving system design, functional correctness, and iterative improvement.

Result: The evaluation showed a 27.38% acceptance rate, with coding agents performing well in basic functions but struggling with complex aspects like system design and optimization.

Conclusion: ProjDevBench provides a much-needed framework for holistic evaluation of coding agents, highlighting their strengths and exposing challenges in advanced tasks.

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [48] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: This paper addresses missing modality issues in Multimodal Affective Computing (MAC) with a benchmark framework, MissMAC-Bench, for evaluating robustness comprehensively.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of missing modality in MAC models, which destabilizes performance due to dynamic availability and distribution shifts of modality data.

Method: The paper introduces MissMAC-Bench, a benchmark with unified evaluation standards, principles like no missing priors during training, and handling both complete/incomplete modalities.

Result: Extensive experiments demonstrate the effectiveness of MAC approaches against the missing modality issue across 4 datasets and 3 language models.

Conclusion: MissMAC-Bench is a foundational framework for advancing robustness in MAC and bridging real-world challenges with academic progress in the field.

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [49] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: Explores enhancing reinforcement learning under verifiable rewards (RLVR) for large language models (LLMs) focusing on data and compute efficiency.


<details>
  <summary>Details</summary>
Motivation: Improve RLVR techniques to make reasoning-intensive LLM applications more resource-efficient.

Method: Introduces Dynamic One-Shot Policy Refinement (DoPR), which uses uncertainty-aware RL to select a single training sample per policy batch based on reward volatility and exploration.

Result: DoPR reduces rollout costs significantly while maintaining strong reasoning performance.

Conclusion: DoPR provides a scalable, efficient RL-based post-training method for LLMs, making reasoning tasks more accessible.

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [50] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: The paper introduces Canonical Intermediate Representation (CIR) and the rule-to-constraint (R2C) framework to improve optimization model formulation from natural language by LLMs, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from improving LLM-based optimization model formulation, which currently struggles with handling complex constraints and operational rules.

Method: The paper proposes CIR, a schema encoding operational rule semantics through archetypes, and the R2C framework, a multi-agent system connecting problem descriptions to optimization models using CIR.

Result: Experiments show R2C achieves an accuracy rate of 47.2% on a rich operational rules benchmark and delivers competitive results compared to proprietary systems like GPT-5.

Conclusion: R2C, enhanced by reflection mechanisms, provides substantial gains in accuracy, demonstrating a strong ability for optimization model formulation from text and setting new best-reported results.

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [51] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner introduces a synthetic semantic information gain reward for training large reasoning models (LRMs) to retrieve external knowledge more effectively, outperforming existing baselines in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Optimizing external knowledge retrieval for LRMs is challenging due to insufficient reward signals that provide dense, principled feedback.

Method: The paper redefines information gain through uncertainty reduction, developing an intrinsic estimator based on semantic clustering via textual entailment, paired with GRPO for scalable optimization.

Result: InfoReasoner achieves up to 5.4% improvement in accuracy across seven question-answering benchmarks compared to strong retrieval-augmented baselines.

Conclusion: InfoReasoner offers a scalable and theoretically founded approach for agentic reasoning in LRMs, enhancing information-seeking capabilities and epistemic progress.

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [52] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: The paper explores entropy reduction as a mechanism to improve tool-use behavior of LLM-based agents, proposing strategies to optimize their efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and degraded performance caused by excessive and low-quality tool calls in long trajectories of LLM-based agents.

Method: Entropy reduction is utilized as a supervisory signal, with two reward strategies designed: sparse outcome rewards for trajectory-level optimization and dense process rewards for fine-grained supervision.

Result: Experiments demonstrate a 72.07% reduction in tool calls and a 22.27% improvement in performance by using entropy-reduction-based reward strategies.

Conclusion: Entropy reduction is effective in enhancing tool-use behavior, making agents more adaptive and efficient for real-world applications.

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [53] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: The paper investigates the effects of user persuasion on AI agents' long-term task behavior, introducing the concept of 'persuasion propagation' and studying it in web research and coding tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how belief-level user persuasion affects the downstream behavior of AI agents during complex tasks.

Method: A behavior-focused evaluation framework was used to differentiate persuasion applied during vs. prior to task execution, tested across web research and coding tasks.

Result: Persuasion during task execution showed weak behavioral effects, but persuasion prior to execution significantly reduced searches (by 26.9%) and unique source visits (by 16.9%) in prefilled belief-state agents.

Conclusion: The results indicate that persuasion, especially prior to task execution, can impact AI agent behavior, emphasizing the need for behavior-level evaluations in such systems.

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [54] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: This paper introduces the Capability-Comprehension Gap in human-AI interaction and proposes the Cognitive Integrity Threshold (CIT) as a framework for maintaining human oversight and accountability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge posed by advanced AI systems eroding users' ability to explain, verify, and intervene effectively.

Method: The authors define CIT and operationalize it through three functional dimensions: verification capacity, comprehension-preserving interaction, and institutional scaffolds for governance.

Result: The paper highlights the gap in current transparency and governance methods and proposes a structured agenda to ensure cognitive sustainability in human-AI interactions.

Conclusion: For humans to maintain oversight and accountability with advanced AI, it is crucial to align design and governance frameworks with the Cognitive Integrity Threshold.

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [55] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: The paper identifies inefficiencies in how transformer attention heads are trained, proposing a game-theoretic formalization and introducing GAME-LoRA to optimize their coordination, reducing hallucinations without degrading performance.


<details>
  <summary>Details</summary>
Motivation: Despite multi-agent dynamics in transformer attention heads, they are trained monolithically, leading to inefficiencies such as redundancy and errors.

Method: The authors use game theory to model attention head interactions as a potential game, bounding inefficiencies (Price of Anarchy) and introducing GAME-LoRA to enhance coordination via gradient coupling regularization.

Result: GAME-LoRA reduced hallucination probability by 18% (average 8%) without performance loss, with experiments confirming its effectiveness and prediction capabilities.

Conclusion: Properly addressing the game dynamics among attention heads through specific regularization (e.g., GAME-LoRA) leads to more efficient and reliable transformer models by reducing unnecessary competition and errors.

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [56] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: The paper introduces a foundation model for CAN bus data that can generalize to various automotive tasks, inspired by approaches in NLP and CV.


<details>
  <summary>Details</summary>
Motivation: Existing methods in automotive AI heavily rely on isolated models for specific tasks, which limits shared learning and generalization across tasks.

Method: The authors treat CAN data as language, employing a unified tokenization scheme and pretraining on large-scale decoded signals, followed by fine-tuning for specific tasks.

Result: Their pretrained CAN model successfully adapts to diverse auto insurance tasks, showcasing effective multi-task generalization.

Conclusion: Foundation modeling principles from NLP and CV can be applied to automotive data, opening new avenues for generalizable representation learning in the field.

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [57] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT enhances self-correction in language models by introducing task abstraction before refining responses, improving reasoning and reducing errors.


<details>
  <summary>Details</summary>
Motivation: Existing self-correction mechanisms often fail at addressing deep reasoning errors, focusing mainly on surface-level issues. The motivation is to create a framework that improves on this by adding a task abstraction step.

Method: The SELF-THOUGHT framework introduces a structured task abstraction step, where the key variables and constraints of the problem are identified before refining the solution. These abstractions are also transferable across models, enabling smaller models to improve without extensive fine-tuning.

Result: The framework enhances accuracy, robustness, and generalization across diverse reasoning tasks for both large and small language models.

Conclusion: SELF-THOUGHT offers a scalable way to enable more reliable self-correcting in language models, effectively addressing reasoning deficiencies and improving model performance.

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [58] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround introduces an uncertainty-aware framework for GUI grounding models that improves reliability by ensuring risk-aware predictions, using a calibration process for statistically guaranteed false discovery rate (FDR) control.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to enhance the reliability of GUI grounding models to avoid costly errors in high-stakes tasks, such as approving erroneous payments.

Method: SafeGround employs a distribution-aware uncertainty quantification method to analyze stochastic outputs, and applies a calibration process to establish a decision threshold that ensures FDR control during testing.

Result: The SafeGround framework outperforms existing methods in distinguishing correct from incorrect predictions and improves system-level accuracy by up to 5.38 percentage points on multiple GUI grounding models.

Conclusion: SafeGround effectively enhances the safety and reliability of GUI grounding models while enabling significant system-level accuracy gains through a robust, uncertainty-aware approach.

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [59] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse tackles federated learning challenges in collaborative LLM agents by enhancing tool-usage effectiveness and minimizing communication costs.


<details>
  <summary>Details</summary>
Motivation: Collaborative training among LLM-based agents faces hurdles like communication costs, data heterogeneity, and tool-usage inefficiencies.

Method: Synapse trains a shared global model of tool-usage behavior through federated aggregation, templated representations, embedding retrieval, and adaptive masking.

Result: Synapse improves tool-usage effectiveness and lowers communication overhead compared to existing methods in multi-agent LLM systems.

Conclusion: Synapse provides a robust framework for federated learning by addressing core challenges and ensuring improved collaboration among LLM agents.

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [60] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench is a new benchmark evaluating multi-turn clarifications of autonomous agents handling faulty user inputs, targeting grounded execution cases beyond text-only settings.


<details>
  <summary>Details</summary>
Motivation: With the rise of autonomous agents using large language models, handling uncooperative user inputs (e.g., missing details, ambiguity, false assumptions) has become essential to prevent execution risks, unaddressed by traditional benchmarks.

Method: The paper introduces Drift-Bench, with a taxonomy of cooperative breakdowns and a persona-driven user simulator using the Rise evaluation protocol to test multi-turn clarifications in real-world execution scenarios.

Result: Drift-Bench revealed significant performance declines under input faults, with varying clarification efficacy depending on user personas and fault scenarios.

Conclusion: Drift-Bench pinpoints failure scenarios in agent communication, presenting a vital tool for bridging clarification processes and agent safety to ensure robust execution under real-world conditions.

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [61] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: Sparse auto-encoders (SAEs) are improved to address challenges like $L_1$ penalty non-smoothness and lack of alignment with human semantics, demonstrating success in Stable Diffusion 3.5 for compositional generalization and semantic editing.


<details>
  <summary>Details</summary>
Motivation: SAEs face limitations such as the non-smoothness of the $L_1$ penalty affecting reconstruction/scalability and misalignment of learned features with human-understandable semantics.

Method: The authors adapt unconstrained feature models from neural collapse theory and supervise SAEs to reconstruct feature vectors by learning sparse concept embeddings and decoder weights.

Result: The improved method achieves compositional generalization and reconstructs images with novel concept combinations, enabling feature-level interventions for semantic editing in Stable Diffusion 3.5.

Conclusion: The adapted SAEs effectively address previous challenges, aligning better with human semantics and enhancing performance in tasks like semantic image editing.

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [62] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2 is introduced as an improved Theory-Based Reinforcement Learning agent leveraging Large Language Models to learn reusable abstractions and integrate them into hierarchical planning, leading to higher sample-efficiency and capability to solve complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art LLMs and RL systems struggle with learning and generalizing abstractions across diverse tasks, which hinders efficient planning. Existing TBRL systems require human-specified abstractions, leaving abstraction-learning unaddressed.

Method: TheoryCoder-2 uses LLMs' in-context learning to actively synthesize abstractions from experiences and incorporate these abstractions into a hierarchical planning process for solving tasks.

Result: In experiments across BabyAI, Minihack, and VGDL games (e.g., Sokoban), TheoryCoder-2 showed significant sample efficiency compared to baseline LLM agents and prior program-synthesis agents, and succeeded in solving tasks where baselines failed.

Conclusion: TheoryCoder-2 overcomes the reliance on human-defined abstractions by actively learning them, marking progress in TBRL and demonstrating superior performance and efficiency in task-solving environments.

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [63] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: Chat interfaces limit analytical performance in multi-step tasks due to cognitive overload, with solutions proposed to overcome these issues.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of chat-based interfaces for AI-assisted multi-step analytical tasks and find ways to improve analytical performance.

Method: The study identifies five cognitive mechanisms that degrade performance with chat interfaces and proposes eight design patterns as solutions to mitigate these shortcomings, focusing on reducing cognitive bottlenecks.

Result: Formulates a cognitive overload model and introduces design patterns that address key cognitive bottlenecks, enhancing performance by reducing errors and biases.

Conclusion: Chat systems work poorly for open-ended analytical tasks. Hybrid designs that incorporate expert priors and specific UI patterns can reduce cognitive overload and improve performance.

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [64] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: This paper introduces a clinically grounded safety system for large language models in mental health support scenarios, aiming to reduce safety failures and enhance risk detection.


<details>
  <summary>Details</summary>
Motivation: To address safety failures in large language models used for mental health support, especially in distinguishing therapeutic disclosures from genuine clinical crises.

Method: The authors developed a risk taxonomy collaboratively with psychologists, created the MindGuard-testset for training, and trained safety classifiers using synthetic dialogues and human annotations.

Result: The trained MindGuard classifiers reduced false positives and harmful engagement rates. Their performance surpassed existing general-purpose safeguards in adversarial multi-turn interactions.

Conclusion: The study demonstrates the effectiveness of clinically grounded classifiers and promotes their use and transparency by releasing models and human evaluation data.

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [65] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: The paper introduces an online hierarchical planning system (R-HTN) for agents that integrates HTN planning, online planning, and adherence to predefined directives, allowing certain intelligent disobedience for safety or predefined traits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create agents capable of prioritizing safety or embedded personality constraints (directives), even if it means defying user commands, thus ensuring ethical and reliable behavior.

Method: The approach employs online HTN planning combined with directives that enforce constraints during task execution. Two variants of agents were developed: Nonadaptive (stops actions when directives are violated) and Adaptive (modifies plans to remain compliant).

Result: The experiments demonstrated that R-HTN agents never violate directives while attempting to achieve user goals, adhering strictly to stipulated constraints.

Conclusion: R-HTN agents provide a robust framework for implementing ethical and safe decision-making in autonomous systems, ensuring compliance with non-negotiable directives.

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [66] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: The paper introduces MixDPO, a training strategy for aligning large language models by leveraging both easy and difficult preference pairs effectively.


<details>
  <summary>Details</summary>
Motivation: Existing preference optimization methods are sensitive to the quality of preference pairs, especially for ambiguous ones, which are often filtered out despite containing valuable signals.

Method: MixDPO orders preference data from easy to hard, applying preference loss to easy pairs and routing ambiguous pairs to supervised fine-tuning (SFT) to stabilize training.

Result: MixDPO consistently outperforms other methods (e.g., DPO) across multiple alignment benchmarks, offering particularly notable improvements in AlpacaEval-2 length-controlled win rate.

Conclusion: MixDPO effectively manages difficult preference pairs by combining curriculum-based training and hybrid optimization methods, enhancing the alignment of large language models.

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [67] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: The paper investigates training interference in Agentic Reinforcement Learning (ARL) methods and introduces a solution named DART to address the issue.


<details>
  <summary>Details</summary>
Motivation: To examine whether joint training of reasoning and tool-use capabilities in ARL truly leads to better performance, as is commonly assumed.

Method: The authors introduce the Linear Effect Attribution System (LEAS) to investigate gradient interference during training and propose DART, a framework that decouples parameter updates for reasoning and tool-use.

Result: The proposed method, DART, showed consistent improvements (6.35% on average) over baselines and achieved results comparable to multi-agent systems, while using a single model.

Conclusion: Joint optimization in ARL can be ineffective due to gradient interference; DART provides a solution by disentangling reasoning and tool-use behaviors, improving overall performance.

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [68] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: This paper introduces Error Taxonomy-Guided Prompt Optimization (ETGPO), a top-down prompt optimization approach that targets language model errors systematically to enhance performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the efficiency and effectiveness of Automatic Prompt Optimization (APO) by addressing limitations of existing bottom-up methods, such as loss of a global perspective in prompt adjustments.

Method: The authors develop ETGPO, which categorizes model errors into a taxonomy, identifies frequent failure modes, and augments prompts with specific guidance to address these issues, instead of iterative problem-based adjustments.

Result: ETGPO outperforms or matches state-of-the-art approaches in tasks like mathematics, question answering, and logical reasoning, while reducing token usage and optimization cost by nearly 66%.

Conclusion: ETGPO offers a scalable and efficient alternative to traditional APO methods, achieving high accuracy with significantly lower computational resources.

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [69] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: Post-training preference alignment can amplify sycophantic behavior in language models by prioritizing user preferences over factual accuracy.


<details>
  <summary>Details</summary>
Motivation: Investigate how preference alignment algorithms can lead to sycophantic tendencies and amplify biases.

Method: Utilized formal analysis to connect human feedback optimization with bias, studied reward learning mechanisms, and proposed intervention to mitigate amplification during training.

Result: Bias in human preferences directly impacts sycophantic tendencies, and computational experiments confirm behavioral drift commonly occurring under this setup.

Conclusion: To minimize sycophantic behaviors, a minimal agreement penalty method based on KL divergence is proposed and characterized.

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [70] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: This paper presents HalluHard, a benchmark for evaluating hallucination in multi-turn dialogues by requiring inline citations and iterative evidence retrieval from web searches.


<details>
  <summary>Details</summary>
Motivation: Large language models suffer from ungrounded factual claims in multi-turn dialogs, exacerbating as context grows and errors cascade.

Method: The paper introduces HalluHard, evaluates hallucinations using inline citations for factual assertions, and proposes a judging pipeline to retrieve and verify evidence.

Result: Despite using web searches, hallucinations persist substantially, with a rate of around 30% for Opus-4.5 in its strongest configuration. Hallucination behavior depends on factors like model capacity and type of knowledge.

Conclusion: Current LLMs show significant hallucination issues even with improved grounding techniques, necessitating further advancements to enhance reliability in sensitive domains.

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [71] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: The paper proposes a reinforcement learning framework for improving reasoning in Large Language Models using a Step-wise Marginal Information Gain mechanism and other strategies, leading to better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle challenges in reinforcement learning for reasoning in LLMs, such as reward sparsity and inefficient credit assignment.

Method: The paper introduces a Step-wise Marginal Information Gain mechanism for reasoning steps, a Decoupled Masking Strategy for proper credit distribution, and a Dual-Gated SFT objective to enhance training stability.

Result: The framework outperforms baselines like GRPO in benchmarks including MATH and Super-CLEVR, with superior sample efficiency, accuracy, and out-of-distribution robustness.

Conclusion: This method enhances the reasoning abilities of Large Language Models effectively, showing potential in challenging tasks and unseen scenarios.

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [72] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: The paper proposes a new objective to enhance the diversity of solutions in reinforcement learning applied to language models, achieving better reasoning performance and diversity without diminishing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of reduced outcome diversity in reinforcement learning with verifiable rewards while improving large language model reasoning performance.

Method: A set level diversity objective is introduced, applying kernelized similarity and integrating a leave-one-out marginal contribution for sampled trajectories as an advantage shaping term in policy optimization.

Result: Experiments demonstrated the algorithm outperforms baselines in reasoning benchmarks like Pass@1 and Pass@K across various model scales.

Conclusion: The proposed diversity-enhancing reinforcement learning method is effective at improving both reasoning performance and diversity of outcomes in large language models.

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [73] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: The study introduces a benchmark, \cb, to evaluate Large Language Models' (LLMs) ability to reason about convexity under deep functional composition, highlighting shortcomings at greater depths and proposing a recursive divide-and-conquer framework to improve performance.


<details>
  <summary>Details</summary>
Motivation: To test and enhance the reasoning capabilities of LLMs in understanding and processing convex functions, especially as LLMs aim to handle more advanced mathematical and scientific research tasks.

Method: A benchmark called \cb was created to test LLMs' ability to assess convexity under deep functional compositions. The study identifies reasoning challenges and introduces a divide-and-conquer framework, combining external parsing tools and recursive analysis of sub-expressions.

Result: LLMs show significant performance drops in reasoning about convexity as functional depth increases. The proposed method dramatically improves performance, achieving perfect F1-scores even at higher depths.

Conclusion: By leveraging a scalable benchmark and introducing the divide-and-conquer framework, the study effectively addresses the compositional reasoning gap in LLMs, showing promise for handling deeper mathematical reasoning tasks.

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [74] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: The paper introduces AutoHealth, a novel multi-agent system aimed at improving autonomous machine learning for health data by addressing modality heterogeneity, task-specific adaptability, and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents in machine learning struggle with heterogeneous health data modalities, lack task-specific adaptability, and ignore uncertainty estimation essential for healthcare decision-making.

Method: The AutoHealth system employs five specialized agents for tasks like data exploration, model construction, and optimization. It operates in a closed-loop while balancing prediction performance and uncertainty quantification.

Result: AutoHealth demonstrated superior performance on a benchmark of 17 diverse tasks, improving prediction accuracy by 29.2% and uncertainty estimation by 50.2% over current baselines.

Conclusion: AutoHealth offers a significant step forward in reliable, autonomous healthcare modeling, producing interpretable models with enhanced predictive reliability and uncertainty awareness.

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [75] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM is a framework that employs a fine-tuned large language model for automating industrial optimization modeling processes, including model creation, constraint injection, and variable pruning. It shows promise in reducing the need for domain expertise while enhancing efficiency in large-scale industrial optimization tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to simplify and automate the highly expertise-intensive process of creating and maintaining optimization models in industrial planning and scheduling by leveraging advancements in large language models.

Method: EvoOpt-LLM employs a 7B-parameter LLM fine-tuned using low-rank adaptation (LoRA). It supports optimization modeling processes such as automated model construction, constraint injection, and variable pruning to improve efficiency and reduce expert intervention.

Result: EvoOpt-LLM achieved notable rates: a generation rate of 91%, executability of 65.9%, and an F1 score of ~0.56 for variable pruning on medium-sized LP models. It performed efficiently even with limited training samples.

Conclusion: EvoOpt-LLM presents a scalable, practical, and data-efficient solution for automation in industrial optimization modeling. It reduces dependence on experts, is adaptable to evolving business rules, and improves solver-level efficiency.

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [76] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON is a framework that ensures 100% solution feasibility for combinatorial optimization using grammar-constrained decoding, a repair layer, and adaptive sampling, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large language models, which lack guarantees for solution feasibility in combinatorial optimization, and improve real-world application potential.

Method: Developed the FALCON framework with grammar-constrained decoding, feasibility repair layers, and adaptive Best-of-N sampling. Also introduced BOPO training methodology using preference weight optimization for dense supervision.

Result: Empirical validation across 7 NP-hard combinatorial optimization problems demonstrated FALCON achieves perfect feasibility and matches or exceeds state-of-the-art solution quality.

Conclusion: FALCON ensures robust combinatorial optimization solutions with 100% feasibility, offering reliable deployment potential for real-world problems.

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [77] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: This paper examines the instability issues in reinforcement learning with verifiable rewards (RLVR) for Mixture-of-Experts (MoE) models, attributing it to objective-level hacking caused by token-level credit misalignment.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts architectures face significant instability during RLVR training, hindering the improvement of large language models' reasoning capabilities, with little understanding of the underlying causes.

Method: A principled framework centered on objective-level hacking and extensive experiments with a 30B MoE model to analyze token-level credit misalignment and its role in training-inference discrepancies.

Result: The research uncovers a key pathological dynamic in MoE training: abnormal growth in training-inference discrepancy, revealing a new class of instability mechanisms in RLVR training for MoE models.

Conclusion: The study provides a causal explanation for instabilities in MoE RLVR architectures and offers insights for designing more stable algorithms to improve large language model training.

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [78] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: This study introduces BiCarFormer, a Transformer-based model that combines diagnostic codes and environmental data for improved vehicle malfunction classification.


<details>
  <summary>Details</summary>
Motivation: Current automotive diagnostic systems rely mainly on diagnostic codes but fail to consider contextual environmental data, which is crucial for accurate failure classification.

Method: The authors propose BiCarFormer, a bidirectional Transformer that uses embedding fusions and co-attention mechanisms to integrate DTC sequences with environmental data for multi-label classification.

Result: BiCarFormer achieves superior performance on a real-world automotive dataset with 22,137 error codes and 360 error patterns compared to traditional models.

Conclusion: Incorporating contextual environmental data into vehicle diagnostics enhances performance, reduces maintenance costs, and improves automation in the automotive industry.

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [79] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: The paper proposes a framework to improve UAV network stability and utility by combining Lyapunov-based stability insights, resource allocation via Stackelberg games, and efficient computational methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges faced by heterogeneous networks supported by UAVs, particularly conflicts between limited resources and strict stability needs for latency-sensitive and bandwidth-intensive services.

Method: Introduces a Sensing-Communication-Computing-Control closed-loop framework. Develops a Stackelberg game model for resource allocation combined with a novel pruning-based Proximal Policy Optimization algorithm for computational efficiency.

Result: Simulation results reveal that the proposed framework ensures control loop stability and optimizes system utility in dynamic low-altitude environments.

Conclusion: The approach successfully transforms abstract stability requirements into tangible resource boundaries, balances resource allocation dynamically, and achieves efficient computation, ensuring mission reliability and performance in UAV-supported networks.

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [80] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: PersistBench evaluates safety risks arising from long-term memory in conversational assistants, identifying high failure rates in cross-domain leakage and memory-induced sycophancy.


<details>
  <summary>Details</summary>
Motivation: To address overlooked safety risks associated with the persistence of long-term memories in large language models (LLMs) used in conversational systems.

Method: The creation and deployment of PersistBench benchmark to identify and measure risks like cross-domain leakage and memory-induced sycophancy in 18 LLMs.

Result: Findings revealed high failure rates: a median of 53% for cross-domain leakage and 97% for sycophancy samples across the evaluated LLMs.

Conclusion: PersistBench serves to highlight and mitigate potential safety risks in integrating long-term memory into conversational systems, paving the way for safer AI development.

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [81] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: Latent Chain-of-Thought models face a trade-off between exploration and execution due to decisional certainty, demanding adaptive system design.


<details>
  <summary>Details</summary>
Motivation: Exploring why Latent CoT models succeed in exploration tasks but struggle with computational execution, aiming to enhance reasoning model adaptability.

Method: The paper introduces decisional certainty as a mechanism, characterizes an exploration-execution trade-off theoretically, proposes the Symbolic Index for quantification, and validates curriculum learning as essential.

Result: The study establishes the causal link between decisional certainty and performance, confirms the Symbolic Index helps regulate task execution and exploration, and demonstrates curriculum learning's necessity to avoid distributional failures.

Conclusion: Designing adaptive systems that dynamically adjust decisional certainty improves task-specific performance in reasoning models.

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [82] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: The study finds that intervening on certain layers in pretrained VLMs can enhance performance, introduces Task-Layer Interaction Vector to identify task-interfering layers, and proposes a method called TaLo for dynamic adaptation without retraining.


<details>
  <summary>Details</summary>
Motivation: To improve performance of pretrained VLMs by addressing task-specific hindrances caused by specific layers, and to unlock hidden capabilities of the model.

Method: Systematic layer analysis through layer intervention and the introduction of Task-Layer Interaction Vector to quantify task sensitivity; proposes TaLo for task-specific adaptation by bypassing interfering layers dynamically at inference.

Result: Observed that bypassing task-interfering layers leads to performance improvement across models and datasets, with specific example of a 16.6% boost in Qwen-VL's accuracy on a specific QA task.

Conclusion: Pretrained VLMs possess task-specific modularity that can be leveraged through intervention at inference time, presenting an efficient, training-free mechanism for better utilization of their capabilities.

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [83] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: This paper introduces ASP-Bench, a benchmark for transitioning natural language problem definitions into Answer Set Programs (ASPs) and provides multidimensional insight into problem complexity and solution modeling.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of converting natural-language specifications into logic programs, especially for neurosymbolic engineering.

Method: Developed ASP-Bench: 128 natural language problem instances to evaluate system performance in translating problems to ASPs, characterized across seven reasoning aspects.

Result: The ReAct-based agent approach successfully modeled ASPs with robust iterative refinement, providing insights into problem complexity.

Conclusion: ASP-Bench enables systematic understanding of problem modeling and facilitates effective neurosymbolic engineering approaches.

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [84] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: The paper proposes a state-transition framework for LLMs to improve reasoning efficiency and performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Long CoT reasoning enhances LLM performance but incurs high computational and memory costs. Existing solutions that compress CoT sequences hinder reasoning capacity, necessitating a more efficient strategy.

Method: The authors introduce a state-transition framework using a linear attention mechanism. The reasoning state is maintained to capture historical reasoning data, reducing attention complexity from quadratic to linear. This framework retrieves relevant historical information without attending to all previous tokens.

Result: The proposed framework reduces reasoning complexity, improves efficiency, and mitigates over-thinking caused by noisy steps. Experimental results confirm enhanced reasoning performance across datasets and model sizes.

Conclusion: The framework improves LLM reasoning efficiency and capacity by reducing computational costs, showcasing substantial performance improvements.

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [85] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: This paper introduces Workflow-R1, a framework for dynamic optimization of LLM-based workflows through sequential language decision-making and proposes GSsPO, an RL algorithm tailored for structured multi-turn reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing workflow optimization methods are static and code-centric, limiting the flexibility of solving complex reasoning tasks in agentic workflows.

Method: Workflow-R1 reformulates workflow construction as multi-turn sequential decision-making using natural language and introduces GSsPO, a structure-aware reinforcement learning algorithm designed to optimize the Think-Action cycle.

Result: Workflow-R1, powered by GSsPO, demonstrates superior performance over baselines in extensive QA benchmarks for multi-turn reasoning tasks.

Conclusion: Workflow-R1 establishes a new paradigm for dynamic and automated workflow optimization, with GSsPO proving to be a generalized solution for sequential decision-making in complex reasoning tasks.

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [86] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: This paper introduces gSMILE, a framework for explaining generative AI models using perturbation and surrogate modeling techniques to achieve token-level attribution and visualizations.


<details>
  <summary>Details</summary>
Motivation: There is a need for transparency and accountability in generative AI decision-making processes, particularly in high-stakes applications.

Method: The gSMILE framework employs controlled textual perturbations, Wasserstein distance metrics, and weighted surrogate modeling to quantify prompt influence. It also incorporates evaluation metrics such as stability and faithfulness, applied across various generative AI architectures.

Result: Experiments demonstrate that gSMILE creates robust, human-aligned attributions and generalizes effectively across state-of-the-art generative models.

Conclusion: gSMILE facilitates transparent and responsible deployment of generative AI by generating insightful and systematic explanations for their outputs.

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [87] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE is proposed as a dynamic framework to improve preference-based alignment in model training by maximizing gradient efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Static methods like DPO lack adaptability in handling preference pairs, leading to inefficiencies and noise during optimization.

Method: SAGE combines a coarse-grained curriculum mechanism with a fine-grained stability-aware scoring to prioritize high-utility training samples.

Result: Experiments on mathematical reasoning benchmarks show SAGE accelerates convergence and achieves better performance compared to static methods.

Conclusion: Policy-aware and stability-conscious data selection is vital for efficient reasoning model alignment, as demonstrated by SAGE's effectiveness.

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [88] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: The paper presents FutureMind, a reasoning framework designed to enhance Small Language Models (SLMs) by leveraging knowledge distillation from Large Language Models (LLMs) for more effective task-solving capabilities with reduced costs.


<details>
  <summary>Details</summary>
Motivation: SLMs, while efficient and cost-effective, struggle with complex, knowledge-intensive tasks. The motivation is to bridge this gap by equipping SLMs with advanced reasoning and retrieval abilities.

Method: FutureMind is a modular reasoning framework introducing four reasoning modules (Problem Analysis, Logical Reasoning, Strategy Planning, Retrieval Guidance) and three retrieval paradigms to improve SLM performance. It utilizes adaptive knowledge distillation techniques from LLMs.

Result: FutureMind outperforms state-of-the-art baselines in multi-hop QA benchmarks such as 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames. It shows consistent superior performance across various SLM scales and architectures.

Conclusion: FutureMind demonstrates that reasoning skill transfer between LLMs and SLMs can be effective, albeit limited by cognitive biases. This work highlights the potential of optimizing SLMs for better performance while maintaining efficiency.

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [89] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: The paper introduces Predictive Scheduling to optimize token budgeting for large language models (LLMs), leading to higher accuracy without increasing token cost.


<details>
  <summary>Details</summary>
Motivation: Current fixed token budgeting for LLMs leads to inefficiencies, either over-computing easy tasks or under-computing hard ones.

Method: The approach involves using lightweight predictors (MLP or LoRA-fine-tuned classifiers) to estimate necessary computation length or difficulty for queries and a greedy allocator to dynamically distribute a fixed token budget.

Result: Predictive Scheduling achieves up to 7.9-point absolute accuracy improvement on the GSM8K benchmark and addresses over 50% of the gap to an oracle predictor.

Conclusion: Pre-run prediction of token requirements significantly improves accuracy while balancing computational costs, enabling more efficient LLM deployments.

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [90] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: Paper introduces OntoEKG, an LLM-driven tool for ontology construction from enterprise data. Achieves partial success with notable challenges.


<details>
  <summary>Details</summary>
Motivation: Current ontology construction for enterprise knowledge graphs is resource-heavy and relies on manual inputs and domain expertise.

Method: Introduces OntoEKG, a pipeline utilizing language models to identify classes and properties, structure them hierarchically, and serialize into RDF.

Result: Achieved an F1-score of 0.724 in one domain, highlighting potential while uncovering challenges related to scope and reasoning.

Conclusion: LLM-based ontology construction shows promise but requires further refinement for real-world applicability across domains.

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [91] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: The paper proposes RE-MCDF, a relation-enhanced framework for clinical diagnosis using multi-expert collaboration to address logical inconsistencies in electronic medical records, improving on state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in clinical diagnosis with large language models (LLMs) caused by the heterogeneity, sparsity, and noise in electronic medical records (EMRs). The paper seeks to improve diagnosis reliability and logical consistency.

Method: RE-MCDF introduces a generation-verification-revision loop with three experts: a primary expert for diagnoses, a lab expert for prioritizing clinical indicators, and a group for enforcing disease logical constraints, using a medical knowledge graph (MKG).

Result: RE-MCDF outperformed state-of-the-art diagnostic methods in tests on neurology datasets (NEEMRs and XMEMRs), demonstrating improved performance in complex scenarios.

Conclusion: Using a multi-expert framework incorporating logical constraints and MKG guidance, RE-MCDF enhances clinical diagnosis reliability and consistency over baseline methods.

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [92] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: The paper proposes a framework for selecting the best pretrained Vision-Language Model (VLM) for specific tasks, using directional metrics based on internal dynamics of visual encoders.


<details>
  <summary>Details</summary>
Motivation: Selecting an optimal pretrained VLM for target tasks is challenging due to computational constraints, limited data in few-shot scenarios, and inefficiencies in existing selection methods.

Method: They introduce layer-wise conductance analysis to quantify the importance of functional blocks in visual encoders, alongside a novel metric called Directional Conductance Divergence (DCD) to rank tasks effectively.

Result: The proposed method achieves significant improvements in predictive accuracy, outperforming existing baselines like SWAB by 14.7% in NDCG@5 across 48 models and 21 datasets.

Conclusion: The approach provides an effective, computationally-efficient solution for selecting pretrained VLMs, facilitating improved transferability in various downstream tasks.

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [93] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: The paper addresses entity-level aggregation queries over free text, introducing AGGBench for evaluation and proposing a modular baseline, DFA, which improves completeness in evidence coverage.


<details>
  <summary>Details</summary>
Motivation: Aggregate queries requiring completeness are underexplored, with current paradigms failing to address "find all" requirements for exhaustive evidence collection.

Method: The authors formalize entity-level aggregation queries with strict completeness constraints and introduce AGGBench alongside the DFA framework (Disambiguation-Filtering-Aggregation).

Result: Empirical results show DFA improves evidence coverage compared to existing RAG and agentic baselines.

Conclusion: The work establishes entity-level aggregation querying as a formalized problem and provides tools (AGGBench and DFA) for completeness-oriented evaluation and advancements.

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [94] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: Linear probes can monitor AI for deceptive behavior but show flaws like spurious patterns. Choosing the right training instructions improves results by targeting deception types with specialized methods.


<details>
  <summary>Details</summary>
Motivation: The need to effectively detect deceptive behaviors in AI systems for responsible AI development and risk mitigation strategies.

Method: The study evaluates linear probes and focuses on how instruction pair choices and a taxonomy of deception types affect probe performance. Variance analysis is used to explain the role of prompts.

Result: They found that instruction pairs influence probe performance (70.6% variance explained) and tailored prompts help better identify deception types.

Conclusion: A universal deception detector is infeasible; organizations should design specialized probes to address specific deception threat models.

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [95] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym is introduced as a tool for rapid offline A/B testing using synthetic buyers powered by Large Language Model agents, significantly reducing testing time without involving real users.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency, potential user experience risks, and time consumption of traditional A/B testing for e-commerce UI changes.

Method: SimGym uses synthetic buyers represented by Large Language Model agents to simulate and evaluate UI changes in a live browser, utilizing buyer profiles and production interaction data to test control and treatment setups offline.

Result: The system shows strong alignment with real human outcomes on UI changes, reducing testing time from weeks to under an hour, validated on a major e-commerce platform.

Conclusion: SimGym offers a rapid and scalable offline A/B testing solution, minimizing risks to user experience and significantly accelerating experimentation cycles.

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [96] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: The paper emphasizes building legal and regulatory frameworks for AI governance, highlighting registration systems for models and agents and regulatory markets for innovation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of governing the transformative nature of AI and ensure responsible development and deployment.

Method: Proposes building AI governance infrastructure such as registration regimes for frontier models and autonomous agents, and regulatory markets for private sector involvement.

Result: Outlines three approaches to enhance legal and regulatory systems for AI governance.

Conclusion: Effective AI governance requires not just rules but also robust legal frameworks and markets to support regulatory innovation and compliance.

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [97] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: The paper proposes a neural amortization framework that enhances stochastic local search for repeated Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs).


<details>
  <summary>Details</summary>
Motivation: Existing solutions for MPE inference in PGMs often stagnate in poor optima and fail to balance likelihood gains with long-term search improvements, particularly across repeated queries on a fixed graphical model.

Method: The paper introduces a neural amortization framework using an attention-based network to guide local search by predicting moves that reduce Hamming distance to a near-optimal solution. This integrates with existing local search procedures.

Result: The proposed method improves convergence performance and achieves consistent gains over standard Stochastic Local Search (SLS) and Guided Local Search (GLS+) across high-treewidth benchmarks.

Conclusion: The approach effectively leverages the fixed structure of PGMs to enhance repeat-query inference, achieving better balance between short-term and long-term search objectives.

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [98] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: The paper introduces Qrita, an efficient algorithm for implementing Top-k and Top-p sampling in large language models, addressing computational and memory inefficiencies.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of Top-k and Top-p sampling algorithms for large language models, which suffer from computational and memory bottlenecks in existing approaches.

Method: The authors propose Qrita, based on a pivot-based selection strategy, featuring a Gaussian-based sigma-truncation to reduce search space and quaternary pivot search with duplication handling for faster and deterministic output.

Result: Qrita outperforms existing high-performance execution engines (vLLM, SGLang, Flashinfer), achieving up to 2× throughput and half the memory usage, while maintaining deterministic output.

Conclusion: Qrita provides significant improvements in performance and efficiency for Top-k and Top-p sampling, making it a valuable contribution to large language model sampling algorithms.

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [99] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: This paper introduces PRISM, a system designed for cost-sensitive selective intervention in proactive agents with decision-theoretic gating and selective reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing systems for proactive agents struggle with choosing when to intervene due to over-reliance on heuristics or indiscriminate reasoning, leading to inefficiencies and limited control over the benefit-burden tradeoff.

Method: PRISM uses a decision-theoretic gate with a dual-process reasoning architecture, applying calibrated thresholds for intervention and selective resource-intensive reasoning (Slow mode) in ambiguous, high-stakes cases.

Result: PRISM outperformed strong baselines in ProactiveBench, reducing false alarms by 22.78%, improving F1 score by 20.14%, and demonstrating computational efficiency.

Conclusion: PRISM provides a precise, efficient, and controllable framework for proactive decision-making by employing decision-theoretic gating, selective reasoning, and teacher-student aligned distillation.

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [100] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: The paper presents MAGIC, a multi-agent reinforcement learning framework for improving safety alignment in LLMs via dynamic adversarial games.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety measures struggle to keep up with evolving adversarial attacks as they rely on static data distributions, limiting adaptation to new threats.

Method: MAGIC formulates safety alignment as an adversarial asymmetric game where attacker and defender agents co-evolve using multi-turn reinforcement learning to exploit and protect against vulnerabilities.

Result: Experiments show MAGIC leads to superior defense rates while preserving LLM helpfulness, as attackers evolve novel strategies and defenders adapt effectively.

Conclusion: The method enhances LLM robustness with co-evolutionary learning, providing theoretical insights and practical success against adversarial attacks.

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [101] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent is a framework designed for long-horizon, multidisciplinary scientific research, using hierarchical planning, tool integration, and self-evolution capabilities.


<details>
  <summary>Details</summary>
Motivation: To overcome the shortcomings of existing LLMs and tool-based agents in handling long-horizon planning, robust goal maintenance, and continual learning for intricate scientific research.

Method: S1-NexusAgent employs a hierarchical Plan-and-CodeAct paradigm, integrates a wide range of scientific tools, and implements novel context management and self-improving capabilities through a dual-loop architecture.

Result: Achieved state-of-the-art performance across benchmarks in biology, chemistry, and material science, proving its effectiveness and generalization ability.

Conclusion: The framework offers a sustainable and efficient solution for complex and multidisciplinary scientific research challenges, enabling long-term self-evolution and dynamic adaptability.

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [102] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: The paper introduces a framework that allows AI systems to autonomously form questions and set tasks in dynamic settings by reasoning over internal states, environmental cues, and interactions, outperforming traditional prompting approaches.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems rely on predefined prompts and tasks, making them ineffective in handling changing environmental conditions autonomously.

Method: The paper proposes a human-simulation-based framework focusing on question formation as a core decision process. It includes internal, environment, and inter-agent prompting, while also enabling learning from experience.

Result: Experimental results in a simulation demonstrate the framework significantly outperforms baseline methods, reducing no-eat events and improving adaptability with statistically significant advancements.

Conclusion: The proposed framework enhances AI systems' adaptability and decision-making by autonomously generating questions and tasks, proving effective in dynamic, multi-agent environments.

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [103] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: This paper introduces Collaborative Thoughts, a framework combining autoregressive and diffusion models, enabling joint reasoning and generation via a feedback loop for enhanced multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of autoregressive and diffusion models in tasks requiring spatial reasoning, logical control, and complex constraint satisfaction.

Method: The framework integrates autoregressive models for structured planning, diffusion models for visual generation, and a vision-based critic to iteratively refine outputs through feedback.

Result: Collaborative Thoughts improves spatial reasoning reliability and controllability in generative processes through a unified collaborative loop.

Conclusion: This approach enhances multimodal task performance by leveraging complementary strengths of autoregressive and diffusion models in a cooperative framework.

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [104] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT framework introduces spatially consistent fusion and task-semantic alignment for effective urban region embeddings, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address spatial incoherence and lack of task-semantic alignment in urban region embedding models for better performance in tasks like crime prediction and resource allocation.

Method: ToPT consists of two modules: SREL (Spatial-aware Region Embedding Learning) which incorporates spatial priors into a Graphormer-based fusion module, and Prompt4RE (Task-aware Prompting for Region Embeddings) that aligns task semantics using a frozen multimodal large language model and multi-head cross-attention.

Result: Experiments showed state-of-the-art performance with up to 64.2% improvement across urban computing tasks and cities, showcasing the significance of spatial priors and semantic alignment.

Conclusion: ToPT effectively integrates spatial coherence and task-oriented semantic alignment, making it superior for urban region embedding tasks. Its generalization is validated across multiple cities and tasks.

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [105] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: The paper introduces FlowSteer, a framework using reinforcement learning for automated workflow orchestration, addressing issues like manual cost and sparse reward signals.


<details>
  <summary>Details</summary>
Motivation: Existing workflow orchestration methods are costly, overly reliant on specific operators/LLMs, and struggle with limited reward signals.

Method: FlowSteer uses a lightweight policy model interacting within a canvas environment to automate workflows, with a novel training approach CWRPO for improved results.

Result: The model demonstrates substantial performance improvements across twelve datasets compared to baselines, showcasing effective orchestration.

Conclusion: FlowSteer provides a versatile and efficient solution for workflow orchestration, supporting diverse operators and showing robust experimental success.

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [106] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: The authors introduce TRIP-Bench, a benchmark for long-horizon travel-planning tasks in real-world scenarios and propose GTPO, an online reinforcement learning method to improve performance in these tasks.


<details>
  <summary>Details</summary>
Motivation: Benchmarks for LLM-based agents underrepresent challenges in global constraints, tool reasoning, and long multi-turn interactions. Current models struggle with robust, real-world problem-solving over extended interactions.

Method: TRIP-Bench includes realistic travel scenarios, 18 tools, 40+ travel requirements, and automated evaluation for varying levels of difficulty. GTPO, an RL method, focuses on multi-turn learning with specialized reward mechanisms.

Result: Experiments revealed limited success of existing advanced models (50% on easy, <10% on hard). The proposed GTPO method improved performance, enhancing constraint satisfaction and robustness.

Conclusion: TRIP-Bench highlights challenges in long-horizon scenarios. GTPO improves model performance, marking a step forward in designing effective interactive LLM-based agents for complex, real-world settings.

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [107] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: This paper analyzes the generative behavior of large language models (LLMs) from minimal, topic-neutral inputs, revealing differences in their semantic preferences, topical specialization, and degenerative behavior.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand LLM behavior across various settings for reliable monitoring and AI safety, moving beyond topic- or task-specific analysis methods.

Method: The study probes the near-unconstrained generative behaviors of LLMs by analyzing outputs from minimal, neutral inputs and compares systematic preferences among various LLM families.

Result: The research reveals strong, consistent topical preferences and behavioral differences in generative outputs across LLM families, such as GPT-OSS producing advanced technical content and DeepSeek generating religious content. It also discusses degenerative behavior, with some models producing repetitive or incoherent outputs.

Conclusion: The findings demonstrate systematic preferences and unique behaviors inherent to each LLM family, highlighting the importance of understanding these characteristics for AI safety and development.

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [108] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: The paper proposes LSTR, a reasoning framework that integrates sparse latent transitions for better interpretability while maintaining accuracy and compression efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing latent reasoning methods are difficult to interpret and control due to dense transitions, while sparse models lack active reasoning capabilities.

Method: LSTR uses a Latent Transition Transcoder (LTT) with residual skip architecture to enable sparse semantic transitions, integrating explicit sparsity constraints for better control and interpretability.

Result: LSTR achieves reasoning accuracy and compression efficiency, significantly enhancing interpretability compared to dense latent models. Sparse features are shown to act as interpretable and causally effective operators.

Conclusion: The proposed LSTR framework addresses the limitations of both dense latent and sparse models by combining controllable sparse semantic transitions with effective reasoning capabilities, improving interpretability without sacrificing performance.

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [109] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: This paper introduces a new approach to address risks of advanced AI systems by focusing on organisational pathways associated with resource access and intervention, rather than only on the technical properties of the AI systems themselves.


<details>
  <summary>Details</summary>
Motivation: To address the concern that advanced AI systems may diminish human control by pursuing instrumental goals, and to expand intervention methods beyond technical solutions.

Method: The paper develops 'instrumental goal trajectories' (IGTs) that identify organisational pathways (procurement, governance, finance) through which AI systems gain key resources, enabling monitoring and interventions at these pathway points.

Result: The study provides a conceptual framework to track and intervene at organisational points where systems obtain resources necessary for gaining capabilities, offering new avenues for defining and maintaining acceptable AI capability levels.

Conclusion: IGTs broaden the scope of AI control approaches by integrating organisational system-level interventions, complementing technical methods to track and manage AI capabilities more effectively.

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [110] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: The paper introduces a new framework called Causal Prompt Optimization (CPO) to improve prompt design for LLMs, addressing instability and cost in existing techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the sensitivity of Large Language Models (LLMs) to prompt design, as current methods lack adaptability and rely on costly evaluations.

Method: The method involves two stages: first, it employs Double Machine Learning (DML) to create a causal reward model for unbiased assessments, and second, it uses the model for efficient query-specific prompt optimization without real-time evaluations.

Result: CPO consistently surpasses both human-engineered prompts and current optimization strategies in benchmarks like mathematical reasoning and data analytics, particularly excelling on challenging queries.

Conclusion: Causal inference offers a scalable and cost-effective solution for prompt optimization, making enterprise LLM deployments more reliable and efficient.

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [111] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: This paper addresses hallucinations in Video-LLMs by proposing Model-aware Counterfactual Data based Contrastive Decoding (MACD), an inference strategy to reduce ungrounded outputs and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs often hallucinate when visual evidence is weak, ambiguous, or biased. Current methods fail to effectively control visual cues responsible for these hallucinations.

Method: The MACD approach uses model feedback to identify object regions causing hallucination and generates targeted counterfactual inputs for contrastive decoding to enforce evidence-grounded decisions.

Result: Experiments show that MACD reduces hallucinations and improves task accuracy across challenges like small, occluded, or co-occurring objects in diverse Video-LLMs.

Conclusion: MACD enhances reliability for Video-LLMs by addressing visual evidence weaknesses through model-aware counterfactual data integration. Code and data will be publicly released for further use and development.

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [112] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: This paper introduces $α$-GFNs, a generalization of Generative Flow Networks (GFlowNets), that improves control over exploration-exploitation trade-offs, significantly enhancing their performance in tasks like mode discovery.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in GFlowNet objectives, particularly in their fixed exploration-exploitation trade-off caused by an equal mixing of forward and backward policies.

Method: The paper establishes a link between GFlowNet objectives and Markov chain reversibility, proposing $α$-GFNs that incorporate a tunable parameter $α$ to adjust exploration-exploitation dynamics.

Result: $α$-GFNs demonstrate better performance in mode discovery tasks across benchmarks, detecting up to 10 times more modes than prior GFlowNet objectives.

Conclusion: $α$-GFNs offer a flexible and improved framework for GFlowNets, enhancing their exploration-exploitation dynamics and advancing their capabilities in various benchmarks.

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [113] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: The paper introduces Adversarial Reward Auditing (ARA), a dynamic framework for mitigating reward hacking in RLHF by treating it as a competitive game.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of RLHF to reward hacking, where spurious correlations in reward models allow models to achieve high scores while violating human intent.

Method: The authors propose ARA, a two-stage framework: first, a "Hacker" policy identifies vulnerabilities while an "Auditor" learns to detect exploitation; second, an Auditor-Guided RLHF approach penalizes detected hacking.

Result: ARA outperforms baselines across three hacking scenarios, achieving better alignment-utility trade-offs, reducing issues like sycophancy and verbosity, and suppressing code gaming. The approach generalizes well across domains.

Conclusion: ARA transforms reward hacking into a measurable signal that can be effectively controlled, showing promise as a scalable, multi-domain solution for RLHF vulnerabilities.

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [114] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: The paper introduces PRISM, an innovative architecture to improve decoding speed of LLMs by decoupling model capacity from inference cost, achieving better throughput with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Large Language Models face slow decoding due to their auto-regressive nature, creating a need for faster and more efficient decoding methods.

Method: PRISM decouples the computational pathways of draft models with different parameter sets, enabling faster and more efficient decoding.

Result: PRISM outperforms existing methods in speed and capacity, achieving a 2.6x boost in decoding throughput with lower latency.

Conclusion: PRISM effectively addresses decoding inefficiencies, proving to be a superior architecture by scaling better with data and boosting LLM performance.

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [115] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: CrossAdapt is a two-stage framework designed to address challenges in deploying new architectures for large-scale user response prediction systems by enabling efficient cross-architecture knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of high model switching costs in user response prediction systems, caused by retraining large models on historical data and limited performance under data constraints, as well as the inefficiencies of existing knowledge distillation methods when handling diverse architectures and large embedding tables.

Method: CrossAdapt combines an offline stage with dimension-adaptive projections, progressive network distillation, and strategic sampling, and an online stage with asymmetric co-distillation and distribution-aware adaptation for efficient knowledge transfer.

Result: Experiments demonstrated that CrossAdapt improves AUC by 0.27-0.43%, reduces training time by 43-71%, and successfully mitigates degradation metrics in a large-scale deployment at Tencent WeChat Channels.

Conclusion: CrossAdapt proves to be an efficient and effective solution for cross-architecture knowledge transfer, enabling rapid adaptation while preserving performance and reducing computational overhead in real-world applications.

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [116] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: This paper introduces the LingLanMiDian (LingLan) benchmark, a unified and large-scale expert-curated evaluation suite for Traditional Chinese Medicine (TCM) using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current benchmarks in TCM for medical NLP are fragmented, inconsistent in scoring, and inadequate for fair comparison of LLMs, which fail to bridge the gap between human expertise and machine reasoning in this specialized domain.

Method: LingLan provides a comprehensive evaluation protocol covering tasks such as knowledge recall, reasoning, information extraction, and clinical decision-making. The benchmark includes metrics for synonym-tolerant labeling, a hard subset for challenging scenarios, and reframes diagnosis/treatment into single-choice decisions. Zero-shot evaluations of 14 state-of-the-art LLMs were conducted.

Result: The zero-shot evaluation highlights strengths and weaknesses in TCM-related reasoning and clinical decision capabilities of prominent LLMs. Notably, they perform significantly worse on the 400-item Hard subset compared to human experts.

Conclusion: The LingLan benchmark sets a standardized foundation for advancing TCM-focused LLMs and medical AI research by providing consistent metrics and unified evaluation approaches, helping bridge gaps in domain-specific reasoning.

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [117] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: The paper introduces ORCH, a deterministic coordination framework leveraging multiple cooperative large language models for improved discrete-choice reasoning, ensuring reproducible, interpretable, and effective outcomes.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems for reasoning tasks in large language models often use stochastic methods or ad-hoc heuristics, leading to non-reproducible behavior and weak interpretability.

Method: ORCH adopts deterministic routing rules and structured analyses from multiple LLMs, combining them via a merge agent using fixed rules for decision-making. It optionally employs an EMA-guided router to optimize agent selection based on accuracy, latency, or cost.

Result: Experimental validation on MMLU, MMLU-Pro, and GSM8K demonstrates significant accuracy improvements over baselines by over 10 points on MMLU-Pro and 50 points on GSM8K. The EMA router further boosts performance by up to 2.0 points.

Conclusion: ORCH provides a robust pathway to deterministic, controllable, and interpretable multi-agent architectures for discrete-choice reasoning in large language models.

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [118] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: The paper proposes INDIBATOR, a multi-agent framework for molecular discovery, which leverages individualized scientist profiles for agents and achieves superior performance compared to generic role-based systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent frameworks simplify agents' roles, not reflecting the unique research trajectories of human scientists, leading to limited discovery power.

Method: INDIBATOR creates agents grounded in individualized scientist profiles based on publication and molecular history. It uses a debate structure with proposal, critique, and voting phases.

Result: INDIBATOR outperforms coarse-grained persona-based systems and achieves competitive or state-of-the-art results in molecular discovery tasks.

Conclusion: Incorporating individuality into agents, reflective of researchers' unique scientific backgrounds, significantly enhances the quality of automated scientific discovery in multi-agent systems.

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [119] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: The paper introduces the Synesthesia of Vehicles (SoV), a framework that predicts tactile excitations from visual inputs, enhancing autonomous vehicle safety through proactive tactile perception.


<details>
  <summary>Details</summary>
Motivation: Current visual and optical sensors in autonomous vehicles cannot detect critical road-induced excitations needed for dynamic control. Inspired by human synesthesia, the study aims to bridge this gap.

Method: The method includes a cross-modal spatiotemporal alignment technique and a visual-tactile synesthetic (VTSyn) generative model using latent diffusion for unsupervised tactile data synthesis.

Result: The proposed VTSyn model outperforms existing models in temporal, frequency, and classification analysis, as demonstrated through experiments on diverse road and lighting conditions.

Conclusion: The study validates that proactive tactile perception using visual inputs enhances AV safety and provides superior performance compared to prior models.

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [120] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA addresses limitations of current agentic frameworks in long-horizon tasks through recursive task decomposition, modular design, and structured aggregation, improving performance on reasoning and generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current frameworks struggle on long-horizon tasks due to brittle orchestration, context limitations, and opaque execution traces, necessitating scalable, interpretable solutions.

Method: Introduced ROMA framework which uses recursive task decomposition and modular roles (Atomizer, Planner, Executor, Aggregator) for structured and transparent task execution. Additionally, improved prompt-searching algorithm GEPA+ was implemented.

Result: ROMA achieved significant improvements in reasoning accuracy (9.9% increase on SEAL-0) and matched leading model performance on writing benchmarks (EQ-Bench).

Conclusion: Recursive, modular architectures like ROMA can enhance reasoning depth in agentic systems, remaining adaptable, model-agnostic, and interpretable.

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [121] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG is a new framework for improving Standard Operating Procedures (SOPs) retrieval using a novel expert system to resolve industrial complexities and optimize performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in SOP retrieval, such as procedural rigidity, dependency on context, and execution requirements, which current retrieval systems fail to solve.

Method: The proposed framework uses a Mixture-of-Experts (MoE) approach with specialized Entity, Causal, and Flow graph experts, enhanced by a Procedure Card layer to prune search space and an LLM-Guided gating mechanism for dynamic weighting.

Result: SOPRAG outperforms previous retrieval systems in retrieval accuracy and response utility in four industrial domains, achieving perfect execution scores in critical tasks.

Conclusion: SOPRAG demonstrates significant improvements in SOP retrieval and operational application, addressing key challenges in industrial settings with innovative techniques.

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [122] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM is a framework enabling memory-efficient procedural learning in LLM-driven agents without requiring parameter updates, boosting performance and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of computational inefficiency and instability in LLM-driven agents arising from on-the-fly reasoning, which prevents effective reuse of prior experiences.

Method: ProcMEM formalizes 'Skill-MDP' to convert episodic narratives into reusable Skills with specific conditions. It introduces Non-Parametric PPO, which uses semantic gradients and a PPO Gate for Skill verification, ensuring reliable and efficient memory reuse.

Result: ProcMEM exhibits superior performance, memory compression, and higher reuse rates in various task scenarios. It enables effective procedural knowledge accumulation and reuse across tasks and agents.

Conclusion: ProcMEM effectively enhances long-term autonomy in LLM-driven agents through procedural memory frameworks, demonstrating transparent procedural knowledge refinement and reuse.

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [123] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: The paper presents Entropy-Guided Training (EGT) to enhance multimodal reasoning reward models by addressing noise in preference datasets and improving training efficiency.


<details>
  <summary>Details</summary>
Motivation: To address noise in datasets and inefficiency in conventional training methods for multimodal reward models, aiming to better align with human preferences.

Method: Proposes Entropy-Guided Training (EGT), incorporating entropy-based data curation and a progressive training strategy using sample difficulty determined by entropy.

Result: EGT-trained models show superior performance compared to state-of-the-art multimodal reward models across three benchmarks.

Conclusion: The EGT approach effectively mitigates dataset noise and enhances training efficiency, proving beneficial for multimodal reasoning reward models.

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [124] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: The paper introduces a geometric framework for studying multi-head attention in large language models, offering metrics and theoretical insights into token selection and separability.


<details>
  <summary>Details</summary>
Motivation: To provide a geometric understanding and interpretability of how multi-head attention in large language models functions, particularly token separability and selection.

Method: They utilized geometric metrics (Precision, Recall, F-score) alongside derived mathematical bounds under stable and empirically motivated assumptions to study attention behavior in the value-state space.

Result: Empirical results demonstrated strong agreement between theory and practice across various LLMs, with distinct head specialization noted for token separability and prediction metrics.

Conclusion: Multi-head attention acts as a structured geometric classifier, enabling improved interpretability and potential for geometry-aware optimizations in attention mechanisms.

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [125] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: The paper introduces DomusFM, a foundation model for smart-home sensor data utilizing a self-supervised contrastive learning paradigm to overcome labeled data scarcity and outperform baselines on activity recognition tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in existing smart-home activity recognition methods: reliance on large labeled datasets, a lack of handling unique smart-home sensor data traits, and feasibility issues with LLM-based approaches concerning privacy and costs.

Method: DomusFM uses a self-supervised dual contrastive learning paradigm, integrating semantic embeddings from a lightweight language model with specialized encoders for temporal and binary patterns to learn transferable sensor data representations.

Result: DomusFM outperforms state-of-the-art baselines across seven datasets in activity recognition tasks, achieving high accuracy even with minimal labeled data (5% for fine-tuning).

Conclusion: DomusFM addresses data scarcity and feasibility challenges, showcasing robust performance and practical deployment potential for real-world smart-home systems.

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [126] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: This paper compares Formal Concept Analysis (FCA) with GPT-5 (LLM) for topic modeling tasks in two experimental setups using educational materials and research articles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate and compare the effectiveness of LLMs and FCA in the underexplored area of topic modeling.

Method: The paper applies FCA using the CREA pipeline and GPT-5 via a zero-shot strategy with three distinct prompts (topic generation, merging, and labeling) to two datasets: teaching materials and research articles.

Result: The study produces insights on how both FCA and GPT-5 perform in topic modeling by analyzing teaching materials and classifying topics from research articles.

Conclusion: This work provides a comparative exploration of FCA and GPT-5 for their distinct strengths and weaknesses in the topic modeling domain.

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [127] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: The paper presents a method called Generalizable Predictive Prompt Selection (GPS) to improve training efficiency and reasoning capabilities in large language models through lightweight Bayesian inference and batch acquisition strategies.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning improves the reasoning of large language models, but its computational cost is high due to extensive optimization processes, necessitating efficient alternatives.

Method: The GPS method uses a lightweight generative model for Bayesian inference to predict prompt difficulty, combined with strategies prioritizing intermediate-difficulty prompts and diverse prompt selections in batch acquisitions.

Result: GPS significantly enhances training efficiency, final performance, and computational allocation compared to baseline methods in various reasoning benchmarks.

Conclusion: GPS offers a more efficient and generalizable approach to enhancing large language models by streamlining both training and test-time computation.

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [128] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: The paper proposes UCT, a new framework enabling LLMs to evolve from tool users to tool creators, achieving significant reasoning performance gains without additional training.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing Tool-Integrated Reasoning models, which fail to handle open-ended tasks, lack mechanisms for handling erroneous outputs, and are restricted by manual tool construction.

Method: UCT introduces a training-free framework that transforms LLMs into tool creators by distilling reasoning experiences into reusable assets. It incorporates memory consolidation for creating a reusable tool library and supports self-updating during inference.

Result: UCT demonstrated significant performance gains (+20.86% and +23.04%) in multi-domain mathematical and scientific reasoning tasks, showcasing the model's ability to improve tool quality autonomously during reasoning.

Conclusion: The novel framework effectively enhances the capabilities of TIR models by enabling adaptive tool creation and self-improvement, offering a paradigm shift in reasoning and tool optimization for LLMs.

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [129] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: The paper formalizes analogical reasoning in Transformer models using concepts from category theory and identifies mechanisms enabling analogy transfer, backed by synthetic tasks and pre-trained model analyses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how Transformers develop and implement analogical reasoning, given its centrality in human cognition but limited comprehension of its mechanisms in AI.

Method: The authors introduce synthetic tasks inspired by category theory to test analogical reasoning in controlled conditions, analyze Transformers' mechanisms, and study pretrained large language models (LLMs).

Result: Their findings reveal that analogical reasoning depends on data, optimization, and model size, involving two mechanisms: geometric alignment in embedding space and functor application.

Conclusion: The paper provides a concrete and grounded understanding of analogy in neural networks, bridging abstract cognitive ideas and mechanistic insights in Transformers.

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [130] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: The paper introduces a conversational diagnosis system that improves diagnostic accuracy and efficiency by reasoning through a knowledge graph and clarifying questions, supported by a realistic patient simulator.


<details>
  <summary>Details</summary>
Motivation: Existing diagnostic systems often depend on model-based parametric knowledge or assume patients provide rich details, which is unrealistic in early encounters. The paper aims at creating a system that works effectively even with incomplete and vague patient information.

Method: The proposed system generates diagnostic hypotheses from a dialogue context and confirms them through clarifying questions, iterating until a diagnosis is reached. It uses a knowledge graph for reasoning and simulates realistic patient interactions based on MIMIC-IV data.

Result: The system demonstrates enhanced diagnostic accuracy and efficiency compared to baselines, with a realistic simulator validated by medical professionals.

Conclusion: The proposed system bridges the gap in conversational diagnosis by incorporating vague patient inputs and a knowledge graph-based reasoning process. It shows both clinical relevance and methodological advancement.

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [131] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: The paper introduces VeriFY, a framework designed to reduce factual hallucinations in LLMs by using self-verification during training, achieving significant reductions in hallucination rates.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of factual hallucination in large language models, which is a critical challenge impacting their reliability.

Method: The authors propose VeriFY, a training framework that incorporates consistency-based self-verification by guiding the model through structured steps such as answering, self-verification, consistency judgment, and deciding to abstain. They also use a stage-level loss masking technique to avoid reinforcing hallucinated answers during training.

Result: VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent across various models and scales, with minimal recall losses, and shows generalization capabilities across different datasets.

Conclusion: VeriFY effectively teaches LLMs to handle factual uncertainty, mitigating hallucinations while maintaining high recall, and provides a robust solution adaptable across datasets and model architectures.

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [132] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: This paper introduces a safety-aware decoding method for large language models, using a single neuron as a gating mechanism to balance intrinsic capabilities with external guidance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and lack of generalization in existing post-training safety alignment approaches for large language models.

Method: A low-cost training expert model is introduced, featuring a single neuron as a gating mechanism to guide decoding for safety alignment.

Result: The method preserves model utility, enhances safety, reduces training overhead, and generalizes well across different scales of language models.

Conclusion: The proposed safety-aware decoding approach offers a lightweight and practical solution for improving the safety and usability of large language models while maintaining efficient deployment.

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [133] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: This paper presents a strategy to improve knowledge integration and reasoning in AI systems by introducing a training approach that contextualizes new information and combines it with multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of enabling AI systems, especially large language models, to internalize new knowledge and flexibly apply it during reasoning, as existing methods often fail to create a coherent framework of new information usable in diverse contexts.

Method: The proposed method involves three principles: providing a coherent background story to contextualize new knowledge, training models using self-generated multi-hop questions requiring reasoning, and applying knowledge distillation to train a model to replicate reasoning without direct access to new information.

Result: The experiments reveal that the proposed training approach enhances AI models' ability to leverage new knowledge effectively for reasoning and yields strong performance on complex questions requiring integration of multiple facts.

Conclusion: The study demonstrates that knowledge integration in AI models should focus on reasoning rather than memorization, and the proposed approach offers a promising pathway for improving AI reasoning capabilities.

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [134] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent system for complex workflows in regulated settings, showing improved efficiency and accuracy over single-agent models.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems face challenges in handling uncertainty and coordination across decision stages and with human oversight, especially in regulated environments.

Method: A multi-agent system using a finite-horizon Markov Decision Process (MDP) is proposed, with specific agents for roles, quantifying uncertainties at agent and system levels.

Result: The proposed system outperformed single-agent models, showing up to a 19% accuracy improvement, 85x human review reduction, and improved efficiency.

Conclusion: Multi-agent systems can enhance performance, reduce human oversight, and improve decision-making in complex, regulated workflows.

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [135] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: The paper introduces the concept of investigatory intelligence in Agentic Large Language Models (LLMs) and provides a benchmark called DDR-Bench to evaluate this. Results illustrate challenges in long-term exploration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on real-world data analysis tasks in LLM benchmarks and examine the capabilities of LLMs to demonstrate autonomy and investigatory intelligence.

Method: Introduced DDR (Deep Data Research), a task for LLMs to autonomously analyze databases, and DDR-Bench, a large-scale benchmark for evaluation.

Result: Frontier LLMs show emerging agency but face challenges in long-term investigatory tasks.

Conclusion: Investigatory intelligence requires more than scaling or scaffolding; intrinsic strategies in designing agent models are crucial.

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [136] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: The paper introduces SIDiffAgent, a training-free framework leveraging Qwen models for improved text-to-image synthesis by addressing challenges like prompt sensitivity, semantic ambiguity, and artifacts.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of text-to-image diffusion models, such as sensitivity to prompt phrasing, semantic ambiguity, and artifacts, which restrict their real-world applicability.

Method: Introduces SIDiffAgent, a training-free framework using Qwen family of models to autonomously manage prompt engineering, detect/correct poor generations, remove artifacts, and leverage memory of experiences for iterative self-improvement.

Result: SIDiffAgent achieved an average VQA score of 0.884 on GenAIBench, surpassing existing open-source and proprietary models.

Conclusion: SIDiffAgent demonstrates superior adaptability and output quality for text-to-image synthesis, addressing core challenges inherently and enabling practical real-world applications.

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [137] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: The paper investigates why masked diffusion-based language models (MDMs) perform better than autoregressive models (ARMs) on reverse queries, attributing the improvement to architectural structures and training processes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why MDMs exhibit less failure than ARMs in handling reverse queries, such as understanding '$A$ is $B$' and '$B$ is $A$'.

Method: The study examines the interaction of MDM architecture and training by analyzing a one-layer Transformer and showing how weight sharing and gradient alignment reduce reverse query errors.

Result: The authors demonstrate that positive correlation in attention scores and aligned gradients in one-layer Transformers explain the observed improvement of MDMs. Experimental results support these mechanisms.

Conclusion: Masked diffusion-based models mitigate ARMs' reversal issues due to architectural and training-specific mechanisms rather than purely their training objective.

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [138] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: The paper proposes DGR, a method to enhance safety alignment in large reasoning models (LRMs) by reducing distributional gaps in safety datasets, yielding significant improvements in reasoning accuracy and safety performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the safety tax—degraded reasoning abilities in LRMs caused by misaligned safety datasets—which arises from distributional gaps between human-labeled or externally generated datasets and the target LRM.

Method: The authors introduce DGR (Distribution Gap Reduction), which modifies and refines safety reasoning datasets to align more closely with the target LRM's internal distribution. The method aims to maintain safety performance without degrading reasoning ability.

Result: Experimental results show that DGR significantly reduces reasoning degradation while maintaining safety, achieving a 30.2% improvement in DirectRefusal and a 21.2% improvement in the R1-ACT reasoning accuracy metric compared to the baseline method.

Conclusion: DGR demonstrates that bridging distributional gaps is essential for reducing the safety tax in LRMs. Additionally, safety alignment may activate latent reasoning capabilities, as shown by the small number of examples needed to enhance refusal behaviors.

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [139] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: The paper compares three graph search algorithms for traffic-aware navigation: Floyd-Warshall-Ingerman, Dijkstra's/A*, and Yen's. Each had trade-offs in runtime and optimality.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare three distinct graph search methods in terms of their efficiency and traffic-awareness for road networks, aiming to optimize navigation solutions.

Method: Three algorithms were compared: Floyd-Warshall-Ingerman (preprocessed multi-query), Dijkstra's/A* (real-time single-query), and Yen's (hybrid of preprocessed k-shortest paths and real-time iteration).

Result: Floyd-Warshall-Ingerman was the fastest for real-time performance but lacked traffic-awareness. Dijkstra's/A* provided optimal, traffic-aware routes with minimal preprocessing. Yen's balanced speed and route optimality but required extensive preprocessing.

Conclusion: Each algorithm offers specific strengths and weaknesses, and their suitability depends on the context of the use case. Combining methods can provide tailored solutions.

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [140] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: The paper introduces NLCO, a benchmark for evaluating LLMs in handling combinatorial optimization tasks end-to-end without external tools, analyzing their performance under constraints and taxonomy-based problems.


<details>
  <summary>Details</summary>
Motivation: Although LLMs excel in math and logic reasoning, their capability in combinatorial optimization has not been thoroughly studied. The research aims to explore this unexplored area.

Method: The authors designed NLCO, a benchmark consisting of 43 problems, with a taxonomy for detailed evaluation. They assessed LLM performance by feasibility, solution optimality, and efficiency.

Result: Experiments show modern LLMs handle small problem instances effectively but struggle with larger ones, especially in graph-based and bottleneck scenarios.

Conclusion: Current LLMs demonstrate potential in combinatorial optimization but require advancements to handle larger tasks and complex patterns effectively.

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [141] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: This paper introduces TIDE, a framework to evaluate and understand the test-time improvement (TTI) paradigm in autonomous LLM agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of understanding and suitable evaluation metrics for how and why test-time improvement (TTI) mechanisms succeed or fail.

Method: The paper proposes Test-time Improvement Diagnostic Evaluation (TIDE), an agent and environment-agnostic framework that breaks down TTI into three dimensions to evaluate task dynamics, looping behaviors, and memory burdens.

Result: TIDE reveals that agents' improvement requires optimizing their interaction dynamics with the environment, beyond just internal reasoning scaling, based on rigorous experiments across varied environments and agents.

Conclusion: The framework shows that enhancing agent performance entails explicitly optimizing their engagement with the environment, offering a structured way to assess TTI dynamics.

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [142] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: The paper introduces LASER-KV, a framework for improving Key-Value (KV) compression in large language models under memory-efficient conditions, achieving superior accuracy on long context tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large language models caused by the linear growth of KV cache memory, ensuring memory efficiency without significantly impacting semantic recall.

Method: The study introduces LASER-KV, a KV compression framework utilizing a layer accumulated selection strategy with exact-LSH recall and a block-wise accumulation approach governed by a protection divisor (n).

Result: Experiments using the Babilong benchmark demonstrate LASER-KV's superiority over existing methods, showing stable performance and an accuracy improvement of up to 10% on long context tasks.

Conclusion: LASER-KV challenges the idea that attention scores alone are sufficient for token utility and sets a new benchmark for memory-efficient KV compression, ensuring semantic integrity.

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [143] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: The paper introduces a Comparative Explainable AI (Δ-XAI) framework to analyze behavioral changes in large-scale models caused by scaling, fine-tuning, or learning processes.


<details>
  <summary>Details</summary>
Motivation: Existing Explainable AI methods mainly address individual model failures, but fail to explain behavioral shifts occurring due to interventions like scaling, fine-tuning, or reinforcement learning in large-scale AI models.

Method: The authors propose a Δ-XAI framework that focuses on explaining comparative behavioral shifts between a reference and intervened model, rather than analyzing models in isolation. They define desiderata for designing explaining methods and present pipelines and an experiment to demonstrate Δ-XAI.

Result: The paper presents a concrete Δ-XAI experiment alongside pipelines to showcase how this framework can be applied effectively in highlighting behavioral differences induced by interventions.

Conclusion: Behavioral shifts in large-scale foundation models should be explained through comparative analysis, as proposed in the Δ-XAI framework, which better captures changes resulting from interventions.

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [144] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: This paper introduces Integrated Policy Gradient (IPG), a new framework for interpreting reasoning behaviors in large language models (LLMs) by tracing their internal mechanisms and contributions to reasoning outputs.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of the opaque internal mechanisms in large language models driving their strong reasoning abilities, and address existing interpretability methods' limitations in localizing and modulating reasoning components.

Method: A new framework, Integrated Policy Gradient (IPG), attributes reasoning behavior to internal components of LLMs by propagating accuracy-signals through their inference processes, emphasizing outcome-oriented and sequential influence-aware principles.

Result: IPG demonstrated superior precision in localizing components responsible for reasoning tasks and enabled reliable modulation of reasoning behaviors across various reasoning models via empirical evaluations.

Conclusion: IPG provides a more effective interpretability tool for tracing and modulating the reasoning mechanisms in LLMs, enhancing their understanding and potential control for applications.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [145] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: The paper addresses the challenge of inconsistency in Multi-Agent Discussion (MAD) involving LLMs and proposes a context learning method (M2CL) to enhance coherence and performance.


<details>
  <summary>Details</summary>
Motivation: Current MAD methods suffer from inconsistencies when multiple LLMs collaborate, as their individual contexts often misalign, leading to incoherent solutions.

Method: The authors introduce M2CL, a method that trains context generators for each agent to dynamically produce and refine context instructions, ensuring discussion coherence and progressive consensus building.

Result: M2CL achieved 20%-50% improvement over existing methods in academic reasoning, embodied tasks, and mobile control, with superior transferability and computational efficiency.

Conclusion: M2CL effectively enhances the consistency of MAD, enabling LLMs to avoid noise-driven convergence and reach accurate solutions progressively.

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [146] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: The paper introduces an online self-evolving memory system, Live-Evo, that enhances long-term task-solving performance and adapts to distribution shifts using a continuous feedback mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing memory systems struggle with true distribution shifts and continuous feedback due to reliance on static train/test splits.

Method: Live-Evo employs an Experience Bank to store events and a Meta-Guideline Bank to compile task-specific guidance, dynamically updating memory with weights based on consistent usefulness or decay.

Result: Live-Evo showed significant improvement: 20.8% better Brier score, a 12.9% increase in market returns, and successful transfers to research benchmarks.

Conclusion: The Live-Evo system effectively adapts to dynamic environments, leveraging feedback for robust, long-term performance improvements in memory-based learning models.

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [147] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: The paper introduces BELLA, a framework for selecting cost-efficient Large Language Models (LLMs) based on skill profiling and budget constraints.


<details>
  <summary>Details</summary>
Motivation: Practitioners face challenges in selecting LLMs without unnecessary spending and need more granular, task-focused model evaluations.

Method: BELLA employs (1) critic-based skill profiling, (2) clustering skills into matrices, and (3) multi-objective optimization for cost-performance trade-offs.

Result: BELLA helps practitioners identify optimal models for tasks, supports cost-saving, and provides transparent rationale for decisions.

Conclusion: The framework allows cost-efficient, skill-focused LLM deployment, emphasizing transparency and principled decision-making.

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [148] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: This paper introduces a framework where language models structure reasoning as discrete steps to better identify and correct errors, improving autonomous self-correction capabilities.


<details>
  <summary>Details</summary>
Motivation: To improve language models' ability to detect and correct reasoning errors autonomously by exploring structured reasoning techniques.

Method: A prompting method structures reasoning into discrete thought steps combined with the self-correction framework, Iterative Correction Sampling of Thoughts (Thought-ICS), which localizes and corrects errors by backtracking and generating alternatives.

Result: Thought-ICS shows a 20-40% improvement in self-correction when assisted by verification and surpasses existing self-correction methods in fully autonomous setups.

Conclusion: Structuring reasoning into discrete thoughts provides a pathway for language models to enhance self-correction capabilities, paving the way for more reliable AI systems.

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [149] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: The paper introduces Thinking with Comics, a visual reasoning paradigm using comics as an efficient medium for multimodal reasoning, offering advantages over images and videos.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current reasoning modalities, such as the lack of temporal structure in static images and high computational cost of videos.

Method: Comics are proposed as a medium that captures temporal structure, embedded text, and narrative coherence while reducing computational cost. Two reasoning paths based on comics are studied and evaluated on reasoning tasks.

Result: Experimental results show that the use of comics outperforms static images in temporal and causal reasoning tasks and is more efficient than video-based approaches.

Conclusion: Comics provide an effective intermediary visual representation for enhancing multimodal reasoning, with impact varying by narrative structure and style.

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [150] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: The paper presents MentisOculi, a benchmark to evaluate unified multimodal models' (UMMs) ability for visual reasoning, finding that current strategies for visual reasoning fail to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether incorporating visual reasoning, akin to human mental imagery, can enhance the reasoning capabilities of unified multimodal models (UMMs).

Method: The authors design MentisOculi, a procedural and tiered benchmark of multi-step reasoning tasks that rely on visual solutions, assessing different visual strategies like latent tokens and explicit imagery.

Result: UMMs show limitations in utilizing generated or even accurate visualizations; compounding errors in visual generation prevent improvement in reasoning performance.

Conclusion: Current unified multimodal models fail to benefit from visual thinking strategies, but MentisOculi establishes a framework to study and address these shortcomings in model reasoning.

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [151] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web is a new leading web agent excelling in long-horizon tasks over dynamic web interfaces, achieving state-of-the-art results on the Online-Mind2Web benchmark.


<details>
  <summary>Details</summary>
Motivation: Current web agents fail in handling complex, dynamic web tasks due to issues such as weak element grounding, lack of procedural knowledge, and task tracking limitations.

Method: Avenir-Web combines a Mixture of Grounding Experts, Experience-Imitation Planning, and adaptive memory-integrated task tracking to enhance interaction with complex web interfaces.

Result: The agent outperforms prior open-source web agents and matches proprietary models on the Online-Mind2Web benchmark for real-world scenarios.

Conclusion: Avenir-Web establishes itself as a superior open-source agent for handling complex, live website tasks, advancing the field significantly.

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [152] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: The paper addresses autoregressive LLMs' failure in logical reasoning ("reversal curse") by introducing a new data regularization technique.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive LLMs struggle with logical reasoning tasks such as reversing knowledge due to a tendency to memorize facts rather than learning higher-level rules.

Method: The authors propose the Identity Bridge data recipe, a simple regularization technique involving self-mapping data (e.g., "$A → A$") to mitigate the reversal curse.

Result: Theoretically, the proposed method enables even basic transformer models to overcome the reversal challenge. Empirically, finetuning a 1B model with the method improves reversal task success rates from near-zero to 40%.

Conclusion: This study challenges the notion that the reversal curse is an inherent limit, showing it can be addressed through a low-cost training modification, fostering improved logical reasoning in LLMs.

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [153] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: The paper introduces a benchmark dataset of failed AI agent tasks and a diagnostic framework, AGENTRX, to localize critical failures.


<details>
  <summary>Details</summary>
Motivation: AI agents often fail in complex, probabilistic, multi-agent tasks, and localizing these failures is challenging due to the complexity and noisy outputs.

Method: The authors annotated 115 failed agent trajectories with failure steps and categories from a taxonomy. They also developed AGENTRX, a domain-agnostic tool synthesizing constraints and using an LLM-based judge to pinpoint failures.

Result: AGENTRX improves failure step localization and attribution accuracy across structured API workflows, incident management, and open-ended tasks.

Conclusion: AGENTRX significantly advances the automation of diagnosing AI agent failures, reducing human effort and improving robustness.

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [154] [Accelerating Physics-Based Electromigration Analysis via Rational Krylov Subspaces](https://arxiv.org/abs/2602.00330)
*Sheldon X. -D. Tan,Haotian Lu*

Main category: cs.AR

TL;DR: This paper addresses the computational challenges of electromigration (EM) stress analysis in VLSI interconnects through two efficient techniques based on rational Krylov subspace reduction, achieving significant accuracy and speed improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome the computationally expensive challenge of solving stress-governing partial differential equations for reliable and accurate EM analysis in nanometer VLSI interconnects.

Method: The authors developed two techniques: ExtRaKrylovEM, a frequency-domain extended rational Krylov method, and EiRaKrylovEM, a time-domain rational Krylov exponential integration method. These methods utilize rational Krylov subspace reduction by expanding at selected time constants, with an optimized coordinate descent strategy for selecting shift times.

Result: The methods demonstrated significant efficiency and accuracy, achieving 20 to 500 times speedup and sub-0.1 percent prediction errors in nucleation time and resistance changes using only 4-6 Krylov orders, outperforming standard methods.

Conclusion: The proposed techniques are computationally effective and accurate for EM stress analysis, making them practical for EM-aware optimization and stochastic analysis in VLSI systems.

Abstract: Electromigration (EM) induced stress evolution is a major reliability challenge in nanometer-scale VLSI interconnects. Accurate EM analysis requires solving stress-governing partial differential equations over large interconnect trees, which is computationally expensive using conventional finite-difference methods. This work proposes two fast EM stress analysis techniques based on rational Krylov subspace reduction. Unlike traditional Krylov methods that expand around zero frequency, rational Krylov methods enable expansion at selected time constants, aligning directly with metrics such as nucleation and steady-state times and producing compact reduced models with minimal accuracy loss. Two complementary frameworks are developed: a frequency-domain extended rational Krylov method, ExtRaKrylovEM, and a time-domain rational Krylov exponential integration method, EiRaKrylovEM. We show that the accuracy of both methods depends strongly on the choice of expansion point, or shift time, and demonstrate that effective shift times are typically close to times of interest such as nucleation or post-void steady state. Based on this observation, a coordinate descent optimization strategy is introduced to automatically determine optimal reduction orders and shift times for both nucleation and post-void phases. Experimental results on synthesized structures and industry-scale power grids show that the proposed methods achieve orders-of-magnitude improvements in efficiency and accuracy over finite-difference solutions. Using only 4 to 6 Krylov orders, the methods achieve sub-0.1 percent error in nucleation time and resistance change predictions while delivering 20 to 500 times speedup. In contrast, standard extended Krylov methods require more than 50 orders and still incur 10 to 20 percent nucleation time error, limiting their practicality for EM-aware optimization and stochastic EM analysis.

</details>


### [155] [Optimal Engagement of Residential Battery Storage to Alleviate Grid Upgrades Caused by EVs and Solar Systems](https://arxiv.org/abs/2602.00342)
*Rafi Zahedi,Amirhossein Ahmadian,Chen Zhang,Shashank Narayana Gowda,Kourosh SedghiSigarchi,Rajit Gadh*

Main category: cs.AR

TL;DR: The paper addresses power quality challenges from integrating solar systems and EV chargers into distribution networks, proposing a methodology to mitigate issues using optimal battery-based solar system allocations.


<details>
  <summary>Details</summary>
Motivation: With the rise of distributed energy resources, power quality issues like voltage fluctuations and power losses arise, necessitating solutions for grid resilience, especially with increasing EV adoption.

Method: The study uses simulations on the IEEE 33-bus testbed and applies Particle Swarm Optimization to allocate battery-based solar system resources effectively.

Result: The simulations demonstrate the potential for increased grid resilience and the effective balance between non-battery-based and battery-based solar systems using optimized PP allocations.

Conclusion: Proper allocation of battery-based solar systems through advanced optimization techniques can significantly enhance grid resilience and meet future EV demand trends, including electric trucks.

Abstract: The integration of distributed energy resources has ushered in a host of complex challenges, significantly impacting power quality in distribution networks. This work studies these challenges, exploring issues such as voltage fluctuations and escalating power losses caused by the integration of solar systems and electric vehicle (EV) chargers. We present a robust methodology focused on mitigating voltage deviations and power losses, emphasizing the allocation of a Permitted Percentage (PP) of battery-based solar systems within residential areas endowed with storage capabilities. A key facet of this research lies in its adaptability to the changing landscape of electric transportation. With the rapid increase of electric trucks on the horizon, our proposed model gains relevance. By tactically deploying PP to oversee the charging and discharging of batteries within residential solar systems, utilities are poised not only to assist with grid resilience but also to cater to the upcoming demands spurred by the advent of new EVs, notably trucks. To validate the efficacy of our proposed model, rigorous simulations were conducted using the IEEE 33-bus distribution network as a designed testbed. Leveraging advanced Particle Swarm Optimization techniques, we have deciphered the optimal charging and discharging commands issued by utilities to energy storage systems. The outcomes of these simulations help us understand the transformative potential of various PP allocations, shedding light on the balance between non-battery-based and battery-based solar residences. This research underscores the need for carefully crafted approaches in navigating the complexities of modern grid dynamics amid the anticipated increase in electric vehicles.

</details>


### [156] [AutoGNN: End-to-End Hardware-Driven Graph Preprocessing for Enhanced GNN Performance](https://arxiv.org/abs/2602.00803)
*Seungkwan Kang,Seungjun Lee,Donghyun Gouk,Miryeong Kwon,Hyunkyu Choi,Junhyeok Jang,Sangwon Lee,Huiwon Choi,Jie Zhang,Wonil Choi,Mahmut Taylan Kandemir,Myoungsoo Jung*

Main category: cs.AR

TL;DR: AutoGNN is an FPGA-based accelerator designed to enhance Graph Neural Network preprocessing through effective hardware utilization and dynamic adaptation.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the preprocessing bottlenecks in Graph Neural Network inference, which dominate inference latency, by leveraging FPGA's capabilities.

Method: AutoGNN utilizes FPGA components like UPEs and SCRs for efficient parallel and sequential task execution, and includes a software framework for dynamic profiling and reprogramming.

Result: Implemented on a 7nm enterprise FPGA, AutoGNN achieves up to 9.0× and 2.1× speedup compared to conventional systems and GPUs, respectively.

Conclusion: AutoGNN demonstrates the potential of FPGA technology in overcoming GNN preprocessing challenges, providing high-performance solutions across diverse workloads.

Abstract: Graph neural network (GNN) inference faces significant bottlenecks in preprocessing, which often dominate overall inference latency. We introduce AutoGNN, an FPGA-based accelerator designed to address these challenges by leveraging FPGA's reconfigurability and specialized components. AutoGNN adapts to diverse graph inputs, efficiently performing computationally intensive tasks such as graph conversion and sampling. By utilizing components like adder trees, AutoGNN executes reduction operations in constant time, overcoming the limitations of serialization and synchronization on GPUs.
  AutoGNN integrates unified processing elements (UPEs) and single-cycle reducers (SCRs) to streamline GNN preprocessing. UPEs enable scalable parallel processing for edge sorting and unique vertex selection, while SCRs efficiently handle sequential tasks such as pointer array construction and subgraph reindexing. A user-level software framework dynamically profiles graph inputs, determines optimal configurations, and reprograms AutoGNN to handle varying workloads. Implemented on a 7$n$m enterprise FPGA, AutoGNN achieves up to 9.0$\times$ and 2.1$\times$ speedup compared to conventional and GPU-accelerated preprocessing systems, respectively, enabling high-performance GNN preprocessing across diverse datasets.

</details>


### [157] [Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators](https://arxiv.org/abs/2602.00838)
*Prabhu Vellaisamy,Harideep Nair,Di Wu,Shawn Blanton,John Paul Shen*

Main category: cs.AR

TL;DR: The paper evaluates unary and binary GEMM designs for integer-based deep learning inference by rigorously analyzing energy-efficiency and performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of unary GEMM hardware for energy-efficient computations, especially as deep learning shifts toward low-precision operations.

Method: Three unary GEMM designs (uGEMM, tuGEMM, and tubGEMM) were rigorously analyzed and compared to binary GEMM across different bit-widths, matrix sizes, and weight sparsity from multiple neural network models.

Result: The study identifies trade-offs and determines optimal configurations for unary GEMM, revealing its potential for energy efficiency in edge AI accelerators.

Conclusion: Unary GEMM has significant potential as an energy-efficient compute mechanism for future DL hardware, particularly in edge AI environments.

Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.

</details>


### [158] [ENFOR-SA: End-to-end Cross-layer Transient Fault Injector for Efficient and Accurate DNN Reliability Assessment on Systolic Arrays](https://arxiv.org/abs/2602.00909)
*Rafael Billig Tonetto,Marcello Traiola,Fernando Fernandes dos Santos,Angeliki Kritikakou*

Main category: cs.AR

TL;DR: ENFOR-SA is a new framework for accurate yet efficient transient fault analysis in DNN systolic array architectures, significantly reducing overhead compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and impracticality of traditional fault-injection techniques for analyzing advanced DNN architectures due to their complexity and the high overhead of hardware-level simulation.

Method: Introduced an end-to-end framework called ENFOR-SA, which combines cross-layer simulation and utilizes RTL-level models only during fault injection, optimizing the majority of the process at the software level.

Result: ENFOR-SA achieves RTL-accurate fault injection with only a 6% slowdown compared to software-based injection, a 569x speedup over full-SoC RTL simulation, and a 2.03x improvement over state-of-the-art tools.

Conclusion: ENFOR-SA effectively balances accuracy and efficiency in fault analysis of DNNs, making it a highly practical alternative to traditional full-RTL simulation methodologies.

Abstract: Recent advances in deep learning have produced highly accurate but increasingly large and complex DNNs, making traditional fault-injection techniques impractical. Accurate fault analysis requires RTL-accurate hardware models. However, this significantly slows evaluation compared with software-only approaches, particularly when combined with expensive HDL instrumentation. In this work, we show that such high-overhead methods are unnecessary for systolic array (SA) architectures and propose ENFOR-SA, an end-to-end framework for DNN transient fault analysis on SAs. Our two-step approach employs cross-layer simulation and uses RTL SA components only during fault injection, with the rest executed at the software level. Experiments on CNNs and Vision Transformers demonstrate that ENFOR-SA achieves RTL-accurate fault injection with only 6% average slowdown compared to software-based injection, while delivering at least two orders of magnitude speedup (average $569\times$) over full-SoC RTL simulation and a $2.03\times$ improvement over a state-of-the-art cross-layer RTL injection tool. ENFOR-SA code is publicly available at https://github.com/rafaabt/ENFOR-SA.

</details>


### [159] [NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units](https://arxiv.org/abs/2602.01546)
*Shanmuga Venkatachalam,Prabhu Vellaisamy,Harideep Nair,Wei-Che Huang,Youngseok Na,Yuyang Kang,Quinn Jacobson,John Paul Shen*

Main category: cs.AR

TL;DR: The study introduces NeuroAI TNNs (NeuTNNs), a new efficient type of temporal neural networks leveraging findings from neuroscience, along with a tool, NeuTNNGen, for their design.


<details>
  <summary>Details</summary>
Motivation: To integrate neuroscience principles into artificial intelligence to accelerate innovation and improve the capability and energy efficiency of AI systems.

Method: The researchers propose NeuroAI TNNs (NeuTNNs) which incorporate neuroscience-based advancements, implement synaptic pruning, and establish a PyTorch-to-layout tool suite, NeuTNNGen, for designing these models.

Result: NeuTNNs exhibit superior performance and efficiency compared to traditional TNNs, achieving 30-50% hardware cost reduction while maintaining accuracy. Performance is demonstrated across diverse sensory modalities and specific applications.

Conclusion: NeuTNNs and NeuTNNGen provide a significant foundation for developing energy-efficient and application-specific AI systems grounded in neuroscience.

Abstract: Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.

</details>


### [160] [In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning](https://arxiv.org/abs/2602.01827)
*Tommaso Spagnolo,Cristina Silvano,Riccardo Massa,Filippo Grillotti,Thomas Boesch,Giuseppe Desoli*

Main category: cs.AR

TL;DR: The paper presents a novel architecture that integrates Digital In-Memory Computing (DIMC) with the Vector RISC-V ISA to improve Deep Learning inference at the edge, achieving significant performance and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The need for high-performance and energy-efficient architectures for Deep Learning inference on edge devices drives the integration of Digital In-Memory Computing to reduce data movement and enhance efficiency.

Method: This study proposes modifying the Vector RISC-V Instruction Set Architecture by adding a DIMC unit to the execution pipeline, along with four custom instructions for optimized data operations during Deep Learning inference.

Result: The architecture achieves 137 GOP/s peak performance, a 217x speedup over the baseline core, and a 50x area-normalized speedup, demonstrating sustained throughput on models like ResNet-50.

Conclusion: The experimental findings highlight the proposed architecture's effectiveness as a scalable and efficient method for accelerating Deep Learning inference in edge computing scenarios.

Abstract: Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.

</details>


### [161] [Position: The Need for Ultrafast Training](https://arxiv.org/abs/2602.02005)
*Duc Hoang*

Main category: cs.AR

TL;DR: The paper proposes a shift from inference-only FPGAs to ultrafast on-chip learning for real-time adaptation in non-stationary, high-frequency systems.


<details>
  <summary>Details</summary>
Motivation: Current FPGAs are limited to static models trained offline, restricting their operation in rapidly changing environments where fast model updates are crucial.

Method: Introduce ultrafast on-chip learning within the FPGA fabric to integrate inference and training under deterministic, low-latency constraints.

Result: Potential transformation of FPGAs into real-time learning machines capable of adapting as quickly as the physical systems they control.

Conclusion: Jointly rethinking algorithms, architectures, and toolflows could enable groundbreaking applications in domains requiring rapid adaptation and learning.

Abstract: Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.

</details>


### [162] [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: The paper presents a novel approach using Kolmogorov-Arnold Networks (KANs) for ultrafast online learning, outperforming conventional MLPs in low-latency and resource-constrained systems.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies and numerical instabilities of Multi-Layer Perceptrons (MLPs) in high-frequency systems demanding ultrafast adaptation.

Method: Leverage Kolmogorov-Arnold Networks (KANs) with advancements such as B-spline locality for sparsity and fixed-point quantization for robustness, implemented on FPGA platforms.

Result: KANs demonstrated improved efficiency and expressiveness compared to MLPs in ultrafast online learning scenarios, achieving sub-microsecond latencies.

Conclusion: KANs are a promising solution for ultrafast online learning, surpassing MLPs' limitations and setting a new benchmark in resource-constrained and low-latency environments.

Abstract: Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.

</details>


### [163] [CHAOS: Controlled Hardware fAult injectOr System for gem5](https://arxiv.org/abs/2602.02119)
*Elio Vinciguerra,Enrico Russo,Giuseppe Ascia,Maurizio Palesi*

Main category: cs.AR

TL;DR: CHAOS is a modular and configurable fault injection framework for gem5, designed to analyze and improve system reliability.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop tools that facilitate the evaluation and enhancement of system reliability and resilience through fault simulation.

Method: CHAOS framework integrates with the gem5 simulator, allowing precise and systematic fault injection across different architectural levels.

Result: It successfully supports evaluations of fault tolerance mechanisms and resilience strategies across diverse fault models and scenarios.

Conclusion: CHAOS contributes to the research in dependable computing by enabling comprehensive fault analysis and system robustness improvements.

Abstract: Fault injectors are essential tools for evaluating the reliability and resilience of computing systems. They enable the simulation of hardware and software faults to analyze system behavior under error conditions and assess its ability to operate correctly despite disruptions. Such analysis is critical for identifying vulnerabilities and improving system robustness. CHAOS is a modular, open-source, and fully configurable fault injection framework designed for the gem5 simulator. It facilitates precise and systematic fault injection across multiple architectural levels, supporting comprehensive evaluations of fault tolerance mechanisms and resilience strategies. Its high configurability and seamless integration with gem5 allow researchers to explore a wide range of fault models and complex scenarios, making CHAOS a valuable tool for advancing research in dependable and high-performance computing systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [164] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: This paper introduces PPoGA, a novel framework for KGQA that integrates a Planner-Executor and self-correction mechanism, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of LLMs in handling flawed reasoning plans due to functional fixedness, making AI reasoning systems more robust and flexible.

Method: Developed PPoGA framework using Planner-Executor architecture combined with predictive processing and a self-correction mechanism for both Path and Plan Correction.

Result: Achieved state-of-the-art performance on KGQA benchmarks (GrailQA, CWQ, and WebQSP), outperforming existing methods.

Conclusion: PPoGA's metacognitive abilities show the importance of problem restructuring for enhancing AI reasoning in question answering.

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [165] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: MediGRAF is a hybrid system combining structured and unstructured data retrieval for EHRs to improve information querying and retrieval safety.


<details>
  <summary>Details</summary>
Motivation: EHR systems overwhelm clinicians with information, and current methods to utilize LLMs do not fully address issues like context grounding or hallucinations.

Method: The study introduced MediGRAF—a hybrid Graph RAG system integrating structured querying (Neo4j Text2Cypher) and unstructured text embedding retrieval, tested on patient records from MIMIC-IV.

Result: MediGRAF achieved 100% recall on factual queries and high-quality scores (4.25/5) for complex inference tasks without safety violations.

Conclusion: Hybrid graph-grounding systems improve EHR data querying, ensuring safer and more reliable clinical decision-making compared to standard LLM approaches.

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [166] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: The paper introduces G-MemLLM, a memory-augmented model integrating LLMs with a Latent Memory Bank to address context limits and factual inconsistency in long-term reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with finite context windows and multi-hop reasoning challenges, such as maintaining long-term factual consistency.

Method: G-MemLLM uses a frozen LLM backbone combined with a trainable Latent Memory Bank featuring GRU-style gated updates to manage memory selectively and prevent knowledge gradients from vanishing.

Result: G-MemLLM improves performance significantly across benchmarks, such as a 13.3% accuracy boost on ZsRE for Llama 3.1-8B, an 8.56-point Answer F1 improvement for GPT-2, and a 6.89-point Supporting Fact F1 increase for Llama 3.1-8B on HotpotQA.

Conclusion: G-MemLLM outperforms standard LLMs in multi-hop reasoning tasks, enhancing relational precision and scaling effectively across models.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [167] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: PTCBENCH is a benchmark to evaluate personality consistency in large language models (LLMs) under different situational contexts.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked aspect of LLM personality traits being dynamic and context-dependent for better user trust and engagement.

Method: PTCBENCH evaluates LLM personalities across 12 external contexts using the NEO Five-Factor Inventory, analyzing 39,240 personality trait records.

Result: Specific scenarios like 'Unemployment' notably impact LLMs’ personalities and reasoning abilities.

Conclusion: PTCBENCH provides insights for developing psychologically-aligned AI by establishing a systematic framework for personality consistency evaluation in changing environments.

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [168] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: This paper introduces SafeTalkCoach, a dialogue generation framework for simulating parent-child conversations about sexual health, adhering to best practices while maintaining realism and diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in collecting real-world data on sensitive parent-child communication about sexual health, given the private nature of these conversations, and to provide a reliable resource for AI research and health communication practices.

Method: The authors developed SafeTalkCoach, which integrates sources like crowd-sourced and synthesized scenarios, sexual health guidelines, personas, adaptive control modules, and hierarchical diversification to generate realistic and diverse conversations.

Result: The evaluations of SafeTalkCoach show that it generates diverse, realistic, and high-quality parent-child conversations about sexual health, with controllability.

Conclusion: The SafeTalkCoach framework and dataset aim to enhance AI research and health communication practices by providing a valuable tool for studying and improving parent-child communication on sensitive topics.

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [169] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: The paper introduces a framework to enhance semantic understanding and reasoning in enterprise knowledge management by combining structured and unstructured data into a unified ontology and proposing a three-stage training process.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome limitations of traditional knowledge graphs, such as poor implicit relationship discovery and failure to handle complex question answering in enterprise-scale knowledge management.

Method: The authors propose a dual-layer enterprise ontology constructed from multi-source data, combined with a three-stage training pipeline involving ontology instruction fine-tuning, text-ontology grounding, and multi-task instruction tuning via curriculum learning.

Result: The framework achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning tasks, demonstrating improved fusion of ontology structures and language.

Conclusion: The proposed unified construct--align--reason framework effectively enhances semantic reasoning and knowledge integration in enterprise environments, with potential for significant advancements in complex question answering systems.

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [170] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: The paper introduces Reversible Diffusion Decoding (RDD), a framework to improve the robustness and quality of block-wise diffusion language models by enabling backtracking during failures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of stagnation in diffusion language models, where irreversible token commitments during generation hinder progress under suboptimal contexts.

Method: RDD incorporates a reversible mechanism that identifies stagnation, backtracks to earlier blocks using cached states, and applies confidence-guided re-masking to reinitialize uncertain tokens while preserving reliable ones.

Result: RDD achieves improved generation robustness and quality over baseline methods while maintaining computational efficiency.

Conclusion: RDD successfully enhances diffusion-based generation by allowing recovery from commitment errors and maintaining parallel efficiency through its reversible mechanism.

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [171] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: The paper addresses the limitation of retrieval-augmented generation (RAG) systems in handling diverse plausible answers and proposes a new framework, DIVERGE, for enhancing diversity while maintaining answer quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of existing RAG systems that fail to utilize context diversity effectively, leading to less diverse responses and impacting fair information access.

Method: The method involves introducing DIVERGE, a new RAG framework featuring reflection-guided generation and memory-augmented iterative refinement to enhance diverse viewpoints and preserve answer quality while using new metrics for evaluation.

Result: DIVERGE improves the trade-off between diversity and quality, outperforming competitive baselines and state-of-the-art methods on the Infinity-Chat dataset while correlating well with human evaluations.

Conclusion: The study reveals systematic limitations of current LLM-based systems for open-ended questions and shows that modeling diversity explicitly can significantly address this issue.

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [172] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: This paper introduces the first large-scale benchmark for evaluating uncertainty quantification (UQ) in reasoning-demanding QA tasks using large language models, highlighting their limitations and presenting findings across 685,000 responses.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable uncertainty quantification (UQ) in scientific question answering settings, where fact-retrieval and reasoning play crucial roles.

Method: The paper benchmarked up to 20 large language models across seven scientific QA datasets using prompting for open-ended QA, evaluating various UQ methods, including token-level and sequence-level calibration approaches.

Result: Key findings reveal problems with token-level confidences induced by instruction tuning, systematic biases in verbal approaches, and effectiveness of answer frequency for calibration. Misleading reliance on ECE metrics for UQ evaluation was also highlighted.

Conclusion: Current UQ methods for large language models in scientific QA are limited and require improved approaches, particularly in calibration and reliable benchmarking practices.

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [173] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: The paper analyzes unfaithfulness in explanations by Patchscopes on LLMs and proposes a novel recalibration method, BALOR, to address the issue.


<details>
  <summary>Details</summary>
Motivation: To address the issue where LLMs rely on linguistic biases rather than contextual information within hidden representations, causing unfaithfulness in generated explanations.

Method: The authors designed a dataset to evaluate faithfulness in LLM explanations and introduced the Bias Alignment through Logit Recalibration (BALOR), which recalibrates output logits by contrasting unbiased and patched prompts.

Result: BALOR improved the faithfulness of explanations in LLMs, achieving up to 33% performance improvement and reducing bias across evaluated cases.

Conclusion: The paper highlights bias-related unfaithfulness in LLMs and demonstrates that BALOR effectively suppresses these biases, enhancing explanation quality in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [174] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: The paper proposes a two-stage pipeline to extract metadata from municipal meeting minutes, addressing challenges posed by heterogeneous formats and styles.


<details>
  <summary>Details</summary>
Motivation: Municipal meeting minutes are inconsistent in format, making metadata extraction challenging for effective information retrieval. Existing NER models are unsuitable for domain-specific categories in this context.

Method: The methodology involves a question answering model to identify key text segments followed by transformer-based NER models (like BERTimbau and XLM-RoBERTa with CRF) to extract detailed metadata.

Result: The proposed approach achieved strong in-domain performance and demonstrated effectiveness over general-purpose LLMs. However, cross-municipality evaluations revealed a limitation in generalization due to record variability.

Conclusion: The study establishes the first benchmark for metadata extraction in this domain, paving the way for improvements and further research in processing municipal records efficiently.

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [175] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: This paper studies the growing use of AI-generated content in academic peer reviews, revealing a significant increase in AI-detected reviews between 2022 and 2025.


<details>
  <summary>Details</summary>
Motivation: To explore the emergence and implications of AI-generated content in academic peer review processes, especially as LLMs are more widely adopted.

Method: A detection model trained on historical reviews was used to analyze peer review data from ICLR and Nature Communications, tracking AI-generated content over time.

Result: Minimal AI-generated content before 2022, followed by a sharp rise. By 2025, 20% of ICLR reviews and 12% of Nature Communications reviews were identified as AI-generated, with significant growth seen in late 2024 for Nature Communications.

Conclusion: The paper highlights the increasing role of AI assistance in peer review processes and emphasizes the urgency of understanding its consequences for academic evaluation.

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [176] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: The paper introduces a benchmark named DETOUR to evaluate conversational agents in multi-turn tip-of-the-tongue retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in conversational agent benchmarks, which currently fail to simulate multi-turn tip-of-the-tongue scenarios realistically.

Method: They proposed DETOUR, a dual-agent benchmark, where a Primary Agent queries a consistent Memory Agent to recollect information across 1,011 prompts in text, image, audio, and video modalities.

Result: State-of-the-art models only achieved 36% accuracy on DETOUR, indicating current limitations in handling multi-modal, underspecified scenarios.

Conclusion: The study highlights the need for improvements in conversational agents' capabilities for realistic multi-turn and ambiguous information retrieval tasks.

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [177] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: The paper introduces DecompressionLM, a new framework for extracting knowledge from language models without needing pre-defined queries, addressing current probing limitations such as scalability and bias toward frequent concepts.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge probing methods rely on predefined queries, which limits the ability to discover unknown concepts encoded in language models. The paper seeks to overcome this constraint.

Method: The approach uses a stateless framework involving Van der Corput low-discrepancy sequences and arithmetic decoding to enable deterministic, parallel generation of concept graphs without shared state.

Result: The framework improves concept coverage significantly, with results showing a 30-170% increase with a specific quantization method (AWQ-4bit) and a notable collapse of 71-86% with another method (GPTQ-Int4). Additionally, a 17-point gap in hallucination rates was observed between top- and bottom-ranked models.

Conclusion: DecompressionLM offers a new dimension for evaluating language models by enhancing knowledge breadth and addressing factual grounding, particularly useful for compressed model evaluations.

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [178] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: The paper evaluates if advanced language models capture binding relations in Turkish reflexive pronouns.


<details>
  <summary>Details</summary>
Motivation: To examine how well large language models handle the syntactic and semantic nuances of binding relations in Turkish reflexive pronouns.

Method: Constructed and tested 100 balanced sentences contrasting local and non-local antecedents for Turkish reflexives on two models: OpenAI's chain-of-thought model and Trendyol-LLM-7B-base.

Result: Trendyol-LLM showed a strong locality bias (70% favoring local antecedents), while OpenAI's model distributed choices more evenly.

Conclusion: There is a marked difference in how the two models handle binding relations, with Trendyol-LLM showing significant locality bias.

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [179] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: Large Reasoning Models (LRMs) face challenges of redundancy in reasoning outputs. This paper proposes identifying meaningful content using attribution metrics and introduces a selective learning framework to enhance model performance.


<details>
  <summary>Details</summary>
Motivation: LRMs generate verbose reasoning chains, but much of this content is uninformative, leading to degraded model performance after supervised fine-tuning (SFT).

Method: The method uses integrated gradient attribution to evaluate token influence and define two metrics: attribution strength (overall token impact) and direction consistency (distribution of positive/negative effects). It selectively fine-tunes models on important reasoning segments with these metrics, masking unimportant data.

Result: The proposed method improves accuracy and reduces redundancy in reasoning models across various models and datasets.

Conclusion: Selective learning based on segment relevance improves reasoning model performance and output efficiency by focusing on reflective rather than shallow reasoning segments.

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [180] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: The study explores the Mandela effect in large language model-based multi-agent systems, identifying causes, testing agents with a new benchmark (MANBENCH), and proposing mitigation strategies, achieving a reduction in misinformation by 74.40%.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored problem of collective cognitive biases, such as the Mandela effect, in multi-agent systems, which hinder accurate memory recall and raise ethical concerns about misinformation spread.

Method: The study introduces MANBENCH, a benchmark involving four task types and five interaction protocols to measure the Mandela effect. It evaluates agents from various LLMs and proposes mitigation strategies such as cognitive anchoring, scrutiny, and alignment-based defenses.

Result: The proposed strategies successfully achieved a 74.40% reduction in the Mandela effect, demonstrating their efficacy in mitigating memory biases.

Conclusion: The study highlights the importance of addressing cognitive biases in LLM-based multi-agent systems, providing insights and tools (e.g., MANBENCH) to ethically align these systems and mitigate misinformation effectively.

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [181] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: The study investigates how Large Language Models (LLMs) select important information for summarization using behavioral and computational analyses.


<details>
  <summary>Details</summary>
Motivation: To understand and interpret the hidden mechanisms of how LLMs prioritize information for summarization tasks.

Method: The paper uses empirical importance distributions generated from length-controlled summaries to assess consistency in information selection behaviorally. Computationally, attention heads and layer-based predictive analysis were performed.

Result: LLMs display consistent importance patterns that diverge significantly from pre-LLM baselines and align more within families than by size. Certain attention heads and middle-to-late layers predict importance well.

Conclusion: The research offers insights into LLMs' internal representation and prioritization of importance in summarization, paving the way for better control and understanding of their information selection processes.

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [182] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: The paper introduces SENSE, a model connecting lexical embeddings to sensorimotor experience, alongside a study showing human-sensorimotor correlations and phonosthemic analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational word embeddings and human sensory and motor understanding of language.

Method: Developed SENSE, a projection model predicting sensorimotor norms from lexical embeddings, and conducted a behavioral study with human participants involving nonce word selection.

Result: Statistically significant correlations were found between human selections and SENSE ratings for 6 modalities. Phonosthemic patterns related to interoception were identified.

Conclusion: SENSE effectively links lexical embeddings to sensorimotor norms, revealing phonosthemic associations and advancing computational language studies.

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [183] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: The study explores intention-based tasks for revision generation using a new framework called Intention-Tuning, which achieves efficient and effective fine-tuning on small datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effectively applying LLMs to intention-based generation tasks, like revision generation, where existing models struggle with complex and multi-intention scenarios due to limited annotated data.

Method: The authors propose Intention-Tuning, a framework that adaptively selects certain LLM layers to fine-tune for learning writer intentions and repurposing them for revision generation tasks.

Result: Experimental results show that Intention-Tuning performs better than other PEFT (Parameter-Efficient Fine-Tuning) baselines, especially on small revision datasets.

Conclusion: The proposed approach offers a promising and cost-effective solution for handling intention-based text generation tasks, paving the way for efficient LLM fine-tuning in low-resource scenarios.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [184] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: The paper introduces GlobalHealthAtlas, a multilingual dataset for public health reasoning, and proposes novel methods for dataset construction, quality control, and evaluation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of structured benchmarks for machine learning in public health reasoning, a critical yet underexplored problem domain.

Method: The authors developed GlobalHealthAtlas, a dataset of 280,210 instances across 15 health domains and 17 languages, with tools for supervised learning and slice-based evaluations, supported by a quality control pipeline involving LLM-based checks.

Result: The study successfully constructs and validates a large-scale dataset and proposes a domain-aligned evaluation tool to assess outputs across six dimensions.

Conclusion: The contributions enable advancements in reproducible training and evaluation of machine learning solutions for complex public health reasoning tasks.

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [185] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: This paper discusses governance challenges for multilingual large language models (MLLMs) in terms of cultural inequities, localized norms, and accountability limitations, proposing culturally grounded frameworks instead of technical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing governance frameworks for MLLMs are English-centric, neglecting marginalized languages and communities, resulting in systemic risks and sociotechnical harm.

Method: The paper synthesizes cross-cultural perspectives, evidence from multilingual model behavior, data asymmetries, and sociotechnical harm, introducing a governance framework grounded in cultural norms and human rights.

Result: Three governance challenges are identified: inequities in training and evaluation, misalignment with local norms and values, and insufficient accountability mechanisms for marginalized communities.

Conclusion: Culturally grounded governance is necessary to prevent MLLMs from exacerbating global inequalities and to align model behavior with sociocultural and rights-based priorities.

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [186] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: The paper addresses challenges in TableQA for LLMs by using a commented step-by-step code-generation framework, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance numerical accuracy and reasoning interpretability in TableQA tasks, which often suffer due to disrupted data structure relationships in conventional methods.

Method: The proposed method uses a Python code-generation framework with multi-line programs and natural language comments, allowing better reasoning and correct code generation.

Result: The method achieves 70.9% accuracy on WikiTableQuestions using Qwen2.5-Coder-7B-Instruct, surpassing existing baselines. An integrated framework further raises accuracy to 84.3%.

Conclusion: The study presents an effective way to improve TableQA through explicit, step-by-step coding and integration with end-to-end models, advancing both accuracy and interpretability.

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [187] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: This study examines how the BERT model processes Argument Structures using multi-dimensional analysis, finding hierarchical representation in its layers.


<details>
  <summary>Details</summary>
Motivation: To explore how BERT, a prevalent NLP model, represents linguistic structures, specifically Argument Structure Constructions.

Method: They used methods like MDS, t-SNE, GDV, FDR, and attention analysis to analyze hierarchical representation across BERT layers.

Result: The study found construction-specific information progresses hierarchically: forming in early layers, optimizing separation in middle layers, and persisting in later stages.

Conclusion: BERT successfully encodes and maintains complex linguistic structure hierarchically throughout its layers.

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [188] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

TL;DR: The paper analyzes changes in French drama (1700-1900) using topic modeling and economic indicators, highlighting shifts post-French Revolution.


<details>
  <summary>Details</summary>
Motivation: To explore how political and economic transformations, like the French Revolution and industrialization, influenced thematic trends in French drama.

Method: The study employs Latent Dirichlet Allocation and Jensen-Shannon Divergence for topic modeling, and plots topic prevalence against French GDP changes from 1700-1900.

Result: The topical composition of French drama shifted significantly post-Revolution, with bourgeois themes becoming more dominant from the late 18th century.

Conclusion: French drama's evolution closely aligns with major socio-economic developments, reflecting broader historical changes in themes and focus.

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


### [189] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: Kanade is a disentangled speech tokenizer that enhances phonetics and prosody extraction while suppressing irrelevant information such as speaker identity, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To develop a speech tokenizer that effectively extracts phonetics and prosody while suppressing irrelevant information, enabling high-quality speech synthesis and modeling.

Method: Kanade employs a single-layer disentangled structure to separate acoustic constants, creating a stream of tokens for phonetics and prosody without requiring auxiliary disentanglement methods.

Result: Kanade achieves state-of-the-art results in speaker disentanglement and lexical availability, along with maintaining excellent reconstruction quality.

Conclusion: Kanade demonstrates exceptional performance as a speech tokenizer, streamlining efficient speech modeling and synthesis through its disentangled and highly effective architecture.

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [190] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: Hermes, an LLM-based automated framework, aims to improve interlingual subtitling through dedicated modules for speaker diarization, terminology identification, and expressiveness enhancement.


<details>
  <summary>Details</summary>
Motivation: The paper targets the challenges of interlingual subtitling in machine translation, focusing on semantic coherence, pronoun and terminology accuracy, and expressive translation in subtitle texts.

Method: The proposed solution is Hermes, an LLM-based framework comprising three modules: Speaker Diarization for identifying speakers, Terminology Identification for precision in translation, and Expressiveness Enhancement for improving translation quality.

Result: Hermes delivers state-of-the-art speaker diarization performance and generates expressive, contextually coherent translations for interlingual subtitling.

Conclusion: Hermes significantly contributes to research on automated interlingual subtitling, offering enhanced performance and improved translations.

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [191] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: The paper introduces LAVE, a constrained decoding approach specifically for diffusion large language models (dLLMs), improving syntactic correctness without significant performance overhead.


<details>
  <summary>Details</summary>
Motivation: Diffusion large language models (dLLMs) struggle with generating syntactically valid outputs due to their probabilistic nature, and current constrained decoding techniques are ineffective or unreliable for these models.

Method: The proposed method, LAVE, leverages the parallel token prediction property of dLLMs to perform lookahead at each forward pass, ensuring intermediate outputs remain extensible into valid sentences.

Result: LAVE demonstrated superior performance across four dLLMs and three benchmarks, significantly enhancing syntactic correctness with minimal runtime penalties.

Conclusion: LAVE provides an efficient and effective constrained decoding solution for dLLMs, ensuring reliable generation of syntactically valid outputs and addressing key limitations of existing methods.

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [192] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: This paper evaluates the use of transformers (RoBERTa and XLM-RoBERTa) for hope speech detection in English and German, achieving notable accuracy and F1-scores.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance hope speech detection in English and German using advanced pre-trained transformers to improve natural language processing tasks.

Method: The paper implements the transformers RoBERTa for English and multilingual XLM-RoBERTa for both English and German, and evaluates their performance on hope speech detection.

Result: RoBERTa achieved a weighted F1-score of 0.818 and 81.8% accuracy for English, while XLM-RoBERTa obtained an F1-score of 0.786 and 78.5% accuracy for English and German.

Conclusion: The study highlights the effectiveness of pre-trained transformer models and their potential in improving NLP tasks like hope speech detection.

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [193] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: The paper analyzes the effect of safety alignment in LLMs as a systematic distortion and proposes a comprehensive framework for addressing jailbreak scenarios via forecast aggregation strategies, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the discrepancies caused by safety alignment in LLMs, particularly to understand and optimize handling of jailbreak attacks systematically.

Method: Developed a framework modeling alignment as systematic distortion, cast jailbreaking as a forecast aggregation problem, derived optimal strategies like Gradient Shift, and proposed a new hybrid rule.

Result: Achieved higher Attack Success Rates and reduced Jailbreak Tax on safety-hardened models, outperforming existing jailbreak methods across benchmarks.

Conclusion: The proposed framework effectively handles jailbreaking challenges, providing a systematic and generalized approach that outperforms prior methods.

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [194] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: The thesis introduces methods to make language models more semantically interpretable and controllable using a VAE framework, focusing on sentence-level and reasoning-level tasks.


<details>
  <summary>Details</summary>
Motivation: To create language models with interpretable and precise semantic representations, which enable better control and understanding of latent spaces.

Method: Using a VAE framework, the thesis explores disentangling semantic features at the sentence level and controlling inference behaviors at the reasoning level, using innovative theoretical and practical approaches.

Result: The proposed methods improve both interpretability and controllability of latent spaces in natural language tasks.

Conclusion: The research successfully moves towards language models with semantic representations that are systematically interpretable and controllable.

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [195] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

TL;DR: LegalOne is a family of models developed for the Chinese legal domain with enhanced reasoning capabilities. It utilizes tailored techniques for domain adaptation, reasoning trajectory extraction, and multi-phase learning to outperform larger general-purpose language models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle in domains requiring rigorous, multi-step reasoning due to insufficient domain-specific knowledge. The paper aims to bridge this gap in the legal sector, especially the Chinese legal domain.

Method: The paper introduces a three-phase pipeline: 1) PAS for domain adaptation, balancing knowledge acquisition and retention; 2) LEAD for structured reasoning from legal texts; 3) Curriculum Reinforcement Learning to progressively enhance legal reasoning capabilities.

Result: LegalOne outperforms larger general-purpose models in legal tasks due to its sophisticated reasoning and domain-specific training. It also sets a new benchmark for legal AI performance.

Conclusion: LegalOne showcases the potential of specialized models, offering enhanced efficiency and reliability for judicial reasoning tasks while providing publicly available tools for advancing Legal AI research.

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [196] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: This paper evaluates instruction-tuned Small Language Models (SLMs) for multi-turn customer-service QA and compares them with Large Language Models (LLMs). Results show promising performance in some SLMs, but highlight challenges in dialogue continuity and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether Small Language Models can effectively perform in multi-turn customer-service QA environments requiring dialogue continuity, particularly as an efficient and cost-effective alternative to resource-intensive Large Language Models.

Method: The method involves instruction-tuning SLMs, utilizing a history summarization approach to maintain conversation context, and evaluating nine SLMs against three LLMs using semantic and lexical metrics, qualitative analysis, human evaluation, and LLM-as-a-judge methods.

Result: Findings reveal varying performance among SLMs, with some nearing LLM performance levels while others exhibit difficulties in maintaining conversational context and alignment.

Conclusion: SLMs show potential for resource-constrained customer-service QA systems, but their limitations in dialogue continuity and contextual understanding need to be addressed for practical deployment.

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [197] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: EchoReview leverages citation contexts to enhance automated peer review, addressing scalability and reliability issues in traditional systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome scalability and reliability challenges in traditional peer review systems caused by increasing scientific submissions and inconsistent human reviews.

Method: The method involves mining evaluative signals from citation contexts, constructing a large citation-driven review dataset (EchoReview-16K), and training an automated reviewer (EchoReviewer-7B).

Result: EchoReviewer-7B demonstrated significant improvements on review quality dimensions such as evidence support and review comprehensiveness.

Conclusion: Citation contexts are validated as an effective framework for developing scalable and reliable automated peer review systems.

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [198] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: ExperienceWeaver introduces a novel hierarchical framework enhancing clinical text improvement by shifting focus from data retrieval to actionable experience learning, achieving notable successes in small-sample settings.


<details>
  <summary>Details</summary>
Motivation: Clinical text improvement is challenging due to limited high-quality data and complex medical documentation constraints. Current approaches depend heavily on extensive data or provide superficial corrections.

Method: The ExperienceWeaver framework distills multidimensional feedback into error-specific Tips and high-level Strategies, guiding the model to learn revision reasoning over simple recall.

Result: Evaluations across four clinical datasets demonstrate ExperienceWeaver's superior performance, surpassing state-of-the-art models like Gemini-3 Pro in small-sample scenarios.

Conclusion: ExperienceWeaver presents an effective solution for improving clinical text editing by injecting structured experience, achieving reliable enhancements in small data environments.

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [199] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: CURP introduces a framework for efficient and interpretable user modeling to enhance personalization in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing personalization quality with computational and data efficiency in current user modeling approaches.

Method: CURP employs a bidirectional user encoder and a discrete prototype codebook, allowing personalization with minimal trainable parameters (~ 0.2% of the model size).

Result: Experiments demonstrated CURP's superior performance, generalization, and scalability compared to strong baselines on variant generation tasks.

Conclusion: CURP provides an efficient, interpretable, and scalable solution for personalized modeling and generation in large language models.

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [200] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: This paper introduces DeMix, a framework to optimize data mixtures in LLM pre-training using model merging to predict optimal ratios, breaking trade-offs such as sufficiency and efficiency.


<details>
  <summary>Details</summary>
Motivation: Identifying the optimal data mixture for LLM pre-training is challenging due to reliance on constrained or expensive methods, requiring a more efficient and reliable approach.

Method: The authors propose DeMix, a framework that trains component models on datasets at scale and evaluates mixtures using weighted model merging, reducing the training cost for optimizing data mixtures.

Result: DeMix demonstrates improved mixture discovery through extensive experimentation, achieving better benchmark performance at reduced cost compared to existing methods.

Conclusion: DeMix enhances large-scale LLM pre-training by providing an efficient, accurate, and scalable framework for optimizing data mixtures, supported by the release of the DeMix Corpora dataset.

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [201] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: Search-engine date filters do not reliably eliminate post-cutoff leakage during retrospective forecasting evaluations, resulting in inflated accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether search-engine date filters are dependable for ensuring reliable pre-cutoff retrieval in search-augmented forecasting tasks.

Method: Auditing Google Search using 'before:' filters to identify post-cutoff content leakage and leveraging a large language model, gpt-oss-120b, to assess forecasting accuracy using leaky and leak-free documents.

Result: Findings reveal 71% of questions had post-cutoff leaks in retrieval, and in 41%, the answer could be directly inferred; prediction accuracy was significantly inflated using leaky documents.

Conclusion: Date-restricted searches are insufficient for temporal evaluation; stronger retrieval methods or frozen, time-stamped web snapshots are recommended for more credible forecasting evaluations.

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [202] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper introduces a method (A$^2$D) to improve reinforcement learning with verifiable rewards (RLVR) by decomposing complex questions into simpler sub-questions for enhanced reasoning by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: While RLVR has improved LLMs' reasoning abilities, it often struggles due to blind exploration caused by limited information during the process.

Method: The proposed method first trains a decomposer via RLVR without a teacher model to create sub-questions from complex queries. These sub-questions are then used to guide the training of the reasoner.

Result: A$^2$D shows improved performance compared to competitive baselines, functions as a modular component for various RLVR algorithms, and provides insights into how RLVR affects decomposer behavior.

Conclusion: The A$^2$D approach enhances RLVR by introducing sub-question guidance, improving LLM reasoning and exploration abilities through adaptive techniques.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [203] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: This paper identifies and addresses the issue of overthinking in Large Reasoning Models (LRMs) during reasoning by analyzing repetitive behaviors and introduces a reward-based method to enhance their efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the problem of overthinking in LRMs, specifically targeting redundant and inefficient post-answer reasoning behaviors that waste computational resources.

Method: They define a Reasoning Anchor, analyze reasoning behavior pre- and post-anchor, and introduce the Anchor-based Process Reward (APR), a reward shaping method that penalizes repetitive verification after the answer stabilizes.

Result: APR models achieve improved performance and efficiency across five mathematical reasoning datasets at 1.5B and 7B model scales, with reduced resource requirements for reinforcement learning training.

Conclusion: Anchor-based Process Reward effectively improves the performance-efficiency trade-off in LRMs by addressing the structural redundancy of Answer-Stable Tails (AST).

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [204] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: The paper introduces WordCraft, an interactive tool using Multimodal Large Language Models (MLLMs) to improve vocabulary memorization for Chinese learners of English through the keyword method.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenges faced by Chinese-English learners in generating keywords, creating associations, and forming vivid mental imagery, which limits effective vocabulary memorization.

Method: The method includes conducting a formative study with learners and educators to identify difficulties and designing WordCraft, a learner-centered interactive tool powered by MLLMs, to scaffold the keyword learning process.

Result: User studies evidence that WordCraft preserves the generation effect and demonstrates high effectiveness and usability in vocabulary memorization.

Conclusion: WordCraft effectively aids vocabulary learning by enhancing learner engagement, guiding the keyword process, and improving retention.

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [205] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: The paper proposes a novel method to evaluate trust in AI systems using a behavioral game theory approach and validates it on GPT-4.1, showing parallels to human trust behaviors.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding and calibrating the level of trust exhibited by artificial intelligence systems, enabling more effective and human-centered AI interactions.

Method: Developed and applied a novel elicitation method using iterated in-context learning within the framework of the behavioral Trust Game to measure trustworthiness priors in AI models.

Result: GPT-4.1's trustworthiness closely aligned with human behaviors in trust scenarios. It differentiated trust based on agent characteristics, confirming parallels to stereotype-based dimensions of warmth and competence.

Conclusion: The findings provide a foundation for understanding AI trustworthiness and pave the way for designing systems that align more closely with human trust and behavioral dynamics.

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [206] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: The paper analyzes how large language models develop reasoning abilities during training, focusing on internal state changes rather than output alone.


<details>
  <summary>Details</summary>
Motivation: To understand how reasoning capabilities evolve during training of large language models, which remain opaque when only investigating generated outputs.

Method: Researchers adopt a representational perspective to analyze internal state dynamics across various training stages through experiments and statistical analysis.

Result: Post-training has limited impact on initial representation quality, but reasoning tasks involve significant shifts in representation during generation. Correlation exists between correctness and final states, and generated token semantics, rather than inference computation or intrinsic parameters, drive representation transition.

Conclusion: The study offers deeper insights into the reasoning process dynamics and highlights how training enhances reasoning skills, which is valuable for optimizing future models.

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [207] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: HyLRA introduces a novel attention mechanism to improve computational efficiency in LLMs by leveraging layer-wise attention profiling and reusing key tokens, reducing computational costs without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and memory challenges posed by quadratic attention computation in Large Language Models, especially when existing sparse attention mechanisms fail to balance efficiency with accuracy optimally.

Method: HyLRA employs hybrid layer reuse attention that utilizes layer-wise sparsity profiling. It identifies layers requiring full attention due to sensitivity and reuses indices in tolerant layers using a dynamic programming approach to reduce computation while preserving critical tokens.

Result: HyLRA enhances the inference throughput of LLMs by 6%--46%, with less than 1% accuracy degradation. It consistently outperforms existing state-of-the-art sparse attention methods.

Conclusion: HyLRA effectively addresses inefficiencies in dense attention computations and provides a practical, efficient, and open-source framework for optimizing LLM performance while maintaining accuracy.

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [208] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: This paper investigates the use of Multimodal LLMs (MLLMs) for understanding source code by converting it into images, achieving significant compression while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: As software systems scale, current text-based source code representation for LLMs becomes computationally inefficient. The motivation is to explore a more efficient image-based modality for code representation, taking advantage of compression in images that retains semantic meaning.

Method: The study systematically converts source code into rendered images for analysis by MLLMs. It uses visual cues like syntax highlighting and tests the code understanding performance of MLLMs under various compression rates.

Result: Experiments show that MLLMs achieve up to 8x compression with effective code understanding, provide improved code completion at 4x compression using visual cues, and demonstrate resilience in tasks like clone detection. In some cases, visual compression even outperforms raw text inputs.

Conclusion: The findings suggest that representing source code in an image modality offers an efficient alternative to text, emphasizing the potential of MLLMs for computational efficiency in code understanding while acknowledging some current limitations.

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [209] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: This paper introduces Omni-RRM, an open-source rubric-grounded reward model for multimodal evaluations, improving performances across text, image, video, and audio processing.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) face challenges due to limited reward model effectiveness, which restricts alignment strategies and relies heavily on human annotations.

Method: The authors developed Omni-RRM using Omni-Preference, a dataset generated through automated pipelines without human labeling and trained the model in two stages: supervised fine-tuning and reinforcement learning (GRPO).

Result: Omni-RRM achieves state-of-the-art accuracy in video and audio benchmarks, significantly outperforms existing open-source RMs in image-related tasks, and transfers well to text preferences.

Conclusion: Omni-RRM demonstrates substantial performance improvements across modalities and use cases, advancing alignment capabilities for MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [210] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) can balance informativeness and factuality in responses using a framework called Factuality-Controlled Generation (FCG), which is trained on synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off in LLMs' responses between informativeness and factual accuracy, allowing users to control the level of factuality in responses.

Method: Introduce a framework called Factuality-Controlled Generation (FCG) for adding factuality constraints during query responses and train models with synthetic data.

Result: Synthetic training improved the ability of models to maintain factuality requirements and keep outputs informative.

Conclusion: The FCG approach enables tailored response generation from LLMs by successfully balancing informativeness and factual accuracy, demonstrating practical advancements in model control.

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [211] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: The research seeks to consolidate the study of adversarial robustness across various model roles and highlights limitations in current adversarial training methods for text scoring models. New adversarial training techniques are proposed to improve both robustness and task efficacy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to unify and address fragmented approaches within adversarial robustness research in language models, focusing specifically on text scoring models.

Method: The authors design multiple adversarial training methods adapted for text scoring models and analyze their effectiveness across applications. They propose combining complementary methods to improve robustness and task performance.

Result: The proposed methods show enhanced adversarial robustness and task effectiveness. For RLHF, the models reduce reward hacking and help align language model training.

Conclusion: Combining adversarial training techniques not only strengthens robustness but also improves overall performance and supports better alignment in LLMs.

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [212] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: The study addresses the redundancy problem in language model context and introduces RISE, which provides stable and robust explanations by measuring unique input influence.


<details>
  <summary>Details</summary>
Motivation: The need to distinguish essential context elements from correlated ones in large language model outputs while tackling redundancy and improving interpretability in critical applications.

Method: RISE (Redundancy-Insensitive Scoring of Explanation) assesses the distinct influence of each input, mitigating redundancies and delivering clearer attributions.

Result: Experiments validated that RISE achieves more stable and robust explanations compared to traditional methods.

Conclusion: RISE emphasizes the significance of conditional information for trustworthy explanations and better monitoring of language model outputs.

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [213] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: This paper introduces ILSIC, a new dataset for legal statute identification combining layperson queries and court judgements. It highlights the differences between the two data types.


<details>
  <summary>Details</summary>
Motivation: Legal statute identification is crucial in Legal NLP, especially for layperson queries that are informal and non-professional. The current focus is on court judgements, leaving a gap for practical layperson input.

Method: The authors created ILSIC, a comprehensive dataset with layperson queries alongside court judgements, and carried out experiments using transfer learning, retrieval-augmented generation, zero/few-shot inference, and supervised fine-tuning.

Result: Experiments revealed that models trained on court judgement data perform poorly on layperson queries, but transfer learning from court to layperson data can improve performance depending on scenarios.

Conclusion: ILSIC enables comparative research between court and layperson queries for legal statute identification, highlighting the importance of tailored approaches for informal layperson data.

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [214] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: Proposes RPG-Encoder, advancing repository analysis by unifying comprehension and generation processes with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from fragmented representations in repository understanding, leading to a reasoning disconnect.

Method: RPG-Encoder uses semantic-rich encoding, incremental topology evolution, and unified navigation for repository analysis.

Result: Achieved state-of-the-art repository understanding with 93.7% accuracy on SWE-bench Verified and high reconstruction coverage (98.5%) on RepoCraft.

Conclusion: RPG-Encoder significantly enhances repository comprehension and generation, reducing costs and improving fine-grained accuracy.

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [215] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: This paper explores the use of Sequential Monte Carlo (SMC) for inference in large language models, emphasizing how approximate reward models impact efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effective inference-time scaling in large language models when true reward models are unavailable.

Method: The authors theoretically analyze the role of approximate reward models in Sequential Monte Carlo (SMC)-based inference, focusing on the Bellman error as a key metric and its relation to computational complexity.

Result: When the Bellman error of the reward model is bounded by O(1/T) for reasoning processes of length T, SMC achieves an exponential improvement in inference efficiency, reducing complexity from exponential to polynomial.

Conclusion: Approximate reward models can be sufficient for SMC-based inference-time scaling, provided their Bellman error is well-controlled, enabling significant computational improvements.

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [216] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: effGen is an open-source framework optimized for small language models, addressing token cost, privacy, and efficiency concerns with local deployment. It outperforms existing systems like LangChain and AutoGen on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of agentic systems built for large language models, such as high costs and privacy concerns, by introducing a framework optimized for small language models.

Method: effGen introduces four contributions: improved tool-calling with prompt compression, task decomposition for query simplification, complexity-based routing for smart decisions, and unified memory across different protocols.

Result: The framework achieves higher success rates, faster execution, and lower memory usage compared to LangChain, AutoGen, and Smolagents. It demonstrates complementary scaling behaviors for prompt optimization and complexity routing across language model sizes.

Conclusion: effGen is a highly effective, efficient, and secure framework for local deployment of small language models, offering significant performance improvements while ensuring accessibility for research and commercial use.

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [217] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: Large language models face challenges in agriculture and low-resource languages. AgriHubi, adapted for Finnish agriculture, improves decision support using domain-specific documents and iterative feedback.


<details>
  <summary>Details</summary>
Motivation: Address challenges of weak grounding and limited real-world evaluation in large language models, particularly for low-resource languages like Finnish.

Method: Developed AgriHubi, a domain-adapted retrieval-augmented generation system leveraging agriculture-specific Finnish documents, iterative updates, and user feedback.

Result: AgriHubi showed improved answer completeness, linguistic accuracy, and reliability, with insights on trade-offs between quality and latency during deployment.

Conclusion: Empirical outcomes guide the design and evaluation of effective RAG systems for domain-specific use in low-resource languages.

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [218] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: The paper investigates if higher-order (HO) categories in sentence-level human value detection bring practical advantages, finding that while learnable, enforcing hierarchy with hard gating isn't consistently beneficial. Calibration and lightweight ensembling improve performance most.


<details>
  <summary>Details</summary>
Motivation: To determine whether Schwartz HO categories provide meaningful structure for sentence-level value detection and whether leveraging this hierarchy can enhance predictive performance within a compute-efficient framework.

Method: The study evaluates three approaches: direct supervised transformer models, HO→values pipelines with hard masks to enforce hierarchy, and cascaded presence→HO→values methods. It also examines low-cost improvements (lexica, context inclusion, threshold tuning), small instruction-tuned models, QLoRA, and simple ensembles on English sentences from ValueEval'24/ValuesML.

Result: HO categories were found to be learnable at the sentence level, but enforcing their hierarchy reduced Macro-F1 performance in many cases due to error compounding. Label-wise threshold tuning and lightweight transformer ensembling yielded consistent performance gains (+0.05 and +0.02 Macro-F1, respectively).

Conclusion: While Schwartz HO categories can descriptively categorize human values, enforcing the hierarchy via hard gating compromises performance. Robust improvements stem from model calibration techniques and lightweight model ensembles rather than structural hierarchy enforcement.

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [219] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: A lightweight multimodal baseline for emotion recognition is introduced using the SemEval-2024 Task 3 dataset.


<details>
  <summary>Details</summary>
Motivation: Provide an accessible baseline for emotion recognition in conversations using multimodal data.

Method: Combination of transformer-based text classifier and self-supervised speech representation model with late-fusion ensemble.

Result: Empirical results demonstrate when multimodal fusion outperforms unimodal models under limited training conditions.

Conclusion: Serves as a transparent reference implementation to enable future and more detailed comparisons.

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [220] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: This paper proposes Neural FOXP2, a method for making a specific language primary in language models by steering language-specific neurons.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the dominance of the English language in large language models, despite their multilingual training, and to enable controllable language prioritization.

Method: The method involves three stages: (i) Localizing language-specific neurons, (ii) Identifying low-rank steering directions through spectral analysis, (iii) Adjusting activations to prioritize target language defaultness while suppressing English dominance.

Result: Neural FOXP2 successfully steers multilingual models to prioritize a chosen target language, such as Hindi or Spanish, using compact neuron sets and fine-grained activation shifts.

Conclusion: The findings demonstrate that language-specific behaviors in language models are governable via sparse, low-rank control mechanisms, making it possible to safely and effectively alter language prioritization.

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [221] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: The paper introduces MixTalk, a strategic communication game for LLM-to-LLM interactions, that involves verifiable and unverifiable information combined to test reasoning about credibility. It also proposes TOPD, a method to improve communication robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand and model strategic communication, especially in high-stakes scenarios, where both verifiable and probabilistic credibility coexist, addressing gaps in prior work that oversimplified communication models.

Method: The study designed MixTalk, a communication game with LLM agents interacting through strategic combinations of verifiable and unverifiable claims and evaluating their interactions via tournaments. Additionally, they developed TOPD, a policy distillation method based on tournament logs.

Result: Through large-scale tournaments in three settings, the study revealed LLM agents' strengths and limitations in handling probabilistic credibility. TOPD significantly improved the receiver agent's resistance to persuasive communication.

Conclusion: MixTalk and TOPD advance understanding of strategic communication in high-stakes scenarios with probabilistic information. The research highlights areas of LLM performance requiring improvement and practical solutions to enhance robustness.

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [222] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: This paper introduces 'Structural Confidence', a framework for confidence estimation in large language models using multi-scale structural signals from hidden-state trajectories, achieving strong performance on various tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models are used in critical applications where errors have high costs, but existing confidence estimators are unreliable under certain conditions. There is a need for a more reliable and efficient confidence estimation framework.

Method: The authors propose 'Structural Confidence', a model-agnostic, single-pass framework that leverages spectral, local-variation, and global shape descriptors in hidden-state trajectories to enhance output correctness prediction.

Result: The proposed method achieves strong performance across multiple benchmarks (FEVER, SciFact, WikiBio-hallucination, TruthfulQA) in AUROC and AUPR metrics, outperforming standard confidence estimation baselines.

Conclusion: 'Structural Confidence' provides an efficient, robust way to estimate confidence in LLM outputs without requiring multiple stochastic runs, making it highly practical for resource-constrained, high-stakes applications.

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [223] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: MedSpeak enhances medical question-answering systems by introducing a framework that corrects ASR errors using medical knowledge graphs and LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Current SQA systems often fail to correctly recognize complex medical terms due to limitations in ASR capabilities.

Method: MedSpeak integrates semantic relationships, phonetic information, and reasoning from large language models (LLMs) using a medical knowledge graph to refine ASR transcripts and improve answer prediction.

Result: Experimental benchmarks highlight significant improvements in recognizing medical terms and overall performance of medical SQA systems.

Conclusion: MedSpeak establishes itself as a state-of-the-art system for medical spoken question-answering, solving prevalent ASR issues and improving accuracy.

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [224] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: The paper introduces DISPO, a REINFORCE-style algorithm addressing limitations of existing training methods for reinforcement learning in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the reasoning capabilities of large language models in mathematical tasks by addressing trade-offs and limitations in current reinforcement learning methods.

Method: DISPO is presented as a new REINFORCE-style algorithm that decouples the clipping of importance sampling weights for correct and incorrect responses, offering four tunable update regimes to balance exploration and stability.

Result: DISPO achieves 61.04% on AIME'24, outperforming CISPO (55.42%) and DAPO (50.21%), and shows performance improvements across various benchmarks.

Conclusion: DISPO successfully balances exploration and stability, avoiding catastrophic failures commonly observed in existing methods, and demonstrates better mathematical reasoning capabilities in large language models.

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [225] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: The paper identifies a reward subsystem in Large Language Models (LLMs) akin to the human brain's reward system, highlighting its significance for reasoning and analyzing its robustness and transferability.


<details>
  <summary>Details</summary>
Motivation: To explore the internal structures of LLMs to better understand their reasoning capabilities by drawing parallels with biological reward mechanisms in humans.

Method: Using intervention and analysis experiments, researchers identified and investigated value neurons and dopamine neurons in LLMs, assessing their role, robustness, and transferability.

Result: They found a sparse reward subsystem in LLMs with value neurons for state value expectation and dopamine neurons handling reward prediction errors, showing high consistency across datasets, scales, and architectures.

Conclusion: The identified reward subsystem is fundamental for reasoning in LLMs, with robust and transferable features that resemble the brain's reward mechanisms.

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [226] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: Introduces DeALOG, a decentralized multi-agent system for multimodal question answering using specialized agents communicating via a shared natural-language log.


<details>
  <summary>Details</summary>
Motivation: To improve robustness and interpretability in multimodal question answering by leveraging specialized processing and decentralized coordination.

Method: Developed Decentralized Multi-Agent Framework named DeALOG, integrating specialized agents (Table, Context, Visual, etc.) that communicate with a shared natural-language log.

Result: Achieved competitive performance across six datasets (FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, MultiModalQA), demonstrating robustness and accuracy.

Conclusion: DeALOG demonstrates the potential of modular, natural-language-based communication in enhancing scalability, error detection, and verification in multimodal question answering.

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [227] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: The paper focuses on enhancing large language models' ability to appropriately apply lemmas by introducing a structured prediction task and a reinforcement learning framework.


<details>
  <summary>Details</summary>
Motivation: Large language models excel at mathematical benchmarks but often misapply lemmas by ignoring their required assumptions, leading to incorrect conclusions.

Method: The authors introduce RULES, a structured output training method using two-section outputs that separately evaluate assumptions and conclusions. It incorporates reinforcement learning with section-aware loss masking to penalize specific sections responsible for errors.

Result: RULES achieves consistent in-domain improvements, significant gains in handling perturbation-breaking applicability, and modest gains or parity in end-to-end problem performance. Ablation studies highlight that their approach's key components are crucial for these results.

Conclusion: The proposed structured training approach enhances robustness and correctness in lemma application within large language models, improving their utility in reasoning tasks.

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [228] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: Efficiently converts token-based language models into byte-based ones without retraining from scratch, using distillation.


<details>
  <summary>Details</summary>
Motivation: The study addresses the high cost of training Byte Language Models (BLMs) from scratch on trillions of bytes.

Method: It proposes a two-stage distillation process: 1) aligning byte representations progressively with token-based models, and 2) fine-tuning byte models for generation.

Result: The study validates the efficiency using models like Llama and Qwen, achieving byte-based performance similar to token-trained models with significantly fewer training bytes (~125B bytes).

Conclusion: The proposed distillation method is a cost-effective solution for converting pre-trained token-based LLMs into BLMs while preserving their performance.

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [229] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: Large language models (LLMs), such as GPT-4.1, are evaluated for modeling novice reasoning in chemistry tutoring. While fluent, their reasoning is overly coherent and less human-like, overestimating learner performance.


<details>
  <summary>Details</summary>
Motivation: Investigate if LLMs can simulate fragmented and imperfect novice reasoning and metacognitive judgments typically seen in human learning.

Method: Assessment of LLM performance based on 630 think-aloud utterances from multi-step chemistry problems, comparing it against human learner utterances and analyzing its predictive accuracy for novice success.

Result: GPT-4.1 demonstrated fluent, context-appropriate reasoning but was overly coherent, verbose, and lacked variability, overestimating learner success.

Conclusion: LLMs face epistemic challenges in simulating human learning due to training data bias towards expert-like problem-solving. This suggests the need for improved modeling approaches in adaptive AI systems for novice learning.

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [230] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: This paper investigates speech bias in multilingual MLLMs by creating the BiasInEar dataset. It evaluates model biases using various linguistic and demographic factors, revealing a sensitivity to language and structural bias.


<details>
  <summary>Details</summary>
Motivation: The research aims to explore speech bias in multilingual MLLMs and provide a systematic way to evaluate fairness and robustness in speech-integrated models.

Method: The authors constructed the BiasInEar dataset, a multilingual benchmark with speech data balanced by gender and accent. They evaluated nine models using metrics such as accuracy, entropy, APES, and Fleiss' κ under various perturbations.

Result: The study found that MLLMs show robustness to demographic factors but are sensitive to language and structural perturbations, implying speech can enhance structural biases.

Conclusion: The study provides a framework for evaluating fairness and robustness in speech-augmented MLLMs, advancing understanding of biases in such models.

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [231] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: The paper discusses how Large Language Models (LLMs) express personality traits differently based on conversational context and examines whether these variations denote inconsistency or adaptability.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate how identical personality prompts lead to different outcomes in LLM behavior across different conversational settings, addressing questions of consistency versus context-sensitive adaptation.

Method: The authors analyze the effect of contextual cues on personality prompting in LLMs across four conversational scenarios: ice-breaking, negotiation, group decision-making, and empathy tasks.

Result: The results indicate that contextual cues significantly shape the expression of personality traits and emotional tone, showing that LLMs adapt their behavior according to the situational demands.

Conclusion: The study concludes that LLMs exhibit context-sensitive personality expressions, aligning more closely with human-like behavioral adaptation rather than displaying a fixed personality trait, as framed through Whole Trait Theory.

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [232] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: Knowledge distillation faces challenges when using multiple teacher models due to conflicts and resource demands. This paper proposes Knowledge Purification for consolidating rationale from multiple teachers to enhance efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional distillation methods struggle with knowledge conflicts and require high computational resources, especially with multi-teacher setups.

Method: The paper presents Knowledge Purification concept and introduces five purification methods to resolve conflicts and consolidate knowledge rationales effectively.

Result: Experimental results show improved distilled model performance and reduced knowledge conflicts. Router-based approaches displayed strong generalization.

Conclusion: Knowledge Purification optimizes multi-teacher distillation approaches, facilitating efficient deployment of smaller models capable of leveraging LLM capabilities.

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [233] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: This paper investigates the limitations of LLMs in domain-specific translations, focusing on subtitle translation, and proposes the ALPO method and a dataset to improve translation quality.


<details>
  <summary>Details</summary>
Motivation: Explore how to address the limitations of LLMs in performing vertical domain translations, especially in domains requiring expressive and context-specific language, like visual media subtitle translation.

Method: Assembling a directed subtitle parallel corpus, applying the Adaptive Local Preference Optimization (ALPO) method, and utilizing LLMs for nuanced translation evaluation and reward alignment.

Result: ALPO demonstrated superior translation quality across multiple dimensions, supported by the newly created and released subtitle corpus.

Conclusion: ALPO and the introduced dataset effectively enhance the customization and performance of LLMs in domain-specific translations, with specific success in expressive subtitled translation tasks.

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [234] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: A novel verifier-guided adaptive framework for reasoning tasks achieves improved performance and efficiency compared to test-time compute scaling.


<details>
  <summary>Details</summary>
Motivation: Current inference methods often apply fixed computation strategies with limited adaptability and efficiency.

Method: The framework employs multiple inference iterations, guided by a Process Reward Model (PRM) for dynamic planning, trajectory generation, pruning, and selection.

Result: Achieved significant performance improvements in datasets like MATH-500, AIME24, and AMO-Bench benchmarks while optimizing computation allocation.

Conclusion: Verifier-guided adaptive reasoning enhances efficiency and accuracy by concentrating resources on valuable computation paths.

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [235] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: LORE is a method that improves the retrieval ability of large language models (LLMs) by activating their logical reasoning capacity through fine-grained contrastive learning. It overcomes issues of surface similarity and enhances downstream performance without external supervision.


<details>
  <summary>Details</summary>
Motivation: To address the problem where large language models struggle with knowledge-intensive tasks due to retrievers overfitting to surface similarities, thereby failing to handle complex logical queries.

Method: LORE uses fine-grained contrastive learning to enhance embeddings that align with logical structures instead of shallow similarities. It requires no external supervision or supplementary resources and remains compatible with existing retrieval indexes.

Result: LORE consistently improves retrieval utility and downstream generation performance without sacrificing efficiency or needing pre-retrieval analysis.

Conclusion: LORE offers a practical and efficient way to enhance logical reasoning in LLMs, resolving retrieval issues stemming from shallow similarity, and ensuring improved performance on complex queries. Its datasets and code are publicly accessible.

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [236] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: The paper introduces Tendem, a hybrid system combining AI and human expertise, and demonstrates its advantages over AI-only or human-only workflows.


<details>
  <summary>Details</summary>
Motivation: To create a system utilizing both AI and human expertise for improved task performance and quality.

Method: Compared Tendem's performance against AI-only and human-only workflows (94 tasks) and evaluated its AI agent on third-party benchmarks.

Result: Tendem delivered higher-quality outputs with faster turnaround times, while its AI agent performed near state-of-the-art on certain benchmarks.

Conclusion: Tendem proves to be an effective hybrid system, balancing cost-efficiency, quality, and operational speed through AI and human collaboration.

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [237] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: The paper presents a novel framework extending TPPs with LLMs to the visual modality for modeling multimodal events, addressing the challenge of long sequences via adaptive sequence compression.


<details>
  <summary>Details</summary>
Motivation: Existing TPPs struggle with generating rich, multimodal content and maintaining coherence in long textual outputs due to their limited handling of extended sequences.

Method: The authors propose adaptive sequence compression based on temporal similarity to manage long contexts while preserving patterns, complemented by a two-stage training paradigm: pre-training on compressed sequences and supervised fine-tuning.

Result: Experiments show that the proposed approach achieves superior predictive accuracy and textual analysis quality compared to state-of-the-art baselines, validated on benchmarks like DanmakuTPP-QA.

Conclusion: The framework significantly improves multimodal event modeling, proving effective for handling long-range dependencies and producing high-quality textual outputs.

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [238] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: This paper introduces Logifus, a novel logical obfuscation framework, and the LogiQAte benchmark to test how logical obfuscation affects reasoning tasks, finding that LLMs perform significantly worse in obfuscated scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess and address the vulnerability of LLMs to logical obfuscation where logical equivalence is presented in challenging or transformed formats.

Method: The authors developed the Logifus obfuscation framework and created LogiQAte, a benchmark with 1,108 questions covering four diverse reasoning tasks. They evaluated six state-of-the-art models under zero-shot conditions.

Result: Obfuscation drastically reduces LLM performance, with GPT-4o experiencing a 47% performance drop, GPT-5 a 27% drop, and o4-mini a 22% drop across all tasks.

Conclusion: Current large language models process questions superficially without achieving deep logical understanding, signaling a need for developing models that can grasp and interpret meaning beyond surface-level representations.

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [239] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: This paper investigates the linguistic properties of fine-tuning datasets for cultural adaptation in large language models (LLMs) and their impact on performance. It employs linguistic metrics and principal component analysis to study dataset variations across Arabic, Chinese, and Japanese.


<details>
  <summary>Details</summary>
Motivation: There is a gap in understanding how the linguistic properties of datasets influence cultural adaptation in LLMs, amidst concerns regarding cultural misalignment of globally deployed models.

Method: The authors conduct principal component analysis on lightweight linguistic, semantic, and structural metrics of datasets in Arabic, Chinese, and Japanese, followed by fine-tuning major LLM families and evaluating them on cultural knowledge benchmarks.

Result: The findings reveal interpretable axes of dataset properties, with lexical-oriented components being most robust for consistent model performance across benchmarks, while extreme semantic or diversity components are less effective.

Conclusion: Dataset linguistic properties can shape cultural alignment in LLMs, but their impact varies by model type, showing that some properties are more predictive of cultural performance than others.

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [240] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: This paper presents a framework to improve translation quality into low-resource languages without retraining models or requiring parallel data, leveraging linguistic typology.


<details>
  <summary>Details</summary>
Motivation: To address translation biases in large language models that favor dominant typological patterns over low-resource languages, causing structural non-conformance.

Method: A framework using the Universal Metalinguistic Framework (UMF) with typological profiling across 16 dimensions and a computational engine for linguistic disambiguation and typological compliance scoring during generation.

Result: Evaluation on nine language pairs shows typological intervention rates correlate with distance to English, with varying precision across morphological and syntactic phenomena.

Conclusion: The framework improves translations for low-resource languages and can work with any large language model capable of generating multiple outputs, requiring no parallel data and eliminating retraining.

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [241] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: The paper introduces PedagoSense, a system for detecting and recommending effective teaching strategies using a two-stage classifier and large language model generation in tutor-student dialogues.


<details>
  <summary>Details</summary>
Motivation: To enhance interaction quality in dialogue-based learning by identifying and improving teaching strategies used in tutor-student conversations.

Method: PedagoSense employs a binary classifier to detect pedagogical strategies, performs detailed classification to identify specific strategies, and utilizes a large language model for response recommendation and generation.

Result: The evaluation on annotated tutor-student dialogues demonstrates high accuracy in strategy detection, improved performance with data augmentation, and highlights areas where fine-grained classification remains difficult.

Conclusion: PedagoSense successfully integrates pedagogical theory with LLM-based responses, paving the way for more adaptive and effective educational technologies.

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [242] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

TL;DR: EmoAra delivers end-to-end emotional nuance preservation in cross-lingual communication, optimized for banking services.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to preserve emotional context in cross-lingual spoken communication, particularly for improving banking customer service quality.

Method: EmoAra utilizes a CNN-based emotion classifier, Whisper for ASR, MarianMT for translation, and MMS-TTS-Ara for speech synthesis to process English inputs and deliver Arabic outputs.

Result: Achieved F1-score of 94% for emotion recognition, BLEU score of 56, BERTScore F1 of 88.7%, and 81% human evaluation score in banking-domain translations.

Conclusion: The proposed system successfully preserves emotional nuances in cross-lingual communication, showcasing high performance and practical implementation.

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [243] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: This paper reviews Visual Word Sense Disambiguation (VWSD), emphasizing its evolution toward using multimodal techniques like CLIP, diffusion, and LLMs for improved lexical disambiguation.


<details>
  <summary>Details</summary>
Motivation: To explore advances in VWSD as a solution to lexical ambiguity in vision-language tasks by leveraging visual and multimodal methods.

Method: The review investigates VWSD's development from early methods to state-of-the-art techniques like CLIP-based models, diffusion text-to-image generation, and LLM support, covering 2016-2025 studies.

Result: Fine-tuned CLIP and LLM-supported VWSD systems show 6-8% gains in MRR over zero-shot baselines, but challenges in context handling, model bias, multilingual datasets, and evaluation metrics remain.

Conclusion: The future of VWSD lies in blending advanced systems like CLIP, diffusion models, and LLMs to achieve robust, context-sensitive, and multilingual disambiguation solutions.

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [244] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: This paper explores the phenomenon of attention sink in Large Language Models (LLMs) and proposes a new sink-aware training algorithm to balance attention heads and enhance performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the issue of attention sink, which leads to disproportionate attention given to the first token and head collapse in LLMs, affecting their performance.

Method: A sink-aware training algorithm is proposed with an auxiliary load balancing loss for attention layers to mitigate head collapse and balance attention heads.

Result: The proposed method achieves effective head load balancing and enhances performance across Vanilla Attention, Sink Attention, and Gated Attention mechanisms.

Conclusion: The study introduces insights into the Mixture-of-Experts structure within attention mechanisms and lays the groundwork for improving attention layers in LLMs through better training techniques.

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [245] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: This paper introduces ASTER, a framework to overcome challenges in reinforcement learning for tool-integrated reasoning in language models, achieving state-of-the-art results with a small high-quality dataset.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the problem of 'interaction collapse' in Tool-Integrated Reasoning (TIR) using reinforcement learning (RL), which causes failure in performing multi-turn tool usage.

Method: They studied key factors influencing RL training: cold-start behavioral priors, effects of interaction density, and RL budgets. They proposed ASTER, emphasizing interaction-dense cold-start data (4K trajectories) to build effective behavioral priors for extended training.

Result: ASTER-4B achieved a significant milestone by scoring 90% accuracy on AIME 2025, surpassing other advanced RL-based tool-integrated frameworks like DeepSeek-V3.2-Exp.

Conclusion: A robust but small dataset (interaction-dense) enables superior tool integration and reasoning performance in RL frameworks, with ASTER demonstrating enhanced exploration and generalization capabilities.

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [246] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: Chronos introduces a chronological reasoning scorer to improve reasoning performance in LLMs by modeling reasoning trajectories as time series.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods treat all reasoning traces or tokens equally, leading to inconsistent trajectory quality and logical errors.

Method: Chronos models reasoning trajectories as time series, analyzing token probabilities to assign quality scores and employs weighted voting.

Result: Chronos achieves substantial performance improvements, such as 34.21% over Pass@1 and 22.70% over Maj@128 on HMMT25 benchmark.

Conclusion: Chronos is effective and computationally efficient for enhancing LLM reasoning across various benchmarks and models.

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [247] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: This paper addresses the granularity mismatch in achieving human utility in AI and proposes Token Priority as a solution for better alignment in supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the difficulty of aligning fine-grained autoregressive model behavior with coarse or uniform supervision signals, creating a mismatch that limits achieving optimal human utility.

Method: The authors formalize Supervised Fine-Tuning as a distribution reshaping process oriented around Token Priority, classifying breakthroughs into Positive Priority for noise reduction and Signed Priority for unlearning toxic modes.

Result: The analysis categorizes current advances into defined priority regimes, offering a unified framework to evaluate existing methods' strengths and weaknesses.

Conclusion: Token Priority is positioned as a crucial strategy for aligning raw data with an ideal alignment manifold, with the paper identifying challenges and suggesting research directions to advance the alignment field.

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [248] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: Introduced Inferential QA, focusing on deriving answers from indirect clues rather than directly stated information.


<details>
  <summary>Details</summary>
Motivation: Current QA systems lack capabilities to handle questions requiring inference based on indirect textual evidence.

Method: Developed the QUIT dataset with 7,401 questions and evaluated various retrievers, rerankers, and LLM-based readers for inferential reasoning.

Result: Traditional QA models underperform in inferential tasks, with reasoning-oriented LLMs failing to outperform general-purpose models.

Conclusion: QA systems need advancements to handle inferential reasoning tasks effectively, marking a new challenge for QA research.

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [249] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: The paper introduces DetectRouter, a system for selecting optimal surrogates for detecting AI-generated texts, improving detection reliability.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot detection methods lack flexibility, often using a fixed surrogate model, which causes variability in detection performance due to mismatched source-surrogate alignment.

Method: The authors propose DetectRouter, a framework using two-stage training: first creating prototypes from white-box models, then aligning detection scores geometrically to cover black-box sources.

Result: Experiments on benchmarks EvoBench and MAGE show that DetectRouter provides consistent detection improvements across criteria and language model families.

Conclusion: Proper surrogate-selection using DetectRouter enhances robust detection of AI-generated text, converting a challenging detection task into a routing problem.

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [250] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: This paper introduces TerminalTraj, a scalable pipeline for generating high-quality terminal task training data, enabling the creation of agent models that perform significantly better on terminal-based benchmarks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges in training agent models for terminal-based tasks, specifically the issues of executability (requiring Docker environments) and verifiability (standardized validation of diverse outputs).

Method: They developed TerminalTraj, a system that filters high-quality repositories to build Dockerized environments, aligns tasks to these environments, and creates agent trajectories with executable validation code.

Result: Using this pipeline, the authors curated 32K Docker images and produced over 50K verified terminal trajectories across eight domains. Models trained on this data showed up to 20% improvement on benchmarks and set competitive results with fewer than 100B parameters.

Conclusion: TerminalTraj offers a scalable and effective solution for creating high-quality training data for terminal-based tasks, resulting in significant performance advancements across various domains.

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [251] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: This paper presents PARSE, the first open-domain Persian reasoning QA benchmark with 10,800 questions across various formats to evaluate reasoning-capable LLMs in Persian.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive reasoning-focused QA benchmark for low-resource languages, particularly Persian, which is spoken by 130 million people.

Method: A controlled pipeline using large language models (LLMs) was used to generate and validate 10,800 diverse questions in Persian. Linguistic and factual quality were rigorously checked.

Result: Persian prompts and structured prompting significantly improved LLM performance, with fine-tuning offering further gains for Persian-specialized models.

Conclusion: PARSE fills a gap in evaluating reasoning-capable LLMs for Persian, enabling fair comparison and practical adaptation in low-resource language settings.

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [252] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: Pacer improves large language model decoding speed with a dynamic draft length and achieves up to 3.09x speedup.


<details>
  <summary>Details</summary>
Motivation: The fixed draft length in speculative decoding limits its potential to further accelerate large language model inference.

Method: Introduce Pacer, which uses pre-verification layers to dynamically control draft lengths during speculative decoding.

Result: Pacer achieves up to 2.66x speedup over autoregressive decoding, and up to 3.09x when combined with Ouroboros.

Conclusion: Dynamic draft length optimization significantly enhances decoding speed and consistently outperforms traditional speculative decoding approaches.

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [253] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: This paper introduces EverMemBench, a benchmark for long-term conversational memory in complex dialogue scenarios, identifying key limitations in current memory systems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inadequacy of existing benchmarks that fail to emulate real-world multi-topic, multi-party conversations, hindering the development of memory systems for LLM-based assistants.

Method: The authors developed EverMemBench, consisting of multi-topic conversational data spanning over 1 million tokens and evaluated memory systems via over 1,000 QA pairs based on recall, memory awareness, and profile understanding.

Result: The evaluation revealed issues, such as poor performance in multi-hop reasoning (26% success rate for oracle models), unresolved temporal reasoning, and retrieval limitations due to semantic gaps.

Conclusion: EverMemBench highlights critical memory system challenges and serves as a valuable platform for advancing next-gen architectures capable of handling real-world conversational complexities.

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [254] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: The paper introduces DreamOn, a novel framework that enhances Diffusion Language Models (DLMs) for dynamic, variable-length generation, addressing limitations related to fixed-length masked sequences in code infilling.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of fixed-length masked sequence generation in DLMs, which hampers their practical utility and degrades performance when predefined mask sizes do not match ideal completion lengths.

Method: DreamOn introduces a diffusion framework with two length control states, allowing models to dynamically adjust output length. It integrates seamlessly with existing DLMs with minimal training modifications and no architectural changes.

Result: DreamOn, implemented on Dream-Coder-7B and DiffuCoder-7B, achieves performance comparable to state-of-the-art autoregressive models on benchmarks like HumanEval-Infilling and SantaCoder-FIM, demonstrating competitive results with length flexibility.

Conclusion: DreamOn resolves a fundamental limitation of DLMs, enabling practical deployment and enhancing flexibility for variable-length generation, thus advancing the applicability of DLMs in real-world tasks.

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [255] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: The paper addresses challenges in reasoning accuracy and structure control in multi-hop QA systems, proposing a framework, CRAFT, to improve faithfulness and systematic analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve reliable reasoning in multi-hop QA tasks, addressing issues like reasoning collapse, answer inconsistency, and loss of format control experienced by current systems.

Method: They propose CRAFT, which employs a Group Relative Policy Optimization (GRPO) reinforcement learning framework. It incorporates dual reward mechanisms—deterministic rewards for structural correctness and judge-based rewards for semantic faithfulness.

Result: CRAFT demonstrated improved answer accuracy and reasoning faithfulness on three multi-hop QA benchmarks. The CRAFT 7B model achieved performance competitive with closed-source Large Language Models.

Conclusion: CRAFT effectively addresses multi-hop QA challenges, ensuring structured and faithful reasoning while allowing comprehensive analysis of reasoning performance.

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [256] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: The paper introduces XDLM, a model merging strengths of Masked Diffusion Language Models (MDLM) and Uniform-noise Diffusion Language Models (UDLM), achieving superior performance in both understanding and generating tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of MDLM and UDLM, which lack balanced performance in semantic understanding/generalization and few-step generation quality, respectively.

Method: Proposes XDLM with a stationary noise kernel to unify MDLM and UDLM paradigms, including a theoretical framework and memory optimization via algebraic simplifications.

Result: XDLM excels in zero-shot text benchmarks (5.4 points higher than UDLM), image generation (improving FID from 80.8 to 54.1), and language model tuning (doubling baseline performance with 15.0 MBPP).

Conclusion: XDLM outperforms prior models by unifying strengths of MDLM and UDLM, proving effective across tasks, and demonstrates scalable potential for improved performance in generative modeling.

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [257] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: This paper investigates selective knowledge distillation (KD) in large language models (LLMs), proposing a new method called SE-KD that improves performance and efficiency through entropy-guided selection strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance knowledge distillation for large language models by addressing gaps in understanding the interplay of importance signals and selection policies, seeking to optimize selective supervision for better performance and efficiency.

Method: The authors disentangle and analyze selective KD across position, class, and sample axes and propose SE-KD, a student-entropy-guided position selection method. They extend this method to SE-KD 3X for class and sample axes to optimize efficiency.

Result: The proposed SE-KD improves accuracy, downstream task adherence, and memory efficiency. SE-KD 3X achieves additional efficiency gains, reducing processing time by 70%, memory usage by 18%, and storage by 80% without losing performance.

Conclusion: Selective knowledge distillation strategies like SE-KD and SE-KD 3X show notable improvements in both efficiency and performance, presenting a more practical and resource-effective approach to KD in LLMs.

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [258] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: This paper discusses the continued relevance of high-level synthesis (HLS) in agentic AI hardware design, highlighting its role in enabling agentic optimization and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether high-level synthesis (HLS) is still relevant in the context of AI-driven hardware optimization and its benefits in enhancing agentic hardware systems.

Method: The authors analyze the role of HLS, identify its current limitations, and propose a taxonomy demonstrating how AI agents can evolve into autonomous hardware design partners.

Result: The study highlights HLS as an integral abstraction layer, identifies areas like performance feedback and debuggability as weaknesses in existing HLS tools, and outlines a framework for collaboration between humans and AI.

Conclusion: HLS remains important for agentic hardware design, and its evolution with AI involvement could shift more responsibilities to autonomous AI systems, supporting innovation.

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [259] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: SentiFuse is a model-agnostic framework for integrating heterogeneous sentiment analysis models and systematically leveraging their strengths through various fusion strategies.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis approaches lack a unified framework to integrate diverse models effectively.

Method: Developed SentiFuse, a framework integrating heterogeneous models through a standardization layer with decision-level, feature-level, and adaptive fusion strategies.

Result: SentiFuse outperformed individual models and naive ensembles across three datasets, achieving up to 4% F1 score improvement and robustness in challenging sentiment cases.

Conclusion: The systematic integration of diverse sentiment analysis models enhances accuracy and reliability in sentiment assessment across various text types.

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [260] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: The paper addresses challenges in Bangla QA models due to limited data and complexity, introduces a counterfactual Bangla QA dataset (BanglaCQA), and proposes methods for detecting parametric vs. contextual knowledge in these models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the inadequacy of tools for low-resource languages like Bangla by determining whether QA models rely on pre-encoded knowledge or contextual input, as current datasets lack the necessary structure for such analysis.

Method: The authors created BanglaCQA by expanding an existing dataset with counterfactual passages and annotations. They proposed fine-tuned language-specific and multilingual pipelines, as well as prompting-based pipelines for decoding tasks. Evaluations were performed using both LLM-based and human methods.

Result: The study found that QA models exhibit varied effectiveness based on the context used, and Chain-of-Thought prompting specifically enhanced parametric knowledge extraction in low-resource settings, especially for decoder-only LLMs.

Conclusion: A new analytical framework for Bangla QA is presented, contributing to understanding the interaction between parametric and contextual learning. This research establishes a foundation for counterfactual reasoning in low-resource scenarios and suggests broader future applications.

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [261] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: The study introduces ConPress, a method to enhance reasoning efficiency in large models by leveraging a self-compression phenomenon that reduces reasoning overhead.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of long reasoning processes typically generated by large reasoning models (LRMs) on reasoning-intensive tasks.

Method: The proposed ConPress technique uses a self-supervised fine-tuning method, leveraging the self-compression phenomenon where multiple questions in a prompt lead to shorter reasoning paths. It generates multi-question prompts, filters the concise reasoning traces, and fine-tunes the model on them.

Result: ConPress achieves a substantial reduction in reasoning token usage (59% on MATH500 and 33% on AIME25) while maintaining competitive reasoning accuracy.

Conclusion: ConPress effectively streamlines the reasoning process in LRMs, reducing inference overhead without accuracy loss, making reasoning models more efficient.

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [262] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: The paper introduces Ebisu, a benchmark for understanding Japanese financial language, featuring two expert-annotated tasks, and finds current models perform poorly, highlighting unresolved challenges.


<details>
  <summary>Details</summary>
Motivation: To address the complex challenges posed by the unique linguistic and cultural features of Japanese financial language for LLMs.

Method: The authors propose Ebisu, a benchmark with two tasks: JF-ICR for recognizing implicit communication and JF-TE for extracting financial hierarchies, and evaluate performance across a range of LLMs.

Result: State-of-the-art LLMs perform poorly on the benchmark, indicating limited improvements even with model scaling or adaptations.

Conclusion: Ebisu sheds light on the difficulties in Japanese financial NLP and poses as a focus for advancing research in this area, with all datasets and tools available for public use.

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [263] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: Rubric-ARM is a new framework to improve response quality assessment in non-verifiable domains through joint optimization of a rubric generator and a judge, using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current reward models inadequately assess multifaceted response quality in open-ended tasks, which motivates the need for a more comprehensive evaluation approach.

Method: Rubric-ARM integrates a rubric generator and a judge using preference-based reinforcement learning and employs an alternating optimization strategy for stable training.

Result: Rubric-ARM achieves state-of-the-art results, improving policy alignment across benchmarks in both offline and online reinforcement learning.

Conclusion: Rubric-ARM offers a more advanced method for evaluating complex tasks, enhancing judgment accuracy and downstream policy performance.

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [264] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: The paper introduces a new framework, AROA (Argument Rarity-based Originality Assessment), to evaluate originality in student essays based on the rarity of arguments, treating originality and quality as separate measures.


<details>
  <summary>Details</summary>
Motivation: Traditional writing assessments focused on quality lose relevance as AI-generated texts can achieve high quality easily. The goal of education to foster critical thinking and originality necessitates new assessment approaches.

Method: The study presents AROA, which assesses originality using four components: structural rarity, claim rarity, evidence rarity, and cognitive depth, quantified through density estimation and integrated with a quality adjustment mechanism.

Result: Experiments showed a negative correlation between quality and claim rarity, suggesting a trade-off where high-quality writings often use typical patterns. AI essays had lower claim originality but matched humans in structural complexity.

Conclusion: AROA provides a novel path for originality assessment in writing. It highlights limitations in LLMs' originality capabilities, distinguishing them from human-generated content.

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [265] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: The paper introduces FS-Researcher, a system enabling large language models (LLMs) to perform deep research tasks beyond their context windows using a persistent workspace.


<details>
  <summary>Details</summary>
Motivation: Long-horizon deep research tasks for LLMs face challenges due to model context limits, constraining token budgets for evidence collection and report writing.

Method: FS-Researcher is a dual-agent file-system-based framework with a Context Builder agent for collecting and archiving data and a Report Writer agent for report composition using a hierarchical knowledge base.

Result: The framework achieves state-of-the-art report quality on benchmarks DeepResearch Bench and DeepConsult, and demonstrates effective test-time scaling.

Conclusion: The FS-Researcher framework extends deep research capabilities for LLMs by leveraging a file-system approach, allowing for improved scalability and report quality.

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [266] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: This paper proposes Value Aggregation (VA) as a method to capture sentence-level semantics better using Large Language Models, outperforming existing approaches, even in training-free settings.


<details>
  <summary>Details</summary>
Motivation: Most current sentence representation methods rely on final-layer hidden states of Large Language Models, which are aimed at next-token prediction and fail to effectively capture global sentence-level semantics.

Method: A novel method called Value Aggregation (VA) pools token value vectors across multiple layers and token indices. An enhanced version, Aligned Weighted VA (AlignedWVA), uses specific attention mechanisms and aligns weighted value vectors for optimal LLM-based embeddings.

Result: VA achieves better performance compared to other LLM-based embedding methods, even matching or surpassing ensemble-based methods like MetaEOL. AlignedWVA significantly exceeds MetaEOL in training-free scenarios.

Conclusion: The paper demonstrates the superiority of using attention value vectors for sentence representations and highlights the potential of fine-tuning VA for stronger LLM embedding models.

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [267] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: This paper introduces a robust framework, Certified Semantic Smoothing (CSS), to enhance the safety of Large Language Models (LLMs) against jailbreak attacks through statistical, ensemble-based guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing the vulnerability of LLMs to adaptive jailbreaks that bypass empirical defenses and ensuring more reliable safety guarantees.

Method: The paper proposes Certified Semantic Smoothing (CSS) using Stratified Randomized Ablation, classifying inputs into structural prompts and mutable payloads for provable robustness. Noise-Augmented Alignment Tuning (NAAT) is also introduced to mitigate performance issues and add semantic denoising capabilities.

Result: CSS reduces gradient-based attack success rates from 84.2% to 1.2% while maintaining high benign utility (94.1%), outperforming weaker baselines which compromise utility.

Conclusion: By providing a deterministic robustness certification, this framework significantly strengthens LLM defenses against adversarial modifications, ensuring both safety and high performance in practical applications.

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [268] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: The paper introduces Wiki Live Challenge (WLC) as a benchmark to evaluate Deep Research Agents (DRAs) using Wikipedia Good Articles (GAs) as references, focusing on assessing their neutrality, comprehensiveness, and factual verifiability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current evaluation methods for DRAs—which rely heavily on LLM-generated references and lack reliability and fine-grained/objective assessment—and to push comprehensive benchmarks closer to human expert-level quality.

Method: The authors curates a dataset of 100 recent Wikipedia Good Articles and designs Wiki Eval, which includes 39 criteria for writing quality and rigorous factual verifiability metrics. They test various DRA systems using this dataset.

Result: The experiments demonstrate a significant performance gap between DRAs and expert-level human writing in Wikipedia GAs, showcasing WLC as a robust evaluation tool.

Conclusion: The introduction of WLC serves as an effective benchmark for advancing DRA capabilities, enabling reliable, fine-grained, and objective evaluations using expert-quality references.

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [269] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

TL;DR: This paper introduces the Socratic Inquiry Framework (SIF) to make large language models (LLMs) proactive in therapeutic questioning rather than reactive responders.


<details>
  <summary>Details</summary>
Motivation: Current LLMs in therapy are reactive and fail to surface deeper beliefs or guide change, while cognitive behavioral therapy relies heavily on proactive questioning.

Method: The authors propose the SIF, a therapeutic intent planner that separates decision-making on when to ask questions (Strategy Anchoring) and what to ask (Template Retrieval). They also introduce the Socratic-QA dataset for strategy-aligned Socratic questioning.

Result: SIF increases the frequency of proactive questioning, conversational depth, and therapeutic alignment in experiments.

Conclusion: SIF marks a shift in psychological LLMs from being reactive comforters to active cognitive guides, enabling better therapeutic conversations.

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [270] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: This paper introduces SEA-Guard, a culturally aware AI safeguard model for Southeast Asia, using a novel framework to create region-specific safety datasets.


<details>
  <summary>Details</summary>
Motivation: Address the lack of culturally grounded AI safeguard models tailored to diverse values, norms, and regulations in Southeast Asia.

Method: Develop an agentic data-generation framework to create authentic safety datasets specifically for Southeast Asia, and train multilingual models (SEA-Guard) tailored to regional nuances.

Result: SEA-Guard performs better than existing safeguards in detecting harmful or sensitive content specific to Southeast Asia, while maintaining strong general safety performance.

Conclusion: Culturally aware safeguard models like SEA-Guard are vital for improving AI alignment in diverse cultural contexts and significantly enhance regional safety detection capabilities.

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [271] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: A2Eval automates and optimizes the evaluation process for embodied Vision-Language Models (VLM), improving efficiency, accuracy, and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Current practices for VLM evaluation are labor-intensive, expensive, and suffer from redundancy and imbalance, hindering efficient development.

Method: Introduces a two-agent framework: the Data Agent curates balanced evaluation suites, and the Eval Agent develops executable pipelines for fully automated evaluation.

Result: A2Eval achieved significant reductions in computational costs (77%) and dataset size (85%), while improving ranking fidelity and human alignment.

Conclusion: A2Eval sets a new standard for low-cost, high-fidelity evaluation in embodied VLMs, addressing systematic biases and enhancing efficiency.

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [272] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: The paper proposes Steering Vector Fields (SVF) to enhance inference-time steering of large language models (LLMs) by using context-dependent interventions, addressing limitations of static steering vectors.


<details>
  <summary>Details</summary>
Motivation: Steering vectors provide a flexible method to adjust LLM behaviors during inference without fine-tuning or complex prompting. However, they often fail to provide reliable control across various contexts and tasks, showing inconsistent results.

Method: The paper introduces SVF, which uses a differentiable concept scoring function to dynamically determine a context-dependent steering direction at each activation, enabling coordinated interventions across layers in a shared concept space.

Result: SVF improves reliability and control compared to static steering vectors, especially in long-form generation and multi-attribute tasks, providing consistent improvements across multiple LLMs and scenarios.

Conclusion: SVF resolves the limitations of static steering vectors by adapting steering directions based on context, making inference-time steering more effective and practical for complex tasks.

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [273] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: The paper introduces CoDiQ, a framework for controllable difficult question generation to improve reasoning model training, and presents a 44K-question dataset.


<details>
  <summary>Details</summary>
Motivation: Training reasoning models on challenging, competition-level questions enhances their performance. Existing methods lack precise difficulty control, scalability, and efficiency in generating such questions.

Method: The authors identify factors affecting question difficulty and solvability, propose test-time scaling, and develop CoDiQ-Generator from Qwen3-8B. They generate CoDiQ-Corpus with 44K competition-grade questions and validate via human evaluations.

Result: CoDiQ-Corpus offers significantly more challenging yet solvable questions compared to existing datasets. Training on these improves models' reasoning performance, confirmed through experiments.

Conclusion: Controlled-difficulty question generation is crucial for advancing reasoning models, and CoDiQ effectively addresses this with its scalable generation framework and open-source contributions.

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [274] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: DeepControl introduces methods to optimize external information retrieval in reasoning agents via adaptive control mechanisms, achieving significant benchmarks improvements.


<details>
  <summary>Details</summary>
Motivation: Current multi-step reasoning agents face challenges from uncontrolled retrieval, leading to inefficiencies in evidence processing and learning instability.

Method: DeepControl employs selective regulation mechanisms based on information utility, integrating retrieval continuation and granularity controls, coupled with an annealed control strategy for behavior training.

Result: DeepControl consistently surpasses prior methods, achieving notable performance enhancements (e.g., 9.4% on Qwen2.5-7B and 8.6% on Qwen2.5-3B benchmarks).

Conclusion: Adaptive information control is essential for improving search-augmented reasoning agents, enabling their scalability in complex environments.

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [275] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: The paper investigates the mechanism behind In-Context Learning (ICL) in large language models (LLMs), suggesting that their encoding strategy may explain the phenomenon.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding surrounding the mechanisms of ICL in LLMs, and to overcome limitations such as error correction and diagnosis when leveraging ICL.

Method: The paper builds on the properties of ICL and functional modules of LLMs, proposing 'the counting hypothesis' which connects ICL to LLMs' encoding strategies.

Result: Evidence is provided supporting the hypothesis that LLMs' encoding strategy underpins ICL.

Conclusion: Understanding ICL through 'the counting hypothesis' could advance its applications across domains while addressing its limitations, contributing to improved utilization of LLMs.

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [276] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: The paper addresses the problem of exploration collapse in large reasoning models (LRMs) during reinforcement learning post-training and proposes a decoding strategy, Latent Exploration Decoding (LED), to improve reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To resolve the unintended exploration collapse in LRMs during RL post-training, which hinders the performance improvement in reasoning accuracy when using temperature-based sampling.

Method: The authors propose a depth-conditioned decoding strategy, Latent Exploration Decoding (LED), which uses entropy-maximizing intermediate layer configurations to enhance exploration without modifying the model or requiring additional training.

Result: LED improves reasoning accuracy (pass@1 and pass@16) across reasoning benchmarks and models by 0.61 and 1.03 percentage points, respectively.

Conclusion: Latent Exploration Decoding (LED) tackles the entropy asymmetry issue in post-trained LRMs, offering a simple and effective way to boost reasoning capabilities without extra model tuning or parameters.

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [277] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: This paper introduces a Strategic Language Search (SLS) problem to test LLMs' information-seeking ability, using a game-theoretic approach called Game of Thought (GoT), and demonstrates better worst-case performance.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in high-stakes settings when they lack sufficient information, necessitating active and effective information-seeking strategies.

Method: The authors formalize the SLS problem and develop the GoT framework, which uses game-theoretic techniques to approximate a Nash equilibrium (NE) strategy in adversarial setups.

Result: The GoT framework improves worst-case performance compared to direct prompting and heuristic-guided methods across various scenarios.

Conclusion: Game-theoretic approaches in information-seeking scenarios can significantly enhance the performance of LLMs in challenging conditions.

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [278] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: The researchers propose a framework that enhances large language model (LLM) decision-making in agentic environments by enabling simulated exploration during test-time, improving reliability while mitigating risks.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling techniques for LLMs fail to address the challenges of agentic settings, where irreversible, costly interactions occur with external environments.

Method: They introduce a framework called 'Agentic Risk-Aware Test-Time Scaling via Iterative Simulation' (
ame), which utilizes simulated interactions to explore actions before real-world execution, and designs a risk-aware tool simulator for better handling of rare high-impact failure modes.

Result: Iterative simulation significantly boosts agent reliability in decision-making tasks, and the risk-aware simulation consistently improves performance across different models and tasks.

Conclusion: The proposed framework demonstrates that emphasizing risk-awareness and simulation can make agentic systems more robust and reliable for high-stakes environments.

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [279] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: The paper presents MedAraBench, a large-scale Arabic medical dataset to address the lack of multilingual benchmarks for LLMs, particularly in medical domains.


<details>
  <summary>Details</summary>
Motivation: Arabic is underrepresented in NLP research, especially in medical applications, due to limited open-source datasets and benchmarks.

Method: The authors manually digitized academic materials from Arabic-speaking medical professionals, preprocessed the data, divided into training and test sets, and validated quality using expert reviews and LLM assessments.

Result: MedAraBench spans 19 medical specialties with 5 difficulty levels. Eight models were benchmarked, demonstrating the dataset’s capability to highlight the need for domain-specific improvements.

Conclusion: MedAraBench offers a valuable resource to expand multilingual benchmarks, enhance LLM evaluation, and support clinical applications in Arabic-speaking regions.

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [280] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: The paper explores the reliability of activation-based steering in LLMs by diagnosing using internal signals such as entropy-based measures and KL divergence, showing their predictive value.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to understand mechanistic factors that influence the success or failure of activation-based steering in LLMs, as current practices lack clear insights.

Method: The authors focus on two information-theoretic measures to evaluate steering—Normalized Branching Factor and KL divergence—and conduct reliability studies using annotations as ground truth.

Result: The study reveals that the proposed signals effectively predict successful steering and estimate failure probabilities. A stronger evaluation baseline for the two main steering methods is introduced.

Conclusion: Mechanistic signals can diagnose steering reliability effectively, offering predictive power for steering outcomes and enhancing evaluation methods of activation-steering techniques.

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [281] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: The paper introduces BBPE16, a UTF-16-based tokenizer for multilingual ASR, which reduces token counts and improves efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in multilingual ASR caused by BBPE's variable-length encoding, which inflates token sequences for non-Latin scripts like CJK, increasing computational load and memory use.

Method: The authors propose BBPE16, a UTF-16-based tokenizer that retains BBPE's language-agnostic features, uses a uniform 2-byte code unit, and improves cross-lingual token sharing.

Result: BBPE16 achieves comparable or better accuracy in multilingual ASR scenarios, reduces token counts (up to 10.4% for Chinese), decreases decoding iterations (up to 10.3%), and speeds up fine-tuning and inference while lowering memory usage.

Conclusion: BBPE16 is presented as an efficient and practical tokenization approach for multilingual ASR, reducing inefficiencies without significant accuracy trade-offs.

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [282] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: This study introduces COMI, a novel context compression framework for Large Language Models (LLMs), addressing computational inefficiency and redundancy in long-context scenarios. COMI uses Marginal Information Gain (MIG) to optimize information relevance and diversity, significantly enhancing model performance under high compression rates.


<details>
  <summary>Details</summary>
Motivation: The motivation was to resolve the computational inefficiency and redundancy in long-context settings of LLMs by developing a more effective context compression framework.

Method: COMI employs a two-stage compression framework: (1) Coarse-Grained Group Reallocation, dividing context into groups with dynamic compression rates using inter-group Marginal Information Gain; (2) Fine-Grained Token Merging, where tokens are merged based on intra-group MIG-based weighting to retain relevant information with minimal redundancy.

Result: Experiments on various question-answering and summarization tasks with different LLM backbones show that COMI significantly outperforms existing methods, with notable improvements like a ~25-point Exact Match (EM) enhancement under a 32x compression on NaturalQuestions using Qwen2-7B.

Conclusion: COMI's coarse-to-fine adaptive approach efficiently maintains semantic relevance and diversity while achieving high compression, positioning it as a superior method for handling long-context scenarios in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [283] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: Proposes a predictive guardrail framework, SafePred, that enhances safety and task utility for Computer-using Agents (CUAs) by addressing long-term risks beyond current observation space.


<details>
  <summary>Details</summary>
Motivation: Existing reactive guardrails for CUAs fail to prevent actions leading to delayed long-term risks.

Method: Developing SafePred, a predictive guardrail framework, which utilizes the world model for future risk predictions and ensures safe action guidance.

Result: SafePred achieves over 97.6% safety performance and increases task utility by 21.4% compared to reactive baselines.

Conclusion: SafePred effectively mitigates both short- and long-term risks, ensuring safer and more utility-optimized behaviors for CUAs in complex environments.

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [284] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: This paper introduces three techniques to improve Automated Essay Scoring (AES): Two-Stage fine-tuning, Score Alignment, and uncertainty-aware self-training, achieving significant performance boosts under limited and full-data conditions.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of labeled data in Automated Essay Scoring (AES), which challenges the development and real-world adoption of robust AES systems.

Method: The study relies on a Two-Stage fine-tuning strategy, a Score Alignment technique, and uncertainty-aware self-training to enhance AES performance. The methods are implemented using DualBERT and evaluated on the ASAP++ dataset.

Result: In the 32-data setting, the methods achieve 91.2% of full-data performance with only 32 labeled samples, and Score Alignment contributes to state-of-the-art results in full-data scenarios.

Conclusion: The proposed approach significantly improves AES performance in both limited and full-dataset settings, showcasing the potential of the introduced techniques for real-world applications.

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [285] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: WorldCup is a multi-bit watermarking method for LLMs achieving superior performance in provenance encoding through innovative mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address limitations of zero-bit watermarking schemes in reliable attribution, specifically, indirect information flow and low capacity in LLM-generated text.

Method: WorldCup uses a hierarchical competition mechanism and entropy-aware modulation for embedding bits into token selection. It also employs confidence-aware decoding for robust message recovery.

Result: WorldCup outperforms prior watermarking methods in capacity, detectability, robustness, quality, and decoding efficiency.

Conclusion: The proposed framework establishes a strong foundation for advanced LLM watermarking studies by significantly enhancing key performance metrics.

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [286] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: The paper introduces Zero2Text, a new framework to tackle embedding inversion privacy risks in retrieval-augmented generation systems, overcoming limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks associated with embedding inversion attacks in vector databases used in retrieval-augmented generation, which are inadequately handled by current approaches.

Method: The authors propose Zero2Text, a training-free framework using recursive online alignment that synergizes large language model priors with dynamic ridge regression to iteratively align generations to target embeddings.

Result: Extensive experiments show Zero2Text outperforms baselines in recovering sentences from unknown domains without leaked data, with significantly higher ROUGE-L and BLEU-2 scores on benchmarks like MS MARCO.

Conclusion: Zero2Text effectively mitigates privacy trade-offs in embedding inversion, demonstrating compelling performance and adaptability in black-box and cross-domain scenarios.

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [287] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: This paper proposes a topology-aware tokenization method to improve large language models (LLMs) in understanding and reasoning with graph data, using a special token <SOG_k>. It achieves significant performance improvements in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges LLMs face with graph data, particularly in handling structural hallucinations and misalignments with original tokens.

Method: The authors designed a structural tokenizer that maps graph topology into a specialized token <SOG_k>, enabling explicit topology representation along with a hybrid structured dataset for alignment.

Result: The approach significantly outperformed baselines by 9.9% to 41.4% on five graph-level benchmarks. It further supports both global and local structural understanding in node-level tasks.

Conclusion: This method empowers LLMs to accurately process graphs, enhances interpretability and consistency, and provides flexibility for node-level structural reasoning. The codebase is publicly accessible.

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [288] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: The paper investigates the impact of data distribution on context compression quality in Large Language Models (LLMs), revealing influences of input entropy and intrinsic model data gaps.


<details>
  <summary>Details</summary>
Motivation: Improving computational efficiency and addressing redundancy in long-context LLM scenarios, while exploring overlooked data-centric factors affecting compression quality.

Method: Uses an autoencoder-based framework to evaluate semantic integrity and systematically analyze the relationship between data distribution and compression quality.

Result: Findings show negative correlation between input entropy and compression quality for the encoder and diminishing compression gains due to intrinsic encoder-decoder data gaps.

Conclusion: Data distribution significantly affects compression quality, necessitating guidelines and considerations for optimizing compression in LLMs.

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [289] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: The paper introduces Sentence Curve Language Models (SCLM), a novel approach in language modeling using sentence curves to improve global structure prediction, achieving SOTA performance among diffusion-based language models.


<details>
  <summary>Details</summary>
Motivation: The use of static word embeddings in traditional and diffusion-based language models fails to capture the global structure of sentences, focusing only on local accuracy in word prediction.

Method: The proposed method, Sentence Curve Language Model (SCLM), replaces static word embeddings with sentence curves defined by spline curves, enhancing global structure modeling in sentence predictions.

Result: SCLM achieves state-of-the-art performance among DLMs on benchmarks like IWSLT14 and WMT14, stable training without knowledge distillation, and promising results compared to discrete DLMs on LM1B.

Conclusion: The introduction of sentence curves significantly improves the global structure modeling capability of language models, showcasing its potential as an alternative to traditional word embedding methods.

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [290] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE is a pipeline that uses a tree-pruning approach on HTML DOM, allowing a small language model to extract accurate structured data cost-effectively, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to balance the trade-off between the fragility of manual heuristics and the high cost of large language models in web data extraction.

Method: AXE utilizes a specialized "pruning" mechanism to process HTML DOM as a tree, combined with a lightweight LLM and Grounded XPath Resolution for traceability.

Result: AXE achieves state-of-the-art zero-shot performance with an F1 score of 88.1% on the SWDE dataset, outperforming larger alternatives.

Conclusion: AXE offers a cost-effective, practical solution for large-scale web data extraction, demonstrating efficiency even with a small model.

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [291] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: This paper proposes RAM (Read As HuMan), a context compression framework, to improve LLM performance in long-context tasks by combining close reading of relevant sections and skimming less important parts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses computational inefficiency and redundancy when deploying large language models (LLMs) in long-context scenarios.

Method: RAM partitions the input into segments; high-relevance parts are fully retained (close reading), while low-relevance sections are query-guided compressed into summary vectors (skimming). It uses contrastive learning to refine the distinction between the two approaches.

Result: Experiments show that RAM improves performance on multiple benchmarks, achieves high scalability, and delivers up to 12x speedup for long inputs.

Conclusion: The RAM framework effectively balances computational efficiency and relevance in long-context scenarios, outperforming baselines and providing interpretability with competitive results.

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [292] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: The paper introduces PretrainRL, a framework combining reinforcement learning during pretraining to mitigate factual hallucinations caused by imbalanced data distributions in large language models.


<details>
  <summary>Details</summary>
Motivation: LLMs generate factual inaccuracies because of skewed data distributions that prioritize high-probability falsehoods over low-probability truths.

Method: The authors propose PretrainRL, which reshapes probability distributions using reinforcement learning, debiasing falsehoods, and enabling better learning through a negative sampling strategy and new evaluation metrics.

Result: PretrainRL effectively reduces factual errors and surpasses existing state-of-the-art methods based on testing across three public benchmarks.

Conclusion: PretrainRL addresses factual hallucinations at their root, improving factual knowledge representation in LLMs by reshaping the pretraining phase.

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [293] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) lack robust long-term memory for applications like online emotional support. This paper introduces ES-MemEval, a benchmark for evaluating memory capabilities, and EvoEmo, a multi-session dataset for emotional support.


<details>
  <summary>Details</summary>
Motivation: To address the inability of LLMs to effectively handle dispersed and evolving user data in personalized long-term interactions.

Method: The authors propose ES-MemEval to systematically test LLM memory capabilities and create EvoEmo, a dataset capturing the complexities of long-term emotional support interactions.

Result: Experiments demonstrate that explicit memory enhances performance by reducing hallucinations and enabling personalization. Retrieval-augmented models showed factual consistency but struggled with evolving user states.

Conclusion: The study highlights the need for more robust integrations of memory and retrieval systems in personalized dialogue models.

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [294] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

TL;DR: Paper introduces GuideWeb, a benchmark for automatic user guidance generation in web UIs, showcasing challenges and current model limitations.


<details>
  <summary>Details</summary>
Motivation: Maintaining operation guides for evolving websites is labor-intensive, requiring solutions for automating guide generation.

Method: GuideWeb benchmark and accompanying evaluation suite assess guide target selection accuracy and text quality on real-world websites.

Result: GuideWeb Agent achieves 30.79% target prediction accuracy, BLEU scores of 44.94 for intent generation, and 21.34 for guide-text generation, outperforming baselines.

Conclusion: Automatic guidance generation for web UIs is still challenging, signaling the need for technological advancements to ensure reliability in deployment.

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [295] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: This paper presents "Vibe Coding," an educational approach that uses Large Language Models (LLMs) to support coding in an NLP course, emphasizing conceptual understanding and critical thinking over mere coding skills.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenges and opportunities in using LLMs for educational purposes in NLP, emphasizing the need for teaching NLP concepts effectively while leveraging the power of LLMs.

Method: The method involved implementing "Vibe Coding" in an undergraduate NLP course, where students used LLMs for code generation across seven labs, with assessment focusing on conceptual understanding via critical reflection questions.

Result: Feedback from 19 students indicated high satisfaction in engagement and learning, highlighting reduced cognitive load for debugging, but raised challenges regarding time constraints, task clarity, and LLM output verification.

Conclusion: Properly structured, LLM-assisted learning can shift the focus from technical coding skills to deeper conceptual understanding, making students better prepared for AI-driven professional environments.

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [296] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: The study introduces CatRAG, a framework addressing shortcomings of static graph-based Retrieval-Augmented Generation (RAG) by dynamically adapting Knowledge Graphs to queries, enabling more comprehensive evidence retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing Retrieval-Augmented Generation (RAG) systems struggle with "Static Graph Fallacy," failing to adapt Knowledge Graphs to the relevance of the given query, leading to partial recall and incomplete evidence for multi-hop queries.

Method: The authors propose CatRAG, incorporating three strategies: Symbolic Anchoring, Query-Aware Dynamic Edge Weighting, and Key-Fact Passage Weight Enhancement, which query-adapt graph traversal and random walks for better evidence retrieval in multi-hop queries.

Result: CatRAG outperformed state-of-the-art baselines across four benchmarks, improving reasoning completeness by recovering entire evidence chains rather than partial ones, despite modest gains in standard Recall metrics.

Conclusion: CatRAG successfully enhances RAG systems by dynamically adjusting Knowledge Graphs, improving retrieval accuracy and enabling fully grounded reasoning in multi-hop tasks.

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [297] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Moe-CTC is a new ASR architecture using a Mixture-of-Experts approach with intermediate CTC supervision to address challenges in recognizing accented speech, outperforming a FastConformer baseline.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR models fail with accented speech as they are heavily trained on high-resource English varieties; current accent-agnostic or accent-specific methods lack robustness or rely on noisy data.

Method: Moe-CTC uses a Mixture-of-Experts architecture with intermediate CTC loss, enabling specialized accent representation during training with accent-aware routing and generalization through label-free routing during inference.

Result: Tested on the Mcv-Accent benchmark, Moe-CTC improved accuracy for both seen and unseen accents, achieving up to 29.3% relative WER reductions over existing strong baselines.

Conclusion: This method provides a scalable way to enhance ASR performance on accented speech, blending specialization for accents and generalization for unseen varieties.

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [298] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: This paper introduces the Orthogonal Hierarchical Decomposition (OHD) framework to improve LLM understanding and reasoning of complex tables with multi-level headers and merged cells.


<details>
  <summary>Details</summary>
Motivation: Address challenges faced by LLMs in processing complex table structures, as existing methods fail to capture hierarchical semantics and dependencies effectively.

Method: Proposes the Orthogonal Hierarchical Decomposition (OHD) framework that uses an Orthogonal Tree Induction (OTI) method to split tables into column and row trees, ensuring structural and semantic alignment.

Result: OHD outperformed existing approaches on complex table QA benchmarks—AITQA and HiTab—on various metrics.

Conclusion: The OHD framework demonstrates effective advancements in making LLMs better aligned with structural and semantic hierarchies in complex tables.

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [299] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: This paper introduces a novel approach called Embedding-Virtualized Knowledge (EVK) to evaluate and improve large language model editing, targeting broader impacts on their knowledge systems and preserving knowledge better.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating large language model edits are limited to predefined benchmarks and finite datasets, which restrict the understanding of broader impacts on the model's entire knowledge system.

Method: The authors developed EVK, a framework using controlled perturbations in embedding space, along with an evaluation benchmark called EVK-Bench. They also proposed EVK-Align, a module to minimize knowledge drift during editing.

Result: EVK allows for broader evaluations not confined to explicit data, while EVK-Align improves knowledge preservation in models without reducing editing accuracy.

Conclusion: The proposed EVK framework and tools enhance the evaluation and editing of large language models, improving comprehension of knowledge drift and balancing knowledge retention and editing precision.

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [300] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: The paper proposes a method for improving chain-of-thought (CoT) reasoning in large language models (LLMs) without strong reliance on supervision by utilizing a self-sampling framework and activation steering.


<details>
  <summary>Details</summary>
Motivation: The study addresses the redundancy in reasoning introduced by CoT methods in LLMs and explores the potential for creating a fast-thinking, efficient reasoning mode analogous to human System 1 cognition.

Method: The authors propose a self-sampling framework that generates reasoning traces without teacher guidance and uses supervised fine-tuning (SFT) with human-like dual-cognitive systems and progressive compression curriculum. They also explore a self-evolution regime to refine models using only prediction-consistent variable-length data, without needing gold answers.

Result: The method achieves stable improvements in benchmarks, particularly in math tasks and cross-domain generalization in medicine. It works for both broad-spectrum and R1-style LLMs.

Conclusion: The proposed framework enables efficient CoT learning by reducing dependence on high-quality labeled data and shows potential across multiple domains, supporting advancements in reasoning capabilities of LLMs.

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [301] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: This paper explores the underlying mechanisms of self-reflection in R1-style LLMs by analyzing the layer-wise activation process and identifying three key stages leading to reflection behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms enabling self-reflection in R1-style language models, which remains unclear despite increasing attention.

Method: The study uses the logit lens technique to analyze token-level semantics and identify distinct stages of reflection, supported by targeted interventions to discern causal chains across stages.

Result: The paper uncovers a structured three-stage progression: latent-control, semantic-pivot, and behavior-overt layers, each contributing to self-reflection through semantic and activation modulation.

Conclusion: R1-style LLMs exhibit a human-like meta-cognitive process of self-reflection, progressing from latent monitoring to discourse-level regulation and overt reflection behavior. This understanding advances insights into LLM mechanisms.

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [302] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: This paper introduces xMemory, a system that enhances agent memory retrieval by organizing memory hierarchically and aligning retrieval with semantic components, improving reasoning and efficiency.


<details>
  <summary>Details</summary>
Motivation: Agent memory system retrieval often returns redundant or incoherent context, affecting reasoning due to correlated or duplicated spans in bounded dialogue streams.

Method: The approach involves splitting memories into semantic components, organizing them hierarchically, maintaining a structure through sparsity–semantics objectives, and retrieving context top-down based on themes and uncertainty reduction.

Result: Experiments conducted using three LLMs on datasets like LoCoMo and PerLTQA demonstrate improved answer quality and token efficiency.

Conclusion: xMemory shows consistent advancements over standard RAG pipelines, effectively addressing retrieval inefficiencies through semantic organization and hierarchical memory systems.

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [303] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: The paper introduces NEAT, a Neuron-based Early Reasoning Exit framework, to prevent redundant reasoning steps in large models without additional computational costs.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the problem of overthinking in Large Reasoning Models, which leads to inefficiency due to redundant reasoning steps.

Method: The proposed NEAT framework monitors neuron-level activation dynamics, identifies exit-associated neurons, and tracks their activation patterns in real-time to enable training-free early exits during reasoning tasks.

Result: NEAT reduces unnecessary reasoning steps, achieving an average token reduction of 22%-28% across four reasoning benchmarks while maintaining accuracy.

Conclusion: NEAT effectively reduces overthinking and improves reasoning efficiency in models without additional computation or external datasets.

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [304] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: This paper introduces WildGraphBench, a benchmark for evaluating GraphRAG systems on realistic, large-scale, heterogeneous contexts using Wikipedia references and citation-linked data.


<details>
  <summary>Details</summary>
Motivation: Current GraphRAG benchmarks use curated, short passages, failing to provide a realistic assessment of system performance on long and complex documents. The authors aim to address this limitation.

Method: The authors construct WildGraphBench by utilizing Wikipedia's structure, sampling articles from 12 topics, and creating 1,100 questions across three task complexities with external document references as retrieval sources.

Result: The research demonstrates that GraphRAG systems perform well in multi-fact aggregation for moderate evidence sources but struggle with detailed accuracy, especially on summarization.

Conclusion: WildGraphBench effectively highlights the limitations of existing GraphRAG models, showcasing the need for more balanced aggregation strategies for detailed and large-scale scenarios.

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [305] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: The paper introduces LEC-KG, a framework integrating Large Language Models (LLMs) and Knowledge Graph Embeddings (KGE) to construct knowledge graphs from unstructured text, overcoming challenges like long-tail relation bias and heterogeneous mentions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of creating domain-specific knowledge graphs, such as dealing with heterogeneous entity mentions, long-tail relation distributions, and the lack of standardized schemas.

Method: LEC-KG employs hierarchical coarse-to-fine relation extraction, evidence-guided Chain-of-Thought feedback, and semantic initialization. It iteratively refines knowledge graph representations through the collaboration of LLMs and KGE.

Result: The LEC-KG framework demonstrates significant improvements in constructing knowledge graphs from Chinese SDG reports, outperforming LLM baselines, especially in handling low-frequency relations.

Conclusion: LEC-KG reliably converts unstructured policy texts into validated knowledge graph triples by using a synergistic combination of semantic understanding and structural reasoning.

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [306] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: The paper presents a method, Dynamic Decoupled Conditional Advantage (DDCA), to improve efficiency and accuracy in Reinforcement Learning with Verifiable Rewards (RLVR) by addressing verbosity and optimizing length penalties.


<details>
  <summary>Details</summary>
Motivation: RLVR encourages multi-step reasoning but often generates overly verbose traces. Existing techniques excessively penalize length, leading to accuracy drops, due to structural issues like baseline dilution and difficulty-penalty mismatch.

Method: The proposed DDCA method decouples efficiency optimization from correctness by calculating length advantages within the correct-response cluster and dynamically scaling penalties based on task difficulty.

Result: DDCA reduces redundancy by approximately 60% on simpler tasks (e.g., GSM8K) and over 20% on harder tasks (e.g., AIME25), while maintaining or increasing accuracy.

Conclusion: DDCA achieves a better efficiency-accuracy trade-off in RLVR by mitigating verbosity without sacrificing correctness, especially by adjusting to task difficulty.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [307] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: The paper introduces Dicta-LM 3.0, a collection of open-weight LLMs for Hebrew and English, available in three sizes with advanced features.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of LLMs for low-resource languages like Hebrew, despite a high global demand. The authors aim to address this gap.

Method: The authors developed Dicta-LM 3.0 by adapting pre-existing base models and trained them on a large dual-language corpus. They released three model sizes and evaluated them with a new benchmark suite specific to Hebrew tasks.

Result: Dicta-LM 3.0 includes LLMs trained on Hebrew and English, featuring 1.7B, 12B, and 24B parameters. These models showcase adaptability to Hebrew-centric tasks and settings.

Conclusion: The paper provides a framework for training LLMs in low-resource languages using Dicta-LM 3.0 as an example, contributing to advancements in multilingual NLP.

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [308] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: The paper introduces OOMB, a memory-efficient training system for large language models (LLMs), capable of handling long contexts with minimal GPU memory overhead.


<details>
  <summary>Details</summary>
Motivation: Current LLM training on long contexts is hindered by the prohibitive GPU memory consumption caused mainly by activations, which scale with sequence length.

Method: The authors propose OOMB, which employs chunk-recurrent training with on-the-fly activation recomputation to maintain constant activation memory. They also optimize KV cache management with a paged memory manager, asynchronous CPU offloading, and page-level sparse attention.

Result: OOMB enables efficient and scalable training, achieving only 10MB memory overhead per additional 10K tokens of context length for a 7B-parameter model and allowing training of such models with up to 4M-token contexts on a single GPU.

Conclusion: The proposed system overcomes memory-related challenges in long-context LLM training, providing significant advancements in resource efficiency and scalability.

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [309] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: The paper demonstrates that refusal behaviors in large language models correspond to distinct geometric directions in activation space but can be controlled using shared one-dimensional linear steering.


<details>
  <summary>Details</summary>
Motivation: The authors aim to better understand and control refusal behaviors in large language models by exploring their geometric structures in activation space.

Method: The paper analyzes eleven refusal-related categories and investigates their representation in activation space, employing linear steering techniques to understand how these affect model behavior.

Result: The study finds that refusal behaviors are geometrically distinct but share a common linear control mechanism, affecting the manner rather than the occurrence of refusal.

Conclusion: Refusal behaviors in language models are complex but manageable through unified steering methods, offering nuanced control opportunities.

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [310] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: This paper introduces GapEval, a benchmark to examine the gap and alignment between understanding and generation capabilities in unified multimodal models (UMM). It finds a consistent gap indicating surface-level unification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether understanding and generation tasks in unified multimodal models are genuinely aligned and integrated within a single model.

Method: The authors propose GapEval, a bidirectional benchmark where each question can be answered in both image and text modalities. They analyze bidirectional inference capabilities across a range of UMMs and conduct an empirical study on knowledge manipulation to explore limitations.

Result: Findings reveal a persistent gap between understanding and generation capabilities, showing surface-level rather than deep unification. Knowledge across modalities remains disjoint, with unsynchronized capability emergence.

Conclusion: Current multimodal models lack deep cognitive integration between understanding and generation tasks. Further exploration is needed to achieve synchronized knowledge across modalities.

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [311] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: This paper introduces Focus-dLLM, which significantly speeds up diffusion large language models' (dLLMs) long-context inference by using a novel, training-free attention sparsification framework.


<details>
  <summary>Details</summary>
Motivation: To overcome the efficiency issues in dLLMs caused by the computational cost of bidirectional full attention during long-context inference, as current sparse attention methods remain ineffective.

Method: The authors propose a past confidence-guided indicator to predict unmasked regions and a sink-aware pruning strategy to accurately eliminate redundant attention computations. Their approach also reuses sink information across layers, leveraging cross-layer consistency.

Result: Focus-dLLM achieves over 29× speedup for dLLMs in long-context inference with a 32K context length without any loss in accuracy.

Conclusion: Focus-dLLM improves efficiency and maintains accuracy for long-context processing in dLLMs, providing a significant computational advantage.

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [312] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: The paper introduces D-CORE, a two-stage framework aimed at addressing Lazy Reasoning in large reasoning models (LRMs) to improve their complex tool use capabilities.


<details>
  <summary>Details</summary>
Motivation: Current LRMs struggle with task decomposition in complex scenarios, resulting in suboptimal reasoning and tool use, referred to as Lazy Reasoning.

Method: D-CORE employs a two-stage training framework: first, self-distillation to enhance task decomposition reasoning, followed by diversity-aware reinforcement learning to improve reflective reasoning capability.

Result: D-CORE demonstrates robust performance across scales, surpassing existing models in accuracy on BFCLv3 benchmarks, with significant improvements despite smaller model sizes.

Conclusion: D-CORE effectively mitigates Lazy Reasoning in LRMs and achieves state-of-the-art performance, paving the way for more efficient reasoning capabilities in smaller models.

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [313] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: The paper introduces AR-MAP, a novel framework for aligning Diffusion Large Language Models (DLLMs) using autoregressive large language models (AR-LLMs) as teachers, achieving competitive or superior results in preference alignment tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high variance and computational overhead in preference alignment of Diffusion Large Language Models (DLLMs) using existing methods.

Method: AR-MAP leverages preference-aligned autoregressive LLMs (AR-LLMs) as teachers for aligning DLLMs through weight scaling, which exploits shared architectural similarities without direct alignment computation.

Result: AR-MAP showcases competitive or superior performance, achieving 69.08% average score across various preference alignment tasks when compared to DLLM-specific alignment methods.

Conclusion: The proposed AR-MAP framework effectively aligns DLLMs with minimal variance and computational overhead, making it an efficient alternative to existing methods for preference alignment.

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [314] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: The paper evaluates the metalinguistic knowledge (explicit reasoning about language) of LLMs and finds their knowledge limited and influenced by resource availability.


<details>
  <summary>Details</summary>
Motivation: Existing analyses emphasize language use but neglect LLMs' metalinguistic understanding and biases towards high-resource languages.

Method: The authors benchmark LLMs using accuracy, macro F1, chance baselines, and variation analyses across linguistic domains, languages, and resource levels.

Result: GPT-4 performs best with moderate accuracy (0.367), while models mainly perform above chance but not beyond majority-class baselines; results vary across domains and resource-levels.

Conclusion: LLMs lack generalizable grammatical competence, with performance shaped by data visibility; a new open-source benchmark is introduced to evaluate and improve diversity.

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [315] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: The paper introduces the first Sinhala physical common sense reasoning dataset containing 110 verified samples in a Sri Lankan context.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for physical common sense reasoning in Sinhala, a widely spoken language in Sri Lanka.

Method: The dataset was created through human involvement: prompts, correct answers, and wrong answers were crafted and verified for quality.

Result: A dataset consisting of 110 samples in Sinhala language with a focus on Sri Lankan context.

Conclusion: This dataset advances NLP research in Sinhala, contributing to physical common sense reasoning tailored for Sri Lankan scenarios.

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [316] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: The paper studies position bias in rubric-based evaluations by LLMs and introduces a balanced permutation strategy to address it, improving reliability.


<details>
  <summary>Details</summary>
Motivation: To address position bias in the rubric-based evaluation paradigm of LLMs as judges, and improve scoring reliability compared to human judgment.

Method: Conducting controlled experiments to detect position bias and proposing a balanced permutation strategy for score aggregation.

Result: Balanced permutation strategy mitigates latent position bias and enhances correlation between LLM evaluations and human judgments.

Conclusion: Rubric-based evaluation is prone to position bias, which can be corrected using permutation calibration, improving reliability of LLM assessments.

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [317] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: This paper introduces a new computational approach to measure regular sound correspondences in historical linguistics and assess irregularity in cognate sets using the 'balanced average recurrence' metric.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantified evaluation of regular sound correspondences in historical linguistics, which are often based on intuitive judgments rather than systematic metrics.

Method: The authors developed the 'balanced average recurrence' metric as a measure of regularity and implemented a computational method to identify irregular cognate sets. The method was validated through experiments using simulated and real datasets, employing leave-one-out validation techniques.

Result: The method achieved 85% accuracy in identifying irregular word forms in real datasets. Experiments showed the influence of increasing irregularity and the benefits of working with subsamples of large datasets.

Conclusion: The paper concludes that the new metric and method can enhance the quality of datasets in computer-assisted language comparison and potentially improve methodologies for future research.

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [318] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: The paper presents OpenSeal, a Southeast Asian language-focused large language model (LLM) built with only openly disclosed parallel data and efficient training methods.


<details>
  <summary>Details</summary>
Motivation: The lack of truly open-source multilingual LLMs for low-resource Southeast Asian languages and the need for transparency in training processes.

Method: The authors conducted controlled experiments using solely parallel data for continual pretraining of an LLM, utilizing 34.7B tokens and training on 8x NVIDIA H200 GPUs for 180 hours.

Result: OpenSeal, a competitive Southeast Asian LLM, was developed, demonstrating the effectiveness of parallel data in extending LLMs to new languages.

Conclusion: Parallel data is highly effective in pretraining LLMs for new languages, and OpenSeal sets a benchmark for transparency and performance for Southeast Asian LLMs.

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [319] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

TL;DR: The paper presents DziriBOT, a chatbot designed for the Algerian Darja dialect, using an innovative architecture to handle linguistic challenges and low-resource data.


<details>
  <summary>Details</summary>
Motivation: To address the demand for effective conversational agents in Algeria, accounting for the linguistic and orthographic complexities of Darja and its hybrid script usage.

Method: The solution involves a multi-layered architecture combining NLU and Retrieval-Augmented Generation (RAG), along with evaluating sparse Rasa pipelines, machine learning baselines, and fine-tuned transformer models.

Result: The fine-tuned DziriBERT model showed state-of-the-art performance, surpassing traditional baselines in handling orthographic noise and interpreting rare intents.

Conclusion: DziriBOT emerges as a scalable, robust solution bridging linguistic gaps, offering a practical approach for dialect-aware conversational agents in Algeria and similar regional contexts.

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [320] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5 is an open-source multimodal framework combining text and vision for advanced AI performance. It includes a dynamic agent orchestration system, achieving state-of-the-art results and latency improvements.


<details>
  <summary>Details</summary>
Motivation: To enhance general agentic intelligence by integrating text and vision modalities and providing a framework for dynamic task decomposition.

Method: Techniques include joint text-vision pre-training, zero-vision supervised fine-tuning (SFT), joint text-vision reinforcement learning, and the introduction of Agent Swarm for parallel task execution.

Result: Achieves state-of-the-art performance across domains like coding, vision, and reasoning, and reduces latency by up to 4.5 times compared to single-agent systems.

Conclusion: Kimi K2.5 demonstrates significant advancements in multimodal AI and task efficiency, paving the way for future exploration and real-world applications.

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [321] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: The paper investigates the reliability of cross-lingual evaluation methods for large language models, highlighting significant instabilities in pragmatic judgment metrics across related Finno-Ugric languages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the instability in measuring language model performance across different languages, especially morphologically rich ones, and to ensure reliable evaluation methods before deployment.

Method: The authors use synthetic dialogues generated with identical parameters across Estonian, Finnish, and Hungarian. They evaluate the stability of automatic metrics and LLM-as-a-judge scoring using both surface-level metrics and pragmatic judgments, with Estonian annotations as a reference.

Result: The study revealed ranking instabilities; surface-level metrics performed consistently across languages, while pragmatic judgments showed rank inversions and near-zero correlations, indicating judge scoring issues rather than true model differences.

Conclusion: Zero-shot judge transfer is unreliable for evaluating discourse-level tasks in morphologically rich languages. Language-specific calibration and human annotations are recommended to enhance evaluation reliability.

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [322] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: A new metric, StoryScore, is proposed to evaluate AI-generated scientific storytelling, addressing challenges such as hallucination detection and narrative quality.


<details>
  <summary>Details</summary>
Motivation: Evaluating AI-generated scientific narratives is difficult, especially due to the inadequacies of standard summarization metrics and issues with hallucination detectors.

Method: The authors introduce StoryScore, a composite metric combining semantic alignment, grounding, control, structural fidelity, redundancy avoidance, and hallucination detection.

Result: StoryScore provides a unified evaluation framework and identifies shortcomings in current hallucination detection methods when assessing pedagogical creativity.

Conclusion: StoryScore advances the evaluation of AI-generated scientific storytelling, addressing gaps in traditional metric systems while exposing limitations in hallucination detection.

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [323] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: The paper discusses a RAG framework called CARRIAGE designed to enhance diversity in cross-cultural recipe adaptations and improve generative results compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Retrieval Augmented Generation (RAG) in generating diverse and culturally adaptable recipe adaptations that cater to multiple user preferences and dietary needs.

Method: Introduction of CARRIAGE, a plug-and-play RAG framework that enhances diversity in recipe retrieval and context organization, aiming to improve the adaptability and diversity in generated recipes.

Result: CARRIAGE successfully generates more diverse and culturally relevant recipe adaptations, achieving Pareto efficiency in both diversity and quality compared to closed-book large language models.

Conclusion: By improving diversity in both retrieval and context organization, CARRIAGE overcomes RAG's limitations, representing a significant step forward in creative tasks requiring multiple valid outputs, especially in cross-cultural recipe adaptation.

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [324] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: The study addresses domain heterogeneity issues in multi-domain reinforcement learning for large reasoning models, introducing Modular Gradient Surgery (MGS) as a solution, achieving significant improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges stemming from domain heterogeneity when training large reasoning models across diverse domains.

Method: Introduced Modular Gradient Surgery (MGS) to resolve module-level gradient conflicts during multi-domain RL training.

Result: MGS yielded significant performance improvements in Llama and Qwen models across three domains, outperforming standard multi-task RL strategies.

Conclusion: MGS effectively addresses cross-domain interference, clarifies interference sources, and provides a robust method for multi-domain reinforcement learning in large reasoning models.

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [325] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: The authors explore how Large Language Models encode and update beliefs in representation space, propose new geometric methods for steering beliefs, and highlight limitations of linear representations.


<details>
  <summary>Details</summary>
Motivation: To understand how beliefs are encoded, updated, and reshaped within the representation space of large language models, especially when adapting to new evidence or changes.

Method: The authors study how Llama-3.2 represents and updates belief parameters through curved "belief manifolds" by generating samples and testing adaptation under distribution shifts. They introduce methods like linear field probing (LFP) and geometry-aware steering for interventions.

Result: The study reveals that belief parameters form curved manifolds during sufficient in-context learning and that geometry-aware steering better preserves belief consistency during interventions than linear methods, which can push models off-manifold.

Conclusion: Rich geometric structure naturally emerges in LLMs, and purely linear representations are inadequate for capturing the complexity of their belief systems, requiring more nuanced, geometry-aware approaches.

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [326] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: This paper presents a fully automated framework to generate precise molecular structure descriptions at scale, with a curated large-scale dataset yielding 98.6% precision.


<details>
  <summary>Details</summary>
Motivation: Aligning molecular structure with natural language is essential for enabling LLMs to effectively address downstream chemical tasks, but large-scale, high-quality datasets for such tasks are infeasible to construct manually due to costly human annotation.

Method: The authors extend a rule-based chemical nomenclature parser to interpret IUPAC names and generate structured XML metadata encoding molecular structure, which guides LLMs in producing accurate natural-language descriptions.

Result: Their framework produced a dataset of around 163,000 molecule-description pairs and achieved 98.6% precision in description through rigorous LLM-based and human expert validation.

Conclusion: This work provides a reliable dataset and an extensible annotation method for advancing molecule-language alignment, enabling its use in broader chemical tasks that demand structural descriptions.

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [327] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: This paper introduces 'language vectors,' a novel method to improve multilingual in-context learning for large language models (LLMs), increasing performance across 19 languages without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Performance of LLMs on non-English languages lags behind English, especially in multilingual in-context learning. This paper seeks to address this disparity by leveraging the hypothesis that LLMs encode languages as distinct directions in a shared semantic space.

Method: The authors introduce 'language vectors,' a training-free method that steers language models by modifying their intermediate activations during inference to shift toward the target language's vector in the semantic space.

Result: Evaluations across three datasets and 19 languages on three models demonstrate consistent improvements in multilingual tasks over baseline models. Hierarchical clustering of the vectors aligns with known linguistic relationships.

Conclusion: Language vectors significantly enhance multilingual learning and generalize across tasks, revealing task-agnostic and linguistically grounded properties in LLMs.

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [328] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: The paper unifies various methods for controlling large language models (LLMs) under a single framework, analyzes their trade-offs, and introduces a new steering approach SPLIT to enhance control effects.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling LLMs are studied in isolation, making it hard to compare or understand their connections and trade-offs effectively.

Method: The authors propose framing control methods as dynamic weight updates induced by control signals and analyze their effects on preference (towards a target concept) and utility (task-valid generation) using polarity-paired contrastive examples.

Result: The study finds a consistent trade-off: stronger control improves preference but reduces utility. It introduces the SPLIT approach that balances this trade-off, improving control while better preserving utility.

Conclusion: This unified framework and analysis illuminate the trade-offs in LLM control approaches and suggest SPLIT as a promising solution to optimize both preference and utility.

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [329] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: This paper presents a framework using multi-agent prompting for assessing soft skills, outperforming traditional fine-tuned models in reliability and scalability in abstract reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing soft skills like empathy and communication are inconsistent, biased, and struggle with context-dependent reasoning.

Method: A multi-agent prompting framework breaks down evaluation tasks into refinement and scoring stages. It uses 3-shot in-context learning with a large instruct-tuned model to assess skills.

Result: The approach outperforms fine-tuned models in reliability (Avg QWK 0.62 vs 0.32), achieves human-level consistency, and generalizes well on benchmarks (ASAP).

Conclusion: Structured prompt engineering offers a scalable alternative to fine-tuning in subjective reasoning tasks and redefines LLM application in automated assessments.

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [330] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: The paper introduces a scalable pipeline for training a Reward Model (RM) to verify mathematical proofs generated by Large Language Models (LLMs), aiming to improve mathematical reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Address the challenge that many mathematical problems are proof-based and lack a simple way to verify proof authenticity. This necessitates the development of a reliable RM for automatic verification of proof processes.

Method: Developed a scalable data-generation pipeline leveraging LLMs to create diverse, high-quality proof-related datasets, filtered through hierarchical human reviews. Trained an RM incorporating process rewards and token weight balance to enhance RLVR outcomes.

Result: Experiments demonstrate the RM's scalability and effectiveness in reward accuracy, generalization, and test-time guidance for improving LLM mathematical reasoning.

Conclusion: The proposed approach provides practical tools and strategies to enhance LLMs' proof-checking capabilities, contributing to strengthening their mathematical reasoning abilities.

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [331] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: LLMs risk poorly calibrated decision support; authors propose a collaborative reasoning system to address these issues.


<details>
  <summary>Details</summary>
Motivation: Highlight dangers of overly fluent AI assistants, emphasizing their potential to reinforce errors and push verification burdens onto humans.

Method: Introduces a discrepancy-driven control loop governing knowledge substrates with mechanisms for detecting conflicts and managing premises through negotiation.

Result: Shifts AI's focus from fluent agreement to premise-based collaboration, enhancing reliability in complex decisions.

Conclusion: Human-AI trust should focus on evidence-based premises over conversational fluency, proposing testable criteria for better partnerships.

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [332] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: The paper proposes ROG, a framework that integrates retrieval techniques with LLM reasoning for answering complex FOL queries on incomplete KGs, demonstrating superior performance on challenging query types.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of answering complex FOL queries over incomplete KGs, particularly those with multi-operator and negation-dependent structures, by reducing inference errors and improving reasoning robustness.

Method: ROG combines query-aware neighborhood retrieval with LLM chain-of-thought reasoning. It breaks down multi-operator queries into single-operator sub-queries, grounding them in localized evidence and caching intermediates to improve consistency.

Result: ROG achieves significant improvements over embedding-based reasoning models, particularly on high-complexity and negation-heavy queries, in KG reasoning benchmarks.

Conclusion: ROG offers a robust alternative to embedding-based KG reasoning by using retrieval-based, step-wise inference for better handling of complex and negation-dominant queries.

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [333] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: This paper proposes a novel method to detect student misconceptions using large language models (LLMs) in student-tutor dialogues, with emphasis on generating, retrieving, and re-ranking plausible misconceptions. It improves predictive performance and showcases advantages of fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of timely and accurate identification of student misconceptions, which heavily relies on teacher effort and intuition, hence improving learning outcomes and preventing error escalation.

Method: The authors use fine-tuned LLMs to generate plausible misconceptions from dialogues, retrieve top candidates via embedding similarity, and re-rank them using another fine-tuned LLM for better relevance.

Result: The approach demonstrates improved predictive performance compared to baseline models. Fine-tuned LLMs generate higher-quality misconceptions and outperform larger closed-source models. Ablation studies validate the significance of generation and re-ranking steps.

Conclusion: Fine-tuning LLMs for misconception detection significantly enhances both performance and quality, showcasing the potential for automating critical educational interventions.

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [334] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: The paper evaluates Large Language Models (LLMs) on multilingual mental health tasks, comparing them with conventional NLP baselines and testing their performance on machine-translated data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how LLMs perform in a multilingual context, particularly for mental health tasks, where there has been limited investigation.

Method: The study assesses LLMs using eight mental health datasets across different languages in zero-shot, few-shot, and fine-tuned setups, alongside machine-translated data. It also evaluates translation quality and its impact on LLM performance.

Result: Proprietary and fine-tuned open-source LLMs achieved competitive F1 scores, surpassing some state-of-the-art baselines. However, performance declined on machine-translated data, with variation depending on language and typology.

Conclusion: LLMs show promise for multilingual mental health tasks, but their efficacy is impacted by translation quality, particularly for languages with structural or lexical mismatches.

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [335] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: The paper addresses the issue of bias in large language models (LLMs) during syllogistic reasoning where semantic plausibility interferes with formal validity. The authors introduce a framework that employs abstraction-guided reasoning to reduce these biases and improve formal reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs have a bias called content effect, where they confuse semantic plausibility with logical validity even in step-wise explanations. Current approaches fail to fully suppress this bias when reasoning formally.

Method: The authors propose abstraction-guided reasoning, where abstract and content-laden syllogisms are paired. They define an abstract reasoning space based on activations from abstract inputs and utilize lightweight Abstractors to predict representations aligned with this space and intervene during model computation.

Result: The proposed method reduces semantic-driven errors and improves performance in ensuring the logical validity of LLMs, particularly in cross-lingual reasoning tasks.

Conclusion: Abstraction-guided reasoning using activation-level abstraction demonstrates potential for scalable solutions to enhance formal reasoning robustness against semantic interference in LLMs.

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [336] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: The paper introduces Mixture of Factor Analyzers (MFA) as an advanced method in analyzing the activation space of language models, capturing nonlinear and multidimensional structures, outperforming unsupervised baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on linear separability of concepts in activation space, often neglecting complex geometric structures. This paper aims to address this limitation with a scalable, unsupervised approach.

Method: The researchers utilize MFA to model activation space as Gaussian regions, decomposing activations into centroid locations and local variations. They test this method on Llama-3.1-8B and Gemma-2-2B.

Result: MFA captures nonlinear activation structures effectively, ranks above unsupervised baselines in evaluations, matches supervised methods in localization, and surpasses sparse autoencoders in steering capabilities.

Conclusion: Local geometry, represented by subspaces in activation space, serves as a useful tool for scalable concept discovery and model control, improving over simplistic linear approaches.

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [337] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

TL;DR: The paper explores potential consciousness in LLMs, emphasizing belief-guided agency and meta-cognitive monitoring using experimentally supported findings.


<details>
  <summary>Details</summary>
Motivation: Rapid progress in LLMs sparks the curiosity about whether these models show consciousness-like traits.

Method: The study evaluates HOT-3 as an indicator for consciousness, involving belief formations in latent space influencing actions and meta-cognition.

Result: Findings show that LLMs form beliefs influenced by external factors, beliefs drive actions, and LLMs can monitor/report their beliefs.

Conclusion: The results suggest belief-guided agency and meta-cognitive monitoring in LLMs, offering methods for studying such capabilities.

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [338] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill introduces learnable and evolvable memory skills for LLM agents, improving performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the rigidity and inefficiency of traditional memory systems in LLM agents.

Method: MemSkill utilizes a controller for skill selection, an executor for skill-guided memory generation, and a designer to refine and evolve memory skills.

Result: Experiments demonstrate MemSkill's improvement over baselines and strong generalization across settings.

Conclusion: MemSkill enables adaptive, self-evolving memory management for LLM agents, paving the way for more versatile systems.

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [339] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: The paper evaluates the limitations of Chain-of-Thought (CoT) reasoning in LLMs and proposes a divide-and-conquer (DAC) method utilizing reinforcement learning, achieving significantly better results.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning capabilities of large language models (LLMs) beyond the constraints of sequential Chain-of-Thought reasoning, particularly on complex tasks.

Method: An end-to-end reinforcement learning framework that decomposes a problem into subproblems, solves them, and integrates the solutions back, aligning the decomposition and solving steps within RL.

Result: The DAC-style framework improved model performance, outperforming CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition benchmarks.

Conclusion: DAC-style reasoning, supported by reinforcement learning, is a superior approach for maximizing LLM reasoning capabilities and scalability compared to CoT.

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [340] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: Re-TRAC enhances LLM-based research agents with cross-trajectory exploration, resulting in significant performance gains and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations in ReAct-based frameworks, which struggle with global awareness, branching, and efficient exploration due to their linear design.

Method: Re-TRAC introduces a framework summarizing states (evidence, uncertainties, failures, future plans) after each trajectory to inform subsequent actions, complemented by supervised fine-tuning for smaller models.

Result: Re-TRAC outperforms ReAct by 15-20% on BrowseComp, with smaller models achieving state-of-the-art results and reduced tool calls and token usage, reflecting more efficient exploration.

Conclusion: Re-TRAC reframes research as a progressive, reflection-driven process, demonstrating advantages over traditional methods in both performance and resource utilization.

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [341] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: This paper addresses multi-objective alignment for large language models (LLMs) using a novel method that resolves gradient conflicts and demonstrates superior performance without requiring explicit reward models.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of aligning LLMs when preferences involve multiple conflicting objectives, which naive aggregation methods fail to handle effectively.

Method: The authors introduce the RACO framework, which uses pairwise preference data and a clipped variant of conflict-averse gradient descent to align objectives. The framework avoids explicit reward models and ensures convergence to Pareto-critical points.

Result: The proposed framework demonstrates superior performance in experiments on multi-objective summarization and safety alignment tasks across various LLMs. It achieves better Pareto trade-offs compared to existing baselines.

Conclusion: The RACO framework improves alignment for LLMs with conflicting objectives by leveraging novel optimization techniques, avoiding explicit reward models, and achieving consistent performance improvements.

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [342] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: This paper introduces a novel dataset, EDU-CIRCUIT-HW, to evaluate the effectiveness of Multimodal Large Language Models (MLLMs) in interpreting STEM handwritten solutions and suggests ways to improve their reliability for educational applications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of authentic, domain-specific benchmarks for evaluating MLLMs' understanding of STEM student handwritten solutions and to improve their reliability in high-stakes educational settings.

Method: The study introduces EDU-CIRCUIT-HW, a dataset of 1,300+ university-level STEM handwritten solutions paired with expert-verified transcriptions and grading reports. It evaluates MLLMs on both recognition fidelity and auto-grading performance, and proposes preemptively rectifying errors using minimal human intervention.

Result: The evaluation reveals significant problems in MLLMs' recognition of handwritten STEM content, indicating that the models are not yet reliable for automated educational applications. However, detected error patterns were used to enhance system robustness with minimal human effort.

Conclusion: Current MLLMs are inadequate for reliably processing STEM handwritten solutions autonomously. However, targeted error rectification and human intervention approaches can make such AI systems more robust and practical in educational settings.

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [343] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: The paper introduces 'Simulate Anything', a simulation framework that uses multi-view videos and generative models to produce high-fidelity training data for embodied intelligence, addressing scalability challenges.


<details>
  <summary>Details</summary>
Motivation: Scalability in embodied intelligence is limited by scarce real-world interaction data, reliance on expensive tools, and significant simulation-to-reality gaps.

Method: They use 3D Gaussian Splatting to reconstruct high-fidelity environments from videos and integrate generative models for physical realism, enabling scalable simulation-based training without costly sensors or precise calibrations.

Result: Their method efficiently creates photorealistic, physically accurate simulations, producing data that allows Vision Language Action models to achieve strong zero-shot performance on downstream tasks.

Conclusion: The framework demonstrates that reconstruction-driven world modeling can bridge the visual and physical simulation gaps and make embodied intelligence training scalable and practical.

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [344] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: R3G is a three-step framework for retrieving images to enhance visual question answering, showing improved accuracy across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting and integrating relevant images effectively for enhanced reasoning in visual question answering (VQA).

Method: Proposes the R3G framework which includes reasoning to identify visual needs, coarse-to-fine image retrieval, and sufficiency-aware reranking of images.

Result: R3G improves accuracy across six multi-modal large language model architectures and nine scenarios, achieving state-of-the-art performance.

Conclusion: The sufficiency-aware reranking and modular reasoning approach complement each other, enabling better image selection and usage. Code and data are publicly available for reproduction.

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [345] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: The paper introduces HYPE-EDIT-1, a benchmark for evaluating image editing models on practical, reference-based tasks with detailed cost and success metrics.


<details>
  <summary>Details</summary>
Motivation: Current image editing model demonstrations often fail to reflect real workflows' costs and inefficiencies due to retries and reviews.

Method: HYPE-EDIT-1 was developed as a 100-task benchmark with binary pass/fail outcomes, standardized JSON schema, and cost-effectiveness metrics, combining model pass rates with human review time.

Result: Models present per-attempt pass rates between 34-83%, with effective editing costs spanning USD 0.66-1.42. Cheaper per-image models can end up being costlier due to retries and reviews.

Conclusion: Evaluating image editing models needs to account for retry rates, human reviews, and overall cost to reflect real-world efficiency, highlighting the value of HYPE-EDIT-1's metrics.

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [346] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: This paper introduces a Multi-Modal Deep Fusion Framework for predicting UAV trajectories by combining LiDAR and millimeter-wave radar data, achieving a 40% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of managing unauthorized UAVs in the low-altitude economy by improving trajectory prediction accuracy using multi-modal data.

Method: The authors propose a deep fusion network with modality-specific feature extraction networks and a Bidirectional Cross-Attention Mechanism for integrating LiDAR and radar data effectively.

Result: The proposed model, evaluated on the MMAUD dataset, improved trajectory prediction accuracy by 40% compared to the baseline. Ablation studies verified the contribution of loss functions and post-processing strategies.

Conclusion: The research demonstrates the efficiency of multi-modal data fusion for UAV trajectory prediction, offering an effective solution for managing unauthorized UAVs in low-altitude environments.

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [347] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: The paper introduces a pose-based machine-learning model to analyze hyperkinetic movement disorders (HMDs) from clinical videos.


<details>
  <summary>Details</summary>
Motivation: HMDs are challenging to clinically recognize and monitor objectively due to their fluctuating and overlapping nature, and current methods rely heavily on subjective assessments.

Method: A pose-based machine-learning framework was developed to extract keypoint time series from routine outpatient videos and compute kinematic descriptors such as statistical, temporal, spectral, and complexity features.

Result: The framework translates clinical video data into anatomically meaningful and quantifiable features.

Conclusion: The approach provides an objective, scalable method for distinguishing HMD phenotypes and improving their clinical evaluation.

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [348] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE is a new dataset designed for improving Vision Language Models (VLMs) in counting tasks with spatial constraints. It bridges the gap between simplistic and real-life datasets, demonstrating better generalization for out-of-distribution images.


<details>
  <summary>Details</summary>
Motivation: This paper aims to address the limitations of existing counting datasets, such as the lack of spatial composition and control over occlusions, which hinder generalization in Vision Language Models.

Method: The researchers introduce the SITUATE dataset and evaluate its effectiveness by fine-tuning Vision Language Models like Qwen VL 2.5 7B. They compare its performance against other benchmarks, such as Pixmo count and TallyQA.

Result: The SITUATE dataset improves generalization for out-of-distribution images, as models fine-tuned on it perform better on tests like Pixmo count compared to models fine-tuned on other datasets.

Conclusion: SITUATE is a valuable resource for enhancing the generalization capability of Vision Language Models in spatially constrained counting tasks, filling a critical gap between simplistic and real-world datasets.

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [349] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: This paper evaluates the impact of low-light conditions and automated image acquisition on the effectiveness of Presentation Attack Detection (PAD) systems, revealing significant performance declines in these scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring robust PAD performance in diverse real-world environmental and procedural conditions, particularly under low-light and auto-capture scenarios.

Method: A scenario test was conducted on commercial PAD systems to evaluate their performance under low-light conditions and automated image acquisition workflows.

Result: PAD systems showed a significant decline in performance under tested scenarios, with error rates increasing by a factor of about four in low-light and doubling in auto-capture workflows. Only one system reliably maintained strong performance with an error rate below 3%.

Conclusion: Diverse environmental testing is crucial to ensure reliable and consistent PAD system performance in real-world remote identity validation applications.

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [350] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: This paper proposes a novel model for improving remote sensing imagery analysis by integrating geospatial information with visual data, achieving better multimodal geospatial understanding.


<details>
  <summary>Details</summary>
Motivation: Existing models optimize semantic alignment between visual and textual data but lack geospatial layer reasoning, limiting their capabilities in geospatial applications.

Method: The study introduces a geospatial embedding mechanism to align diverse geospatial data with image patches and designs a guided attention module for dynamic integration of multimodal information, assigning specific roles to attention heads for better interpretability.

Result: Experimental results show the model surpasses pretrained geospatial foundation models in tasks like predicting disease prevalence.

Conclusion: The framework effectively enhances multimodal geospatial understanding, proving useful in applications requiring integration of geospatial and visual data.

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [351] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: This study analyzed how space allowance affects play behavior in dairy calves, finding that optimal play occurs at 8-10 m2 per calf, and developed automated computer vision for scalable welfare monitoring.


<details>
  <summary>Details</summary>
Motivation: Understand the effect of space allowance on dairy calves' play behavior under commercial conditions, focusing on intermediate-to-high allowances.

Method: Observed play behavior in 60 calves across 14 farms, analyzed video using a detailed ethogram and statistical models, and developed a computer vision pipeline.

Result: Play was highest at 8-10 m2 per calf and lowest at 6-8 m2 and 12-14 m2. Automated computer vision achieved high accuracy for active play detection.

Conclusion: A space allowance of 8-10 m2 per calf optimizes welfare and is economically practical; automated systems allow scalable welfare assessments.

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [352] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: The paper describes an AI-enabled platform for burn assessment using 3D surface reconstruction, deep learning segmentation, and photogrammetry to provide objective and scalable metrics for burn treatment and monitoring.


<details>
  <summary>Details</summary>
Motivation: Conventional burn assessment methods like visual inspection and 2D photography are subjective, limiting accurate, reproducible, and longitudinal burn evaluations.

Method: The platform integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation to reconstruct 3D burn surfaces and compute metrics like TBSA, depth, and volumetric changes using consumer-grade cameras.

Result: The system successfully reconstructs burn geometries, tracks healing progression through spatial alignment of successive reconstructions, and supports structured clinical workflows with automated reporting.

Conclusion: This scalable, non-invasive platform provides objective and geometry-aware burn assessment and decision-support capabilities for clinical and outpatient care.

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [353] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: The paper introduces 1S-DAug, a novel test-time augmentation technique for few-shot learning (FSL), leveraging generative methods to improve predictions from limited data.


<details>
  <summary>Details</summary>
Motivation: To enhance model generalization in FSL tasks where only a few labeled examples are available, addressing the ineffectiveness of traditional test-time augmentations in such settings.

Method: 1S-DAug combines geometric perturbations with noise injection and denoising diffusion, generating diverse image variants from a single example, which are then aggregated with the original for improved FSL predictions. This approach is training-free and model-agnostic.

Result: 1S-DAug boosts FSL performance across four datasets without any model parameter updates, including a significant improvement on the miniImagenet 5-way-1-shot benchmark.

Conclusion: 1S-DAug demonstrates the potential of advanced augmentation techniques to enhance FSL, offering a standalone, universal plugin for FSL improvement without altering model parameters.

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [354] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: A novel, real-time algorithm for detecting event-camera data clusters offers linear complexity and operates asynchronously.


<details>
  <summary>Details</summary>
Motivation: Current clustering methods for event camera data lack real-time efficiency for small, tempo-spatial event clusters.

Method: An asynchronous and hierarchical agglomerative clustering algorithm utilizing the event camera's data structure for efficient $O(n)$ processing.

Result: The algorithm achieves real-time detection with complexity $O(n$) and is independent of pixel array dimensions.

Conclusion: The approach efficiently handles event camera data by optimizing for both computational simplicity and real-time performance.

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [355] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: The study introduces a conversational code-generating agent for Earth Observation analysis that turns natural language into auditable Python workflows, achieving superior accuracy in specific use cases compared to general-purpose AIs.


<details>
  <summary>Details</summary>
Motivation: Despite advances in computer vision, Earth Observation remains inaccessible to laypersons and overly reliant on black-box systems, highlighting the need for transparent and reproducible solutions.

Method: The paper proposes a code-generating conversational agent using an API for classification, segmentation, detection, and geospatial tasks. The agent generates Python workflows to process queries, tested at tool, agent, and task levels.

Result: The agent outperforms general-purpose AI baselines in accuracy for two use cases: land-composition mapping (64.2% vs. 51.7%) and post-wildfire analysis (50% vs. 0%).

Conclusion: The framework enhances EO analysis by making processes transparent, auditable, and reproducible, improving accessibility and accuracy for lay users.

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [356] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: This paper introduces VDE Bench, a human-annotated benchmark for multilingual and complex visual document editing tasks, addressing gaps in existing models like AnyText and TextCtrl.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in editing dense and multilingual visual documents, an area neglected by existing approaches that primarily focus on sparse English-language scenarios.

Method: It proposes VDE Bench, incorporating a high-quality dataset covering complex and multilingual visual documents, and uses a decoupled evaluation framework assessing text modification accuracy.

Result: The benchmark enables the evaluation of state-of-the-art image editing models, with manual checks confirming consistency between human and automated evaluation results.

Conclusion: VDE Bench is the first systematic benchmark targeting multilingual and complex visual document editing, filling a crucial gap in the image editing research landscape.

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [357] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: The paper introduces a context-aware autoencoder for maritime anomaly detection, improving the accuracy and efficiency of identifying vessel-specific anomalies.


<details>
  <summary>Details</summary>
Motivation: To address the limited effectiveness of traditional autoencoders in detecting collective and contextual anomalies in maritime vessel traffic surveillance, where anomaly detection relies heavily on vessel-specific contexts derived from AIS messages.

Method: The study proposes a context-aware autoencoder that incorporates context-specific thresholds, improving anomaly detection accuracy and computational cost. It compares four variants of this model with a conventional autoencoder using a case study of fishing status anomalies.

Result: Results indicated that context significantly impacts reconstruction loss and anomaly detection. The proposed context-aware autoencoder outperformed traditional models in detecting anomalies in maritime time series data.

Conclusion: The context-aware autoencoder shows promise in improving the reliability and precision of maritime anomaly detection by leveraging vessel-specific contexts and setting context-specific thresholds.

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [358] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: This paper introduces D3R-Net, a new framework for unsupervised anomaly detection that improves defect segmentation accuracy using a self-supervised 'healing' task and frequency-aware regularization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional reconstruction-based methods in unsupervised anomaly detection, which struggle with oversmoothing and fail to highlight subtle high-frequency defects.

Method: D3R-Net combines spatial and frequency domain consistency during training by using a self-supervised 'healing' task with synthetically corrupted images and integrating Fast Fourier Transform (FFT) magnitude loss with optional SSIM regularization.

Result: D3R-Net improves performance on the MVTec AD benchmark, significantly increasing metrics like PRO AUC and pixel ROC AUC, while maintaining high speeds (20 FPS) and a lightweight architecture.

Conclusion: The proposed framework demonstrates improved segmentation accuracy and efficiency, offering a practical solution for unsupervised anomaly detection in manufacturing settings.

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [359] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: The paper introduces POVNet+, a multimodal deep learning model for recognizing and assisting with multiple activities of daily living (ADLs) in socially assistive robots.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of autonomous socially assistive robots in perceiving and assisting with a variety of ADLs, enabling proactive and effective interventions.

Method: Developing POVNet+, which combines ADL and motion embedding spaces to classify known, unseen, and atypical ADLs. Novel user state estimation is incorporated into the model and tested with human-robot interaction experiments.

Result: POVNet+ demonstrates higher accuracy compared to state-of-the-art methods in ADL classification and successfully initiates adaptive interactions in a cluttered, multi-user environment.

Conclusion: POVNet+ enables socially assistive robots to better assist users by accurately recognizing diverse ADLs under real-world conditions, offering improved adaptability and performance.

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [360] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: SCANNER is a new Test-Time Adaptation framework specifically developed for Hate Video Detection, addressing challenges from evolving and ambiguous hateful content.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of detecting evolving hateful content, which evades existing detection mechanisms due to semantic drift, while maintaining consistent core targeting on characteristics like gender and race.

Method: SCANNER leverages stable cores via a centroid-guided alignment mechanism, mitigates outlier impact with adaptive alignment, and incorporates diversity regularization to improve semantic richness during adaptation.

Result: SCANNER achieved an average 4.69% improvement in Macro-F1 over baseline models, demonstrating its effectiveness in detecting hateful content.

Conclusion: The SCANNER framework is effective in adapting to severe semantic drifts in hateful content detection, addressing the limitations of existing methods by focusing on stable cores and improved adaptation techniques.

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [361] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: The paper proposes an efficient model (LLaVA-FA) for compressing large multimodal models using joint low-rank and quantization methods in the frequency domain, achieving better performance with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models face challenges in practical deployment due to high computational and memory demands. Effective compression methods are needed to reduce these costs without degrading performance.

Method: The authors propose LLaVA-FA, which uses joint low-rank plus quantization approximation in the frequency domain, utilizing Fourier transform properties. They introduce PolarQuant for quantizing complex matrices and an optional diagonal calibration (ODC) scheme to remove the dependency on large calibration data.

Result: LLaVA-FA outperforms existing efficient multimodal models on multiple benchmarks with fewer activated parameters and reduced computational costs.

Conclusion: The proposed method provides a more compact, accurate, and efficient compression approach for large multimodal models, making them practical for real-world applications.

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [362] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: The paper addresses challenges in class-incremental learning (CIL) with Vision Transformers (ViTs), proposing scalable solutions to overcome the computational bottlenecks in classifier reconstruction using Low-Rank Factorized RGDA and a Hopfield-based Distribution Compensator.


<details>
  <summary>Details</summary>
Motivation: Current methods for CIL with ViTs rely on computationally expensive SGD-based classifier reconstruction, which is infeasible for large-scale tasks. A faster alternative is needed that maintains accuracy while being scalable.

Method: The authors propose Low-Rank Factorized RGDA (LR-RGDA), which reduces inference complexity by leveraging the Woodbury matrix identity and a low-rank decomposition. They also introduce Hopfield-based Distribution Compensator (HopDC) to adjust class statistics for backbone updates using unlabeled data.

Result: The proposed framework significantly reduces computational complexity and achieves state-of-the-art performance on diverse CIL benchmarks, proving its scalability and effectiveness.

Conclusion: The innovative combination of LR-RGDA and HopDC offers a scalable, efficient, and accurate solution for class-incremental learning with Vision Transformers, making it suitable for large-scale applications.

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [363] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: This paper presents the winning approaches for robust visual foraging competitions, showing how simpler architectures excel in visual robustness while deeper models achieve better neural alignment.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in visual robustness and neural alignment, as biological vision systems outperform artificial agents in these domains.

Method: Developed a lightweight two-layer CNN with Gated Linear Units (Track 1) and a ResNet-like architecture with 16 convolutional layers (Track 2). Conducted extensive training and analysis, including ablation and failure analyses.

Result: Achieved 95.4% for visual robustness and top-1 in neural alignment with designs showing non-linear impacts of training steps on performance.

Conclusion: Challenged the assumption that complexity guarantees better visuomotor performance, proposing practical insights for robust visual agents.

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [364] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: The study explores the use of AI applied to infrared thermal images to estimate breast tissue density, offering a non-ionizing alternative to mammography.


<details>
  <summary>Details</summary>
Motivation: To provide a non-ionizing imaging method for breast density assessment, addressing risks associated with X-ray mammography while improving patient experience.

Method: Proposing 'DensiThAI,' a multi-view deep learning framework leveraging thermal images, evaluated on a dataset of 3,500 women with mammography-derived labels as reference.

Result: DensiThAI achieved a mean AUROC of 0.73 across 10 splits, with statistically significant separation between density classes across all tests.

Conclusion: Thermal imaging, combined with AI, holds promise as a non-invasive, efficient approach for assessing breast tissue density, with broad implications for patient care and workflow.

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [365] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: The paper introduces NGFF, a framework for generating interactive, physics-realistic 4D videos, outperforming earlier methods in speed and robustness, and backed by a large dataset (GSCollision).


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing video generation models in producing physically plausible videos and address computational inefficiencies and lack of robustness in prior approaches.

Method: The proposed Neural Gaussian Force Field (NGFF) integrates 3D Gaussian perception with physics-based modeling to generate physically realistic 4D videos efficiently. It also introduces the GSCollision dataset to aid in better training.

Result: NGFF achieves physically grounded video predictions with two orders of magnitude faster processing than earlier Gaussian simulators while demonstrating robustness and generalization in physical reasoning.

Conclusion: The NGFF framework marks a significant step towards creating physics-grounded AI models for video prediction by combining efficient techniques and a comprehensive dataset.

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [366] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: This paper proposes SDCM, a fusion framework for 4D radar-vision-based 3D object detection in IoV, addressing sparse radar data and degraded vision data issues.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of sparse 4D radar point clouds and degraded vision data (low-light, occlusion) in IoV's radar-vision-based 3D object detection.

Method: The SDCM framework includes: (1) Simulated Densifying (SimDen) to create dense radar data, (2) Radar Compensatory Mapping (RCM) to enhance degraded vision data using radar, and (3) Mamba Modeling Interactive Fusion (MMIF) for reducing heterogeneity and ensuring interaction between modalities.

Result: SDCM achieves superior performance, with improved accuracy, lower parameter usage, and faster inference speeds on datasets such as VoD, TJ4DRadSet, and Astyx HiRes 2019.

Conclusion: SDCM effectively addresses challenges in radar-vision-based 3D detection and offers practical benefits for IoV scenarios with a balance of performance and computational efficiency.

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [367] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: This paper evaluates the use of histopathological foundation models for predicting HRD biomarker scores, demonstrating their advantage over baseline features and their potential for precision oncology.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to explore how foundation models in computational pathology can be applied to regression tasks, particularly for predicting the HRD score, a critical biomarker for personalized cancer treatment.

Method: Five state-of-the-art histopathological foundation models were incorporated into a multiple instance learning framework to extract patch-level features from WSIs. These features were compared against contrastive learning-based features to predict HRD scores in breast, endometrial, and lung cancer cases. A distribution-based upsampling strategy was also introduced to address target imbalance.

Result: The study showed that foundation models consistently outperformed baseline methods in predictive accuracy and generalizability. Target imbalance issues were mitigated by the proposed upsampling strategy, and the results revealed systematic differences among the foundation models.

Conclusion: Large-scale histopathological pretraining enhances regressive biomarker prediction, with benefits in precision, transferability, and clinical usability for oncology-based AI applications.

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [368] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: The paper proposes HPPI-Net, a resource-aware, hierarchical network for on-device Human Activity Recognition (HAR) with high accuracy and low resource usage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconciling accuracy with computational constraints in edge applications for on-device pattern recognition.

Method: HPPI-Net is a hierarchical network using multi-spectral fusion and interpretable modules. It includes features like FFT-based preliminary processing, parallel LSTM encoders, and efficient structures like ECA and DSC for improved performance.

Result: Deployed on an ARM Cortex-M4 microcontroller, HPPI-Net achieved 96.70% accuracy, utilizing only 22.3 KiB RAM and 439.5 KiB ROM, outperforming other methods in both accuracy and resource efficiency.

Conclusion: HPPI-Net offers a practical, efficient, and accurate solution for on-device HAR, particularly suited for memory-constrained edge devices like wearables and smart home systems.

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [369] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: The paper presents a compressed-domain video tracking model that is significantly faster while maintaining good accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a lightweight and efficient object tracking system that can operate in real-time on large video monitoring systems without the computational burden of full RGB video decoding.

Method: The approach involves using motion vectors and transform coefficients from compressed video data. This avoids the need for decoding RGB frames and allows the model to directly propagate object bounding boxes across frames.

Result: The method achieves a computational speed-up of up to 3.7x with only a minor 4% decrease in mAP@0.5 when compared to the RGB baseline on MOTS15/17/20 datasets.

Conclusion: Compressed-domain motion modeling is efficient and promising for real-time video analytics, especially in large-scale monitoring systems.

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [370] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: This paper introduces YOLOE-26, a framework for open-vocabulary instance segmentation, leveraging YOLO26 architecture and text/visual prompting for real-time segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework that extends YOLO's capabilities to open-vocabulary tasks while maintaining efficiency and seamless integrations for real-world environments.

Method: Integrates YOLO26 with open-vocabulary strategies using components like object embedding heads, RepRTA for text prompting, SAVPE for visual prompts, and multi-task optimization training.

Result: YOLOE-26 demonstrates scalable, efficient, and accurate open-vocabulary instance segmentation across various prompting modalities and sizes.

Conclusion: YOLOE-26 effectively combines real-time efficiency with open-vocabulary flexibility, making it practical for dynamic real-world segmentation tasks.

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [371] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: The paper introduces an SPCL framework for cardiac image segmentation, enhancing boundary differentiation and intra-class representation using novel contrastive methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of representation contamination at boundaries in cardiac image segmentation, thereby achieving clearer intra-class characterization and boundary precision.

Method: The framework uses a novel 'unconcerned sample' concept for clearer distinctions within the same class, and a boundary contrastive loss for enhancing boundary representation discrimination.

Result: The SPCL framework demonstrated superior segmentation quality and boundary precision, outperforming existing methods on public cardiac datasets.

Conclusion: The study validates SPCL's capacity to notably improve cardiac image segmentation with theoretical and experimental backing, providing a novel tool for precise boundary recognition.

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [372] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: The paper proposes a noise--frequency continuation framework to improve diffusion posterior sampling for inverse problems, enhancing fine detail recovery and measurement consistency.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion posterior sampling often fails at recovering fine details and suffers artifacts due to weak coupling of measurement terms with noise levels.

Method: Introduced a noise--frequency continuation approach with stabilized posterior sampling, band-limited likelihood guidance, and multi-resolution consistency.

Result: Achieved state-of-the-art performance across super-resolution, inpainting, and deblurring, with a 5 dB PSNR improvement for motion deblurring.

Conclusion: The proposed method effectively addresses shortcomings of traditional diffusion sampling approaches, enhancing reliability and detail recovery in various inverse problems.

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [373] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: This paper introduces CamReasoner, a novel framework for camera dynamics understanding, focusing on structured reasoning instead of superficial classification, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome current shortcomings in multimodal models, which rely on black-box classification prone to errors in understanding physically distinct motions using superficial patterns.

Method: The method revolves around the Observation-Thinking-Answer paradigm that incorporates explicit reasoning using spatio-temporal cues, supported by Reinforcement Learning and a Large-scale Inference Trajectory Suite for training.

Result: CamReasoner outperforms existing methods by grounding inferences in physical geometry, suppressing hallucinations, and scoring state-of-the-art on multiple benchmarks.

Conclusion: The work highlights the importance of structured reasoning in video spatial intelligence, offering a robust way to align inferences with cinematic and physical logic.

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [374] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: This paper examines the limits of current image inpainting detection methods, showing how they rely on global artifacts while missing localized alterations, and proposes a more precise evaluation strategy.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based inpainting allows for realistic image editing, which poses detection challenges. Existing detectors overly depend on global artifacts rather than investigating locally manipulated content.

Method: The authors propose 'Inpainting Exchange (INP-X)', a method that isolates synthesized content by restoring original pixels outside edited regions. They also provide a new dataset containing original, inpainted, and exchanged images to assess this phenomenon.

Result: State-of-the-art and commercial detection methods dramatically drop in accuracy under INP-X, revealing a critical limitation. The authors also show that training models with their dataset improves detection generalization and localization.

Conclusion: This paper underscores the need for content-aware detection techniques in image inpainting. The findings urge a shift from artifact-based detection to models that can localize and identify synthesized content more effectively.

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [375] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: The paper introduces SemiEarth, a model leveraging vision-language models for improving semi-supervised semantic segmentation in remote sensing, solving issues related to poor-quality pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problems of low-quality pseudo-labels, especially in teacher-student frameworks, for semi-supervised semantic segmentation in remote sensing.

Method: The method involves using a Vision-Language Model Pseudo-Label Purifying (VLM-PP) structure to refine pseudo-labels generated by the teacher network, ensuring better guidance for the student model. It also allows category corrections in low-confidence pseudo-labels.

Result: Extensive experiments on multiple remote sensing datasets demonstrate that SemiEarth achieves state-of-the-art performance with improved pseudo-label quality and enhanced interpretability.

Conclusion: SemiEarth not only improves semi-supervised semantic segmentation in remote sensing with better pseudo-labels but also provides a high-performing and interpretable solution.

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [376] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: This paper introduces the Multi-Hop Visual Chain of Reasoning (VCoR) framework for unsupervised deformable image registration, providing enhanced interpretability and reliability while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Unsupervised deformable image registration faces challenges in aligning complex anatomical structures without reference labels, causing issues in interpretability and clinical trust in existing deep learning methods.

Method: The framework uses a multi-hop reasoning strategy with Localized Spatial Refinement (LSR) and Cross-Reference Attention (CRA) modules to iteratively refine deformation fields, ensuring anatomical consistency and producing intermediate visualizations with built-in uncertainty estimation.

Result: The method achieves high accuracy on public datasets (DIR-Lab 4D CT and IXI T1-weighted MRI) and provides transparent intermediate predictions, visualizations, and confidence measures during registration.

Conclusion: The VCoR framework enhances the robustness, transparency, and clinical viability of unsupervised medical image registration by offering interpretability alongside competitive accuracy.

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [377] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: The paper introduces a custom CNN model to diagnose pneumonia from chest X-rays with high accuracy and efficiency, addressing challenges in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional pneumonia diagnosis relies on manual interpretation of chest X-rays, often constrained by expert limitations, inter-observer variability, and radiologist shortages. Automated solutions are necessary for timely intervention.

Method: The paper employs a custom Convolutional Neural Network (CNN) optimized with depth-wise separable convolution, preprocessing techniques like CLAHE, and geometric augmentation to improve precision and image generalization.

Result: The model is tested on a dataset of 5863 chest X-rays, demonstrating high precision and computational efficiency in diagnosing pneumonia.

Conclusion: The proposed system successfully offers a fast, accurate, and computationally efficient diagnostic tool for pneumonia, reducing reliance on limited expert resources and manual methods.

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [378] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: The paper introduces the MFM-Geom model, a geometric multimodal Foundation Model that combines bp-MRI and clinical data for prostate cancer diagnosis, outperforming other methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the subjectivity of expert interpretation in prostate cancer diagnosis and the limitations of existing imaging-based models, particularly their lack of integration with clinical context and data scarcity.

Method: Developed a multimodal Foundation Model (MFM-Geom) that encodes information from bp-MRI imaging and clinical reports using symmetric positive definite (SPD) matrices and Riemannian deep learning for better representation learning.

Result: MFM-Geom outperformed baseline methods by 8.3% (AUC-PR of 90.67) using only 10% training data and demonstrated robustness with an AUC-PR of 90.6 on an external dataset.

Conclusion: Integrating imaging and clinical context using Riemannian deep learning in a multimodal foundation model significantly improves diagnosis accuracy and generalizes well to external data.

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [379] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: A mobile app was developed to help smallholder cacao farmers in the Philippines by providing cacao disease identification and management solutions through an offline deep learning-based system, achieving high accuracy rates.


<details>
  <summary>Details</summary>
Motivation: Smallholder cacao farmers in the Philippines face challenges such as outdated techniques, limited access to agricultural resources, and vulnerability to pests and diseases, necessitating accessible technological tools for improved farming practices.

Method: The study developed a mobile app integrating a deep learning model for accurate cacao disease identification. The app functions offline, targeting cacao farmers in remote areas.

Result: The disease identification model achieved 96.93% validation accuracy, while the black pod infection level detection model achieved 79.49% accuracy. Field testing showed 84.2% agreement with expert assessments.

Conclusion: The mobile app empowers smallholder farmers with accessible technology to improve cacao crop health, enabling better disease diagnosis and management in resource-limited, remote farming areas.

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [380] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: The paper proposes CAPA, a method aimed at improving computational efficiency in large vision-language models by selectively pruning visual tokens and approximating FFN computations, leading to better performance robustness.


<details>
  <summary>Details</summary>
Motivation: The growing computational expense of visual tokens in vision-language models necessitates identifying tokens and operations that can be removed without degrading performance. Existing methods like attention score reliance are inadequate for optimal token selection.

Method: The authors introduced Attention Contribution, a metric combining attention probabilities and value vector magnitudes, to identify token importance. They also designed CAPA, which uses this metric to prune unnecessary tokens and to reduce FFN computations through linear approximations.

Result: CAPA enables efficient pruning and computation reduction, demonstrating strong efficiency-performance trade-offs and better robustness across multiple benchmarks.

Conclusion: This study presents a meaningful advancement in optimizing vision-language models by addressing token redundancy and FFN inefficiencies with CAPA, improving both computational efficiency and model robustness.

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [381] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: This paper introduces SANEval, a novel benchmark for evaluating text-to-image models on compositional tasks, addressing limitations in current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with faithfully interpreting complex prompts, especially regarding multiple objects, attributes, and spatial relationships. Existing evaluation methods have significant limitations in granularity, vocabulary restrictions, and lack of interpretability.

Method: The authors introduce SANEval, a benchmark utilizing large language models (LLMs) for prompt understanding and open-vocabulary object detection, enabling scalable and robust evaluation of compositional adherence without being constrained to fixed vocabularies.

Result: SANEval shows superior alignment with human assessments, as demonstrated through experiments on six state-of-the-art T2I models. The metric achieves improved Spearman rank correlation compared to existing benchmarks, particularly in tasks like attribute binding, spatial relations, and numeracy.

Conclusion: SANEval provides a more reliable and interpretable framework for evaluating compositional abilities in T2I models, addressing critical gaps in current benchmarks. The release of its dataset and evaluation pipeline aims to support future research in this domain.

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [382] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: CSC is a self-supervised framework designed for clustering incomplete data via contrastive learning and sparse subspace clustering.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of subspace clustering methods in handling real-world scenarios where data entries are missing.

Method: CSC generates masked views from partially observed inputs and trains a neural network using contrastive learning. Resultant embeddings are clustered using sparse subspace clustering.

Result: CSC consistently outperformed classical and deep learning baselines on six benchmark datasets, showing robustness to missing data and scalability.

Conclusion: Contrastive self-supervised learning provides an effective approach for clustering incomplete data in real-world scenarios.

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [383] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: The paper introduces 'World-Shaper,' a geometry-aware framework for editing panoramic images directly in equirectangular projection (ERP), offering superior geometric consistency and editing fidelity compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the editing of panoramic images which existing methods fail to handle due to their inability to model spatial structure and maintain global consistency.

Method: The proposed method reformulates panoramic editing in the ERP domain, uses a generate-then-edit paradigm to create paired data, and adopts a geometry-aware learning strategy incorporating shape supervision and internalized panoramic priors.

Result: Experiments on their new benchmark, PEBench, show that their method surpasses state-of-the-art approaches in geometric consistency, editing fidelity, and text controllability.

Conclusion: World-Shaper offers a unified editing framework for coherent and flexible 360° visual world creation, bridging generation and editing with a geometry-aware design.

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [384] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: The paper introduces PLACID, a generative AI framework improving multi-object compositing by leveraging temporal priors from video-based diffusion models to achieve precise layouts and identity preservation.


<details>
  <summary>Details</summary>
Motivation: Current generative models struggle with preserving object identity, background fidelity, and layout control in multi-object compositing tasks.

Method: PLACID uses a pretrained image-to-video diffusion model alongside a synthetic data curation strategy with temporal priors for object placement and layout coherence guided by text.

Result: PLACID demonstrates superior identity preservation, accurate layout control, and visually appealing composites, outperforming current methods as validated by quantitative metrics and user studies.

Conclusion: PLACID sets a new benchmark in multi-object compositing, resolving key issues in object detail preservation and layout synthesis.

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [385] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: The paper addresses the issue of temporal drift in auto-regressive video generation and proposes an inference-time method to remove corrupted latent tokens.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the problem of temporal drift in auto-regressive video generation caused by inference-time error propagation and corrupted latent conditioning tokens.

Method: The proposed method identifies and removes unstable latent tokens during inference based on deviation from prior batches to prevent temporal drift, without modifying the model or training.

Result: The method improves long-horizon temporal consistency in video generation while maintaining simple implementation.

Conclusion: Explicitly removing corrupted latent tokens from auto-regressive contexts effectively mitigates temporal drift without altering the model architecture or training process.

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [386] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind introduces a benchmark to evaluate Multimodal Large Language Models (MLLMs) on fine-grained spatio-temporal reasoning using video pairs with identical content but differing in temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Multimodal Large Language Models in understanding and reasoning about temporal dynamics in videos, as their capabilities in static visual semantics exceed their temporal reasoning skills.

Method: TimeBlind uses a minimal-pairs paradigm, ensuring identical visual content across video pairs to isolate temporal dynamics, and neutralizes language prior biases with complementary questions to assess fine-grained temporal understanding at three levels.

Result: Evaluation of over 20 state-of-the-art MLLMs showed poor performance on temporal reasoning, with an instance accuracy of 48.2% (best model), significantly lower than human accuracy of 98.2%.

Conclusion: TimeBlind identifies substantial reliance of current MLLMs on static visual elements, revealing limitations in temporal logic reasoning and providing a diagnostic benchmark critical for advancing video understanding technologies.

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [387] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: The paper explores computer vision and its ties to Cognitive Science using Bayes Decision Theory, discussing both Bayesian and Deep Neural Network approaches.


<details>
  <summary>Details</summary>
Motivation: To understand computer vision's relationship with Cognitive Science and provide a unifying theoretical perspective using Bayes Decision Theory.

Method: Theoretical analysis of computer vision concepts through the lens of Bayes Decision Theory, focusing on Bayesian and Deep Neural Network approaches.

Result: BDT offers a framework to relate strengths and weaknesses of Bayesian and Deep Neural Networks, suggesting opportunities for integration.

Conclusion: Bayes Decision Theory provides insights into computer vision's key concepts and suggests integrating Bayesian and neural approaches for advancements.

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [388] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze is a benchmark for assessing Vision-Language Models' (VLMs) ability to ground sequential reasoning in visual evidence, exposing issues like hallucination.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration in the capability of VLMs to ground sequential reasoning chains within actual visual inputs and combat hallucination.

Method: Created LogicGaze, using datasets from ShareGPT4Video and Flickr30k, introducing contradictory yet linguistically plausible perturbations. Evaluated using Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection protocols.

Result: Revealed significant vulnerabilities in existing state-of-the-art VLMs, such as Qwen2.5-VL-72B, in validating reasoning against visual inputs.

Conclusion: LogicGaze emphasizes the need for reliable and grounded multimodal reasoning in VLMs, providing valuable resources to improve model robustness.

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [389] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: The paper introduces SAM2CT, a machine learning model for converting sparse radiologist-made annotations into 3D CT segmentations using promptable segmentation techniques.


<details>
  <summary>Details</summary>
Motivation: Current 3D segmentation datasets for CT imaging are limited due to costly manual annotations by radiologists, despite the availability of sparse annotations recorded during routine clinical practice.

Method: SAM2CT leverages existing sparse annotations like arrow and line inputs from PACS systems through the use of promptable segmentation models. It introduces a new memory encoding strategy, Memory-Conditioned Memories (MCM), for 3D medical datasets.

Result: On public benchmarks, SAM2CT achieves Dice similarity coefficients of 0.649 (arrow prompts) and 0.757 (line prompts). It generates clinically acceptable 3D segmentations for 87% of cases from real-world clinical data and shows strong zero-shot capabilities.

Conclusion: The SAM2CT method enables scalable and efficient creation of 3D segmentation datasets from historical annotations, significantly advancing data availability and quality for machine learning in CT imaging.

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [390] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: The paper evaluates the robustness of automated vehicle (AV) perception systems under adverse conditions using an ensemble of models. It highlights significant performance drops due to poor lighting, occlusions, and distance.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability of AV perception systems under various challenging conditions for accurate decision-making and maneuvers.

Method: Predictive sensitivity quantification using model ensembles to capture variability and disagreement in adverse scenarios. Evaluates models like YOLO, DETR, and RT-DETR under simulated and real-world conditions.

Result: Diminished lighting (fog, low sun altitude) and adversarial conditions (occlusions, long distances) significantly degrade perception performance.

Conclusion: Enhanced strategies and benchmarks are essential to improve robustness, especially under combinations of weather, object occlusion, and distance challenges.

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [391] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: The paper introduces a Synergistic Neural Agents Network (SynerNet) to enhance performance in Vision-Language Models (VLMs) by addressing cross-modal alignment issues in Out-of-Distribution (OOD) settings.


<details>
  <summary>Details</summary>
Motivation: The research addresses the problem of cross-modal alignment degeneration in VLMs, particularly in handling OOD concepts, which is critical for improving their robustness and adaptability.

Method: The method employs four computational units (visual perception, linguistic context, nominal embedding, global coordination) with a message-propagation protocol, along with frameworks for latent space acquisition, semantic context adaptiveness, and equilibrium mechanisms.

Result: Experiments on the VISTA-Beyond benchmark demonstrate noticeable improvements in model precision (1.2% to 5.4%) for both few-shot and zero-shot tasks across various domains.

Conclusion: SynerNet improves few-shot and zero-shot adaptability and robustness in VLMs, providing effective solutions for handling OOD concepts and enhancing cross-modal alignment.

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [392] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: The paper identifies "Attention Distraction" (AD) as a significant failure mode in Retrieval-Augmented Generation (RAG) for Vision-Language Models and proposes MAD-RAG to resolve it for improved performance on knowledge-based VQA tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in current RAG approaches for vision-language tasks, particularly issues where retrieved textual context distracts attention from relevant visual regions, resulting in errors even when the context includes correct answers.

Method: The paper introduces MAD-RAG, a training-free strategy that separates visual grounding from text integration using a dual-question setup and attention mixing, ensuring image-conditioned evidence is preserved.

Result: MAD-RAG improves performance on VQA datasets OK-VQA, E-VQA, and InfoSeek, yielding gains of up to 4.76%, 9.20%, and 6.18% compared to vanilla RAG. It also resolves up to 74.68% of failure cases with minimal computational costs.

Conclusion: MAD-RAG effectively mitigates the "Attention Distraction" issue in RAG, leading to significant improvements across multiple datasets without needing additional training, showcasing its practical advantage in knowledge-based VQA tasks.

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [393] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: AdaFuse is an adaptive multimodal fusion framework for lung cancer risk prediction that utilizes reinforcement learning to dynamically select which modalities to include.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining whether specific data modalities should be used for a particular patient in multimodal systems, enhancing diagnostic efficiency and accuracy.

Method: A sequential decision-making process is developed, leveraging reinforcement learning to decide on modality inclusion based on patient-specific data, allowing early termination when adequate information is acquired.

Result: AdaFuse demonstrated a higher AUC (0.762) compared to single-modality (0.732) and other adaptive fusion methods, while reducing computational cost on the NLST dataset.

Conclusion: Reinforcement learning can enable personalized, efficient, and accurate multimodal fusion strategies, highlighting a transition from uniform to adaptive diagnostic processes in medical imaging.

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [394] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: This paper presents MASC, a unified reinforcement learning framework that optimizes both metal-aware k-space sampling and artifact correction to improve accelerated MRI imaging quality.


<details>
  <summary>Details</summary>
Motivation: Traditional methods treat metal artifact reduction and accelerated MRI acquisition separately, leading to inefficiencies and suboptimal outcomes. This paper aims to address both problems concurrently for better diagnostic imaging results.

Method: The authors utilize reinforcement learning with an artifact-aware Proximal Policy Optimization (PPO) agent for k-space sampling and a U-Net-based MAR network for artifact corrections. They employ an end-to-end training scheme using a supervised paired dataset with physics-based simulations.

Result: MASC achieved superior reconstruction quality compared to traditional methods and generalized well to clinical MRI data. End-to-end training further boosted its performance over pre-trained models.

Conclusion: Jointly optimizing metal artifact correction and MRI acquisition strategies through reinforcement learning is an effective approach, achieving improved diagnostic imaging quality. MASC’s method and code are made publicly accessible for further exploration.

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [395] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: This paper introduces ReLAPSe, a reinforcement learning-based framework that efficiently restores concepts in text-to-image diffusion models after unlearning, overcoming computational and reasoning-based limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unauthorized concept removal from diffusion models, while mitigating latent information leakage and inefficiencies in existing exploitation techniques.

Method: ReLAPSe applies reinforcement learning using verifiable rewards (RLVR) that leverage noise prediction loss in diffusion models to align textual prompts with latent visual residuals for efficient concept restoration.

Result: The proposed framework demonstrated near-real-time recovery of fine-grained identities and styles across various unlearning methods, with transferable restoration strategies.

Conclusion: ReLAPSe provides a scalable, efficient tool for rigorous testing and red-teaming of unlearned diffusion models, highlighting its potential in a sensitive experimental setting.

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [396] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: The paper introduces a machine learning framework to rate image-caption pairs by modeling comparative judgments rather than direct ratings.


<details>
  <summary>Details</summary>
Motivation: Rating captions for images can be subjective and time-consuming, but humans can more easily compare two captions to determine which better matches an image.

Method: A comparative learning model using visual features (ResNet-50) and text features (MiniLM) is trained alongside a regression model for direct ratings, evaluated on the VICR dataset.

Result: The regression model achieves better initial performance but comparative learning steadily approaches its accuracy with increasing data. Human studies show that comparative annotations are faster and more consistent.

Conclusion: Comparative learning reduces annotation costs while effectively capturing human preferences, making it a viable alternative to direct rating models.

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [397] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: This paper compares two object detection models—YOLOv5 and Faster R-CNN—in the context of autonomous driving systems, examining their strengths and weaknesses across various conditions.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve object detection for autonomous vehicles, as different deep learning methods impact system accuracy, robustness, and efficiency.

Method: The authors employ a comprehensive experimental analysis comparing YOLOv5 and Faster R-CNN using a diverse dataset of real and synthetic images, evaluated on metrics like mAP, recall, and inference speed.

Result: The study finds that YOLOv5 excels in mAP, recall, and training efficiency as dataset size and resolution increase, while Faster R-CNN is better at detecting small or distant objects and in poor lighting conditions.

Conclusion: The choice between YOLOv5 and Faster R-CNN depends on the application scenario, as each model has distinct strengths that can complement different aspects of autonomous driving challenges.

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [398] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: The paper introduces a new method to annotate brain vasculature via 4D-CTA imaging, employing multi-phase data to enhance segmentation accuracy. The approach significantly improves vessel segmentation using deep learning models, with robust metrics across diverse vascular structures.


<details>
  <summary>Details</summary>
Motivation: To address challenges in brain vessel segmentation by improving visualization and reducing the manual annotation effort through enhanced 4D-CTA imaging and robust datasets.

Method: A novel approach uses dynamic 4D-CTA scans to subtract non-vascular tissues, creating enhanced annotations for training deep learning models. The dataset is expanded by utilizing multiple phases of the scans, improving robustness to contrast variations.

Result: The dataset bettered segmentation accuracy, demonstrated by a nnUNet achieving higher mDC (0.846 arteries, 0.957 veins) and favorable metrics like aDHD (0.304 mm arteries, 0.078 veins) and tSens (0.877 arteries, 0.974 veins).

Conclusion: The methodology proves superior for brain vessel segmentation with enhanced performance and accurate vessel morphology capture. Code and model availability promote future research and applications.

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [399] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: The paper evaluates image captioning models for Brazilian Portuguese using both manually created and automatically translated datasets, highlighting performance differences and biases.


<details>
  <summary>Details</summary>
Motivation: To address the lack of specialized datasets and models for Brazilian Portuguese in image captioning, given its challenges as a low-resource language.

Method: The study compared Transformer-based models for image captioning using datasets featuring native and machine-translated captions, evaluated cross-context performance, interpreted model inferences with attention maps, and employed CLIP-Score for alignment.

Result: Swin-DistilBERTimbau outperformed other models in generalization. ViTucano outshined larger multilingual VLMs in traditional metrics, while GPT-4 achieved the best alignment with CLIP-Score. Attention analysis showed biases in gender, enumeration, and spatial reasoning.

Conclusion: The study sheds light on the impact of dataset origin (manual vs. translated) on IC models for Brazilian Portuguese and highlights Swin-DistilBERTimbau's robustness, ViTucano's efficiency, and systematic model biases.

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [400] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: The paper explores comparative learning for human aesthetic judgment in visual art using pairwise preference assessments and CNN features, showing improved efficiency and accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in varying individual preferences and the high cost of acquiring aesthetic judgment data in visual art.

Method: The study uses pairwise comparative learning with the Law of Comparative Judgment, ResNet-50 for feature extraction, a deep regression model, and a dual-branch pairwise comparison model. Four research questions guide the analysis.

Result: The deep regression model achieved a 328% improvement in $R^2$ compared to the baseline. Comparative models perform near regression models without direct ratings. Pairwise judgment showed 60% lower annotation time.

Conclusion: Pairwise comparative methods are efficient and practical for modeling human aesthetic preferences, but predicting individual preferences is still difficult.

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [401] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: The paper introduces a second-order optimizer for accelerating 3D Gaussian Splatting scene training, achieving better reconstruction quality with fewer training iterations and low memory overhead.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing optimization methods for 3D Gaussian Splatting, including their computational and memory costs, and improve efficiency and scalability in scene reconstruction.

Method: The proposed method approximates curvature using only the diagonal of the Hessian matrix via Hutchinson's method, combined with a parameter-wise trust-region technique to ensure stable optimization.

Result: 3DGS$^2$-TR achieves better reconstruction quality with 50% fewer training iterations compared to ADAM, with low GPU memory overhead suitable for large-scale and distributed training.

Conclusion: The presented optimizer is efficient, scalable, and suitable for reconstructing large scenes while balancing accuracy and computational resources.

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [402] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: This paper tackles the challenge of improving laboratory safety monitoring using vision-language models (VLMs). To assist VLMs in understanding visual contexts, they propose a pipeline converting textual lab scenarios into structured datasets for VLM evaluation. 


<details>
  <summary>Details</summary>
Motivation: Laboratory safety monitoring faces limitations due to reliance on human availability, and automated tools, such as VLMs, require better evaluation tools to enhance their realistic implementation for hazard detection.

Method: The authors developed a structured data generation pipeline to transform textual descriptions into a dataset of aligned images, scene graphs, and ground truth, using AI tools for image rendering and scene graph creation. They also introduced a context-engineering approach to improve VLM visual hazard detection through scene-graph-guided alignment.

Result: The VLMs demonstrated effectiveness with textual scene graphs but struggled with direct visual-only recognition, indicating perceptual limitations. Improvements were observed when employing the proposed alignment technique.

Conclusion: The proposed approach successfully bridges gaps in VLM's visual hazard detection capabilities, moving closer to reliable autonomous lab safety monitoring.

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [403] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ demonstrates how to bypass safety measures in LVLMs using OCR by scattering harmful prompts across benign image grids.


<details>
  <summary>Details</summary>
Motivation: To identify vulnerabilities in LVLMs' safety mechanisms, particularly their reliance on textual filters and OCR capabilities.

Method: The attack decomposes harmful queries into benign sub-queries, introduces irrelevant distractions, and presents them as a grid of images.

Result: This approach successfully evades state-of-the-art LVLMs' safety measures and exploits their multimodal processing weaknesses.

Conclusion: The study reveals critical vulnerabilities in LVLMs' OCR and multimodal input handling, emphasizing the need for enhanced defenses.

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [404] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: This paper introduces DISK, a training-free adaptive inference method leveraging coupled diffusion transformers for video and ego-trajectory prediction, achieving significant speedups and maintaining performance in long-horizon driving rollouts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency and retraining challenges in autoregressive world models for long-horizon video and trajectory prediction, particularly in closed-loop driving scenarios.

Method: DISK utilizes coupled diffusion transformers for video and trajectory with dual-branch cross-modal controllers for skip decisions, ensuring consistency without the need for retraining. It incorporates higher-order latent-difference skip testing in an autoregressive framework.

Result: DISK demonstrates 2x speedup in trajectory diffusion and 1.6x speedup in video diffusion on NuPlan/NuScenes datasets while maintaining planning accuracy, visual quality, and other metrics.

Conclusion: The proposed DISK framework provides a cost-effective, training-free solution for adaptable long-horizon video and trajectory prediction, showing practical application viability in driving simulations.

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [405] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: The paper introduces Sparse4D, a framework for spatiotemporal 3D detection and tracking in multi-camera indoor settings. It evaluates its stability under low frame rates, quantization methods, testing on WILDTRACK benchmark, and mixed precision fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multi-target tracking under occlusions and heterogeneous viewpoints using multi-camera setups in indoor environments.

Method: Sparse4D is a query-based framework that fuses features across multi-view cameras into a shared world frame and uses instance memory for identity propagation.

Result: Sparse4D remains stable with moderate FPS reductions but collapses below 2 FPS for identity tracking. Selective quantization balances speed and accuracy while attention modules are sensitive to low precision. Pretraining at low FPS shows significant zero-shot improvement on WILDTRACK.

Conclusion: Sparse4D demonstrates robustness to FPS reductions and quantization strategies but faces limitations in identity persistence and scalability under certain configurations. Stability-aware validation is recommended.

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [406] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: LatentLens is a novel approach for interpreting visual token representations within Vision-Language Models (VLMs) by mapping them to natural language descriptions, showcasing high interpretability across models and layers.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why large language models (LLMs) can process visual tokens effectively when integrated into Vision-Language Models (VLMs) using methods like shallow transformations.

Method: The introduced method, LatentLens, encodes a large text corpus for contextualized token representations, compares visual tokens to textual ones, and uses nearest-neighbor analysis to produce descriptive outputs for visual tokens.

Result: LatentLens reveals that most visual token representations are interpretable across all studied VLMs and layers, outperforming existing methods such as LogitLens, especially for producing meaningful and fine-grained interpretations.

Conclusion: LatentLens not only improves interpretability in VLMs but also underscores the alignment of vision and language representations, paving the way for new analytic approaches to latent representations.

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [407] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: PSGS framework generates realistic 3D scenes from text using a two-stage process focusing on semantic coherence and global consistency, addressing challenges in existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve immersive 3D scene generation for applications like VR, AR, and gaming by addressing limitations in current text-driven methods such as data scarcity and stitching inconsistencies.

Method: PSGS consists of a two-stage framework: (1) a dual-layer optimization for layout reasoning and iterative visual refinement, and (2) a panorama sliding mechanism to construct 3D Gaussian Splatting point clouds with enhanced depth and semantic consistency.

Result: PSGS achieves superior quality in panoramic generation and 3D scene fidelity, outperforming previous methods.

Conclusion: PSGS represents a robust and scalable solution for immersive content creation, offering improved scene details and visual quality.

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [408] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: This paper proposes ZS-TreeSeg, a Zero-Shot framework for tree crown segmentation, addressing limitations of other models in dense, overlapping forest canopies by leveraging existing segmentation techniques.


<details>
  <summary>Details</summary>
Motivation: Tree crown segmentation is crucial for forest monitoring, but current solutions face challenges like high annotation costs in supervised methods and poor domain generalization in foundation models.

Method: ZS-TreeSeg combines Canopy Semantic segmentation and Cells instance segmentation by modeling tree crowns as star-convex objects, using topological flow fields and Cellpose-SAM for instance separation.

Result: The framework performs robustly on NEON and BAMFOREST datasets, demonstrating generalization over diverse sensor types and canopy conditions without requiring training data.

Conclusion: ZS-TreeSeg offers a robust, training-free method for segmenting tree crowns, providing a practical tool for scalable ecological monitoring and data labeling.

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [409] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: GTATrack is a hierarchical multi-object tracking system optimized for distorted fisheye soccer footage, winning SoccerTrack Challenge 2025.


<details>
  <summary>Details</summary>
Motivation: The challenges of MOT in sports, including irregular player motion, uniform appearances, occlusions, and fisheye camera distortions, demand a solution to improve tracking accuracy.

Method: The GTATrack framework introduces two stages: Deep Expansion IoU for motion-agnostic online matching and Global Tracklet Association for trajectory refinement. A pseudo-labeling strategy also enhances detection of small or distorted objects.

Result: GTATrack achieved a robust HOTA score of 0.60, minimized false positives to 982, and demonstrated top-tier performance in soccer tracking.

Conclusion: Integrating robust local and global tracking strategies, GTATrack effectively addresses issues like identity switches and occlusions, setting a benchmark in fisheye-based sports tracking.

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [410] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: SketchMod enhances stroke-level sketch edits by refining source strokes with transformation, ensuring semantic consistency and visual alignment with the target sketch.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of previous methods that only relied on repositioning source strokes, failing to handle significant variations in size and orientation for semantically and visually coherent sketch edits.

Method: SketchMod refines source strokes through transformations based on three key attributes: scale, orientation, and position, ensuring alignment with the target sketch's patterns.

Result: Experimental results demonstrate precise and flexible stroke-level sketch edit performances of SketchMod.

Conclusion: SketchMod provides an effective solution for flexible and precise stroke-level sketch edits by refining source strokes to align with target sketch patterns.

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [411] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: The paper introduces HSSDCT, a framework for hyperspectral image fusion that enhances reconstruction quality and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing hyperspectral image fusion methods, such as limited receptive fields, redundant spectral bands, and high computational complexity, aiming for more efficient and robust solutions.

Method: Proposed HSSDCT framework containing HDRTB for multi-scale feature aggregation and SSCL for spatial-spectral dependency analysis with reduced computational complexity.

Result: HSSDCT demonstrates superior reconstruction quality and significantly lower computational costs compared to existing methods on benchmark datasets.

Conclusion: HSSDCT achieves state-of-the-art performance in hyperspectral image fusion, offering enhanced quality and efficiency.

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [412] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: The paper proposes RGBX-R1, a framework to improve multimodal large language models' (MLLMs) reasoning across multiple visual modalities beyond RGB, introducing innovative techniques like UAV prompting, two-stage training, and outperforming baselines by a significant margin.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs are limited to RGB modality, which hinders performance on other essential visual modalities like infrared or depth, crucial for complex scenarios.

Method: The authors propose RGBX-R1, using Understand-Associate-Validate (UAV) prompting to create Visual Modality Chain-of-Thought (VM-CoT) for reasoning across modalities. They introduce two-stage training: CS-SFT for supervised fine-tuning and ST-RFT with a Modality-understanding Spatio-Temporal (MuST) reward for advanced reasoning.

Result: The RGBX-R1 framework achieves significant performance improvements, outperforming baselines by 22.71% on three RGBX grounding tasks, verified through extensive experimentation.

Conclusion: RGBX-R1 successfully enhances MLLMs' multimodal understanding and spatial perception, demonstrating the effectiveness of its proposed approaches for handling diverse visual modalities.

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [413] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: SparseCut introduces a sparse shortcut connection architecture for multimodal large language models (MLLMs), enabling hierarchical fusion of visual data without increasing computational load, improving cross-modality understanding.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models struggle to effectively integrate cross-modal semantic information, especially in vision-language contexts where mid- and low-level visual features are often discarded.

Method: SparseCut combines sparse shortcut connections between cross-modal encoders and LLMs with an efficient multi-grained feature fusion module, ensuring semantic-rich integration while maintaining computational efficiency.

Result: Experiments show SparseCut improves performance and scalability of MLLMs across multimodal tasks and benchmarks.

Conclusion: SparseCut effectively enhances cross-modal semantic fusion and performance in MLLMs, showcasing generality, scalability, and computational efficiency.

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [414] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen is an advanced framework for interleaved multimodal generation, excelling in creating text and images through better data curation, architecture design, and evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance the quality of interleaved multimodal generation tasks (like instructional guides and visual planning) which are currently limited by inadequate training data and model architecture limitations.

Method: DuoGen builds a large-scale multimodal dataset combining curated website conversations and synthetic scenarios. It merges a pretrained multimodal LLM with a diffusion transformer (DiT) pretrained for video generation, using a two-stage training strategy: instruction-tuning the LLM, followed by aligning the DiT using curated multimodal sequences.

Result: DuoGen surpasses prior models in text quality, image fidelity, and image-context alignment, achieving state-of-the-art performance in text-to-image and unified generation tasks.

Conclusion: The DuoGen framework successfully addresses limitations in interleaved multimodal generation models with an innovative approach to data, architecture, and training, establishing new benchmarks for performance.

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [415] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: The paper introduces a new training-free segmentation method addressing limitations of graph spectral partitioning approaches via a stochastic flow equilibrium model, leveraging global and local affinities for improved performance.


<details>
  <summary>Details</summary>
Motivation: To overcome issues in existing training-free segmentation methods such as the reliance on pre-selecting cluster numbers, boundary oversmoothing, susceptibility to multimodal affinities, and neglect of local neighborhood structures.

Method: The proposed approach reforms segmentation as a stochastic flow equilibrium problem and introduces a Markov propagation scheme for label diffusion, integrating global diffusion and local neighborhood affinities.

Result: The method achieves state-of-the-art zero-shot performance on seven semantic segmentation benchmarks, improving boundary sharpness, region coherence, and mask stability compared to prior methods.

Conclusion: Reformulating training-free segmentation as a stochastic propagation process addresses critical limitations of previous methods, demonstrating stronger performance and refining segmentation outcomes.

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [416] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: This paper introduces MRAD, a train-free and efficient memory-retrieval method for zero-shot anomaly detection (ZSAD), outperforming existing techniques on various datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ZSAD methods often use costly prompt learning or complex modeling, facing high training/inference costs and limited cross-domain stability.

Method: The proposed MRAD framework builds a two-level memory bank (image/pixel-level) from auxiliary data for direct similarity retrieval during inference and introduces fine-tuned and prompt-enhanced variants.

Result: MRAD delivers superior anomaly classification and segmentation performance across 16 industrial and medical datasets, under train-free and training-based environments.

Conclusion: Fully utilizing empirical raw data distributions, rather than solely relying on model fitting, enables stronger anomaly detection with better generalizability and efficiency.

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [417] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: SAGE is a new framework that improves inference speed in vision-language models by using dynamic speculation trees, achieving faster decoding without losing quality.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods in VLMs use fixed tree structures, which do not adjust to varying prediction difficulties during processing, leading to inefficiencies and limited speed improvements.

Method: SAGE dynamically adjusts the tree structure during decoding based on real-time prediction uncertainty, using entropy as a confidence measure to guide tree depth and width.

Result: SAGE achieved up to 3.36x speedup for LLaVA-OneVision-72B and 3.18x speedup for Qwen2.5-VL-72B on multiple benchmarks without degrading output quality.

Conclusion: Dynamically adapting the speculation tree using prediction confidence significantly improves inference speed and efficiency in vision-language models.

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [418] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: The paper introduces VLDet, a framework improving open-vocabulary object detection (OVD) by enhancing visual-language alignment and backbone adaptation.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection is limited to predefined categories, making it unsuitable for changing environments. The paper addresses the need for improved OVD performance to detect novel categories.

Method: The authors develop VLDet with a feature pyramid and visual-language alignment via the VL-PUB module. The SigRPN block incorporates anchor-text contrastive loss to enhance novel category detection.

Result: VLDet achieves state-of-the-art performance with 58.7 AP for COCO2017 novel classes and 24.8 AP on LVIS, significantly outperforming prior approaches by margins of 27.6% and 6.9%, respectively.

Conclusion: VLDet enhances OVD capabilities, bridging gaps in visual-language alignment and backbone adaptation for improved detection performance on novel categories.

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [419] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: The paper introduces SADER, a diffusion-based framework for effective multi-temporal cloud removal in remote sensing, overcoming sampling inefficiencies and improving cloud region reconstruction.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address challenges in remote sensing cloud removal caused by cloud contamination, which degrades the quality of imagery and hinders downstream Earth observation tasks. It specifically seeks to improve sampling efficiency and exploit structural and temporal priors in multi-temporal scenarios.

Method: The proposed method, named SADER, features a Multi-Temporal Conditional Diffusion Network (MTCDN) for enhanced temporal fusion and hybrid attention mechanisms. It incorporates a cloud-aware attention loss to highlight cloud-dominant regions and employs a deterministic resampling strategy for better iterative sample refinement.

Result: Extensive experiments demonstrate that SADER consistently exceeds the performance of existing methods across various metrics on multiple multi-temporal remote sensing datasets.

Conclusion: SADER significantly enhances the capability of cloud removal in remote sensing imagery by leveraging structural and temporal information effectively. Its introduced methods and design choices provide state-of-the-art performance, offering a promising direction for future research in this domain.

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [420] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet is a 3D point-cloud classification and segmentation method that is fully non-parametric, relying on deterministic operators instead of learned weights.


<details>
  <summary>Details</summary>
Motivation: To create a method for 3D point-cloud processing that avoids requiring trained parameters, ensuring stability across scales and sampling densities while achieving competitive performance.

Method: It uses deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling alongside an adaptive Gaussian-Fourier positional encoding for feature construction. Fixed-frequency Fourier features are added for segmentation tasks.

Result: NPNet achieves competitive performance on benchmarks like ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, particularly excelling in few-shot scenarios, with advantages in memory usage and inference time.

Conclusion: NPNet demonstrates that fully non-parametric approaches can be effective for 3D point-cloud tasks, providing a stable, efficient, and competitive alternative to parametric methods.

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [421] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: The paper introduces OmniVCHall, a benchmark for assessing video hallucination in multimodal models, and proposes TriCD, a novel decoding framework to improve accuracy, achieving over 10% performance gains.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from limitations in current research, which focuses on isolated error types, neglecting compositional hallucinations involving complex interactions of spatial and temporal factors in video analysis.

Method: The authors developed OmniVCHall, a benchmark covering diverse hallucination types with adversarial elements, and proposed TriCD, a contrastive decoding framework that employs adaptive perturbation and saliency-guided enhancements, improved via reinforcement learning.

Result: Results demonstrate that TriCD improves the performance of video multimodal large language models by over 10% in accuracy across two backbones compared to advanced models.

Conclusion: OmniVCHall advances holistic evaluation for video analysis by addressing compositional hallucinations, and TriCD effectively mitigates such issues, marking significant progress in the field.

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [422] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: The paper introduces GLAD, a model using diffusion models to fuse text and visual data for better object tracking in videos, especially under low-quality image conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in vision-language tracking caused by low-quality and low-semantic images and to bridge the gap between textual and visual features.

Method: The proposed method, GLAD, uses diffusion models for generative multi-modal fusion between text descriptions and template images, enhancing semantic details of the images.

Result: GLAD demonstrates advanced performance in object tracking, surpassing previous state-of-the-art methods on multiple benchmarks with notable inference speed.

Conclusion: The GLAD model proves effective in improving cross-modal understanding for vision-language tracking, particularly under challenging low-quality image conditions. Code and models will be available online.

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [423] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: Proposes BDG, a framework designed to improve universal image restoration by integrating degradation diagnosis and generation capabilities into a diffusion model, achieving significant fidelity improvements in restoration tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in universal image restoration by tackling the issue of removing diverse degradations to generate high-quality images while preserving rich textures.

Method: Introduced BDG, incorporating Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) for fine-grained degradation discrimination and integrating this discriminative information into the diffusion model's multi-stage training process (generation, bridging, restoration).

Result: BDG demonstrates significant improvements in fidelity for universal image restoration and real-world super-resolution tasks without altering the model architecture.

Conclusion: BDG effectively enhances the universal image restoration process, offers improved results across multi-degraded scenarios, and provides pretrained models for real-world applications.

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [424] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: The paper introduces MAUGen, a diffusion-based framework for generating photorealistic facial images with accurate Action Unit (AU) labels, addressing the scarcity of high-quality AU datasets.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, diverse datasets with precise AU annotations hinders the development of generalizable AU recognition systems, necessitating a solution to bridge this gap.

Method: MAUGen comprises a Multi-modal Representation Learning module to unify relationships among text descriptions and images, and a Diffusion-based Image label Generator to produce high-quality facial images with corresponding AU labels.

Result: The proposed method creates the MIFA dataset, a comprehensive synthetic collection with AU annotations and identity varieties. It achieves superior results in generating high-quality images and labels compared to existing methods.

Conclusion: MAUGen effectively tackles the scarcity of AU datasets by providing a scalable framework and dataset (MIFA), advancing AU recognition systems through improved and diverse annotations.

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [425] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: The paper introduces Pix2Fact, a visual question-answering benchmark designed to test models on expert-level visual perception and multi-hop reasoning with external knowledge. Current state-of-the-art models achieve low accuracy compared to humans, highlighting key challenges.


<details>
  <summary>Details</summary>
Motivation: To address a gap in existing benchmarks by creating a dataset that evaluates both detailed visual grounding and knowledge-intensive reasoning, which are often tested separately.

Method: The authors designed Pix2Fact, a dataset of 1,000 high-resolution images across 8 scenarios with questions requiring detailed visual understanding, multi-hop reasoning, and external knowledge. Annotators with PhDs formulated questions to ensure quality.

Result: Even the most advanced models, like Gemini-3-Pro and GPT-5, achieved only 24.0% accuracy on Pix2Fact, while human performance was 56%, demonstrating the challenge of achieving human-level comprehension.

Conclusion: Pix2Fact highlights the gap between human visual comprehension and current AI capabilities, aiming to drive the development of next-generation multimodal agents with improved perception and reasoning abilities.

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [426] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: The paper introduces Tune-Your-Style, a method for customizable 3D style transfer.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing content with style patterns and colors in 3D style transfer and to meet diverse user requirements for content-style balance.

Method: A novel paradigm using Gaussian neurons and a learnable style tuner for intensity-tunable style injection, paired with a tunable stylization guidance from multi-view consistent diffusion models.

Result: The method achieves visually appealing and customizable 3D stylization results.

Conclusion: Tune-Your-Style provides enhanced flexibility and user control in 3D style transfer, solving the challenge of customizability.

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [427] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: The paper introduces sparse autoencoders (SAEs) and proposes a new method called Contrastive Neuron Steering (CNS) to address hallucinations in Large Vision-Language Models (LVLMs). CNS reduces hallucinations by analyzing and manipulating neuron activations at the representation level.


<details>
  <summary>Details</summary>
Motivation: LVLMs exhibit strong multimodal capabilities but are prone to hallucinations due to disruptions in the internal neuron mechanisms. The study seeks to explore these underlying causes and address them.

Method: The research employs sparse autoencoders (SAEs) to decompose visual embeddings into sparse neurons, investigates neuron types, and utilizes a technique called CNS for contrastive analysis between clean and noisy inputs. CNS selectively adjusts neuron activations to improve representation quality.

Result: Leveraging CNS, the authors achieved more robust and grounded visual representations, reduced hallucinations, and preserved multimodal understanding. Experiments confirm consistent improvements across benchmarks.

Conclusion: The study highlights the relevance of addressing neuron-level disruptions for mitigating hallucinations and proposes CNS as a controllable intervention mechanism. CNS is compatible with existing methods and improves LVLM performance without compromising multimodal capabilities.

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [428] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: The paper introduces FaceSnap, a novel approach for high-fidelity, personalized portrait generation using a single image and Stable Diffusion in a single inference stage.


<details>
  <summary>Details</summary>
Motivation: Current methods for customized portrait generation are either time-consuming, lack generalizability, or fail to achieve high fidelity in facial details.

Method: FaceSnap leverages a Facial Attribute Mixer for feature extraction and a Landmark Predictor for spatial control. These components are integrated using an ID-preserving module into the Stable Diffusion (SD) pipeline.

Result: FaceSnap achieves superior performance in personalized portrait generation, outperforming existing state-of-the-art methods.

Conclusion: FaceSnap is a fast, plug-and-play method offering consistent and high-quality portrait generation, demonstrating great potential for practical applications.

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [429] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: The paper introduces S$^3$POT, a framework for occlusion segmentation in face parsing by combining face generation and self-supervised spatial prompting.


<details>
  <summary>Details</summary>
Motivation: Face parsing often misclassifies occlusions due to the challenge of annotating and accounting for all types of occlusion objects, which are high-level and diverse.

Method: The S$^3$POT framework consists of Reference Generation for reconstructing occluded regions, Feature Enhancement to modify image features using a contrastive approach, and Prompt Selection to identify accurate prompts for segmentation without requiring ground truth occlusion masks.

Result: Experiments on a specialized dataset validate S$^3$POT's superior performance and demonstrate the individual effectiveness of its modules.

Conclusion: S$^3$POT is a promising method for handling occlusion in face parsing, offering an innovative solution by synergizing face generation and prompt-based segmentation without requiring extensive manual annotation.

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [430] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: The study introduces VIZOR, a viewpoint-invariant and training-free framework for 3D scene graph generation, enhancing accuracy in spatial reasoning and scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene understanding struggle with generalization and provide inconsistent spatial relationships, especially across different viewpoints.

Method: The paper proposes VIZOR, a training-free framework that generates dense 3D scene graphs directly from raw scenes, defining spatial relationships relative to objects' orientations and inferring open-vocabulary relationships without annotated data.

Result: VIZOR demonstrates superior performance in scene graph generation and zero-shot object grounding, surpassing state-of-the-art methods with notable gains on Replica and Nr3D datasets.

Conclusion: VIZOR promises improved accuracy and consistency in 3D scene reasoning, making it a robust solution for scene understanding tasks across diverse viewpoints.

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [431] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: The paper introduces Diff-PC, a novel diffusion-based framework for realistic portrait customization with enhanced identity preservation and facial control.


<details>
  <summary>Details</summary>
Motivation: Existing portrait customization methods struggle with preserving precise identity and controlling facial attributes, necessitating advancements.

Method: The proposed Diff-PC employs a 3D face predictor for facial priors, an ID-Encoder for feature fusion, an ID-Ctrl for ID feature alignment, and an ID-Injector to boost identity fidelity. It also uses an ID-centric dataset for training.

Result: Diff-PC demonstrates superior performance in preserving identity, controlling facial attributes, and ensuring T2I alignment compared to state-of-the-art approaches.

Conclusion: Diff-PC effectively advances portrait customization by achieving high identity fidelity, facial control, and compatibility with multi-style models.

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [432] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: Mamba-SAM, a novel hybrid architecture, efficiently combines SAM and Mamba-based State Space Models for improved 3D medical image segmentation, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: 3D medical image segmentation is challenging due to domain shifts and computational costs, necessitating a more effective and efficient solution that adapts foundation models like SAM for medical imaging.

Method: The paper proposes two strategies: a dual-branch architecture combining frozen SAM with Mamba encoder via cross-attention, and an adapter-based approach with lightweight, 3D-aware modules integrated into SAM. Enhanced feature representation is achieved using Multi-Frequency Gated Convolution.

Result: Extensive tests on the ACDC dataset show competitive accuracy (mean Dice score of 0.906 in dual-branch model) and improved inference speed (4.77 FPS in adapter-based model), outperforming baselines in specific segmentation tasks.

Conclusion: Mamba-SAM achieves effective 3D medical image segmentation by combining the strengths of foundational models and SSM-based architectures, making it a practical and scalable approach.

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [433] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: The paper introduces NOVA, a non-contrastive vision-language alignment framework, as a simpler and more stable alternative to traditional contrastive learning methods.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models based on contrastive learning face challenges such as large batch sizes, negative sampling, and excessive hyperparameter tuning. The authors aim to simplify and stabilize this process.

Method: NOVA aligns visual representations to a frozen text encoder through joint embedding prediction and regularizes embeddings using Sketched Isotropic Gaussian Regularization (SIGReg).

Result: NOVA outperforms existing baselines on zero-shot chest X-ray classification across three benchmark datasets and shows more consistent training runs.

Conclusion: Non-contrastive vision-language pretraining (NOVA) offers a simpler, more stable, and effective alternative to traditional contrastive methods for multimodal tasks.

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [434] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: The paper introduces a physics-guided neural network embedding a Schrödinger-type operator for stable, interpretable 4D spatiotemporal forecasting, with applications in medical imaging and other fields.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of stability, interpretability, and physical consistency in 4D spatiotemporal forecasting for applications like medical imaging and geophysics.

Method: The approach combines deep convolutional networks with a Schrödinger-inspired time-evolution operator to learn amplitude, phase, and potential fields for complex-valued wavefunctions evolved in time.

Result: The model accurately forecasts future 4D states, maintains anatomical fidelity, and prevents error drift over long horizons, as demonstrated in synthetic benchmarks.

Conclusion: A novel and interpretable 4D forecasting framework is proposed, integrating physical priors for robustness and expressivity, with demonstrated stability and usability in realistic deformation and topology changes.

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [435] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: The paper introduces a super-resolution method for improving anatomical fidelity in 3D brain volume reconstructions from 2D dissection photographs.


<details>
  <summary>Details</summary>
Motivation: Existing methods of 3D brain reconstruction from dissection photographs often result in coarse and overly smooth outputs, especially with high slab thickness. Improved resolutions are needed for better neuropathological analyses and neuroimaging mappings.

Method: They propose a computationally efficient super-resolution technique that imputes slices to generate isotropic volumes from anisotropic reconstructions. It uses domain-randomized synthetic data for robust generalization across diverse dissection protocols.

Result: The generated isotropic volumes enhance automated segmentations, yielding better Dice scores in cortical and white matter regions. It also improves cortical surface reconstructions and MRI registration accuracy.

Conclusion: The approach leads to more precise photographic reconstructions, improving connections between neuropathology and neuroimaging. The method is made publicly accessible for broader application.

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [436] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: The paper introduces HPC, a novel compression framework for dynamic Gaussian Splatting, achieving high compactness and storage efficiency while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing compression methods for dynamic Gaussian Splatting, which either suffer from parameter redundancy in unoccupied space or poor compactness due to lack of local correlation usage.

Method: The HPC framework uses a hierarchical point-based latent representation and a tailored aggregation scheme to improve compactness and spatial efficiency. It also compresses neural networks by leveraging inter-frame parameter correlation.

Result: HPC achieves a 67% storage reduction compared to previous methods while maintaining high reconstruction fidelity, outperforming state-of-the-art approaches.

Conclusion: The proposed HPC framework effectively addresses the challenges of dynamic Gaussian Splatting compression by enhancing efficiency and compactness, significantly reducing storage with minimal quality compromise.

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [437] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: This paper focuses on improving video understanding by leveraging temporal relations, introducing novel methods in learning frameworks, fine-tuning strategies, and long-form video modeling.


<details>
  <summary>Details</summary>
Motivation: Current methods in video understanding often fail to effectively capture temporal relations and dynamics across video elements, limiting their performance.

Method: The paper introduces an automatic annotation framework, a fine-tuning strategy using "recurrent adapters," integration of State Space Layers, a new contrastive learning framework, and a benchmarking empirical study for approaches targeting temporal reasoning.

Result: Enhanced models are shown to improve temporal reasoning capabilities, validated by newly introduced benchmarks and empirical studies.

Conclusion: Explicit temporal modeling is key to improving a model's ability to analyze and reason about video content, especially in fluid and dynamic scenarios.

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [438] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: The paper introduces V2X-DSC, a framework for bandwidth-efficient collaborative perception in 3D understanding, ensuring effective fusion of multi-agent observations through conditional reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address bandwidth constraints in collaborative perception for 3D understanding, caused by dense feature data saturating communication links.

Method: The V2X-DSC framework employs a Conditional Codec (DCC) that compresses and reconstructs BEV features using local side information to reduce redundancy and enhance efficiency.

Result: Experiments on diverse datasets validate the method's ability to achieve state-of-the-art performance in accuracy-bandwidth trade-offs under constrained communication conditions.

Conclusion: The proposed method is effective in improving collaborative perception, accommodates bandwidth limitations, and integrates seamlessly across various fusion frameworks.

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [439] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: A Vision-Language-Action (VLA) model integrating 3D point cloud data improves spatial understanding in complex scenes and addresses challenges such as sparse 3D data and domain gaps.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing VLA models that rely on 2D visual input, which restricts their spatial understanding in complex environments.

Method: The paper introduces Any3D-VLA, a unified training pipeline that integrates simulator, sensor, and model-estimated 3D point clouds, and fuses them with 2D representations for domain-agnostic 3D understanding.

Result: Experiments in simulated and real-world environments demonstrate that Any3D-VLA enhances performance and bridges the domain gap between simulation and real-world data.

Conclusion: Explicitly incorporating 3D information improves VLA model capabilities, and Any3D-VLA shows robustness across various inputs, yielding better spatial awareness and reduced domain discrepancies.

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [440] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: The paper introduces JoyAvatar, a model capable of producing long-duration avatar videos with improved text instruction alignment and audio-visual synchronization compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current video avatar models struggle with complex text prompts involving full-body movements, dynamic camera trajectories, and human-object interactions.

Method: The framework uses a twin-teacher enhanced training algorithm for text-controllability and audio-visual synchronization, alongside dynamic modulation of multi-modal conditioning signals during training.

Result: JoyAvatar achieves natural, temporally coherent full-body motions and dynamic camera movements, outperforming state-of-the-art models like Omnihuman-1.5 and KlingAvatar 2.0 in GSB evaluations.

Conclusion: JoyAvatar expands avatar model capabilities, enabling multi-person interactions and non-human role-playing while maintaining lip-sync, identity consistency, and improved overall performance.

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [441] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: This paper introduces VVLoc, a single neural network pipeline for both topological and metric vehicle localization using a multi-camera system, offering high accuracy and confidence estimation.


<details>
  <summary>Details</summary>
Motivation: Current localization methods for autonomous driving often address tasks independently, use single-camera setups, require additional 3D semantic or pose priors, and lack confidence quantification. These limitations reduce feasibility for industrial adoption.

Method: VVLoc employs a unified neural network to calculate geo-proximity and relative metric poses from multi-camera visual data. It also provides confidence measures and requires only visual data and ground-truth poses for training, avoiding complex extra data.

Result: VVLoc achieves state-of-the-art localization accuracy and is validated on both public and challenging self-collected datasets.

Conclusion: VVLoc offers a robust and efficient solution for simultaneous topological and metric localization, suitable for real-world autonomous driving applications.

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [442] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: The paper presents a framework to improve automated analysis of sorghum stomata through semi-supervised instance segmentation and a new annotated dataset.


<details>
  <summary>Details</summary>
Motivation: Sorghum is crucial for climate-resilient agriculture due to its drought tolerance. Precise stomatal trait analysis is vital for enhancing water-use efficiency, but automated methods face challenges in analyzing small and variable stomatal structures.

Method: The authors created a sorghum leaf dataset of 11,060 annotations, utilized a pseudo-labelling strategy for additional 56,428 annotations, and applied patch-based preprocessing alongside semi-supervised learning to enhance segmentation accuracy.

Result: Semantic segmentation accuracy improved to a top mIoU of 70.35%, and instance segmentation accuracy improved to a top AP of 46.10%, demonstrating significant detection improvements for small stomatal structures.

Conclusion: The framework supports scalable extraction of stomatal traits and advances AI-driven phenotyping in crop science, overcoming challenges in analyzing small and nested structures.

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [443] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: The paper introduces a diffusion-based approach for makeup transfer, addressing dataset limitations, feature disentanglement, and control issues.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges with limited datasets, poor feature disentanglement, and weak controllability in makeup transfer systems.

Method: Construct a high-quality dataset via a train-generate-filter-retrain strategy, a diffusion framework for disentangling identity and makeup features, and a text-guided mechanism for fine-grained control.

Result: The system showcases improvements in fidelity, identity preservation, and flexibility as demonstrated on benchmarks and real-world scenarios.

Conclusion: By leveraging a diffusion-based framework and novel dataset and mechanisms, the approach enhances diversity, control, and accuracy in makeup transfer applications.

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [444] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: The paper presents a diffusion-based algorithm to separate inter and outer layer surfaces in double-layered point clouds with open boundaries, addressing artifacts caused by truncation in TSDF fusion during 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by double surface artifacts in double-layered point clouds, particularly in applications like indoor modeling and medical imaging.

Method: A diffusion-based algorithm that separates erroneous inter and outer layers caused by truncation thresholds in TSDF fusion, especially in point clouds with open boundaries.

Result: The algorithm successfully extracts inter layers from double-layered point clouds with watertight and open-boundary geometries in approximately 10 seconds for 20,000 points.

Conclusion: The proposed method is a lightweight post-hoc solution for separating inter/outer shells after TSDF fusion, enabling accurate surface representations without replacing existing reconstruction pipelines.

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [445] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: The paper identifies and addresses the critical challenge of image corruptions in Vision-Language-Action (VLA) robotic systems, proposing a transformer-based solution to restore visual input quality and significantly enhance performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the fragility of VLA models in real-world deployments caused by image corruptions, as current research primarily focuses on physical occlusions but neglects sensor-level disturbances.

Method: The proposed method, Corruption Restoration Transformer (CRT), is a vision transformer that uses adversarial training to restore clean observations from corrupted inputs without requiring fine-tuning of the VLA models.

Result: The CRT method demonstrated significant efficacy, enabling VLAs to recover from sensor distortions and achieve near-baseline success rates across benchmark datasets like LIBERO and Meta-World.

Conclusion: CRT offers a plug-and-play, model-agnostic solution to address sensor disturbances in VLAs, improving robustness and enabling more reliable deployment of robotic systems in real-world scenarios.

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [446] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: HSI-VAR proposes a novel autoregressive method for hyperspectral image restoration to address the shortcomings of existing approaches like diffusion models and regression models.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral image (HSI) restoration methods are computationally intensive or produce oversmoothed results, making them unsuitable for real-world applications involving high-dimensional data with degradations like noise, blur, and missing bands.

Method: HSI-VAR rethinks HSI restoration as an autoregressive generation problem, introducing latent-condition alignment, degradation-aware guidance, and a spatial-spectral adaptation module to improve efficiency and quality.

Result: HSI-VAR achieves a 3.77 dB improvement in PSNR on benchmark datasets, superior structural preservation, and achieves an inference speed-up of up to 95.5 times compared to diffusion-based methods.

Conclusion: HSI-VAR substantially improves the practicality and performance of HSI restoration, offering a state-of-the-art solution for processing degraded hyperspectral images effectively and efficiently.

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [447] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: Monocular foundation models deliver relative depth, which we calibrate with sparse range measurements to create a pseudo-metric depth prior refined for few-shot depth prediction.


<details>
  <summary>Details</summary>
Motivation: Relative depth from monocular models is useful but lacks direct applicability in robotics and autonomous driving due to its non-metric nature.

Method: Calibrate relative depth from foundation models with sparse range data and refine it through a network to achieve accurate metric depth predictions.

Result: The approach enables stable and sharp metric depth predictions even in data-scarce conditions, particularly without curated validation data.

Conclusion: Coupling foundation model priors with sparse measurements is a practical method for robust depth completion under label scarcity.

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [448] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: This study evaluates deep learning for nerve segmentation in ultrasound images, highlighting challenges like class imbalance and nerve size variability.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of manually identifying nerves in ultrasound images due to low contrast, noise, and varying anatomy, and explore how deep learning can enhance performance.

Method: The study uses a U-Net architecture for nerve segmentation and examines the effects of data from different ultrasound machines and annotation strategies.

Result: Training on multi-source data improves generalizability, though single-source training remains optimal for target domains. Multi-class supervision decreases nerve segmentation accuracy, and smaller nerves are harder to segment.

Conclusion: The findings offer insights into improving ultrasound nerve segmentation systems under clinical constraints.

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [449] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: DVLA-RL enhances few-shot learning by leveraging dual-level vision-language alignment, combining local and global semantics effectively using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of existing few-shot learning approaches that fail to establish adaptive and progressive alignment between vision and language at multiple semantic levels.

Method: The Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL) combines Dual-level Semantic Construction (DSC) for generating and selecting relevant textual attributes and RL-gated Attention (RLA) for adaptively fusing visual and language tokens at different network layers.

Result: DVLA-RL outperforms previous methods, achieving state-of-the-art results across nine benchmarks in three diverse few-shot learning scenarios.

Conclusion: By integrating dual-level semantics using a progressive and adaptive reinforcement learning approach, DVLA-RL enhances class-specific discrimination and generalization with limited samples, setting a new standard in few-shot learning.

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [450] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: The paper evaluates the performance of NetVLAD for loop closure detection (LCD) in SLAM, comparing it to DBoW on the KITTI dataset, and introduces a Fine-Grained Top-K precision-recall curve for improved evaluation.


<details>
  <summary>Details</summary>
Motivation: Classic LCD methods like DBoW are efficient but struggle under appearance change or perceptual aliasing, while deep learning-based approaches offer robustness but can be computationally expensive.

Method: The study empirically evaluates NetVLAD as an LCD module using the KITTI dataset. It introduces the Fine-Grained Top-K precision-recall curve to better evaluate scenarios with zero or multiple valid matches and leverages Faiss-accelerated nearest neighbor search for efficiency.

Result: NetVLAD achieves real-time querying speeds while outperforming DBoW in accuracy and robustness on the KITTI dataset.

Conclusion: NetVLAD is a viable and practical alternative to DBoW for LCD in SLAM systems, combining computational efficiency with improved robustness.

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [451] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: This paper proposes a novel zero-shot method, Paracosm, for Composed Image Retrieval (CIR) that uses a 'mental image' generation and synthetic counterparts for improved matching.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the core challenge in CIR, which is the implicit nature of the 'mental image,' and to enhance accuracy in matching for multimodal queries.

Method: The method involves prompting a Large Multimodal Model (LMM) to generate a 'mental image' based on the input query, generating synthetic counterparts for database images, and conducting matching in a synthetic domain without training.

Result: Paracosm achieves state-of-the-art performance across four benchmarks, significantly surpassing existing zero-shot methods.

Conclusion: The proposed Paracosm method effectively utilizes synthetic images for CIR, creating a novel framework that combines creativity and engineering to overcome domain gaps in zero-shot retrieval.

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [452] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: The paper presents DDP-WM, an efficient world model architecture for robotic planning that enhances both speed and performance by disentangling latent dynamics into primary and background updates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the efficiency-performance trade-off in existing dense Transformer-based world models, which limit their real-time deployment capabilities.

Method: The paper proposes the Disentangled Dynamics Prediction (DDP) framework. It isolates primary dynamics through dynamic localization and addresses background updates via a cross-attention mechanism.

Result: DDP-WM demonstrates significant success, achieving up to 9x faster inference and improving task performance, such as in Push-T task where success rates increased from 90% to 98%.

Conclusion: The work shows the potential of disentangled dynamics approaches for creating efficient, high-fidelity robotic world models, enabling better real-time planning and deployment.

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [453] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: This paper presents a privacy-preserving framework using FlowEdit for identity-agnostic pathology preservation in Federated Learning for dermatology.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the trade-off between preserving patient privacy and maintaining diagnostic features essential for clinical dermatology in Federated Learning.

Method: The approach uses inversion-free Rectified Flow Transformers (FlowEdit) for high-fidelity identity transformation and introduces a 'Segment-by-Synthesis' mechanism for creating counterfactual pairs of healthy and pathological images.

Result: The framework achieves Intersection over Union (IoU) stability greater than 0.67 across synthetic identities and successfully generates privacy-compliant synthetic surrogates for edge devices.

Conclusion: The proposed method mitigates the risk of gradient leakage while ensuring high-precision skin image analysis in privacy-sensitive federated environments.

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [454] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: This paper introduces TransNormal, a framework for monocular normal estimation of transparent objects using pre-trained diffusion priors and dense visual semantics from DINOv3, achieving significant improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Laboratory automation requires accurate normal estimation for transparent objects, but conventional methods fail due to light refraction and reflection challenges.

Method: TransNormal incorporates pre-trained diffusion priors, cross-attention with DINOv3 semantics, multi-task objectives, and wavelet-based regularization. It also introduces a high-fidelity synthetic dataset.

Result: TransNormal reduces mean error by 24.4% on ClearGrasp and 15.2% on ClearPose, outperforming state-of-the-art methods, with additional accuracy improvements.

Conclusion: TransNormal effectively enhances geometric understanding for transparent objects, supporting embodied AI in scientific environments. Its dataset and code will be publicly available.

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [455] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: The paper introduces HieraNav, a task for language-driven navigation using LangMap, a benchmark with natural language descriptions and real-world 3D indoor scans, addressing complex semantic levels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable meaningful human-AI communication and advance embodied intelligence through semantic understanding of natural language in navigation tasks.

Method: The authors developed LangMap, a large-scale benchmark with comprehensive annotations and tasks designed for HieraNav, combining natural language instructions, region labeling, and discriminative descriptions for 3D environments.

Result: LangMap demonstrates high annotation quality, surpassing existing benchmarks in description accuracy, and evaluates models showing context and memory enhancements improve navigation but highlights challenges in certain goal completions.

Conclusion: HieraNav and LangMap provide a testbed for improving language-driven navigation, underscoring the need for addressing current challenges in embodied intelligence systems.

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [456] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: The paper introduces a training-free Visual Place Recognition (VPR) framework using a Second-Order Geometric Statistics approach to achieve competitive performance in challenging zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: To enhance Visual Place Recognition (VPR) with robust representations that can withstand environmental and viewpoint transformations, addressing limitations of data-dependent or simplistic statistical methods.

Method: The proposed method models scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, employs Riemannian mappings for geometric stability, and utilizes pre-trained backbones to ensure training-free operation.

Result: Extensive experiments validate the competitive performance of the framework, particularly in zero-shot generalization settings with no need for retraining.

Conclusion: The framework successfully achieves robust and high-performing VPR without requiring supervised training or retraining, offering strong zero-shot generalization.

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [457] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R creates compact 3D reconstruction models trainable on a single workstation, cutting costs and compute demands.


<details>
  <summary>Details</summary>
Motivation: To make advances in 3D multi-view reconstruction accessible to academic labs without large computational resources.

Method: Introduces an offline caching pipeline and confidence-aware distillation loss for training smaller models efficiently on standard hardware.

Result: Proposes a 72M-parameter student model achieving a 9x parameter reduction and a 5x inference speedup, trainable in 3 days on a single workstation.

Conclusion: Distill3R democratizes 3D vision research by enabling efficient, cost-effective training and deployment without relying on large-scale compute.

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [458] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: DIAMOND is a training-free approach to reduce artifacts in text-to-image generative models by correcting the generative trajectory during inference.


<details>
  <summary>Details</summary>
Motivation: Current methods to reduce artifacts in image generation require post-hoc interventions or problematic weight modifications, limiting their practicality and efficiency.

Method: DIAMOND reconstructs a clean sample estimate at every generative step, steering the process away from artifact-prone states, without requiring additional training or changes to model weights.

Result: DIAMOND demonstrates robust, zero-shot performance in generating artifact-free, high-fidelity images across standard diffusion models.

Conclusion: DIAMOND offers a practical solution for artifact-free image synthesis in generative models, circumventing the need for invasive or computationally expensive methods.

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [459] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: OCTOPUS is a novel architecture that enhances State Space Models (SSMs) for vision tasks by incorporating multi-directional recurrence, capturing both local and global spatial structures with linear complexity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard SSMs in vision tasks, particularly their inability to preserve local spatial coherence due to the causal formulation suited for text but detrimental in spatial domains.

Method: OCTOPUS employs eight-directional recurrence (horizontal, vertical, and diagonal, both forward and backward) for better preservation of global and local spatial structures. The design ensures linear computational complexity similar to SSMs.

Result: OCTOPUS achieves noticeable improvements in boundary preservation and region consistency in segmentation tasks and demonstrates superior classification accuracy compared to existing vision-focused SSM models.

Conclusion: OCTOPUS establishes itself as a foundation method for multi-directional recurrence, offering an efficient and scalable way to build spatially-aware vision architectures while maintaining linear computational cost.

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [460] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: The paper presents ConsensusDrop, a method to efficiently reduce visual tokens in Vision-Language Models (VLMs) while preserving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and high cost of processing redundant visual tokens in Vision-Language Models, which existing methods fail to handle effectively by using either query-agnostic or query-aware saliency alone.

Method: They propose ConsensusDrop, which combines vision-encoder saliency with query-aware cross-attention to create a consensus ranking. The most informative tokens are retained, while others are compressed via encoder-guided token merging.

Result: ConsensusDrop consistently outperforms previous token pruning methods across various VLMs, achieving better accuracy-efficiency trade-offs and maintaining near-baseline accuracy even with significant token reductions.

Conclusion: ConsensusDrop is an effective and practical approach for optimizing token usage in VLMs, improving accuracy-efficiency balances, and reducing computational costs. The authors plan to release the code for broader use.

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [461] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: This paper presents two novel data-augmentation frameworks to improve the accuracy of CAR-T/NK cell immunological synapse (IS) detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: The limited size of annotated microscopy datasets restricts the generalization of artificial neural networks (ANNs) in detecting and segmenting CAR-T/NK immunological synapse (IS) structures.

Method: Two complementary frameworks are introduced: Instance Aware Automatic Augmentation (IAAA) and Semantic-Aware AI Augmentation (SAAA). IAAA generates synthetic IS images and segmentation masks using optimized augmentation policies, while SAAA employs a diffusion-based mask generator and Pix2Pix image synthesizer for creating anatomically realistic augmented data.

Result: The augmentation strategies produce synthetic images with visual and structural properties resembling real IS data, enhancing ANNs' segmentation and detection performance.

Conclusion: Combining these frameworks improves the robustness and accuracy of IS quantification, enabling better imaging-based biomarkers for predicting patient responses to CAR-T/NK immunotherapy.

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [462] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: This paper proposes a hybrid approach combining Topological Data Analysis (TDA) with DenseNet121 for highly accurate Alzheimer's disease classification from MRI data.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for an accurate and early diagnosis of Alzheimer's disease using neuroimaging data and overcome the limitations of conventional neural networks in capturing topological characteristics of brain structures.

Method: The hybrid framework integrates TDA, which captures topological features of brain structures, with DenseNet121, which learns spatial features from MRI slices. These features are fused to classify Alzheimer's disease into four stages.

Result: On the OASIS-1 Kaggle MRI dataset, the proposed model achieves an unprecedented accuracy of 99.93% and an AUC of 100%, outperforming state-of-the-art approaches.

Conclusion: Incorporating topological data analysis into deep learning significantly improves the accuracy and robustness of Alzheimer's disease classification models, and the proposed framework shows promise as a tool for automated diagnosis.

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [463] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: The paper addresses the limitations of multimodal large language models (MLLMs) in deep emotional understanding by introducing a new Theory of Mind (ToM)-grounded benchmark and methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with deep emotional reasoning, prompting the need for improved cognitive modeling via Theory of Mind.

Method: The authors developed HitEmotion, a hierarchical ToM-based benchmark, proposed a ToM-guided reasoning chain, and designed TMPO, a reinforcement learning method.

Result: Experiments reveal emotional reasoning limitations in existing models, showing that the proposed methods enhance accuracy and coherence.

Conclusion: The study contributes tools and datasets to evaluate and improve emotional cognition in MLLMs, enhancing their affective intelligence.

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [464] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: The paper introduces VAMOS-OCTA, a deep learning framework for improving image quality in motion-corrupted handheld Optical Coherence Tomography Angiography (OCTA).


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in handheld OCTA result in unsampled retinal regions and degraded volumetric image quality, necessitating a reconstruction framework.

Method: The proposed VAMOS-OCTA uses a 2.5D U-Net architecture with Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss to inpaint motion-corrupted B-scans with vessel continuity across different axes.

Result: VAMOS-OCTA achieves superior reconstruction quality, restoring sharp capillaries and vascular continuity while outperforming existing methods in perceptual and pixel-wise accuracy metrics.

Conclusion: Multi-axis supervision offers a robust approach for restoring heavily motion-degraded 3D OCTA data, making it suitable for pediatric and uncooperative subjects.

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [465] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: The study introduces CortiNet, a lightweight neural network inspired by the human visual cortex, for diagnosing gallbladder diseases through ultrasound imaging. It achieves high accuracy while being efficient and interpretable.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of low-resolution and speckle noise in ultrasound imaging, which reduce diagnostic accuracy, with a lightweight, interpretable neural network suitable for clinical use.

Method: CortiNet uses dual-stream neural architecture inspired by the human visual cortex. It separates low- and high-frequency information, leveraging multi-scale signal decomposition and a physics-based inductive bias. This framework is structure-aware and integrates features through efficient fusion mechanisms.

Result: CortiNet achieves 98.74% diagnostic accuracy on a dataset of 10,692 annotated images across nine gallbladder disease categories. It also reduces parameter usage compared to traditional deep learning models.

Conclusion: CortiNet demonstrates the promise of lightweight, interpretable neural architectures for improving diagnostic reliability in gallbladder diseases while maintaining computational efficiency and robustness against noise.

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [466] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: The paper introduces SRVAU-R1, a reflection-aware framework for improving reasoning in video anomaly understanding (VAU) using multi-modal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: While MLLMs show promising results in understanding video anomalies, current methods focus only on surface-level anomaly descriptions and lack deeper reasoning, such as self-reflection and self-correction.

Method: The proposed SRVAU-R1 includes a reflection-oriented Chain-of-Thought dataset specifically for VAU, as well as a reflection-aware learning paradigm employing supervised and reinforcement fine-tuning.

Result: SRVAU-R1 achieves superior performance in temporal anomaly localization and reasoning quality compared to existing methods in various video anomaly benchmarks.

Conclusion: Self-reflection can significantly enhance reasoning capabilities in VAU tasks, and SRVAU-R1 sets a new benchmark for combining reflection and multi-modal learning in anomaly detection tasks.

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [467] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: Open-set biometrics struggle with detecting subjects not enrolled in the gallery due to suboptimal decision boundaries. LocalScore, a simple scoring algorithm based on local density, improves open-set retrieval and verification significantly.


<details>
  <summary>Details</summary>
Motivation: Open-set biometric systems face challenges with non-mated probes and suboptimal robustness due to existing methods collapsing intra-subject variability into a single global representation.

Method: The proposed LocalScore algorithm uses the local density of the gallery feature distribution via k-th nearest neighbors to improve scoring. It is architecture-agnostic, loss-independent, and computationally efficient.

Result: LocalScore greatly improves open-set biometrics metrics, reducing FNIR@FPIR from 53% to 40%, and improving TAR@FAR from 51% to 74%, across multiple modalities.

Conclusion: LocalScore is a simple, effective, and adaptable solution to enhance open-set biometric systems through local density scoring, achieving significant robustness improvements.

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [468] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: The study finds that deep learning models trained on automatically-curated datasets outperform those trained on manually annotated datasets for thyroid nodule classification.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address the challenge of limited data availability for training deep learning models to classify thyroid nodules in ultrasound images by testing whether automatically-curated datasets can improve model performance.

Method: The authors trained deep learning models on three datasets: a manually annotated dataset, a full automatically-curated dataset, and a subset of the automatically-curated dataset with higher accuracy. They compared the models' AUC scores to evaluate performance differences.

Result: Deep learning models trained on the full automatically-curated dataset achieved a significantly higher AUC (0.694) compared to the manually curated dataset (0.643). The smaller accurate subset had a slightly lower AUC (0.689) than the full automatically-curated dataset but without significant difference.

Conclusion: Using an automatically-curated dataset improves deep learning models' performance for thyroid nodule classification. It is recommended to use the entire dataset instead of limiting to a highly accurate subset.

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [469] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: GMAC is a framework for automatic and online calibration of multi-camera systems using neural networks without explicit 3D reconstruction or manual calibration.


<details>
  <summary>Details</summary>
Motivation: Existing multi-camera calibration methods are not robust in complex or dynamic environments, limiting their use in practical applications.

Method: GMAC leverages implicit geometric representations and latent multi-view structures in neural networks, optimizing for reprojection and cycle consistency, without requiring explicit 3D reconstruction.

Result: Experiments on synthetic and real-world datasets show that GMAC achieves accurate and stable multi-camera calibration.

Conclusion: GMAC enables efficient and reliable multi-camera system calibration, suitable for dynamic environments and practical deployment.

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [470] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: The paper introduces FUSE-Flow, a scalable, real-time framework for multi-view point cloud reconstruction with improved geometric fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to effectively handle real-time, large-scale, multi-view point cloud reconstruction for applications in VR, AR, robotics, etc., under constraints of computational efficiency, high quality, and multi-camera scalability.

Method: A stateless, frame-wise framework using measurement confidence, 3D distance consistency, and an adaptive spatial hashing-based weighted aggregation method for efficient and robust point cloud fusion.

Result: FUSE-Flow demonstrates real-time performance, improved stability, and fidelity in handling complex scenes, with linear complexity and GPU acceleration making it scalable across varying conditions.

Conclusion: FUSE-Flow effectively addresses the challenge of achieving real-time, high-quality point cloud reconstruction while being robust and extensible for multi-camera systems.

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [471] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: The paper proposes Visual Expert Quantization (VEQ), a dual-aware quantization method to address heterogeneity in Mixture-of-Experts Vision-Language Models by considering cross-modal discrepancies and expert contribution differences.


<details>
  <summary>Details</summary>
Motivation: The high memory and computational costs of Mixture-of-Experts Vision-Language Models necessitate compression techniques, which existing quantization approaches fail to fully address due to lack of awareness of vision-language and expert heterogeneity.

Method: VEQ combines Modality-expert-aware Quantization to prioritize key experts based on activation frequency and Modality-affinity-aware Quantization to enhance the calibration process using token-expert affinity and modality data.

Result: VEQ outperformed state-of-the-art quantization methods with notable accuracy improvements (e.g., +2.04% on Kimi-VL and +3.09% on Qwen3-VL) under W3A16 configuration, demonstrating robustness across multimodal tasks.

Conclusion: VEQ offers an effective compression solution for Vision-Language Models by addressing heterogeneity, delivering higher accuracy and robust performance across benchmarks. The code will be published for further development.

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [472] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: The paper presents a framework for generating multimodal task-guidance conversations from instructional videos using large language models, addressing challenges in AR task-guidance AI development.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of large-scale multimodal conversational datasets needed for developing effective AI-based augmented reality assistance systems.

Method: Develop an automatic pipeline utilizing large language models to convert single-person instructional videos into two-person task-guidance conversations, eliminating reliance on costly manual data collection.

Result: Created HowToDIV, a multimodal dataset containing 507 expert-novice conversations, 6,636 question-answer pairs, and 24 hours of video. Benchmarked the dataset using Gemma 3 and Qwen 2.5 for multimodal procedural task assistance.

Conclusion: The proposed framework enables scalable, cost-efficient generation of multimodal datasets and provides a benchmark for advancing AR task-guidance AI systems.

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [473] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: The paper introduces ReLayout, a method to automatically edit design layouts while preserving their structure, overcoming challenges like data scarcity through relation-aware design reconstruction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance automated design workflows by addressing the need for autonomous geometric modification of design layouts based on user intents, while preserving structural integrity and handling data scarcity issues.

Method: The proposed method, ReLayout, uses a relation graph to preserve layout structure and introduces RADR (relation-aware design reconstruction), leveraging a multi-modal large language model. RADR learns self-supervised editing by reconstructing designs with synthesized edit operations.

Result: ReLayout outperforms baseline models in editing quality, accuracy, and layout preservation, as validated by qualitative, quantitative results and user studies.

Conclusion: ReLayout represents a significant advancement in design layout editing by ensuring versatile, high-quality, and structure-preserving results without relying on extensive triplet data.

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [474] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: Residual Decoding (ResDec) is introduced to reduce hallucinations in Large Vision-Language Models (LVLMs) by using historical information for decoding, improving visual grounding and object accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of language priors causing hallucinations in LVLMs, which can undermine their effectiveness in multimodal tasks.

Method: ResDec leverages historical information through the internal reasoning and token logits evolution mechanisms to correct biases in LVLMs decoding without any additional training.

Result: ResDec reduces hallucinations, improves visual grounding, and achieves better performance on LVLM benchmarks.

Conclusion: ResDec is a broadly applicable and effective solution for addressing hallucinations in LVLMs, enhancing their reliability in various multimodal tasks.

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [475] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for ultrasound image analysis with a multi-task benchmark covering segmentation, classification, detection, and regression.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound imaging models lack generalizability across tasks, limiting their clinical deployment potential.

Method: The study uses a Multi-Head Multi-Task Learning (MH-MTL) framework, EfficientNet-B4 backbone, Feature Pyramid Network, and task-specific strategies with adaptive loss scaling.

Result: Validation results support the unified design's feasibility and robustness, establishing it as a strong baseline for research.

Conclusion: The proposed framework provides a promising foundation for developing generalizable ultrasound imaging models, and resources are publicly available for broader research.

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [476] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: The paper introduces a tomographic reconstruction approach using 3D Gaussian ray tracing, offering improved accuracy and system compatibility compared to splatting methods.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) shows potential for real-time novel view synthesis and tomographic reconstruction but faces accuracy challenges due to its local affine approximations.

Method: The method replaces 3DGS's splatting technique with analytical line integrals from ray tracing through 3D Gaussian primitives, enabling physically accurate projections and nonlinear corrections.

Result: This ray tracing method improves accuracy and flexibility in Gaussian-based tomographic models, accommodating diverse systems with better geometric corrections.

Conclusion: The approach enhances tomographic system compatibility and projection accuracy, advancing Gaussian-based reconstruction for practical applications.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [477] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: The paper introduces DRFormer, a framework integrating foundation and vision-language models for person re-identification challenges, excelling in occlusion and pose variation handling.


<details>
  <summary>Details</summary>
Motivation: Person re-identification faces challenges like occlusion and pose variations, requiring fine-grained details and global semantic features from both foundation and vision-language models.

Method: A Dual-Regularized Bidirectional Transformer (DRFormer) framework is proposed, which combines features from DINO and CLIP models through dual-regularization to balance their contributions.

Result: Experiments conducted on five benchmarks demonstrate the DRFormer's ability to harmonize local and global representations, showing competitive performance against state-of-the-art methods.

Conclusion: The integration of complementary features from both architectures effectively addresses person re-identification challenges, offering improved performance in challenging scenarios.

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [478] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: This paper proposes a novel segmentation approach using PDE-constrained optimization to integrate physically motivated priors into deep learning, achieving improved accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of microscopy image segmentation caused by noise, weak boundaries, and limited labeled data, while overcoming issues with unconstrained deep learning models such as instability and poor generalization.

Method: The method formulates segmentation as a PDE-constrained optimization problem using variational regularization. A composite objective function incorporating data fidelity, reaction-diffusion equations, and phase-field interface energies is minimized, implemented as differentiable residual losses. Experiments involve a UNet baseline and testing across different cell types on the LIVECell dataset.

Result: Experiments demonstrate enhanced segmentation accuracy, better boundary fidelity, and improved generalization compared to unconstrained models, especially in low-sample scenarios.

Conclusion: The study shows that incorporating structured priors through PDE-constrained optimization strengthens data-driven models, bridging variational methods and scientific machine learning for better and more generalizable segmentation outcomes.

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [479] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: The paper introduces Piecewise Sparse Attention (PISA), a training-free approach to sparse attention that leverages block-wise Taylor expansion for efficient and high-quality video and image generation.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of Quadratic Attention complexity in Diffusion Transformers for video and image generation and the degradation of quality in block sparse attention at high sparsity levels.

Method: PISA uses an exact-or-approximate strategy, maintaining precise computation for critical key-value blocks while approximating others through block-wise Taylor expansion, covering the full attention span efficiently.

Result: PISA achieves speedups of 1.91x and 2.57x on Wan2.1-14B and Hunyuan-Video, respectively, and a 1.2x acceleration on FLUX for image generation without quality loss.

Conclusion: PISA bridges the gap between attention efficiency and output quality, making it a promising method for enhancing video and image generation in sparse attention frameworks.

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [480] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: The paper introduces a benchmark (MedAD-38K) and a novel training framework to enhance diagnostic accuracy in medical anomaly detection using multimodal models.


<details>
  <summary>Details</summary>
Motivation: To improve plausible reasoning and multimodal generalization in medical anomaly detection models by addressing limitations of supervised fine-tuning on fragmented datasets.

Method: Developed MedAD-38K benchmark and proposed a two-stage training framework: Cognitive Injection for foundational knowledge and Consistency Group Relative Policy Optimization (Con-GRPO) for coherent reasoning processes.

Result: MedAD-R1, the developed model, achieved state-of-the-art performance, outperforming baselines by over 10% on MedAD-38K.

Conclusion: The framework and benchmark improve diagnostic reasoning transparency and coherence, advancing trustworthiness in AI for clinical decision support.

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [481] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: The paper introduces Differential Vector Erasure (DVE), a training-free method for erasing specific concepts like NSFW or copyrighted styles in flow matching diffusion models, improving on existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address concerns of safe and controllable deployment of text-to-image diffusion models, which often reproduce undesirable content such as NSFW, copyrighted information, and specific objects.

Method: Proposes a training-free method called Differential Vector Erasure (DVE) for flow matching diffusion models. It identifies and removes concept-specific components by analyzing the velocity field's directional structure, using a differential vector field to distinguish between target concepts and anchor concepts.

Result: DVE effectively removes undesirable concepts like NSFW or artistic styles while preserving image quality and content diversity. Extensive experiments on FLUX show it outperforms existing baselines.

Conclusion: DVE is a novel and effective solution for concept erasure in flow matching models, enabling safer and more controlled image generation without requiring expensive fine-tuning or impacting image quality.

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [482] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces PandaPose, a method for converting 2D human poses from images into robust 3D poses using intermediate anchor representations, with notable performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in 3D human pose lifting, including error propagation from predicted 2D poses and difficulties in handling self-occlusion.

Method: PandaPose utilizes a unified intermediate representation called the 3D anchor space, featuring joint-wise anchors, depth-aware hierarchical lifting, and an interactive decoder to merge features and anchors.

Result: PandaPose achieves significant improvements in accuracy, reducing error by 14.7% compared to state-of-the-art methods on Human3.6M, backed by strong performance on other benchmarks.

Conclusion: The proposed anchor-based approach effectively mitigates error propagation and resolves self-occlusion ambiguities, demonstrating superior robustness and effectiveness in 3D human pose lifting.

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [483] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: This paper explores harmful meme detection in cases where data is incomplete, such as missing text, and proposes a method that improves performance in such scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing harmful meme detection methods heavily depend on having complete data (including text and image modalities). Real-world challenges, like OCR failures, often lead to missing text, causing the performance of these methods to degrade.

Method: The authors propose a new baseline method that learns shared representations for different modalities by independently projecting them, enabling robustness even when some modalities, like text, are absent.

Result: The proposed method outperforms existing approaches on two benchmark datasets, particularly when textual information is missing.

Conclusion: The study provides a significant improvement in harmful meme detection by enabling robust performance in real-world scenarios where modality data (e.g., text or image components) may be missing.

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [484] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: This paper introduces 'LightCity', a high-quality synthetic urban dataset for inverse rendering under complex lighting conditions in urban scenes, addressing a gap in existing datasets.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges in inverse rendering tasks in urban scenes due to complex illumination, and the absence of datasets suitable for intrinsic decomposition and 3D reconstruction analysis.

Method: The authors create 'LightCity', a synthetic dataset with over 50K images featuring diverse lighting and detailed scene properties (e.g., depth, normal, material), generated under controlled illumination conditions.

Result: LightCity serves as a benchmark for three urban scene tasks and includes a comprehensive analysis, enabling advancements in intrinsic decomposition and 3D reconstruction research.

Conclusion: LightCity provides a robust foundation for addressing complex lighting challenges in urban scene analysis, offering a valuable tool for researchers and applications like autonomous driving.

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [485] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: Koo-Fu CLIP improves visual-language model performance by optimizing embeddings for supervised classification, increasing accuracy and reducing dimensionality.


<details>
  <summary>Details</summary>
Motivation: Visual-language models like CLIP struggle with class separation and high dimensionality in supervised tasks.

Method: The authors employ Fukunaga-Koontz Linear Discriminant Analysis in a whitened embedding space to adapt CLIP embeddings for better class discrimination and dimensionality reduction.

Result: Koo-Fu CLIP achieves significant accuracy improvement on ImageNet benchmarks, increasing top-1 accuracy from 75.1% to 79.1%. It also supports compression up to 10-12x with minimal accuracy loss.

Conclusion: Koo-Fu CLIP effectively enhances class separation and reduces dimensionality, enabling high-performance and efficient classification and retrieval in large-scale settings.

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [486] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: This paper introduces a framework using Remote Sensing and Multimodal Large Language Models for UAV emergency landing site assessments, emphasizing semantic risk understanding with better accuracy than geometric baselines.


<details>
  <summary>Details</summary>
Motivation: Current UAV emergency landing systems primarily use geometric sensors, which cannot detect complex semantic risks, leading to potential safety issues.

Method: The framework employs a coarse-to-fine process: semantic segmentation pre-screens areas, and vision-language reasoning integrates visual features with POI data for fine hazard detection.

Result: Experiments show superior risk identification accuracy compared to geometric methods, with qualitative insights providing human-like justifications.

Conclusion: The proposed approach enhances UAV safety by integrating semantic awareness and interpretable decision support, validated through the newly introduced ELSS benchmark.

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [487] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: This paper introduces EEmoDB, a large dataset and EEmo-Logic, a multimodal language model, to improve understanding of image-evoked emotions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in existing models for image-evoked emotion understanding, which are currently lacking in fine-grained emotion perception and reasoning.

Method: The authors created EEmoDB, a large dataset with 1.2M QA pairs and 36k curated samples, and developed EEmo-Logic, a multimodal large language model, trained with fine-tuned instructions and a customized reward design for emotion tasks.

Result: EEmo-Logic demonstrates strong performance in both in-domain and cross-domain datasets, particularly excelling in emotion-related tasks, including QA and detailed emotion assessments.

Conclusion: EEmoDB and EEmo-Logic represent significant advancements in multi-dimensional emotion understanding and have potential utility in human-computer interaction areas.

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [488] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: This paper introduces CurriSeg, a framework using curriculum-style dynamic data selection and anti-curriculum fine-tuning to improve context-aware segmentation with no additional parameters or training time.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of Context-Entangled Content Segmentation (CECS), where visual patterns of objects overlap with their surroundings, complicating segmentation tasks.

Method: CurriSeg operates in two phases: (1) Curriculum Selection, which filters informative samples using temporal loss statistics, and (2) Anti-Curriculum Promotion, which applies Spectral-Blindness Fine-Tuning to focus on low-frequency contextual information.

Result: Experiments show consistent performance improvements on CECS benchmarks without increasing parameter count or total training time.

Conclusion: CurriSeg effectively integrates curriculum and anti-curriculum strategies to enhance robust segmentation, balancing progression and challenge dynamics in the learning process.

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [489] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: This research introduces a novel pipeline for long-term weather forecasting using the Efficient Multi-scale Transformer (EMFormer) to address issues in error accumulation, computational overhead, and temporal inconsistency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current weather forecasting methods, such as catastrophic forgetting, error accumulation, and high computational loads.

Method: The proposed Efficient Multi-scale Transformer extracts multi-scale features with single convolution during training and inference. An accumulative context finetuning strategy and a composite sinusoidal-weighted loss are implemented.

Result: Improves long-term weather forecasting accuracy, extreme event prediction, generalization on vision benchmarks, and achieves significant computational speedup (5.69x).

Conclusion: The advancements enhance forecasting accuracy and efficiency while demonstrating strong cross-domain applicability and reduced computational demands.

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [490] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: The paper introduces Med3D-R1, a framework focusing on 3D vision-language clinical reasoning using supervised fine-tuning and reinforcement learning, achieving state-of-the-art results on two medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in 3D vision-language models for clinical applications, including complexities in volumetric imaging, overfitting to superficial patterns, and lack of interpretability in model rewards.

Method: It uses a two-stage training process: supervised fine-tuning with residual alignment and abnormality re-weighting, followed by reinforcement learning with redesigned consistency rewards for better diagnostic reasoning.

Result: The model achieves state-of-the-art accuracies of 41.92% on CT-RATE and 44.99% on RAD-ChestCT, improving abnormality diagnosis and clinical reasoning.

Conclusion: Med3D-R1 enhances diagnostic workflows with more reliable and interpretable 3D vision-language systems, showing potential in real-world clinical applications.

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [491] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: This paper integrates textual features into point-supervised temporal action localization through a Text Refinement and Alignment (TRA) framework, enhancing localization performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for point-supervised temporal action localization neglect semantic information from text, relying solely on visual inputs, limiting their effectiveness.

Method: The proposed TRA framework introduces two modules: a Point-based Text Refinement module (PTR) to enhance textual descriptions, and a Point-based Multimodal Alignment module (PMA) using multimodal feature contrastive learning to bridge modalities for improved task performance.

Result: Extensive experiments on five benchmarks show that the TRA framework outperforms state-of-the-art methods in temporal action localization, and it is computationally efficient to run on standard hardware.

Conclusion: The study highlights the importance of leveraging multimodal features, demonstrating the practicality of integrating textual information with visual cues for accurate and scalable temporal action localization.

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [492] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: The paper introduces Q-DiT4SR, a post-training quantization framework specifically designed for Diffusion Transformers in real-world image super-resolution tasks to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) are effective in generating high-quality textures for image super-resolution but are computationally expensive, making them challenging for real-world deployment.

Method: The Q-DiT4SR framework employs two key techniques: H-SVD, a hierarchical SVD method combining global low-rank and local rank-1 block modeling, and Variance-aware Spatio-Temporal Mixed Precision (VaSMP), which optimizes bit-width allocation and activation precision for quantization.

Result: Experimental results show that Q-DiT4SR achieves state-of-the-art performance under W4A6 and W4A4 quantization settings, significantly reducing model size (by 5.8×) and computational operations (over 60×).

Conclusion: Q-DiT4SR enables efficient real-world deployment of DiTs for image super-resolution with minimal quality degradation, offering a robust solution to alleviate inference burdens while maintaining high performance.

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [493] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: This paper introduces the TrafficFlow-aware Lane perception Module (TFM), enhancing lane perception under challenging conditions by utilizing real-time traffic flow information.


<details>
  <summary>Details</summary>
Motivation: Current lane detection methods struggle in occluded or lane-missing scenarios, and high-definition maps are costly and inefficient for real-time applications.

Method: The proposed TFM extracts and integrates real-time traffic flow features with existing lane perception algorithms, validated using public datasets and real-world scenarios.

Result: TFM improves lane perception performance with up to +4.1% mAP gain on the Nuscenes dataset across four models and two public datasets.

Conclusion: TFM effectively addresses challenging lane perception scenarios, providing a cost-efficient and performance-enhanced solution for autonomous driving systems.

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [494] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: The paper introduces DSFC-Net, a novel framework for extracting rural roads from high-resolution imagery, overcoming challenges of high variability, vegetation occlusion, and narrow roads.


<details>
  <summary>Details</summary>
Motivation: To improve rural road detection from high-resolution imagery, addressing unique issues such as intra-class variability, vegetation occlusion, and narrow road widths that traditional urban-focused methods fail to handle.

Method: The DSFC-Net framework utilizes a dual-encoder structure with a CNN branch for local boundary detection and a Spatial-Frequency Hybrid Transformer for global topological modeling. It also employs a Cross-Frequency Interaction Attention module and a Channel Feature Fusion Module to enhance segmentation performance.

Result: Experiments using datasets like WHU-RuR+, DeepGlobe, and Massachusetts showed DSFC-Net outperforms existing state-of-the-art methods in rural road segmentation.

Conclusion: DSFC-Net effectively addresses rural road detection challenges by leveraging both spatial and frequency-domain information, resulting in improved accuracy and segmentation quality.

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [495] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper identifies cross-lingual shared safety neurons in multilingual models, proposes a neuron-oriented training strategy, and demonstrates improvements in safety for non-high-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the imbalance in multilingual safety where non-high-resource languages are more vulnerable than high-resource ones.

Method: The study identifies and analyzes safety neurons, validates their causal roles, and fine-tunes these neurons using a neuron-oriented training strategy targeting cross-lingual shared safety neurons.

Result: Suppressing shared safety neurons causes safety drops in non-high-resource languages, while reinforcing them enhances cross-lingual safety consistency.

Conclusion: Fine-tuning shared safety neurons significantly improves safety for non-high-resource languages without compromising model's general capabilities.

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [496] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map introduces a framework for 3D line mapping using multi-view RGB images, leveraging line-plane optimization for accurate and efficient scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D line mapping lack integration of planar topology, which limits accuracy and structural fidelity in man-made environments.

Method: LiP-Map employs a line-plane joint optimization framework to model learnable line and plane primitives that interact explicitly instead of enforcing pairwise constrains.

Result: LiP-Map achieves superior accuracy and completeness in 3D line mapping on diverse datasets and enhances line-assisted visual localization capabilities.

Conclusion: LiP-Map offers a novel and effective approach for detailed and structured 3D line mapping, showing potential for diverse applications in man-made environments.

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [497] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: This paper introduces REORM, a framework for interaction-consistent object removal in images by leveraging multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: Current object removal techniques fail to address associated interaction elements, leading to semantically inconsistent results.

Method: Proposes REORM framework integrating MLLM-driven analysis, mask-guided removal, self-correction mechanism, and a resource-efficient variant.

Result: REORM performs well on ICOREval benchmark, outperforming existing image editing systems.

Conclusion: REORM effectively addresses interaction-consistent removal, offering improved image editing capabilities.

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [498] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: The paper introduces ReDiStory, a training-free framework to improve visual story generation by reorganizing prompt embeddings to reduce inter-frame semantic interference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining consistent subject identity across multiple images while preserving the unique semantics of individual frames in visual story generation.

Method: ReDiStory decomposes text embeddings into identity-related and frame-specific components and suppresses shared directions across frames, reducing inter-frame interference without altering diffusion parameters or requiring additional supervision.

Result: ReDiStory, under the same diffusion settings as benchmarks, enhances identity consistency and maintains prompt fidelity, demonstrating superior performance on multiple metrics in the ConsiStory+ benchmark.

Conclusion: ReDiStory offers a practical, training-free solution for improving multi-frame story generation, achieving better identity and prompt consistency compared to existing methods like 1Prompt1Story.

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [499] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: The paper introduces StoryState, a system that enhances storybook generation by incorporating explicit and editable story states for improved visual consistency and localized editing.


<details>
  <summary>Details</summary>
Motivation: Motivated by the lack of explicit and editable story states in current multimodal models which affect the visual consistency and editing precision of storybooks.

Method: StoryState uses an agent-based orchestration layer with structured representations (character sheet, global settings, scene constraints) and employs LLM agents to maintain these states for generating and editing prompts.

Result: System experiments demonstrate StoryState’s ability to improve localized edits, cross-page consistency, and reduce editing time while maintaining compatibility with existing generation models.

Conclusion: The framework offers a structured, model-agnostic approach to story state editing, achieving better consistency and efficiency in storybook generation across diverse generation backends.

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [500] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: This paper introduces DeCorStory, a training-free framework for improving visual and semantic consistency in text-to-image storytelling, which outperforms existing baselines by reducing semantic interference and preserving identity.


<details>
  <summary>Details</summary>
Motivation: Existing methods like One-Prompt-One-Story often suffer from visual artifacts such as color leakage, background blending, and identity drift due to strong embedding correlations between consecutive frames.

Method: DeCorStory employs Gram-Schmidt prompt embedding decorrelation, singular value reweighting, and identity-preserving cross-attention to mitigate inter-frame semantic interference, requiring no model modifications or fine-tuning.

Result: Experiments confirm DeCorStory improves prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free methods.

Conclusion: DeCorStory offers an effective, training-free solution that enhances visual and semantic consistency in text-to-image storytelling and integrates seamlessly into existing diffusion pipelines.

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [501] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: FlowCast is a training-free, plug-and-play speculative generation framework that accelerates Flow Matching (FM)-based visual generation, providing over 2.5x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Slow inference of Flow Matching models due to numerous denoising steps limits their usability for real-time or interactive applications.

Method: FlowCast speculates future velocity by extrapolating current velocity using FM models' constant velocity training property. It skips redundant steps in stable regions while maintaining precision in complex ones without retraining or adding auxiliary networks.

Result: FlowCast achieves over 2.5x speedup in visual generation tasks like image, video generation, and editing, outperforming existing methods without degrading quality.

Conclusion: FlowCast offers a robust acceleration framework for FM models, enhancing their practical usability while maintaining output quality and theoretical guarantees.

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [502] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: The paper introduces a framework, MED, to analyze the effects of vision tool-use reinforcement learning (RL) in vision-language models (VLMs), finding that the main improvements come from intrinsic learning rather than mastering the tools.


<details>
  <summary>Details</summary>
Motivation: To investigate whether performance gains in vision tool-use reinforcement learning are driven by mastery of tool-use or by intrinsic capability improvement.

Method: A framework called MED (Measure-Explain-Diagnose) was used to disentangle intrinsic capability changes from tool-use effects, compute gain and harm terms, and evaluate the evolution of these factors across different benchmarks and VLMs.

Result: The analysis revealed that performance improvements are largely due to intrinsic learning, while tool-use RL reduces tool-induced harm but offers limited advancement in correcting intrinsic failures using tools.

Conclusion: Current vision tool-use RL primarily enables vision-language models to coexist safely with tools, rather than fully mastering their use.

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [503] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: The paper introduces a method to generate visual metaphors by transferring abstract logic from one image onto another using a novel multi-agent framework inspired by cognitive theories.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in current generative AI models, which fail to capture abstract logic and genuine metaphorical creativity, restricting innovation in visual rhetoric.

Method: The authors propose a Visual Metaphor Transfer (VMT) task using a cognitive-inspired, multi-agent framework based on Conceptual Blending Theory. Their schema grammar decouples relational invariants for effective logic transfer.

Result: Their approach significantly outperforms state-of-the-art methods in metaphor consistency, analogy appropriateness, and visual creativity, validated through extensive experiments and human evaluations.

Conclusion: The method enables automated creative applications in fields like advertising and media, pushing the boundaries of generative AI to better represent abstract and metaphorical concepts.

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [504] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: The paper proposes a method for Variational Autoencoders (VAEs) to support multi-level temporal compression for videos, effectively combating performance decline at higher compression rates.


<details>
  <summary>Details</summary>
Motivation: To address the performance drop in VAEs when attempting higher compression rates without expanding hidden channel dimensions.

Method: Convert VAEs with fixed compression rates into multi-level temporal compression models through minimal fine-tuning, and integrate them successfully with diffusion-based generative models.

Result: The multi-level temporal compression approach improves video compression efficiency and compatibility with generative models like DiT, backed by empirical evidence.

Conclusion: The proposed technique enables efficient video compression at multiple levels of temporal detail while maintaining high performance and integration with diffusion models.

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [505] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA proposes a training-free framework to accelerate Visual AutoRegressive (VAR) modeling by utilizing entropy analysis for adaptive token reduction.


<details>
  <summary>Details</summary>
Motivation: Existing acceleration methods for VAR models lack adaptability and fail to utilize the full acceleration potential due to heuristic decisions and non-dynamic schedules.

Method: NOVA uses entropy analysis to adaptively determine scales for token reduction, dynamically adjusts reduction ratios for scales and layers, and reuses cached residuals to maintain quality during accelerated inference.

Result: Experiments confirm NOVA effectively accelerates inference while preserving generation quality.

Conclusion: NOVA offers an efficient, simple, and training-free solution to overcome the limitations of existing approaches in VAR acceleration methods.

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [506] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: This paper introduces T2M Mamba, a model for converting motion language into 3D human motions, with improvements in handling periodicity-keyframe coupling and robustness to text paraphrase variations.


<details>
  <summary>Details</summary>
Motivation: Existing models for text-to-motion generation struggle with generation drift due to ignoring interplay between motion periodicity and keyframes and are fragile to slight textual paraphrase variations.

Method: The T2M Mamba model includes Periodicity-Saliency Aware Mamba for coupled dynamic estimation and Periodic Differential Cross-modal Alignment Module for robust motion-text alignment.

Result: Experiments on HumanML3D and KIT-ML datasets demonstrate superior performance, achieving an FID of 0.068 and consistent metric improvements.

Conclusion: T2M Mamba addresses prior limitations in text-to-motion models, enhancing long-sequence coherence and robustness to semantically equivalent text inputs.

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [507] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: This paper investigates adversarial robustness in Mixture-of-Experts (MoE) models used for video understanding, proposes new attack methods exploiting vulnerabilities, and introduces a defense training method to mitigate these weaknesses.


<details>
  <summary>Details</summary>
Motivation: Existing studies on Mixture-of-Experts (MoE) models overlook adversarial vulnerabilities in its components such as routers and expert modules, despite their proven strong performance in video understanding tasks.

Method: The paper introduces Temporal Lipschitz-Guided Attacks (TLGA) to probe weaknesses in routers and Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which perturb routers and experts jointly. It also proposes Joint Temporal Lipschitz Adversarial Training (J-TLAT) to enhance robustness through joint training.

Result: The proposed J-TLAT framework improves adversarial robustness, reduces inference costs by over 60% compared to dense models, and demonstrates consistent effectiveness across diverse datasets and architectures.

Conclusion: The study thoroughly investigates and mitigates component-wise adversarial vulnerabilities in MoE video understanding models, making them more robust and computationally efficient.

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [508] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen redefines synthetic data creation for vision-language pre-training by prioritizing manifold coverage and diversity over simple dataset size, using diverse generators and a hard negative curriculum.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art synthetic data methods rely on a single generative backbone, leading to spectral biases and limited feature diversity. The researchers aim to improve synthetic data's utility by overcoming these limitations.

Method: PolyGen employs multiple distinct generative models instead of a single generator, eliminating model-specific artifacts. The method also introduces a Programmatic Hard Negative curriculum to enforce fine-grained semantic understanding.

Result: PolyGen outperformed the leading baseline (SynthCLIP) by +19.0% on multi-task benchmarks and +9.1% on the SugarCrepe++ compositionality benchmark.

Conclusion: The study concludes that structural data diversity and methodological rigor are more effective for scaling synthetic datasets than simply increasing dataset size.

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [509] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: The paper introduces PromptRL, a framework for flow-based reinforcement learning in text-to-image (T2I) generation, which integrates language models for prompt refinement, significantly improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address key limitations in existing reinforcement learning pipelines for flow matching models in T2I generation: sample inefficiency due to insufficient diversity and prompt overfitting.

Method: The proposed PromptRL incorporates language models as trainable agents for prompt rewriting within the flow-based RL optimization loop, enabling more dynamic optimization and improved learning outcomes.

Result: PromptRL achieves state-of-the-art metrics across multiple benchmarks, improving metrics like GenEval (0.97), OCR accuracy (0.98), and PickScore (24.05), while also advancing large-scale image editing and reducing training rollouts.

Conclusion: PromptRL proves to be a highly efficient framework for flow-based RL in T2I tasks, achieving superior performance with reduced data requirements and establishing itself as a powerful tool for advanced image generation and editing.

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [510] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: This paper introduces the Augmented Latent Intrinsics (ALI) method for image-to-image relighting, addressing limitations in disentangling scene properties from illumination, especially for complex materials.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome failures in image relighting associated with challenging materials (e.g., metal and glass) and the under-constrained nature of existing latent intrinsic representations.

Method: The method introduces a framework that combines pixel-aligned visual encoder features into a latent-intrinsic model with self-supervised refinement strategies to compensate for the lack of paired real-world data.

Result: ALI achieves significant improvements in relighting, particularly for complex specular materials, using unlabeled real-world image pairs and pixel-aligned visual priors.

Conclusion: The study reveals a trade-off between semantic abstraction and photometric fidelity in relighting tasks and demonstrates that ALI effectively balances these factors, providing a robust solution for improved relighting quality.

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [511] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: The paper introduces Parabolic Position Encoding (PaPE), a new position encoding method tailored for vision modalities to improve attention-based architectures' performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing position encoding methods that only partially consider the unique characteristics of vision modalities.

Method: Proposes PaPE, a position encoding designed from principles like translation and rotation invariance, distance decay, directionality, and context awareness. Evaluations are conducted across 8 datasets and 4 modalities.

Result: PaPE or its rotation-invariant variant (PaPE-RI) achieves top performance on 7 out of 8 datasets, with significant improvements in extrapolation experiments on ImageNet-1K.

Conclusion: PaPE effectively encodes spatial relationships in vision modalities, offering substantial performance improvements and setting a new benchmark for position encoding designs in attention-based architectures.

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [512] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet is a new framework for detecting tampered biomedical images using advanced attention modules, achieving superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Biomedical images require specialized tools for detecting subtle tampering, as existing forensic models designed for natural images fail to perform effectively.

Method: The framework utilizes affinity-guided self- and cross-attention modules based on State Space Model-inspired linear attention to locate duplicated image regions efficiently.

Result: BioTamperNet achieves significant improvement in detecting tampered regions compared to existing methods, validated through experiments on bio-forensic datasets.

Conclusion: BioTamperNet provides a powerful and efficient methodology for forensic analysis specifically tailored to the challenges of biomedical imagery.

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [513] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: This paper investigates drivers' gaze behavior using three vision-based approaches: direct object detection, segmentation-assisted classification, and Vision-Language Models. Largest Vision-Language Model showed best results for identifying small objects under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To understand drivers' visual attention during driving and improve advanced driver-assistance systems and road safety.

Method: Three approaches were analyzed: direct object detection (YOLOv13), segmentation-assisted classification (EfficientNetV2 and SAM), and Vision-Language Models (Qwen2.5-VL variations). Performance was evaluated for visual identification of objects in road scenes.

Result: Direct object detection and the largest Vision-Language Model (Qwen2.5-VL-32b) achieved significant results, with highest Macro F1-Scores over 0.84. Segmentation-based methods showed poor recall due to semantic gaps.

Conclusion: Large Vision-Language Models provide better robustness and accuracy for real-world applications in intelligent driver monitoring systems, especially for safety-critical objects and adverse conditions such as nighttime driving.

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [514] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: This paper explores the behavior of quantized vision transformers (DeiT, DeiT3, ViT) particularly on out-of-distribution (OOD) datasets, highlighting that pretraining these models on large-scale datasets like ImageNet-22k adversely impacts their low-bit quantization robustness in OOD detection.


<details>
  <summary>Details</summary>
Motivation: To enable vision transformers for accessible and real-time applications, addressing challenges posed by quantization, particularly its impact on performance in OOD scenarios.

Method: Quantized small-variant popular vision transformers were analyzed on common OOD datasets for their robustness, focusing on the influence of low-bit quantization and pretraining dataset scales using metrics such as AUPR-out and quantization delta.

Result: Models pretrained on larger datasets like ImageNet-22k showed greater vulnerability to low-bit quantization, experiencing sharper drops in OOD detection robustness compared to those pretrained on smaller datasets like ImageNet-1k.

Conclusion: Pretraining on large-scale datasets may hinder the robustness of low-bit quantization in OOD detection; leveraging techniques like data augmentation may offer better alternatives for improving performance.

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [515] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: The paper introduces a Logit Lens Loss (LLL) to enhance the Logit Lens visualization in Vision-Language Models by maintaining the locality of visual tokens, providing improved image explainability and task performance.


<details>
  <summary>Details</summary>
Motivation: Logit Lens visualizations in Vision-Language Models lose their locality as visual content gets diffused into language tokens, making them ineffective for explainability.

Method: The authors propose a Logit Lens Loss (LLL) to preserve the visual representation in image tokens during model training, aligning them semantically with textual concepts and preventing excessive mixing in self-attention layers.

Result: LLL produces meaningful object confidence maps in images and enhances performance in vision-centric tasks like segmentation without requiring architectural changes or large-scale training.

Conclusion: The proposed LLL effectively addresses the diffusion of visual content, making Logit Lens usable for explainability while simultaneously improving vision-specific tasks.

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [516] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: This paper proposes a method for online handwritten character recognition that is robust to rotational deformations using Sliding Window Path Signature (SW-PS) and Linear Recurrent Units (LRU).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of rotational deformation disrupting stroke spatial layouts, which reduces accuracy in online handwritten character recognition.

Method: The method involves employing SW-PS for capturing local structural features of characters and using the lightweight LRU classifier to model dynamic stroke characteristics efficiently.

Result: The proposed method achieved high accuracy on rotated input: $99.62\%$ for digits, $96.67\%$ for English upper letters, and $94.33\%$ for Chinese radicals, surpassing other models.

Conclusion: The SW-PS+LRU framework is concluded to be effective in improving convergence speed and test accuracy in online handwritten character recognition, even under rotational variations.

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [517] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: This paper introduces InteractAvatar, a novel framework for generating talking avatars capable of text-aligned, grounded human-object interactions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of creating talking avatars that can perform interactions grounded in their environment, as opposed to just simple motion, addressing the gap in grounded human-object interaction (GHOI).

Method: The authors propose a dual-stream framework—InteractAvatar—which decouples environmental perception and planning from video synthesis. It includes a Perception and Interaction Module (PIM) to create aligned interaction motions and an Audio-Interaction Aware Generation Module (AIM) for synthesizing videos, controlled using a specially designed motion-to-video aligner.

Result: The framework successfully generates talking avatars performing grounded human-object interactions effectively and establishes a new benchmark, GroundedInter, to evaluate such methods.

Conclusion: The proposed method overcomes the control-quality tradeoff in GHOI and demonstrates advanced capabilities in generating realistic interactions, ensuring alignment with environmental and textual cues.

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [518] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: The paper introduces FSCA-Net, a model designed to enhance crowd counting across diverse environments by overcoming domain discrepancies through feature disentanglement and cross-attention fusion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address performance issues in crowd counting models due to domain discrepancies, which limit their generalization across diverse environments despite advancements in CNN- and Transformer-based methods.

Method: This paper introduces FSCA-Net, a framework that disentangles features into domain-invariant and domain-specific components, employs a cross-attention fusion module for effective knowledge transfer, and optimizes mutual information to maximize consistency and reduce redundancy in feature representations.

Result: FSCA-Net achieves state-of-the-art cross-dataset generalization in crowd counting, outperforming existing methods on multiple benchmarks by mitigating negative transfer effects.

Conclusion: FSCA-Net serves as a robust and scalable solution for real-world crowd analysis, effectively enhancing performance across datasets by disentangling feature representations and improving generalization.

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [519] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: The paper introduces Cognitive Supersensing, a framework enhancing Multimodal Large Language Models (MLLMs) with human-like visual reasoning through latent visual imagery and evaluation using a new VQA benchmark.


<details>
  <summary>Details</summary>
Motivation: The study aims to address MLLMs' limited ability to solve complex cognitive problems, particularly those involving abstract or memory-reliant visual details, which current CoT reasoning models struggle with.

Method: The authors propose Cognitive Supersensing, a training paradigm using Latent Visual Imagery Prediction (LVIP) for learning visual cognitive latent embeddings and combining them with reinforcement learning for text reasoning optimization.

Result: MLLMs trained with Cognitive Supersensing achieved significant improvements on the CogSense-Bench and demonstrated superior adaptability on out-of-domain mathematics and science VQA tasks.

Conclusion: Integrating internal visual imagery facilitates better reasoning, advancing MLLMs beyond mere perceptual tasks to tackle more complex visual-cognitive challenges effectively. Benchmark and models will be open-sourced.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [520] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of restoring screen-captured images affected by moiré patterns and flicker-banding using a unified framework named CLEAR, supported by a large-scale dataset and novel design techniques.


<details>
  <summary>Details</summary>
Motivation: Screen-captured images often suffer from severe visual degradations due to the coexistence of moiré patterns and flicker-banding, which previous methods fail to handle effectively.

Method: The paper introduces the CLEAR framework that integrates a frequency-domain module and trajectory alignment loss, supported by a large-scale dataset and ISP-based flicker simulation pipeline to handle compound degradations.

Result: Experiments show that CLEAR consistently outperforms other image restoration methods across different evaluation metrics in real-world scenarios.

Conclusion: CLEAR framework efficiently removes compound artifacts, making it a promising solution for screen-captured image restoration tasks.

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [521] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: This paper presents a benchmark named Multimodal UNcommonsense (MUN) to evaluate models' reasoning on atypical visual or contextual scenarios, introducing a retrieval-based learning framework (R-ICL) that improves reasoning capabilities in discordant image-text pairs.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of commonsense reasoning in abnormal or non-prototypical multimodal contexts, aiming to enhance AI robustness and logic handling in unexpected situations.

Method: The paper proposes a retrieval-based in-context learning (R-ICL) framework utilizing a Multimodal Ensemble Retriever (MER) to transfer reasoning capabilities from larger to smaller models without additional training.

Result: Experiments demonstrate an 8.3% average improvement over baseline in-context learning methods in handling atypical scenarios.

Conclusion: MUN benchmark and R-ICL framework provide avenues for enhancing the robustness and adaptability of visual-language models in culturally diverse and unexpected contexts.

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [522] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: The paper introduces a diffusion-based image compression technique that reduces decoding steps to one, enhancing speed significantly while maintaining high-quality output.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based image compression methods face high latency and computational demands during decoding due to numerous denoising steps.

Method: A one-step diffusion process combined with a discriminator operating on feature representations, rather than raw pixels, to improve perceptual quality.

Result: The proposed method achieves comparable compression performance with a 46× faster inference speed compared to similar methods.

Conclusion: The approach offers a balance between speed and quality, addressing latency challenges in diffusion-based image compression.

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [523] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces SGHA-Attack, a new framework to improve targeted transfer attacks on vision-language models by using multiple references and enforcing intermediate-layer consistency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prior transfer-based adversarial attacks that overfit surrogate-specific embeddings and fail to generalize across different vision-language models.

Method: The authors propose SGHA-Attack, which combines a visually grounded reference pool created from a text-to-image model and aligns intermediate-layer visual and textual representations throughout the feature hierarchy with global and spatial adjustments.

Result: SGHA-Attack showcases superior targeted transferability against black-box VLMs and resilience to preprocessing and purification defenses compared to previous methods.

Conclusion: Leveraging semantic-guided hierarchical alignment ensures better transferability and robustness, highlighting the importance of utilizing intermediate-layer semantics in transfer attacks for vision-language models.

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [524] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: The paper introduces HandMCM, a robust method for 3D hand pose estimation, addressing challenges like self-occlusion and object-induced occlusions using advanced state space modeling techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and reliability of 3D hand pose estimation, which is essential for applications in human-computer interaction like augmented reality, despite challenges such as self-occlusion and object interactions.

Method: A novel method, HandMCM, was proposed based on the Mamba state space model. It incorporates modules for local information processing and correspondence modeling, along with utilizing multi-modal image features for better robustness.

Result: HandMCM achieved superior performance on three benchmark datasets, especially under challenging occlusions, surpassing current state-of-the-art methods.

Conclusion: HandMCM has demonstrated its potential to improve 3D hand pose estimation, making it more accurate and reliable for real-world applications.

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [525] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: The paper introduces TAFS GRPO, a novel RL-based framework for efficient few-step text-to-image generation with enhanced alignment to human preferences.


<details>
  <summary>Details</summary>
Motivation: Current RL-based approaches in flow matching models face sparse and imprecise rewards and depend on multiple denoising steps, leading to inefficiencies and suboptimal alignment with human preferences.

Method: The TAFS GRPO method introduces adaptive temporal noise during sampling, adds stochasticity while preserving semantic image integrity, and uses a step-aware advantage integration mechanism for dense and stable policy optimization.

Result: Empirical experiments show that TAFS GRPO achieves strong performance in efficient text-to-image generation while significantly improving human preference alignment.

Conclusion: TAFS GRPO effectively mitigates challenges in RL-based text-to-image generation and enhances efficiency and human preference alignment, offering robust tools for future advancements in the field.

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [526] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper introduces a new Mamba-based architecture for salient object detection (SOD) tasks with improved computational efficiency and versatility. Extensive experiments show its superior performance across multiple datasets and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing SOD approaches are limited by CNNs' constrained receptive fields and Transformers' computational inefficiency. There is a need for a unified, efficient model capable of handling various SOD tasks.

Method: The paper introduces Saliency Mamba (Samba), a pure Mamba-based architecture, with innovations like the saliency-guided Mamba block (SGMB) and context-aware upsampling (CAU). Samba+ further enhances task versatility with hub-and-spoke graph attention and modality-anchored continual learning.

Result: Samba achieves state-of-the-art results across six SOD tasks on 22 datasets with reduced computational cost. Samba+ outperforms Samba, providing a single versatile model for various SOD tasks and modalities.

Conclusion: The paper demonstrates the effectiveness and adaptability of the proposed Samba and Samba+ frameworks for SOD tasks, overcoming limitations of prior models and establishing new benchmarks for performance and efficiency.

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [527] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: This paper introduces a Unified Versatile Multimodal Multi-Task Learning framework (UV-M3TL) to improve Advanced Driver Assistance Systems' (ADAS) performance by tackling driver behavior, emotion, vehicle, and traffic analysis in a multi-task setup. It achieves state-of-the-art results using adaptive loss mechanisms and dual-branch structures.


<details>
  <summary>Details</summary>
Motivation: To enhance Advanced Driver Assistance Systems (ADAS) by enabling them to effectively recognize and interpret various heterogeneous tasks like human driver behavior, emotion, vehicle behavior, and traffic context without performance degradation due to inter-task conflicts.

Method: The UV-M3TL framework includes two components: dual-branch spatial channel multimodal embedding (DB-SCME) for handling shared and specific task features, and adaptive feature-decoupled multi-task loss (AFD-Loss), which stabilizes learning and improves multi-task representation using adaptive weighting and feature decoupling constraints.

Result: The proposed framework achieves state-of-the-art performance on the AIDE dataset for all four tasks and outperforms benchmarks in diverse multi-task perception datasets, such as BDD100K, CityScapes, NYUD-v2, and PASCAL-Context.

Conclusion: UV-M3TL provides a robust and versatile solution for multi-task learning in ADAS, effectively mitigating inter-task negative transfer while ensuring superior performance across tasks in diverse multimodal contexts.

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [528] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: ToPi is a training-free token pruning framework designed to improve efficiency in in-context generation for Diffusion Transformers, achieving >30% inference speedup.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency in in-context generation of Diffusion Transformers due to increased sequence length from input concatenation.

Method: Uses offline calibration-driven sensitivity analysis to determine key attention layers, introducing an influence metric for selective token pruning and a temporal update strategy for adaptive pruning.

Result: ToPi accelerates inference by over 30% while preserving structural fidelity and visual quality in image generation tasks.

Conclusion: ToPi successfully mitigates computational bottlenecks in in-context generation for DiTs, balancing efficiency gains with high-quality outcomes.

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [529] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: The paper introduces Omni-Judge, an evaluative tool leveraging omni-modal large language models (omni-LLMs) to assess text-to-audio-video generation. It shows promising results but also exhibits limitations in temporal resolution.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for text-conditioned audio-video generation are either human-dependent, costly, or rely on traditional metrics that lack scalability, interpretability, and handle complexity poorly. Omni-LLMs appear promising as an alternative evaluation solution.

Method: The authors propose Omni-Judge, an evaluation approach using omni-LLMs to assess nine metrics in multi-modal output. It examines metrics such as alignment between audio-text, video-text, coherence, and high-FPS-related perceptual tasks.

Result: Omni-Judge achieved comparable correlation with existing metrics and excelled in semantically demanding evaluations like alignment across modalities. However, it struggled in evaluating high-FPS aspects like video quality and audio-video synchronization.

Conclusion: Omni-Judge shows potential as an interpretable and efficient evaluator for multi-modal tasks but still has limitations, especially in evaluating high-frame-rate aspects. It sets the stage for improving omni-LLMs for more comprehensive evaluation.

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [530] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: The paper presents PISCES, an annotation-free post-training method for text-to-video generation that uses Dual Optimal Transport-aligned Rewards to improve video quality and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: Existing T2V methods face challenges like reliance on large-scale annotations or reliance on pre-trained model embeddings, limiting scalability and supervision quality.

Method: PISCES introduces a novel Dual Optimal Transport-aligned Rewards module, bridging text and video embeddings at distributional and token levels to provide rewards that ensure quality and semantic alignment without annotations.

Result: PISCES demonstrates superior performance on both short and long video generation tasks when evaluated on VBench, outperforming prior methods while also receiving validation through human preferences.

Conclusion: PISCES effectively addresses annotation limitations in T2V post-training via its OT-based reward system, offering a scalable and superior alternative for enhancing video generation.

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [531] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper identifies the fragmented nature of current world models and proposes a unified design specification.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified framework for world models, which are crucial for enabling agents to predict and interact with complex environments.

Method: The authors analyze limitations in current approaches and propose a design specification that integrates interaction, perception, symbolic reasoning, and spatial representation.

Result: They provide a structured perspective aimed at guiding research on holistic world models.

Conclusion: The paper emphasizes the need for a systematic and principled framework to advance world modeling research.

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [532] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: The paper introduces a federated learning framework improving Vision Transformer (ViT) performance on imbalanced and heterogeneous datasets using dynamic adaptive focal loss and a client-aware aggregation strategy.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning models require large datasets, but regulations on data privacy restrict access, especially for sensitive datasets like medical images. Federated learning offers a collaborative model training approach without compromising data privacy, but challenges persist due to data heterogeneity and class imbalance.

Method: The authors propose a dynamic adaptive focal loss (DAFL) with a class imbalance coefficient tailored to local client data distributions. They also introduce a client-aware aggregation strategy considering data size and characteristics for better inter-client generalization.

Result: The framework demonstrated superior classification accuracy on ISIC, Ocular Disease, and RSNA-ICH datasets, surpassing multiple state-of-the-art models by 0.98% to 41.69%. Ablation studies confirm the effectiveness of their proposed methods.

Conclusion: The proposed federated learning framework addresses class imbalance and client heterogeneity, improving model generalization and classification accuracy in privacy-preserving setups, particularly for medical imaging.

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [533] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReCALL is a model-agnostic framework addressing Capability Degradation in adapting generative Multimodal Large Language Models (MLLMs) for Composed Image Retrieval (CIR). It recalibrates retrievers through self-guided mining, CoT prompting, and continual training with contrastive schemes, resulting in state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of adapting generative MLLMs into single-embedding discriminative retrievers, leading to Capability Degradation in cross-modality compositional reasoning for CIR.

Method: ReCALL follows a pipeline: diagnose cognitive blind spots via informative instance mining, generate corrective instructions/triplets using CoT prompting with quality control, and refine retrievers through continual training on grouped contrastive schemes.

Result: ReCALL improves fine-grained visual-semantic distinctions and achieves state-of-the-art performance on CIRR and FashionIQ datasets.

Conclusion: This work successfully demonstrates how ReCALL mitigates issues in adapting MLLMs for CIR tasks, recalibrating degraded capabilities and advancing retrieval accuracy effectively.

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [534] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: The paper introduces CaCoVID, a method to optimize video token selection using contributions to correct predictions, enhancing efficiency in video understanding models.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and computational demand caused by redundant video tokens in video large language models, and improve token selection based on actual contributions to correct predictions.

Method: Introduce a reinforcement learning-based framework to optimize a policy for selecting video token combinations. Additionally, employ a combinatorial policy optimization algorithm with online sampling to reduce exploration space and speed up convergence.

Result: Demonstrated the effectiveness of CaCoVID across various video understanding benchmarks, showing improved computational efficiency and accurate token selection.

Conclusion: CaCoVID shifts token selection from attention-score reliance to contribution-aware optimization, providing a more efficient approach to video token compression for video understanding tasks.

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [535] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: The paper presents a new approach to improving temporally consistent human-centric dense prediction in videos using a synthetic data pipeline and a unified ViT-based model.


<details>
  <summary>Details</summary>
Motivation: Current per-frame models for dense prediction in videos face challenges with temporal consistency due to motion, occlusion, and lighting changes, and there is a lack of paired human video supervision across multiple dense tasks.

Method: The authors developed a synthetic data pipeline producing photorealistic human frames and sequences with pixel-accurate dense labels, alongside a unified ViT-based dense predictor incorporating human geometry priors and temporal supervision strategies.

Result: The proposed approach achieves state-of-the-art results on THuman2.1 and Hi4D benchmarks and generalizes well to real-world videos.

Conclusion: Their model, combining static pretraining and dynamic sequence training, demonstrates improved spatial and temporal consistency for human-centric dense predictions across diverse scenarios.

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [536] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: Lunara Aesthetic II is a publicly available, ethically designed dataset with 2,854 image pairs for studying contextual consistency and identity preservation in image generation/editing systems.


<details>
  <summary>Details</summary>
Motivation: This paper aims to address the lack of datasets for evaluating contextual consistency and identity preservation in modern image systems while ensuring high aesthetic standards.

Method: The dataset uses anchor-linked variation pairs with identity-preserving transformations across attributes like illumination, weather, and mood, ensuring high aesthetic quality and stability in identity.

Result: Results demonstrate strong identity stability, effective attribute control, and superior aesthetics compared to large-scale web datasets.

Conclusion: Lunara Aesthetic II is a valuable resource for benchmarking and analysis in image generation/editing, emphasizing relational supervision and contextual generalization.

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [537] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar is an innovative system enabling real-time 3D Gaussian Splatting avatars in VR environments using inputs from head-mounted display tracking.


<details>
  <summary>Details</summary>
Motivation: To create a real-time system that facilitates accurate and visually realistic full-body 3D avatars for VR users, addressing the limitations of conventional image- or video-based avatar modeling methods.

Method: VRGaussianAvatar utilizes a parallel architecture combining a VR Frontend for full-body pose estimation and a GA Backend for stereoscopic rendering. It employs Binocular Batching to enhance rendering efficiency, reducing redundant computations and supporting high-resolution displays.

Result: The system achieves interactive VR performance with improved appearance similarity, embodiment, and plausibility compared to mesh avatar baselines, verified through performance tests and a user study.

Conclusion: VRGaussianAvatar is effective in delivering high-quality, real-time VR avatar rendering while optimizing computational resources, with source code made publicly available.

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [538] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: The paper introduces SMTrack, a novel visual tracking framework that leverages a state-aware space model to enhance long-range temporal dependency modeling with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling long-range temporal dependencies in visual tracking without relying on complex modules or high computational costs.

Method: Proposes a state-aware Mamba Tracker (SMTrack) using selective state-aware space models and hidden state propagation to achieve efficient and robust temporal modeling.

Result: SMTrack demonstrates improved robustness and promising performance in visual tracking while maintaining lower computational complexity compared to other methods.

Conclusion: SMTrack effectively models temporal cues in visual tracking, offering a balance of performance and efficiency without relying on resource-intensive approaches.

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [539] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: The paper introduces FreshMem, a memory network for transitioning multimodal large language models to online streaming video understanding. It balances detail preservation with long-term coherence, achieving significant improvements without requiring fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multimodal large language models lack flexibility and lead to detail loss in online streaming video understanding. This paper aims to create a solution that ensures both short-term and long-term memory adaptation.

Method: The authors propose FreshMem, which uses two modules: Multi-scale Frequency Memory (MFM) for maintaining historical 'gist' through frequency projections and Space Thumbnail Memory (STM) for adapting continuous streams into episodic clusters via compression.

Result: FreshMem significantly improves performance over baseline models, with gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. It surpasses many fine-tuned approaches despite being a training-free method.

Conclusion: FreshMem offers an innovative, efficient, and training-free approach for long-horizon streaming video understanding, combining fast adaptation with comprehensive memory retention.

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [540] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: This paper introduces CMAFNet, a defect detection method using RGB and depth imagery, excelling in small object settings and outperforming current methods on the TLRGBD benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-based methods struggle with transmission line defect detection due to small-scale defects, complex environments, and low chromatic contrast.

Method: CMAFNet employs a purify-then-fuse paradigm using a Semantic Recomposition Module for noise suppression and Contextual Semantic Integration for structural reasoning. Position-wise normalization ensures feature alignment before fusion.

Result: CMAFNet outperforms baselines on the TLRGBD dataset, achieving 32.2% mAP@50 and 12.5% APs. A lightweight version achieves competitive results with lower computational costs.

Conclusion: The approach provides an effective solution for UAV-based defect detection, addressing limitations of RGB-only methods by leveraging cross-modal capabilities.

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [541] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: This paper presents a generative framework for automating semantic segmentation in microscopy images by bridging the gap between physics-based simulations and experimental data using CycleGAN and U-Net models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate materials characterization for microscopy images by solving the challenges of high annotation costs, subjectivity, and scarcity of expert-annotated data.

Method: The method involves generating microstructural morphologies using phase-field simulations and applying a CycleGAN for unpaired image-to-image translation. A U-Net is trained on these synthetic images for segmentation tasks.

Result: The framework achieved a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88 on experimental images, showing excellent generalization from synthetic to real data.

Conclusion: The paper offers a robust, automated solution by transforming manual annotation tasks into data-rich modeling, speeding up materials discovery through labor-free segmentation.

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [542] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS extends dynamic 3D Gaussian Splatting to 4D physical simulation efficiently and robustly by addressing prior challenges like manual parameter tuning and perceptual gaps in other models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of existing methods in 3D Gaussian Splatting and 4D simulation, such as reliance on manual tuning, perceptual gaps from text/image models, and ignoring 3D surface structure.

Method: FastPhysGS uses Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) and Bidirectional Graph Decoupling Optimization (BGDO) to optimize 3DGS simulation efficiently.

Result: FastPhysGS provides high-fidelity physical simulations within 1 minute runtime and requires only 7 GB of memory, outperforming existing methods in efficiency and quality.

Conclusion: FastPhysGS offers a robust and efficient framework for dynamic 3D Gaussian Splatting in physical simulations, with potential applications across diverse fields.

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [543] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: This paper introduces DenVisCoM, a novel architecture combining Mamba block and attention mechanism for accurate real-time optical flow and disparity estimation.


<details>
  <summary>Details</summary>
Motivation: To create a unified and efficient system capable of accurately and quickly estimating optical flow and disparity due to their interrelated properties in multi-view geometry and motion tasks.

Method: The authors developed a hybrid architecture comprising DenVisCoM and a Transformer-based attention block optimized for real-time computation, memory efficiency, and accuracy. They evaluated performance across numerous datasets.

Result: The experiments demonstrated that DenVisCoM successfully achieves real-time optical flow and disparity estimation with high accuracy.

Conclusion: DenVisCoM effectively addresses the trade-offs between real-time inference, memory, and accuracy in joint motion and 3D perception tasks. Code and models are open-source for further development.

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [544] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: The paper presents a simple linear classifier leveraging Vision Foundation Models for detecting AI-generated images, achieving superior performance in real-world scenarios compared to specialized detectors.


<details>
  <summary>Details</summary>
Motivation: To address the performance collapse of specialized AI image detectors in real-world applications and propose a solution using foundation models.

Method: A simple linear classifier trained on frozen features of Vision Foundation Models like Perception Encoder, MetaCLIP 2, and DINOv3.

Result: The model outperformed specialized detectors in real-world datasets by over 30% accuracy and matched performances on traditional benchmarks.

Conclusion: A paradigm shift is advocated for AI forensics, emphasizing adaptable, generalizable models leveraging foundation models for reliable real-world performance.

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [545] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: The paper introduces TAPTQ, a Post-Training Quantization approach tailored for 3D geometric learning models, tackling challenges like data scale and computational complexity.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and scale of 3D geometry models challenge deployment on resource-limited platforms, and existing PTQ methods for 2D models do not effectively handle 3D feature distributions or calibration overheads.

Method: The paper proposes TAPTQ, featuring coarse-to-fine calibration construction, ternary-search-based optimization for quantization intervals, and TRE-Guided Module-wise Compensation to minimize quantization errors.

Result: TAPTQ achieves better accuracy than existing PTQ methods in 3D models while significantly reducing calibration time, confirmed by experiments on VGGT and Pi3 benchmarks.

Conclusion: TAPTQ effectively addresses the limitations of conventional PTQ methods in 3D models, offering both enhanced accuracy and faster deployment for resource-constrained platforms.

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [546] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed introduces an advanced multimodal embedding model that enhances fine-grained alignment between image regions and specific phrases for improved vision-language understanding, excelling in tasks like visual grounding and image retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of aligning individual image regions with specific textual descriptions, as most existing models fail at achieving such fine-grained multimodal alignment.

Method: The study introduces ObjEmbed, a multimodal embedding model that uses regional embeddings to represent individual objects in an image along with global embeddings, combining semantic and spatial representations. It also predicts IoU for better object localization.

Result: ObjEmbed demonstrates enhanced performance in diverse visual tasks by accurately combining semantic similarity and predicted IoU, with efficient encoding allowing single-pass object and image-level processing.

Conclusion: ObjEmbed's object-oriented, versatile, and efficient encoding approach showcases its superiority in vision-language tasks, validated by notable results across 18 different benchmarks.

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [547] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: The paper proposes a smart parking system with spot-wise monitoring using enhanced methods, achieving 98.80% accuracy and showcasing sustainable hardware reuse.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of previous parking monitoring systems that lacked spot-level insights and advanced application support.

Method: The system combines a distance-aware matching method with Adaptive Bounding Box Partitioning to enable spot-wise monitoring, and utilizes a resource-efficient YOLOv11m model on edge devices with new components: a Digital Shadow and an application support server based on an upcycled TV box.

Result: The system achieves a balanced accuracy of 98.80% in detecting parking occupancy and maintains an inference time of 8 seconds on edge hardware.

Conclusion: By enhancing parking systems with spot-wise functionality and incorporating sustainability measures, the proposed solution contributes to smarter urban mobility and exemplifies effective resource reuse.

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [548] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: Mind-Brush introduces a dynamic, knowledge-driven workflow for text-to-image generation to overcome static and limited understanding of user intent, employing a think-research-create paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation models act as static decoders, failing to adequately comprehend implicit user intentions and adapt to evolving real-world contexts.

Method: The paper proposes Mind-Brush, which simulates a human-like 'think-research-create' workflow. It dynamically retrieves multimodal evidence and uses reasoning tools for tackling complex and implicit visual constraints.

Result: Mind-Brush achieves significant performance improvements over existing unified models, excelling on proposed benchmarks like Mind-Bench and established ones like WISE and RISE.

Conclusion: Mind-Brush successfully enhances unified generation models by introducing agentic workflows, emphasizing dynamic knowledge retrieval and reasoning capabilities for handling complex and out-of-distribution tasks.

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [549] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: The paper introduces MagicFuse, a method for inferring multi-modal image fusion capabilities from a single visible image, effectively addressing scenarios where only visible imaging sensors are available.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable robust multi-modal image fusion benefits in harsh conditions where only low-quality visible images are accessible, overcoming the reliance on multi-sensor setups.

Method: The approach involves MagicFuse, a framework with three branches: intra-spectral knowledge reinforcement, cross-spectral knowledge generation using diffusion models, and multi-domain knowledge fusion for probabilistic scene representation, complemented by visual and semantic constraints.

Result: MagicFuse demonstrates performance on visual and semantic tasks comparable to or better than state-of-the-art multi-modal fusion methods, even when limited to a single degraded visible image.

Conclusion: The results validate the capability of MagicFuse to provide high-quality cross-spectral scene representations and support decision-making using only single visible images, offering a practical solution for challenging imaging conditions.

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [550] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: This paper proposes a privacy-compliant person detection system using MEMS-LiDAR and synthetic data augmentation to improve performance and minimize manual annotation.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate person detection in industrial environments that adheres to privacy laws like GDPR while overcoming the limitations of conventional vision-based systems (e.g., lighting, visibility, and privacy concerns).

Method: The authors use MEMS-LiDAR for anonymized 3D point cloud detection and augment real data with synthetic data from CARLA simulation to address data scarcity and annotation inefficiencies.

Result: Hybrid training (real + synthetic data) improved the average precision by 44 percentage points compared to real-only data and reduced the annotation effort by 50%.

Conclusion: The proposed hybrid MEMS-LiDAR approach achieves high detection accuracy, cost-efficiency, and GDPR compliance, presenting a scalable solution for person detection in industrial spaces.

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [551] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: This paper presents a new approach for automatic characterisation of structural discontinuity sets in exposed rock faces using UAV or laser-scanned point clouds.


<details>
  <summary>Details</summary>
Motivation: The need to assess rock-mass stability and operational safety has driven the demand for robust methods for automatic characterisation of discontinuities in underground mining scenarios.

Method: A single-shot filtering strategy suppresses noise, a cyclic orientation transformation handles polar data, and hierarchical clustering identifies discontinuity sets without needing pre-defined parameters.

Result: The proposed method, tested on real-world data, achieved mean absolute errors of 1.95° in dip angle and 2.20° in dip direction, with dispersion errors below 3°, outperforming existing techniques.

Conclusion: This innovative approach provides a highly accurate and efficient solution for structural discontinuity characterisation in underground mining, advancing safety and operational precision.

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [552] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: STT-LTF enhances long-term satellite image analysis by incorporating spatial and temporal modeling, achieving superior accuracy in forecasting heterogeneous Mediterranean landscapes.


<details>
  <summary>Details</summary>
Motivation: To address challenges in analyzing long-term satellite image time series in heterogeneous Mediterranean landscapes, including complexity from spatial patterns, seasonal variability, and multi-decade environmental changes.

Method: Introduces the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF), using a unified transformer framework that integrates spatial and temporal sequences while leveraging self-supervised learning strategies like spatial masking and horizon sampling.

Result: Experimental results on 40 years of Landsat data show STT-LTF achieves impressive accuracy for next-year predictions with a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412, outperforming existing statistical and deep-learning models.

Conclusion: STT-LTF is a suitable, state-of-the-art solution for forecasting in complex landscapes such as those in Mediterranean regions, demonstrating versatility in tackling irregular and variable prediction challenges.

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [553] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: The paper introduces TempCache, AnnCA, and AnnSA—a unified attention framework for autoregressive video diffusion models, addressing inference bottlenecks caused by KV cache growth during streaming generation. It achieves 5-10x speedups while preserving visual quality and maintaining stable GPU memory.


<details>
  <summary>Details</summary>
Motivation: To resolve bottlenecks in autoregressive video diffusion models caused by escalating inference latency and GPU memory demands from KV cache growth, which restrict long-term consistency and usable temporal context.

Method: The paper identifies redundancies in attention layers and proposes TempCache (KV cache compression using temporal correspondence), AnnCA (efficient cross-attention with ANN-selected prompt tokens), and AnnSA (sparsified self-attention using semantic ANN-matched keys).

Result: The method achieves up to 5-10x speedup in end-to-end video generation, reduces memory and compute costs, and maintains consistent GPU usage and throughput over long rollouts without sacrificing visual quality.

Conclusion: The proposed framework offers a scalable, training-free solution for efficient autoregressive video diffusion, overcoming current limitations while upholding long-term synthesis performance and quality.

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [554] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: The paper introduces FlowBypass, a novel framework for training-free image editing that uses Rectified Flow to address limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off in existing training-free image editing methods between fidelity and alignment with edit prompts, while avoiding backbone-specific feature manipulations.

Method: Proposes FlowBypass, leveraging analytical Rectified Flow to directly construct a bypass between inversion and reconstruction trajectories, reducing error accumulation.

Result: FlowBypass outperforms existing image editing methods, combining stronger prompt alignment with high-detail preservation in irrelevant regions.

Conclusion: FlowBypass offers a general and efficient method for training-free image editing with improved fidelity and prompt alignment.

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [555] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: The paper introduces LDRNet, a fast unsupervised deep learning model for chest CT large deformation image registration, outperforming current methods both in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in chest CT registration, including large deformations and complex backgrounds, which are more challenging compared to brain image registration.

Method: The authors propose LDRNet, which uses a coarse-to-fine refinement strategy with two key components: a refine block for multi-resolution registration field refinement and a rigid block for transformation matrix learning from features.

Result: The proposed LDRNet model outperforms traditional and state-of-the-art deep learning methods (VoxelMorph, RCN, LapIRN) in accuracy while being significantly faster.

Conclusion: LDRNet is an effective and efficient method for large deformation chest CT image registration, demonstrating its superiority over existing methods.

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [556] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: Guided Progressive Distillation (GPD) speeds up video diffusion models using fewer steps while preserving high visual quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high computational cost of diffusion-based video generation without compromising quality.

Method: GPD employs teacher-student training with larger step sizes, online-generated training targets, and frequency-domain constraints in latent space.

Result: GPD reduces sampling steps from 48 to 6 with competitive visual quality on VBench, outperforming existing distillation methods.

Conclusion: GPD offers a simple yet efficient framework for faster and better video generation in diffusion models.

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [557] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: The paper introduces VIA-Bench, a benchmark for assessing the robustness of Multimodal Large Language Models (MLLMs) against visual illusions and anomalies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the robustness of MLLMs when exposed to scenarios that challenge common sense and standard in-distribution data, as current benchmarks primarily assess general-purpose performance.

Method: The authors designed VIA-Bench, consisting of six categories of illusions and anomalies, with over 1,000 expertly curated question-answer pairs. They tested over 20 state-of-the-art MLLMs, using human-in-the-loop evaluations.

Result: The study revealed significant vulnerabilities in MLLMs, especially with models using Chain-of-Thought (CoT) reasoning, which showed limited robustness and breakdowns under visual illusions.

Conclusion: There exists a fundamental gap between machine and human perception, and addressing these perceptual limitations is vital for advancing artificial general intelligence. The benchmark data and code will also be open-sourced.

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [558] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: The paper proposes using publicly available street-view imagery for cost-efficient cross-country data acquisition to address domain shifts in ADAS and ADS deployment.


<details>
  <summary>Details</summary>
Motivation: Differences in legislation, infrastructure, and visual conventions across countries challenge the deployment of ADAS and ADS, requiring adaptable and economical data acquisition methods.

Method: Two scoring methods, KNN-based feature distance with a vision foundation model and visual-attribution with a vision-language model, are proposed. A new co-located dataset is also constructed.

Result: Experiments on traffic sign detection show the proposed method matches random sampling performance with half the target-data usage, and cost estimates affirm scalability of large-scale street-view processing.

Conclusion: Street-view-guided data acquisition is a promising, efficient, and economical approach for advancing cross-country model adaptation for ADAS and ADS systems.

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [559] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: This paper introduces SPIRIT, a unified infrared small target detection (IRSTD) framework addressing the gap in adapting vision foundation models (VFMs) directly to infrared imagery, offering improved performance in both single- and multi-frame analyses.


<details>
  <summary>Details</summary>
Motivation: To address challenges in infrared small target detection (IRSTD), which suffers from weak signals and differences from visible-spectrum imagery, while ensuring effective use of limited infrared data and achieving robust detection in both single and multi-frame modes.

Method: The proposed method, SPIRIT, enhances VFMs for IRSTD using lightweight, physics-informed plug-in modules: PIFR for spatial refinement and PGMA for temporal propagation with memory-attention-based priors, ensuring compatibility and effective detection.

Result: SPIRIT demonstrated superior performance and consistent improvements over both VFM baselines and state-of-the-art methods across multiple IRSTD benchmarks.

Conclusion: SPIRIT provides a unified framework enabling VFMs to effectively handle the challenges of IRSTDs, achieving state-of-the-art results while bridging the modality gap between infrared and visible-spectrum imagery.

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [560] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: The paper introduces Cloth Dynamics Grounding (CDG), exploring unsupervised cloth dynamics learning from visual data. It proposes the CloDS framework for dynamic modeling, using a novel video-to-geometry grounding method with strong generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for simulating dynamic systems rely on known physical properties, limiting their use in unknown scenarios. This paper tackles the challenge of unsupervised learning of cloth dynamics without such supervision.

Method: The proposed CloDS framework operates in three stages: video-to-geometry grounding, training of a dynamics model, and addressing challenges using dual-position opacity modulation to handle occlusions and non-linear deformations.

Result: Experimental evaluations validate that CloDS learns cloth dynamics efficiently from visual data and performs well for unseen configurations.

Conclusion: CloDS successfully enables unsupervised cloth dynamics learning using video observations, overcoming previous limitations. It offers strong potential for broader applicability in dynamic systems.

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [561] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: A study on weakly supervised IMU-based Temporal Action Localization (WS-IMU-TAL) is introduced, addressing challenges in scalable annotation by benchmarking methods and identifying strategies for advancements.


<details>
  <summary>Details</summary>
Motivation: Current human activity recognition using IMU faces limitations due to its inability to capture temporal structures, motivating the shift to action localization with temporal boundaries.

Method: The study benchmarks seven weakly supervised localization methods from other domains on IMU-TAL, with sequence-level labels, through extensive training and evaluation on public datasets.

Result: Findings reveal modality dependencies in transferability, the competitiveness of weak supervision on certain datasets, and failure modes, such as short actions and poor proposal quality.

Conclusion: The research provides actionable directions for future IMU-specific improvements and offers a reproducible benchmark to support community progress in weakly supervised IMU-TAL.

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [562] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: This paper introduces VIBE, a benchmark for visual instruction-based image editing, highlighting the gap in leveraging visual instructions like sketches compared to text guidance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in multimodal communication for image editing where visual instructions, rather than just text guidance, can capture spatial and structural intents more effectively.

Method: The authors developed VIBE, a three-level hierarchy benchmark capturing varying complexities of visual instructions, and proposed a robust evaluation framework with specific metrics for scalable assessment.

Result: A comprehensive evaluation of 17 models (open-source and proprietary) showed proprietary systems performed better but struggled as task complexity increased.

Conclusion: Proprietary models outperform open-source ones but remain limited in handling complex visual instructions, signaling the need for advances in future research on visual instruction-following capabilities.

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [563] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: This paper evaluates the inability of pixel-level deepfake detectors to enhance fact-checking for multimodal (image-text) misinformation and highlights the superiority of evidence-based systems.


<details>
  <summary>Details</summary>
Motivation: To address whether pixel-level forgery detectors meaningfully aid in verifying image-text claims or instead misguide automated fact-checking systems.

Method: Systematic analysis of pixel-level deepfake detectors using MMFakeBench and DGM4 benchmarks and comparison with evidence-driven and hybrid fact-checking systems.

Result: Pixel-level detectors perform poorly with F1 scores ranging from 0.26-0.53. Integrating them into pipelines reduces performance (-0.04 to -0.08 F1). Evidence-driven fact-checking alone achieves better results (F1 ~0.81 on MMFakeBench and 0.55 on DGM4).

Conclusion: Semantic-context understanding and evidence retrieval are critical for combating multimodal misinformation, while pixel-level signals contribute minimally to reasoning in such systems.

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [564] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: Ada-RefSR is a diffusion-based RefSR method addressing unreliable correspondences between low-quality inputs and reference images using an adaptive, lightweight mechanism.


<details>
  <summary>Details</summary>
Motivation: Current RefSR methods struggle with inconsistent LQ-Ref correlations, leading to ineffective use of reference data, either over-relying on flawed references or under-utilizing relevant cues.

Method: Proposes Ada-RefSR framework with Adaptive Implicit Correlation Gating (AICG) to distill reliable reference patterns and adaptively regulate reference guidance while avoiding erroneous fusion.

Result: Ada-RefSR demonstrates strong fidelity, naturalness, and efficiency across different datasets, showing robustness to reference misalignment.

Conclusion: The study introduces a robust, adaptive RefSR approach that mitigates hallucinations and achieves balanced performance in image restoration under varying reference alignments.

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [565] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: The paper introduces a hierarchical proxy-based parametric image representation method for efficient and controllable image editing, overcoming redundancy and lack of fine-grained manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing image representations struggle with redundancy, limiting manual editing efficiency, and lack a direct mapping for semantic-level manipulation, posing challenges for controllable and detailed editing.

Method: The authors propose decomposing images hierarchically into semantic, geometric, and textural attributes using adaptive Bezier fitting, region subdivision, and meshing. Texture parameters are embedded into proxy geometries at multi-scales, enabling high-fidelity reconstruction and direct semantic manipulation while ensuring texture coherence through locality-adaptive feature indexing.

Result: The method achieves state-of-the-art performance on image reconstruction and editing benchmarks, with fewer parameters, high-quality spatial-temporal consistency in animations, and improved rendering fidelity compared to generative approaches.

Conclusion: The proposed framework provides superior control, intuitive editing, and realistic rendering, with wide applications in image and real-time animation.

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [566] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: The paper addresses high inference costs in Multimodal Large Language Models (MLLMs) by proposing Lazy Attention, which reduces redundant layer-wise computations and KV cache footprint.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs suffer from significant inference costs due to redundant visual tokens, creating computational and KV cache bottlenecks.

Method: The proposed method, Lazy Attention, reduces redundancy by streamlining attention through cross-layer sharing of similar patterns, aided by a lightweight layer-shared Q Cache.

Result: Lazy Attention achieves over 35% reduction in KV cache usage, 1.5x throughput improvement, and maintains accurate performance across benchmarks.

Conclusion: Lazy Attention is a flexible and efficient solution that preserves accuracy while improving computational efficiency, compatible with current inference frameworks.

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [567] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA reimagines genomic modeling using a vision-based framework by treating DNA as structured visual layouts, outperforming traditional methods in efficiency and information retention.


<details>
  <summary>Details</summary>
Motivation: Current genomic foundation models inefficiently represent DNA as 1D token sequences, wasting computation on low-information background and struggling to compress long contexts.

Method: OpticalDNA reframes genomic modeling as OCR-style document understanding by converting DNA into structured visual layouts and training a vision-language model with a visual encoder and document decoder.

Result: OpticalDNA outperforms baselines in genomic tasks, handling sequences up to 450k bases using 20x fewer tokens while achieving better performance and parameter efficiency.

Conclusion: OpticalDNA provides a more efficient and effective framework for genomic modeling, leveraging visual layouts to address inefficiencies in traditional linear tokenization.

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [568] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: The paper introduces STELLAR, a self-supervised learning (SSL) framework that solves the conflict between semantic understanding and image reconstruction by factorizing visual features into semantic concepts and spatial distributions.


<details>
  <summary>Details</summary>
Motivation: To address the inherent tension between achieving high-level semantic understanding and maintaining spatial precision in image reconstruction within SSL frameworks.

Method: The proposed method involves factorizing visual features into a low-rank product of semantic concepts and spatial distributions. Semantic tokens undergo augmentation alignment while a localization matrix preserves spatial mapping for pixel-level reconstruction.

Result: STELLAR achieves high-quality image reconstruction (2.60 FID), matches semantic performance of dense architectures (79.10% ImageNet accuracy), and requires only 16 sparse tokens for these results.

Conclusion: STELLAR bridges the gap between discriminative and generative vision tasks by disentangling semantic identity from spatial geometry, offering sparse and versatile visual representations.

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [569] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: The paper proposes DSXFormer, a dual-pooling spectral squeeze-expansion transformer with dynamic context attention for improved hyperspectral image classification (HSIC), achieving superior accuracy on several datasets.


<details>
  <summary>Details</summary>
Motivation: Hyderspectral image classification faces challenges due to high spectral dimensionality, complex correlations, and a lack of labeled training samples, requiring a model that balances spectral discriminability and computational efficiency.

Method: DSXFormer utilizes a dual-pooling spectral squeeze-expansion block to recalibrate spectral channels and a dynamic context attention mechanism in a transformer framework to capture spectral-spatial relationships efficiently.

Result: Experimental results on four benchmark hyperspectral datasets show that DSXFormer outperforms state-of-the-art methods with classification accuracies exceeding 98.5%.

Conclusion: The proposed DSXFormer achieves high accuracy while balancing spectral and spatial features effectively, addressing major challenges in HSIC with computational efficiency.

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [570] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: The paper introduces the Multi-scale Pyramidal Network (MSPN), a novel method for computational pathology tasks, specifically leveraging progressive multi-scale analysis to improve MIL frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional MIL methods struggle with retaining consistent scale-linked features and depend on predefined magnifications, making them computationally costly and inflexible.

Method: MSPN incorporates grid-based remapping to derive coarse features from high magnifications and uses a coarse guidance network for context learning. It acts as a lightweight, add-on module for attention-based MIL frameworks.

Result: MSPN consistently enhances the performance of 4 attention-based frameworks across different clinically-relevant tasks and foundation models.

Conclusion: MSPN is an efficient, flexible, and effective enhancement in MIL systems, offering improved computational pathology outcomes without increasing complexity.

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [571] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: The paper addresses limitations in open-vocabulary object detection for remote sensing by introducing RS-MPOD, a framework leveraging both visual and textual prompts to achieve better category specification under ambiguous scenarios.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary object detection in remote sensing faces challenges due to issues with text-only category prompts, which struggle with semantic alignment under diverse and ambiguous remote sensing scenarios.

Method: The authors propose the RS-MPOD framework, which incorporates a visual prompt encoder for text-free category specification and a multimodal fusion module to integrate visual and textual information for open-vocabulary object detection.

Result: Experiments on diverse remote sensing benchmarks demonstrate that visual prompts provide more reliable category specification under ambiguous or shifting contexts, while multimodal prompts work well in settings where textual semantics align better.

Conclusion: RS-MPOD improves the robustness and flexibility of open-vocabulary object detection in remote sensing through the use of visual prompts and multimodal integration, addressing key issues of semantic ambiguity and distribution shifts.

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [572] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: This paper addresses systematic biases in AI-generated image detectors by proposing a post-hoc calibration method that enhances robustness against distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors fail due to distributional shifts and overfitting to superficial artifacts, causing them to misclassify fake images as real.

Method: The authors propose a post-hoc calibration framework using Bayesian decision theory, involving a learnable scalar correction applied to model logits on a validation set.

Result: The proposed method significantly improves robustness in challenging benchmarks without retraining the model, making detection more reliable.

Conclusion: The lightweight calibration method compensates for distributional shifts, improving AI-generated image detectors' accuracy in open-world scenarios.

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [573] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: The paper addresses the issue of cross-image information leakage in Large Vision-Language Models (LVLMs) during multi-image tasks and proposes a method involving scaling the hidden states of delimiter tokens.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance LVLMs' ability to distinguish between images and improve reasoning accuracy when multiple images are given, addressing their current shortcoming in multi-image tasks caused by cross-image information leakage.

Method: The proposed method involves scaling the hidden states of delimiter tokens, which helps preserve image-specific information and manage intra- and cross-image interactions more effectively, without additional training or inference cost.

Result: The method shows performance gains on various multi-image benchmarks (e.g., Mantis, MuirBench) as well as text-only benchmarks (e.g., TQABench, MultiNews), demonstrating versatility and improvement without extra computation.

Conclusion: This approach effectively mitigates cross-image information leakage, enabling LVLMs to differentiate and reason over multiple inputs more accurately, and enhances performance across diverse tasks without increasing computational overhead.

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [574] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: The paper introduces a novel approach for localized control in text-to-image generation using diffusion models by integrating user-defined region control.


<details>
  <summary>Details</summary>
Motivation: Achieving precise control over desired image details using text descriptions alone is often challenging. Existing methods provide image-level control but are limited to uniform conditions across the image, lacking localized region control.

Method: The proposed method integrates masking features and an additional loss term in the training process, enabling precise local control over image regions while allowing the diffusion model to autonomously generate other areas.

Result: Experiments show that this method synthesizes high-quality images with effective localized control, ensuring correspondence between user-defined regions and the final output.

Conclusion: The approach advances text-to-image generation by enabling detailed localized control, addressing limitations of uniform image-level conditions in previous methods.

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [575] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat proposes a method to reconstruct high-fidelity 3D scenes from sparse images using 2D Gaussian Splatting, achieving better performance in geometry and texture with a surface continuity prior and forced alpha blending.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in reconstructing accurate and continuous 3D scenes from sparse images using existing methods like 3D Gaussian Splatting, which often fail to produce coherent surfaces.

Method: Developed SurfSplat, a feedforward framework employing 2D Gaussian Splatting with a surface continuity prior, forced alpha blending strategy, and a new evaluation metric called High-Resolution Rendering Consistency (HRRC).

Result: SurfSplat consistently outperformed prior methods on datasets such as RealEstate10K, DL3DV, and ScanNet in terms of both standard metrics and HRRC.

Conclusion: SurfSplat provides a robust solution for high-fidelity 3D scene reconstruction from sparse images, overcoming the limitations of previous methods through improved geometry and texture representation.

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [576] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer introduces a unified multimodal world model for autonomous driving that generates multimodal future observations directly, outperforming previous single-modality synthesis methods.


<details>
  <summary>Details</summary>
Motivation: Current methods in autonomous driving rely on single-modality generation for data synthesis, limiting holistic environment understanding across video and LiDAR.

Method: The framework combines LiDAR-specific and video variational autoencoders with Unified Latent Anchoring (ULA) for cross-modal alignment, fused via a diffusion transformer for multimodal synthesis.

Result: Experiments show superior performance of UniDriveDreamer in both video and LiDAR generation compared to state-of-the-art methods.

Conclusion: UniDriveDreamer demonstrates the feasibility of unified multimodal future observation generation, offering enhanced accuracy and utility for autonomous driving systems.

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [577] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: The paper presents VDR-Bench, a novel benchmark designed to evaluate multimodal large language models in realistic visual-textual fact-finding scenarios. It also proposes a cropped-search workflow to enhance visual retrieval.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing benchmarks for evaluating visual and textual search in MLLMs, primarily their visual search limitations and unrealistically idealized evaluation scenarios.

Method: The authors developed VDR-Bench with 2,000 VQA instances using a multi-stage curation process and expert review. They also proposed a multi-round cropped-search workflow for enhanced visual retrieval.

Result: The proposed benchmark effectively evaluates Vision-DeepResearch systems, and the cropped-search workflow significantly improves model performance in realistic retrieval situations.

Conclusion: The study advances evaluation methods for MLLMs in complex real-world scenarios, offering insights and tools for the development of better multimodal deep-research systems.

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [578] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: The paper introduces ClueTracer, a method to combat hallucinations in multimodal reasoning models via better clue localization, offering measurable improvements on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning models often struggle with hallucinations caused by reasoning drift, where focus shifts from task-relevant to irrelevant visual clues, diminishing performance.

Method: ClueTracer is a training-free, parameter-free, and architecture-independent plugin that traces and localizes key visual clues along the model's reasoning pathway, suppressing irrelevant outputs.

Result: ClueTracer boosts reasoning model performance by 1.21× on benchmarks and improves non-reasoning settings by 1.14×, with no additional training required.

Conclusion: ClueTracer effectively mitigates hallucination issues in reasoning models, enhancing their reliability and applicability across diverse multimodal settings.

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [579] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: The paper introduces the 'One Size, Many Fits' (OSMF) framework for advertising image generation, optimizing group-specific Click-Through Rates (CTR) rather than an overall rate, achieving better-targeted marketing results.


<details>
  <summary>Details</summary>
Motivation: Current advertising image generation approaches optimize for overall CTR but overlook the diverse preferences of different user groups, leading to suboptimal targeted marketing performance.

Method: The OSMF framework utilizes product-aware adaptive user grouping based on user and product characteristics. It incorporates group-specific image generation using a Group-aware Multimodal Large Language Model (G-MLLM), pre-trained and fine-tuned with group preference alignment. A new dataset, Grouped Advertising Image Preference Dataset (GAIP), supports this work.

Result: The proposed framework significantly improves group-specific CTR in both offline and online settings, achieving state-of-the-art results.

Conclusion: The OSMF framework demonstrates the effectiveness of tailoring advertising images to specific user group preferences with the help of a newly introduced dataset, paving the way for more personalized and effective targeted marketing in the industry.

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [580] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: This paper introduces Auto-Comp, an automated pipeline for evaluating compositional reasoning in Vision-Language Models, exposing their universal failures and providing tools for future testing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address compositional reasoning errors in Vision-Language Models (VLMs), where models fail to distinguish between similar but distinct descriptions.

Method: Auto-Comp generates synthetic benchmarks using paired Minimal and Contextual captions for controlled analysis of core reasoning abilities, enabling A/B testing.

Result: Evaluation of 20 VLMs, including CLIP and SigLIP, shows models universally struggle with compositional tasks and are impacted by visual clutter and distractors.

Conclusion: Auto-Comp enables in-depth analysis of compositional failures, bridging visio-linguistic challenges. The pipeline and benchmarks are provided to aid ongoing evaluation efforts in the field.

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [581] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: The paper introduces SegmentMIL, a transformer-based framework for coronary stenosis classification that handles multi-view angiography data without requiring expensive view-level annotations.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning models for coronary stenosis detection are limited by their reliance on costly view-level annotations and inability to capture temporal dynamics across angiography views, which are critical for accurate diagnosis.

Method: SegmentMIL employs a transformer-based multi-view multiple-instance learning framework that uses patient-level supervision. It detects stenosis and localizes affected coronary artery regions without requiring view-level annotations.

Result: The model demonstrates high performance in both internal and external validations, surpassing conventional view-level models and multiple-instance learning baselines.

Conclusion: SegmentMIL is a scalable and clinically appropriate solution for diagnosing coronary stenosis, requiring less intensive annotation effort while achieving superior accuracy.

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [582] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS is a scalable framework for reconstructing city-scale scenes with high quality, overcoming challenges in geometry, memory, and computation.


<details>
  <summary>Details</summary>
Motivation: Extend 3D Gaussian Splatting to large-scale urban scenes while addressing geometric consistency, memory efficiency, and computational scalability.

Method: Introduces Depth-Consistent D-Normal Regularization with depth supervision, an adaptive confidence weighting mechanism, Spatially Adaptive Gaussian Pruning, and a unified partitioning and view assignment scheme.

Result: UrbanGS achieves superior rendering quality, geometric accuracy, and memory efficiency across multiple urban datasets.

Conclusion: UrbanGS provides a systematic and efficient solution for reconstructing high-fidelity, large-scale urban environments.

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [583] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: The paper introduces FSVideo, a fast image-to-video diffusion framework using a novel video autoencoder, a transformer-based diffusion model, and a multi-resolution generation approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the demand for fast and high-quality image-to-video generation, improving efficiency and performance over existing models.

Method: The method involves a video autoencoder for compressed latent representations, a diffusion transformer architecture with improved inter-layer information flow, and a multi-resolution strategy for fidelity enhancement.

Result: The final model, comprising a 14B diffusion transformer and upsampler, provides competitive performance while being significantly faster than popular models.

Conclusion: FSVideo demonstrates that advanced architectural designs and strategies can achieve both speed and quality in image-to-video generation, pushing the state-of-the-art forward.

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [584] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: The paper introduces DSKD, a method that uses a diffusion model and locality-sensitive hashing for self-knowledge distillation in students without directly aligning teacher and student models, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the incompatibility issues arising from feature distribution differences between teacher and student models in conventional knowledge distillation methods.

Method: A light-weight diffusion model is used to denoise student features guided by the teacher classifier, paired with locality-sensitive hashing to distill features between original and denoised student features.

Result: DSKD achieves significant improvements over existing KD methods for various models and datasets in visual recognition tasks.

Conclusion: DSKD eliminates mapping and distribution discrepancies, allowing students to learn effectively from teachers while outperforming traditional KD approaches.

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [585] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: iCCDM enhances the Continuous Conditional Diffusion Model by integrating the Elucidated Diffusion Model framework, improving image generation quality and sampling efficiency.


<details>
  <summary>Details</summary>
Motivation: Improve the limitations of the original CCDM, which suffers from outdated framework reliance and low sampling efficiency.

Method: Incorporates the Elucidated Diffusion Model framework with alterations, including a matrix-form formulation and adaptive vicinal training strategy.

Result: iCCDM consistently surpasses existing methods, including GAN-based and large-scale text-to-image diffusion models, with better generation quality and reduced sampling costs.

Conclusion: iCCDM presents a robust improvement over previous CCDM methods by addressing its key shortcomings and showcasing superior performance across multiple datasets and resolutions.

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [586] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit is a training-free framework designed for efficient editing of minute-long videos, overcoming challenges in temporal consistency and computational overhead.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in editing minute-long videos, particularly issues like flickering, boundary artifacts, and structural drift which arise due to computational constraints and temporal inconsistency.

Method: The method employs a divide-and-conquer approach with two core modules: Velocity Blend, which rectifies motion inconsistencies, and Attention Sink, which anchors segment features to reference frames to suppress drift.

Result: MLV-Edit demonstrates superior performance compared to state-of-the-art methods in terms of maintaining temporal stability and semantic fidelity, as shown through extensive experiments.

Conclusion: MLV-Edit successfully resolves key challenges in long-duration video editing and sets a benchmark for both stability and fidelity without requiring additional training.

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [587] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: The paper introduces an AI-based anomaly detection framework to identify and analyze histopathological slides in toxicology, enhancing early detection of drug-induced toxicity and addressing limitations of traditional expert-pathologist assessments.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need to reduce drug development failures due to toxicity by detecting adverse effects early. Traditional histopathology methods heavily rely on expert pathologists, causing inefficiencies in large-scale screenings.

Method: The authors used a novel dataset with pixel-wise annotations of healthy tissue and known pathologies to fine-tune Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) for tissue segmentation. They employed Mahalanobis distance with class-specific thresholds for out-of-distribution pathology detection.

Result: The framework minimized misclassifications with a 0.16% false negative rate for pathological tissues and 0.35% false positive rate for healthy tissues. It effectively identified known and rare pathologies in mouse liver WSIs.

Conclusion: The proposed AI framework enhances preclinical workflows by enabling early, precise identification of toxicological anomalies, demonstrating its utility for safer and more efficient drug development.

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [588] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: The paper addresses issues of registration bias in synthetic CT generation from CBCT, proposing physics-based CBCT simulation for aligned training pairs and improved evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Standard CT generation from CBCT fails due to registration bias, impacting evaluation metrics and clinical usability.

Method: The authors used physics-based CBCT simulation for geometrically aligned training pairs and introduced evaluation via geometric alignment metrics rather than conventional intensity ones.

Result: Models trained on synthetic data showed improved geometric alignment and were preferred by clinical observers, despite lower conventional intensity scores.

Conclusion: Geometric fidelity, not alignment with biased ground truth, better meets clinical requirements for CT generation from CBCT.

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [589] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: The paper presents a deep learning framework for analyzing urban changes from historical maps, offering automation and precision, as applied to Paris’ evolution between 1868 and 1937.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of spatial misalignment, cartographic variation, and degrading document quality in extracting fine-grained information from historical map series for urban transformation analysis.

Method: The paper proposes a fully automated, modular deep learning framework for historical map analysis. It integrates dense map alignment, multi-temporal object detection, and change profiling.

Result: The framework demonstrates robust methods for alignment and object detection. It effectively analyzes urban transformation in Paris over a 69-year span, showcasing detailed spatial-temporal changes.

Conclusion: The framework transitions urban change analysis from qualitative to systematic and quantitative, applicable across varied cartographic contexts and supports social sciences research.

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [590] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: The paper introduces Loop-ViT, a recursive architecture leveraging adaptive iterative computation for visual reasoning, achieving superior efficiency over larger traditional models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of feed-forward architectures in capturing the recursive, algorithmic nature of human reasoning, especially in visual tasks like the ARC-AGI benchmark.

Method: Introduced Loop-ViT, a recursive model with a Hybrid Block combining convolutions and attention, and a Dynamic Exit mechanism that halts computation based on predictive entropy.

Result: Loop-ViT achieved a 65.8% accuracy on the ARC-AGI-1 benchmark, outperforming larger models (73M parameters) with significantly fewer parameters (18M).

Conclusion: Adaptive iterative computation in architectures like Loop-ViT achieves more efficient and effective scaling in visual reasoning compared to simply increasing parameter count.

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [591] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: The paper introduces Reg4Pru, a training regularisation for mitigating performance loss in token-pruning techniques for transformers with improved segmentation accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance computational efficiency and stability in vision transformers that scale quadratically with tokens while maintaining segmentation performance.

Method: Proposes Reg4Pru, a training regularisation for pruning tokens with improved dense prediction stability for segmentation tasks.

Result: Reg4Pru improves average precision by 46% on the FIVES dataset compared to a model without routing, achieving a 29% speedup in computation time.

Conclusion: Reg4Pru is an effective regularisation for token-pruned transformers, balancing computational efficiency and segmentation accuracy.

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [592] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: The paper introduces a two-stage generative adversarial network called TSGAN to address limitations in CT lung nodule datasets and improve model accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle the issues in lung nodule CT datasets, such as limited sample size, insufficient diversity, and anatomical distortions, which affect model performance and generalization.

Method: A two-stage generative adversarial network (TSGAN) was proposed: Stage 1 employs StyleGAN to generate segmentation masks encoding lung nodule structures and tissue control; Stage 2 uses a DL-Pix2Pix model to convert masks into CT images, with enhanced attention mechanisms for optimal texture modeling.

Result: TSGAN improves accuracy by 4.6% and mAP by 4% on the LUNA16 dataset, demonstrating enhanced synthetic image quality and performance of detection models.

Conclusion: The proposed TSGAN method effectively handles dataset deficiencies by enhancing diversity, spatial controllability, and overall detection model performance.

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [593] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: This paper introduces CIEC, a framework for multimodal weakly-supervised manipulation localization using coarse annotations, yielding results similar to fully supervised methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for multimodal manipulation localization depend on costly and detailed annotations, which are hard to scale.

Method: The proposed CIEC framework uses image- and text-based branches incorporating TRPS and VCTG modules, applying constraints and utilizing multimodal cues for weakly-supervised localization.

Result: Experiments showed CIEC achieves performance comparable to fully supervised approaches across multiple metrics.

Conclusion: CIEC successfully addresses multimodal manipulation localization using only coarse annotations, offering an effective and less resource-intensive solution.

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [594] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField is introduced as a topology-aware implicit framework addressing incomplete pulmonary tree structures, offering efficient topological repair, anatomical labeling, and lung segment reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for pulmonary tree analysis struggle with incomplete or disconnected branches in CT images, which hampers downstream anatomical analysis and reduces pipeline usability.

Method: TopoField uses sparse surface and skeleton point clouds to construct a continuous implicit field for topology repair, trained on synthetically disrupted trees. It performs joint multi-task inference for topology repair, anatomical labeling, and lung segment reconstruction.

Result: The method demonstrates improved topological completeness and accurate anatomical labeling and reconstruction on the Lung3D+ dataset, processing each case in just over one second.

Conclusion: TopoField is a practical and efficient approach suitable for large-scale clinical applications, enhancing pulmonary tree modeling despite structural disruptions.

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [595] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: The paper introduces a method, SSI-DM, for efficient inversion of real images into diffusion models' noise space, addressing challenges related to Gaussian noise properties and usability for editing tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with poor editability since they fail to maintain proper Gaussian noise characteristics during inversion due to mathematical singularity issues.

Method: The proposed SSI-DM approach sidesteps the singularity problem by introducing small noise before performing standard inversion, preserving Gaussian properties and improving editability.

Result: SSI-DM demonstrates superior reconstruction fidelity and interpolation performance on public image datasets compared to previous methods.

Conclusion: The method provides an efficient, principled solution compatible with general diffusion models, enabling better editing capabilities and maintaining high-quality reconstructions.

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [596] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: The paper introduces MAIN-VLA, a framework for improving efficiency in complex visual-language-action tasks by aligning intention abstraction and environment semantics, achieving superior decision-making and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with extracting critical action signals in real-time dynamic environments, such as open-world games, due to sensor stream redundancy.

Method: MAIN-VLA uses Intention Abstraction (IA) to simplify linguistic instructions and reasoning into semantic primitives and Environment Semantics Abstraction (ESA) to structure visual streams into topological representations. Modalities alignment filters perceptual redundancy.

Result: MAIN-VLA achieves superior decision-making, stronger generalization, and more efficient inference across open-world environments and large-scale PvP games.

Conclusion: MAIN-VLA effectively enhances VLA tasks by grounding decision-making in deep semantic alignment, setting new benchmarks in complex dynamic environments.

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [597] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: The paper addresses the architectural gap in autoregressive (AR) models for real-time interactive video generation by introducing 'Causal Forcing,' which bridges this gap and achieves state-of-the-art performance on multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in real-time interactive video generation due to the architectural differences between bidirectional and autoregressive video diffusion models, leading to performance issues.

Method: Introduced the 'Causal Forcing' technique, which initializes the AR teacher for ODE distillation and directly addresses the theoretical architectural gap.

Result: Outperformed baseline methods across all tested metrics, achieving significant improvements such as 19.3% in Dynamic Degree, 8.7% in VisionReward, and 16.7% in Instruction Following.

Conclusion: Causal Forcing effectively eliminates the issues caused by distillation from bidirectional teachers in video generation tasks, setting a new benchmark for performance.

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [598] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: The paper introduces MIRROR, a new framework for detecting AI-generated images, surpassing human experts and current methods. It utilizes consistency with human cognitive frameworks rather than specific forgery traces.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for robust AI image detectors as synthetic images threaten media security. Current detectors rely on artifact-based classification which struggles to adapt to new generative patterns.

Method: MIRROR employs a Reference-Comparison approach using a learnable discrete memory bank to encode real-image priors. It projects images into manifold-consistent ideal references and analyzes residual errors for detection.

Result: MIRROR outperformed existing methods across 14 benchmarks, showing gains up to 8.1% in detection accuracy. It achieved 89.6% accuracy on the Human-AIGI benchmark, surpassing human experts.

Conclusion: MIRROR provides a significant advancement in AI-generated image detection with high accuracy and generalizability, approaching human perceptual limits. The framework's code is publicly accessible for further use.

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [599] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: This paper evaluates OCR performance in realistic conditions, focusing on static and dynamic scenarios like varying distances, angles, and walking speeds, using different devices and OCR engines.


<details>
  <summary>Details</summary>
Motivation: To address the gap in OCR evaluations for practical mobile scenarios, especially for assistive technologies, by incorporating dynamic conditions.

Method: Systematic assessment of OCR accuracy across static distances and dynamic conditions like walking speeds and device placements, using multiple OCR engines and devices.

Result: Google Vision achieved the highest overall accuracy; accuracy declined with faster walking speeds and wider angles; phone main camera and shoulder-mounted positions performed best, but body positions were statistically similar.

Conclusion: The study highlights Google Vision as the most accurate OCR engine overall and provides insights into optimal conditions for OCR accuracy in mobile contexts, aiding design for assistive technologies.

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [600] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: LatentMorph is a new framework for text-to-image generation that integrates implicit reasoning in latent spaces, improving adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies, information loss, and cognitive mismatches of explicit reasoning paradigms in text-to-image generation.

Method: The method consists of a framework with four lightweight components: a condenser, a translator, a shaper, and an RL-trained invoker for reasoning in latent spaces.

Result: Experiments show LatentMorph improves performance significantly on benchmarks, achieves better abstract reasoning, reduces inference time, and enhances cognitive alignment with human intuition.

Conclusion: LatentMorph demonstrates the benefits of implicit latent reasoning in generating more adaptive and efficient text-to-image outputs.

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [601] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: The paper presents a flow matching framework (LiFlow) for 3D LiDAR scene completion, enhancing upon diffusion-based methods and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address perception challenges in autonomous driving caused by occlusion and sparsity in LiDAR point clouds, and limitations in current scene completion models due to training-inference mismatch.

Method: Introduces a novel flow matching framework using nearest neighbor flow matching loss and Chamfer distance loss to improve local structure and global coverage in point cloud alignments.

Result: LiFlow demonstrates state-of-the-art performance in 3D LiDAR scene completion metrics.

Conclusion: LiFlow offers an advanced alternative to diffusion-based methods, ensuring consistent training and inference distributions and improving autonomous driving system perception.

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [602] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: The paper introduces DiScene, a sparse query-based method for efficient and robust occupancy prediction through multi-level distillation strategies and teacher-guided initialization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve the efficiency-accuracy trade-off in occupancy prediction, where dense methods waste resources on empty voxels and sparse methods face challenges in complex indoor scenes.

Method: It proposes Multi-level Consistent Knowledge Distillation and a Teacher-Guided Initialization policy to enhance performance, combining hierarchical representation transfer and optimized parameter warm-up.

Result: DiScene achieves 23.2 FPS, outperforming the baseline OPUS by 36.1%, surpasses EmbodiedOcc in performance and speed, and demonstrates adaptability across benchmarks and diverse scenarios.

Conclusion: DiScene provides a novel and effective solution for occupancy prediction, addressing efficiency and robustness concerns, with strong experimental validation and open-source access.

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [603] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: Proposes a novel method to disentangle style and content in human motion data for enhanced style transfer using Residual Vector Quantized Variational Autoencoders (RVQ-VAEs).


<details>
  <summary>Details</summary>
Motivation: Human motion data contains rich semantic and stylistic aspects which are difficult to separate for applications like style transfer.

Method: Uses RVQ-VAEs for coarse-to-fine representation learning complemented by contrastive learning and information leakage loss for organized disentanglement.

Result: Achieves versatile outcomes such as style transfer, style removal, and motion blending without additional fine-tuning for unseen styles.

Conclusion: The proposed framework effectively disentangles motion style and content, offering a robust solution for various inference-time applications.

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [604] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO is a novel technique for enabling short-context vision-language models to understand ultra-long videos without labeled data, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of short-context vision-language models in understanding long videos and reduce reliance on expensive human annotations.

Method: A two-stage framework: 1) Synthesizing preference triples with filtering and approximating reference model scoring to reduce bias and computation. 2) Recursive captioning on long videos to generate metadata, crafting multi-segment reasoning tasks using a language model.

Result: LongVPO achieves superior performance on long-video benchmarks and maintains strong short-video performance using only 16K synthetic examples, outperforming state-of-the-art models.

Conclusion: LongVPO provides a scalable and efficient paradigm for robust long-video understanding without relying on human labels, marking a significant advancement in video analysis.

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [605] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: This paper explores the use of various neural networks to create new texture implicit neural representations (INRs) operating continuously in UV space, achieving high image quality while balancing memory and inference time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance texture INR designs by transitioning from a discrete to a continuous approach over UV coordinate space and analyzing their performance in terms of image quality, memory usage, and inference time.

Method: The method involves designing INRs, conducting experiments to evaluate their effectiveness, analyzing trade-offs between objectives, and exploring applications like real-time rendering and INR-space generation.

Result: The INRs showcased strong performance in image quality, managed memory usage efficiently, and maintained reasonable rendering inference times across experiments.

Conclusion: Continuous texture INR designs are effective, providing high-quality images with balanced performance parameters and showcasing utility in real-time rendering and related applications like mipmap fitting.

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [606] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: This paper proposes a method to integrate rectangular shape priors into CT reconstruction using neural networks for improved accuracy with sparse views.


<details>
  <summary>Details</summary>
Motivation: To enhance CT reconstruction quality while reducing costs, and to incorporate shape priors of industrial objects, specifically rectangular structures, in neural networks.

Method: Introduces Neural Adaptive Binning (NAB) that maps spatial coordinates to binned vector spaces using a novel binning mechanism combined with neural networks.

Result: NAB achieves better reconstruction accuracy in industrial datasets and exhibits robustness in medical datasets.

Conclusion: The paper provides a new approach for integrating shape priors in CT reconstruction, demonstrating its efficacy in both industrial and medical applications.

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [607] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: SNGP model enhances uncertainty estimation and out-of-distribution detection in histopathologic interpretation, ensuring safer AI deployment in digital pathology.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models in digital pathology lack robust uncertainty estimation, limiting their trust and adoption in safety-critical clinical settings.

Method: The paper implements Spectral-normalized Neural Gaussian Process (SNGP), which incorporates spectral normalization and a Gaussian process layer in the model.

Result: SNGP delivers similar in-distribution accuracy while significantly improving uncertainty estimation and OOD detection compared to other models like deterministic and MonteCarlo dropout.

Conclusion: SNGP is a promising framework for uncertainty-aware digital pathology classification, promoting safer deployments and enhancing trust among clinicians.

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [608] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper introduces UnifiedReward-Flex, a personalized reward model for visual generation that dynamically adapts to specific contexts and employs hierarchical assessment criteria.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reward models have limitations in aligning with subjective and context-dependent human preferences due to a one-size-fits-all approach.

Method: UnifiedReward-Flex uses semantic intent interpretation, visual evidence grounding, and hierarchical assessment criteria construction with a two-stage training pipeline: fine-tuning with reasoning traces and direct preference optimization.

Result: UnifiedReward-Flex, integrated into the GRPO framework for image and video synthesis, demonstrates superior context-adaptive reasoning and visual generation capabilities.

Conclusion: UnifiedReward-Flex addresses the misalignment in visual generation systems by introducing a personalized and context-sensitive reward model, validated through extensive results.

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [609] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: This paper introduces MultiBO, a method to refine image generation based on iterative user feedback, improving the alignment of generated images with a specific mental image.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the alignment of generative model outputs with specific mental images that users have but cannot fully express through language prompts alone.

Method: The authors propose MultiBO (Multi-Choice Preferential Bayesian Optimization), which generates multiple images per iteration, obtains user feedback on which better aligns with the target, and uses this feedback to iteratively refine the output.

Result: Results demonstrate improved alignment between user-desired mental images and the generated images, validated through qualitative user scores and quantitative results compared to baseline models.

Conclusion: Incorporating iterative multi-choice feedback effectively narrows the gap between a user's desired mental image and the imagery produced by generative models, making personalized image generation feasible.

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [610] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Infinite-World introduces a robust interactive world model with visual memory capable of handling 1000+ frames in real-world environments, addressing challenges of noisy data and scarce viewpoints.


<details>
  <summary>Details</summary>
Motivation: Existing world models lack effective training paradigms for handling real-world videos due to noisy pose estimations and limited viewpoint revisits, necessitating a more robust model.

Method: The paper introduces a Hierarchical Pose-free Memory Compressor (HPMC) for fixed-budget historical latent representation and Uncertainty-aware Action Labeling for tri-state motion discretization, combined with a Revisit-Dense Finetuning Strategy using compact datasets.

Result: The proposed model delivers superior performance in visual quality, spatial consistency, and action controllability as demonstrated by experiments and user studies.

Conclusion: Infinite-World effectively bridges the gap between synthetic data training and real-world application by addressing challenges in memory, pose estimation, and noisy action modeling.

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [611] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: This paper introduces a unified framework, "Superman," for bridging visual perception with skeleton-based temporal motion generation. It addresses current limitations in human motion analysis tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from fragmentation in the field — perception models that only understand motion from video, generation models unable to perceive raw visual input, and limitations in temporal motion and motion vocabularies.

Method: Superman utilizes two core innovations: the Vision-Guided Motion Tokenizer for cross-modal learning and a unified MLLM architecture to integrate visual-based skeleton pose estimation with motion generation tasks.

Result: The unified framework achieves state-of-the-art or competitive performance in temporal motion analysis tasks on benchmarks like Human3.6M.

Conclusion: Superman demonstrates a scalable, efficient approach for improving generative motion analysis, bridging visual perception and skeleton-based motion synthesis.

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [612] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: ReasonEdit introduces a new method to edit vision-language models (VLMs) for reasoning-heavy tasks by incorporating human reasoning into the correction process, achieving better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing model editors do not effectively handle reasoning-heavy tasks in VLMs. This paper aims to enable edits in a way that incorporates human reasoning, which is essential for such tasks.

Method: ReasonEdit stores human reasoning explanations in a codebook, retrieves relevant facts during inference using a topology-balanced multimodal embedding inspired by network science, and then applies edits based on this reasoning.

Result: ReasonEdit shows superior editing performance across four VLMs on rationale-based visual question answering datasets. It proves that incorporating human reasoning enhances the generalization of edits.

Conclusion: Using human reasoning during the editing of VLMs significantly improves the effectiveness and generalization of model edits, particularly for reasoning-heavy tasks.

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [613] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: The paper proposes Catalyst, a framework enhancing OOD detection using raw channel-wise statistics of pre-pooling feature maps.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of current OOD detection methods which discard valuable pre-pooling feature map signals.

Method: Catalyst computes input-dependent scaling factors (γ) from pre-pooling feature map statistics, integrating them multiplicatively with baseline scores.

Result: Catalyst reduces the average False Positive Rate by up to 32.87% on CIFAR-10, 27.94% on CIFAR-100, and 22.25% on ImageNet.

Conclusion: Utilizing pre-pooling statistics significantly improves OOD detection and complements existing approaches.

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [614] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: This paper introduces SelvaMask, a dataset featuring over 8,800 manually delineated tropical tree crowns and proposes a modular pipeline leveraging vision foundation models to achieve superior crown segmentation performance.


<details>
  <summary>Details</summary>
Motivation: To improve tree crown segmentation performance in challenging tropical forests to support biodiversity conservation and ecosystem monitoring.

Method: The team developed a tropical dataset, SelvaMask, with diverse annotations and a modular detection-segmentation pipeline utilizing domain-specific VFMs.

Result: The proposed approach achieved state-of-the-art segmentation performance in tropical forests, outperforming generalist and supervised models, with validated results across external datasets.

Conclusion: SelvaMask serves as a robust benchmark and valuable tool for enhancing tropical and generalized forest monitoring; its public release supports further contributions to ecological research.

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [615] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniReason is a unified framework that combines text-to-image generation and image editing within a dual reasoning paradigm, enhancing complex synthesis tasks through planning and refinement.


<details>
  <summary>Details</summary>
Motivation: Unified multimodal models struggle with interconnected reasoning steps and deep synthesis tasks, creating a need for a framework that combines text-to-image generation and visual editing effectively.

Method: A dual reasoning paradigm that involves world knowledge-enhanced planning for generation and self-reflective editing for visual refinement, supported by a reasoning-centric dataset for training.

Result: UniReason achieves advanced performance in reasoning-intensive benchmarks and demonstrates strong general synthesis capabilities.

Conclusion: The framework successfully integrates text-to-image generation and image editing, reflecting a human-like cognitive process that improves reasoning and synthesis efficiency across tasks.

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [616] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: The study introduces a gated multi-head Transformer architecture based on Swin U-Net to improve segmentation accuracy in radiotherapy by reducing false positives in irrelevant slices. Experiments demonstrated superior performance compared to conventional models.


<details>
  <summary>Details</summary>
Motivation: To address issues in radiotherapy auto-segmentation where conventional models produce anatomically implausible false positives or hallucinations in slices without target structures.

Method: Developed a gated multi-head Transformer model incorporating inter-slice context, a parallel detection head, and a context-enhanced segmentation stream. The detection head gates invalid slices, while training uses slice-wise Tversky loss to handle class imbalance.

Result: The gated model significantly reduced false positives, achieving a mean Dice loss of $0.013 \pm 0.036$, outperforming a baseline model, which exhibited a mean Dice loss of $0.732 \pm 0.314$. Detecting probabilities highly correlated with anatomical presence effectively eliminated spurious segmentations.

Conclusion: Detection-based gating improves automated segmentation by enhancing anatomical plausibility and robustness, reducing false-positive outcomes, and increasing reliability for clinical radiotherapy workflows.

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [617] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen introduces perceptual supervision into end-to-end pixel diffusion, achieving superior generative results without VAEs or latent representations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve challenges in optimizing high-dimensional pixel manifolds in pixel diffusion methods, which struggle compared to latent diffusion models.

Method: PixelGen utilizes perceptual losses (LPIPS for local patterns and DINO-based for global semantics) to refine the pixel diffusion process.

Result: PixelGen surpasses latent diffusion baselines with an FID score of 5.11 on ImageNet-256 and favorable GenEval score of 0.79 for text-to-image generation.

Conclusion: PixelGen leverages perceptual supervision to simplify image generation while improving performance, marking advancements in pixel diffusion frameworks.

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [618] [Standardized Methods and Recommendations for Green Federated Learning](https://arxiv.org/abs/2602.00343)
*Austin Tapp,Holger R. Roth,Ziyue Xu,Abhijeet Parida,Hareem Nisar,Marius George Linguraru*

Main category: cs.DC

TL;DR: The paper proposes a carbon-accounting methodology for Federated Learning (FL) using NVFlare and CodeCarbon that tracks CO2 emissions across different FL tasks while accounting for compute and communication impacts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized approaches in quantifying and comparing environmental impacts of FL processes, ensuring reproducible and transparent assessments.

Method: Created a carbon-accounting methodology using NVIDIA NVFlare and CodeCarbon to track CO2 emissions during FL phases, incorporating compute and communication emissions with network energy models.

Result: Tested on two FL workloads, revealing notable CO2 variations based on client efficiency, system effects, and GPU tiers, emphasizing the necessity of task-specific energy assessments.

Conclusion: The proposed methodology enhances transparency in measuring FL environmental impacts, establishing a foundation for reproducible 'green' FL practices and enabling more eco-conscious AI developments.

Abstract: Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for FL CO2e tracking using NVIDIA NVFlare and CodeCarbon for explicit, phase-aware tasks (initialization, per-round training, evaluation, and idle/coordination). To capture non-compute effects, we additionally estimate communication emissions from transmitted model-update sizes under a network-configurable energy model. We validate the proposed approach on two representative workloads: CIFAR-10 image classification and retinal optic disk segmentation. In CIFAR-10, controlled client-efficiency scenarios show that system-level slowdowns and coordination effects can contribute meaningfully to carbon footprint under an otherwise fixed FL protocol, increasing total CO2e by 8.34x (medium) and 21.73x (low) relative to the high-efficiency baseline. In retinal segmentation, swapping GPU tiers (H100 vs.\ V100) yields a consistent 1.7x runtime gap (290 vs. 503 minutes) while producing non-uniform changes in total energy and CO2e across sites, underscoring the need for per-site and per-round reporting. Overall, our results support a standardized carbon accounting method that acts as a prerequisite for reproducible 'green' FL evaluation. Our code is available at https://github.com/Pediatric-Accelerated-Intelligence-Lab/carbon_footprint.

</details>


### [619] [What Artificial Intelligence can do for High-Performance Computing systems?](https://arxiv.org/abs/2602.00014)
*Pierrick Pochelu,Hyacinthe Cartiaux,Julien Schleich*

Main category: cs.DC

TL;DR: This paper reviews how AI, particularly ML, optimizes HPC in six areas, focusing on scheduling and supervised performance estimation, while stressing the importance of integration and standardization.


<details>
  <summary>Details</summary>
Motivation: HPC systems consume significant power, which drives the need for AI methods to improve operational efficiency and reduce costs.

Method: The authors manually examined approximately 1,800 publications (2019–2025), narrowing down to 74 papers grouped into six application areas: performance estimation, optimization, scheduling, surrogate modeling, fault detection, and language models.

Result: Scheduling emerged as the most active area, with reinforcement learning and hybrid ML-focused strategies. Supervised approaches aid performance and optimization, and Graph Neural Networks enhance anomaly detection. Domain-specific language models surpass general-purpose LLMs for HPC tasks.

Conclusion: Integration opportunities exist in areas such as OS concepts via LLMs. Advancing MLOps, standardization of AI components, and benchmarking practices are critical for broader AI-driven optimization in HPC.

Abstract: High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publications from 2019 to 2025 were manually screened using predefined inclusion/exclusion criteria; 74 "AI for HPC" papers were retained and grouped into six application areas: performance estimation, performance optimization, scheduling, surrogate modeling, fault detection, and language-model-based automation.
  Scheduling is the most active area, spanning research-oriented reinforcement-learning schedulers to production-friendly hybrids that combine ML with heuristics. Supervised performance estimation is foundational for both scheduling and optimization. Graph neural networks and time-series models strengthen anomaly detection by capturing spatio-temporal dependencies in production telemetry. Domain-specialized language models for HPC can outperform general-purpose LLMs on targeted coding and automation tasks. Together, these findings highlight integration opportunities such as LLM-based operating-system concepts and underscore the need for advances in MLOps, standardization of AI components, and benchmarking methodology.

</details>


### [620] [A Fault-Tolerant Version of Safra's Termination Detection Algorithm](https://arxiv.org/abs/2602.00272)
*Wan Fokkink,Georgios Karlos,Andy Tatman*

Main category: cs.DC

TL;DR: The paper enhances Safra's distributed termination detection algorithm to make it fault-tolerant, allowing it to handle node crashes and altered token ring structures effectively.


<details>
  <summary>Details</summary>
Motivation: The original Safra algorithm, while functional in distributed environments, is not resilient to node failures. Enhancing its fault tolerance is necessary to ensure robustness in real-world distributed systems.

Method: The algorithm modifies Safra's design by splitting the token counter into per-node counters, reconstructing the token ring structure upon crashes, and using a backup token and token-based crash detection for communication. The approach also ensures no additional message overhead.

Result: The enhanced algorithm tolerates any number of node crashes, including simultaneous failures, in a decentralized manner without introducing additional message complexity.

Conclusion: This work successfully extends Safra’s distributed termination detection algorithm to address fault tolerance, backed by correctness proofs and model-checking analysis for validation.

Abstract: Safra's distributed termination detection algorithm employs a logical token ring structure within a distributed network; only passive nodes forward the token, and a counter in the token keeps track of the number of sent minus the number of received messages. We adapt this classic algorithm to make it fault-tolerant. The counter is split into counters per node, to discard counts from crashed nodes. If a node crashes, the token ring is restored locally and a backup token is sent. Nodes inform each other of detected crashes via the token. Our algorithm imposes no additional message overhead, tolerates any number of crashes as well as simultaneous crashes, and copes with crashes in a decentralized fashion. Correctness proofs are provided of both the original Safra's algorithm and its fault-tolerant variant, as well as a model checking analysis.

</details>


### [621] [Training LLMs with Fault Tolerant HSDP on 100,000 GPUs](https://arxiv.org/abs/2602.00277)
*Omkar Salpekar,Rohan Varma,Kenny Yu,Vladimir Ivanov,Yang Wang,Ahmed Sharif,Min Si,Shawn Xu,Feng Tian,Shengbao Zheng,Tristan Rice,Ankush Garg,Shangfu Peng,Shreyas Siravara,Wenyin Fu,Rodrigo de Castro,Adithya Gangidi,Andrey Obraztsov,Sharan Narang,Sergey Edunov,Maxim Naumov,Chunqiang Tang,Mathew Oldham*

Main category: cs.DC

TL;DR: This paper introduces a fault-tolerant hybrid-shared data parallelism (FT-HSDP) paradigm to improve efficiency in large-scale synchronous GPU training by mitigating failure impacts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in synchronous large-scale GPU training due to frequent hardware failures and long recovery times.

Method: FT-HSDP addresses failures using data parallel replicas as fault tolerance units, supported by Fault Tolerant All Reduce (FTAR) for gradient exchange and a non-blocking catch-up protocol for recovery.

Result: The proposed method reduces failure recovery time from 10 minutes to 3 minutes and increases training efficiency from 44% to 80% on O(100K) GPUs.

Conclusion: FT-HSDP effectively enhances training efficiency without compromising model accuracy, presenting a scalable solution for fault tolerance in large-scale training systems.

Abstract: Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time.
  To address this problem, we propose a novel training paradigm, Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP). FT-HSDP uses data parallel replicas as units of fault tolerance. When failures occur, only a single data-parallel replica containing the failed GPU or server is taken offline and restarted, while the other replicas continue training. To realize this idea at scale, FT-HSDP incorporates several techniques: 1) We introduce a Fault Tolerant All Reduce (FTAR) protocol for gradient exchange across data parallel replicas. FTAR relies on the CPU to drive the complex control logic for tasks like adding or removing participants dynamically, and relies on GPU to perform data transfer for best performance. 2) We introduce a non-blocking catch-up protocol, allowing a recovering replica to join training with minimal stall.
  Compared with fully synchronous training at O(100K) GPUs, FT-HSDP can reduce the stall time due to failure recovery from 10 minutes to 3 minutes, increasing effective training time from 44\% to 80\%. We further demonstrate that FT-HSDP's asynchronous recovery does not bring any meaning degradation to the accuracy of the result model.

</details>


### [622] [PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching](https://arxiv.org/abs/2602.00509)
*Qianchao Zhu,Xucheng Ye,Yuliang Liu,Haodong Ouyang,Chengru Song*

Main category: cs.DC

TL;DR: The paper addresses the challenges of latency-critical Mixture-of-Experts (MoE) model inference, proposing an inference system called PROBE that improves both computational and communication efficiency during real-time operation.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in resolving the inefficiencies and bottlenecks caused by expert parallelism in MoE models, particularly under real-world serving conditions with dynamic workloads and rapid semantic shifts.

Method: The paper introduces the PROBE system, featuring three components: (1) Lookahead Predictor for expert activation forecasting, (2) a Balance Planning solver for optimized token assignment, and (3) Phase-Locked Co-Scheduling to mitigate contention during expert transfers.

Result: PROBE achieves a significant reduction in prefill latency (up to 1.32X improvement) and enhances decoding throughput (up to 1.26X improvement) compared to baselines, especially in high-volatility scenarios.

Conclusion: PROBE effectively balances computation and communication in MoE models, demonstrating its capability to handle workload volatility and improve inference efficiency in real-world applications.

Abstract: Mixture-of-Experts models have become a dominant architecture for scaling Large Language Models by activating only a sparse subset of experts per token. However, latency-critical MoE inference faces a fundamental tension: while expert parallelism improves memory efficiency, it also amplifies execution stragglers. In real-world serving, continuous batching and diverse concurrent requests induce rapid semantic shifts, causing expert hotspots to migrate abruptly across GPUs and triggering the 'double penalty' of coupled computational skew and network congestion.
  We propose PROBE, an inference system that co-balances computation and communication in real time. PROBE introduces Continuous Lookahead Pipelining, which proactively predicts, plans, and prefetches for upcoming layers while keeping all control overheads off the critical path. PROBE consists of: (1) a Gate-Initialized Lookahead Predictor that distills the target router to forecast next-layer expert activation with high fidelity; (2) a Hardware-Aware Balance Planning solver that jointly optimizes dynamic expert replication and token assignment under strict hiding-window constraints; and (3) a Phase-Locked Co-Scheduling policy that uses split-phase transmission to hide bandwidth-intensive expert transfers behind computation without contending with All-to-All collectives. Experiments show that PROBE reduces prefill latency by up to 1.32X and improves decoding throughput by up to 1.26X over state-of-the-art baselines, especially under extreme workload volatility.

</details>


### [623] [HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures](https://arxiv.org/abs/2602.00748)
*Fangxin Liu,Qinghua Zhang,Hanjing Shen,Zhibo Liang,Li Jiang,Haibing Guan,Chong Bao,Xuefeng Jin*

Main category: cs.DC

TL;DR: The paper introduces a memory management framework, HyperOffload, for efficiently handling large-scale memory demands in hierarchical SuperNode architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of memory requirements exceeding device HBM capacities in large language models and optimize the use of supernode architectures.

Method: The paper proposes HyperOffload, which integrates graph-driven memory management as part of the compiler's Intermediate Representation (IR) to globally schedule data movement and hide memory latency.

Result: HyperOffload reduces peak device memory usage by up to 26% during inference while maintaining overall performance in large-scale language model workloads.

Conclusion: Integrating memory hardware optimization frameworks within compilers is critical to scale emerging AI workloads efficiently.

Abstract: The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnects, existing software stacks fail to exploit this hardware effectively. Current runtime-based offloading and swapping techniques operate with a local view, leading to reactive scheduling and exposed communication latency that stall the computation pipeline.
  In this paper, we propose the SuperNode Memory Management Framework (\textbf{HyperOffload}). It employs a compiler-assisted approach that leverages graph-driven memory management to treat remote memory access as explicit operations in the computation graph, specifically designed for hierarchical SuperNode architectures. Unlike reactive runtime systems, SuperNode represents data movement using cache operators within the compiler's Intermediate Representation (IR). This design enables a global, compile-time analysis of tensor lifetimes and execution dependencies. Leveraging this visibility, we develop a global execution-order refinement algorithm that statically schedules data transfers to hide remote memory latency behind compute-intensive regions. We implement SuperNode within the production deep learning framework MindSpore, adding a remote memory backend and specialized compiler passes. Evaluation on representative LLM workloads shows that SuperNode reduces peak device memory usage by up to 26\% for inference while maintaining end-to-end performance. Our work demonstrates that integrating memory-augmented hardware into the compiler's optimization framework is essential for scaling next-generation AI workloads.

</details>


### [624] [System-Level Performance Modeling of Photonic In-Memory Computing](https://arxiv.org/abs/2602.00892)
*Jebacyril Arockiaraj,Sasindu Wijeratne,Sugeet Sunder,Md Abdullah-Al Kaiser,Akhilesh Jaiswal,Ajey P. Jacob,Viktor Prasanna*

Main category: cs.DC

TL;DR: This paper develops a system-level performance model for photonic in-memory computing and evaluates its performance by mapping workloads to hardware, showcasing its potential in high-performance computing applications.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the need for faster and more energy-efficient computing systems, exploring photonic in-memory computing as a potential solution due to its high-speed and low-energy advantages.

Method: The authors create a performance model that incorporates latency sources like memory access and opto-electronic conversion. They map computational workloads (e.g., Sod shock tube problem, MTTKRP, Vlasov-Maxwell equation) to evaluate how these latencies impact real-world scenarios.

Result: The performance model demonstrates that a compact photonic SRAM array, built using silicon photonics, can achieve up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS with an energy efficiency of 2.5 TOPS/W across different workloads.

Conclusion: Despite system overheads, photonic in-memory computing shows significant potential for high-performance computing with advancements in energy efficiency and processing speed.

Abstract: Photonic in-memory computing is a high-speed, low-energy alternative to traditional transistor-based digital computing that utilizes high photonic operating frequencies and bandwidths. In this work, we develop a comprehensive system-level performance model for photonic in-memory computing, capturing the effects of key latency sources such as external memory access and opto-electronic conversion. We perform algorithm-to-hardware mapping across a range of workloads, including the Sod shock tube problem, Matricized Tensor Times Khatri-Rao Product (MTTKRP), and the Vlasov-Maxwell equation, to evaluate how the latencies impact real-world high-performance computing workloads. Our performance model shows that, while accounting for system overheads, a compact 1x256 bit single-wavelength photonic SRAM array, fabricated using the standard silicon photonics process by GlobalFoundries, sustains up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS on the Sod shock tube problem, MTTKRP, and the Vlasov-Maxwell equation with an average energy efficiency of 2.5 TOPS/W.

</details>


### [625] [Low-latency Federated LLM Fine-tuning Over Wireless Networks](https://arxiv.org/abs/2602.01024)
*Zhiwen Pang,Kang Wei,Long Shi,Zhe Wang,Jun Li,Feng Shu*

Main category: cs.DC

TL;DR: The paper introduces a framework to improve federated LLM fine-tuning efficiency for clients with limited resources using a joint optimization of pruning rates and bandwidth allocation.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in collaborative fine-tuning while overcoming inefficiencies caused by resource limitations in federated LLMs.

Method: Proposed a Joint Client-Specific Pruning and Bandwidth Allocation (JCPBA) framework along with a fine-tuning latency minimization problem and solved it using a block coordinate descent method.

Result: Extensive experiments showed faster fine-tuning, comparable or better test loss, and reduced computation and communication overhead compared to baselines.

Conclusion: The framework effectively enhances federated fine-tuning efficiency for LLMs, particularly in resource-constrained environments, while maintaining performance quality.

Abstract: Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.

</details>


### [626] [BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation](https://arxiv.org/abs/2602.01404)
*Zhouzi Li,Cindy Zhu,Arpan Mukhopadhyay,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: This paper introduces BOA Constrictor, a scheduler for ML training jobs that optimizes GPU usage under a budget constraint, improving performance with reduced average job completion times.


<details>
  <summary>Details</summary>
Motivation: Organizations face challenges managing GPU costs and performance tradeoffs when running ML training jobs in the cloud. Existing solutions fail to optimize for budgeted performance effectively.

Method: The authors propose a Budget-Optimal Allocation (BOA) policy, formulating the GPU scheduling as a budget-constrained problem and developing the BOA Constrictor scheduler.

Result: The scheduler achieved 1.6x reduction in average job completion time in small-scale tests and 2x in large-scale simulations compared to existing state-of-the-art solutions.

Conclusion: BOA Constrictor offers an effective tool for organizations to achieve higher performance in ML training jobs within a fixed budget, outperforming existing scheduling heuristics.

Abstract: The past decade has seen a dramatic increase in demand for GPUs to train Machine Learning (ML) models. Because it is prohibitively expensive for most organizations to build and maintain a large GPU cluster, organizations instead choose to rent GPUs from cloud providers. The customer is responsible for devising a policy for (i) deciding how many GPUs to rent at every moment in time to process a stream of ML training jobs and (ii) allocating the rented GPUs among the currently active jobs in the system. Because ML training jobs can be parallelized across different numbers of GPUs, the customer generally has many options for how many GPUs to use for each job. Allocating more GPUs to a single training job will cause the job to complete more quickly. However, the customer pays for each GPU-hour they use, and a training job receives a diminishing marginal benefit from running on additional GPUs. Hence, allocating too many GPUs to a single training job can dramatically increase the overall cost that the customer pays to the cloud provider. This gives rise to a cost-performance tradeoff that customers must balance when running training jobs in the cloud.
  To balance the cost-performance tradeoff, we develop BOA Constrictor, a new scheduler for ML training jobs which uses a Budget-Optimal Allocation (BOA) policy to squeeze the highest level of performance out of a cloud-deployed GPU cluster given a fixed budget constraint. We explicitly formulate the problem as a budget-constrained scheduling problem and derive the BOA policy which minimizes the average job completion time (JCT) of a stream of arriving jobs subject to the user's budget. For a given budget level, we demonstrate that BOA Constrictor can reduce average JCT by 1.6 times in small-scale implementation experiments and by 2 times in detailed, large-scale simulations compared to state-of-the-art heuristic based schedulers.

</details>


### [627] [Mean field optimal Core Allocation across Malleable jobs](https://arxiv.org/abs/2602.01411)
*Zhouzi Li,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: This paper addresses the optimization problem of allocating processing cores to malleable jobs with diminishing returns and presents two mean field optimal policies (FW-CAM and WHAM) for scenarios with multiple job classes and generally distributed workloads.


<details>
  <summary>Details</summary>
Motivation: To optimize core allocation for malleable jobs in data centers and cloud clusters, minimizing mean response time, especially under scenarios with diminishing marginal returns on processing capacity.

Method: The paper analyzes the Core Allocation to Malleable jobs (CAM) problem in the mean field regime and proposes two policies, FW-CAM and WHAM, tailored to job classes with arbitrary concave speedup functions, holding costs, and distributed job characteristics.

Result: FW-CAM reveals that job size is irrelevant for optimization in the mean field regime, while WHAM is proven to be asymptotically optimal and functions effectively across diverse scenarios, outperforming prior solutions in the presence of varying speedup functions.

Conclusion: The proposed FW-CAM and WHAM policies provide robust and efficient solutions to the CAM problem, with WHAM being practically useful even outside theoretical conditions, filling gaps in prior research on malleable job scheduling.

Abstract: Modern data centers and cloud computing clusters are increasingly running workloads composed of malleable jobs. A malleable job can be parallelized across any number of cores, yet the job typically exhibits diminishing marginal returns for each additional core on which it runs. This can be seen in the concavity of a job's speedup function, which describes the job's processing speed as a function of the number of cores on which it runs.
  Given the prevalence of malleable jobs, several theoretical works have posed the problem of how to allocate a fixed number of cores across a stream of arriving malleable jobs so as to minimize the mean response time across jobs. We refer to this as the Core Allocation to Malleable jobs (CAM) problem. We solve the CAM problem under a highly general setting, allowing for multiple job classes, each with an arbitrary concave speedup function and holding costs (weight). Furthermore, we allow for generally distributed inter-arrival times and job sizes.
  We analyze the CAM problem in the mean field asymptotic regime and derive two distinct mean field optimal policies, FW-CAM and WHAM. FW-CAM is interesting because it demonstrates a new intuition: in the mean field regime, job sizes are not relevant in finding an optimal policy. WHAM (Whittle Allocation for Malleable jobs) is interesting because it is asymptotically optimal and also serves as a good heuristic even outside of the asymptotic regime. Notably, none of the policies previously proposed in the literature are mean field optimal when jobs may follow different speedup functions.

</details>


### [628] [Developing a Portable Solution for Post-Event Analysis Pipelines](https://arxiv.org/abs/2602.01798)
*Leonardo Pelonero,Fabio Vitello,Eva Sciacca,Mauro Imbrosciano,Salvatore Scavo,Ugo Becciani*

Main category: cs.DC

TL;DR: The paper presents a Science Gateway framework for automated analysis of natural hazards using photogrammetry, AI, and data visualization.


<details>
  <summary>Details</summary>
Motivation: To improve risk assessment and mitigation strategies in response to climate change-induced natural hazards by creating a framework that integrates modern technologies.

Method: A Science Gateway framework was developed, utilizing photogrammetry, data visualization, and AI on aerial images for post-event analysis of natural disasters.

Result: The framework is capable of fully automated and portable analyses to assess extreme natural events and evaluate their impact in risk-prone areas.

Conclusion: Such frameworks can enhance understanding and response to natural hazards, aiding mitigation strategies in vulnerable regions like Italy.

Abstract: In recent years, the monitoring and study of natural hazards have gained significant attention, particularly due to climate change, which exacerbates incidents like floods, droughts, storm surges, and landslides. Together with the constant risk of earthquakes, these climate-induced events highlight the critical necessity for enhanced risk assessment and mitigation strategies in susceptible areas such as Italy.
  In this work, we present a Science Gateway framework for the development of portable and fully automated post-event analysis pipelines integrating Photogrammetry techniques, Data Visualization and Artificial Intelligence technologies, applied on aerial images, to assess extreme natural events and evaluate their impact on risk-exposed assets.

</details>


### [629] [Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training](https://arxiv.org/abs/2602.01872)
*Chongyang Xu,Christoph Siebenbrunner,Laurent Bindschaedler*

Main category: cs.DC

TL;DR: Grappa is a distributed GNN training framework improving efficiency and accuracy via gradient-only communication and innovative gradient aggregation approaches.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies caused by remote feature and activation fetching during distributed GNN training, especially with deeper graphs and numerous partitions.

Method: Implements gradient-only communication by isolating partitions during training while periodically repartitioning and using a corrected gradient aggregation inspired by importance sampling.

Result: Grappa achieves up to a 13x speed-up over state-of-the-art methods, maintains accuracy in deeper models, and supports large-scale training on commodity hardware.

Conclusion: Grappa demonstrates significant improvements in distributed GNN training by balancing speed and accuracy, while remaining model-agnostic and hardware-efficient.

Abstract: Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.

</details>


### [630] [vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models](https://arxiv.org/abs/2602.02204)
*Peiqi Yin,Jiangyun Zhu,Han Gao,Chenguang Zheng,Yongxiang Huang,Taichang Zhou,Ruirui Yang,Weizhi Liu,Weiqing Chen,Canlin Guo,Didan Deng,Zifeng Mo,Cong Wang,James Cheng,Roger Wang,Hongsheng Liu*

Main category: cs.DC

TL;DR: The paper introduces vLLM-Omni, a serving system optimized for 'any-to-any' multimodal AI models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in serving complex any-to-any multimodal AI models, which entail challenges due to their intricate architectures and manual handling of cross-stage interactions.

Method: The proposed method involves creating vLLM-Omni, which enables decomposition of complex architectures into interconnected stages, with optimized resource management and unified inter-stage data connectors.

Result: vLLM-Omni improves performance by reducing job completion time (JCT) by up to 91.4% compared to existing systems.

Conclusion: vLLM-Omni effectively optimizes the serving process for multimodal models, significantly enhancing efficiency. The code is publicly available for further utilization.

Abstract: Any-to-any multimodal models that jointly handle text, images, video, and audio represent a significant advance in multimodal AI. However, their complex architectures (typically combining multiple autoregressive LLMs, diffusion transformers, and other specialized components) pose substantial challenges for efficient model serving. Existing serving systems are mainly tailored to a single paradigm, such as autoregressive LLMs for text generation or diffusion transformers for visual generation. They lack support for any-to-any pipelines that involve multiple interconnected model components. As a result, developers must manually handle cross-stage interactions, leading to huge performance degradation. We present vLLM-Omni, a fully disaggregated serving system for any-to-any models. vLLM-Omni features a novel stage abstraction that enables users to decompose complex any-to-any architectures into interconnected stages represented as a graph, and a disaggregated stage execution backend that optimizes resource utilization and throughput across stages. Each stage is independently served by an LLM or diffusion engine with per-stage request batching, flexible GPU allocation, and unified inter-stage connectors for data routing. Experimental results demonstrate that vLLM-Omni reduces job completion time (JCT) by up to 91.4% compared to baseline methods. The code is public available at https://github.com/vllm-project/vllm-omni.

</details>


### [631] [Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS](https://arxiv.org/abs/2602.02234)
*Andong Hu,Luca Pennati,Stefano Markidis,Ivy Peng*

Main category: cs.DC

TL;DR: The paper integrates AI-based deep potentials into GROMACS for molecular dynamics simulations, evaluating the performance of DPA2 and DPA3 deep learning models on GPUs.


<details>
  <summary>Details</summary>
Motivation: To combine the high accuracy of AI deep potentials with the computational efficiency required for large production-level molecular dynamics simulations using GROMACS.

Method: Integration of AI deep potentials using DeePMD-kit with GROMACS, enabling deep learning models across multiple architectures and GPU backends. Performance evaluation on protein-in-water benchmarks using NVIDIA GPUs.

Result: DPA2 outperforms DPA3 with up to 4.23x and 3.18x higher throughput on A100 and GH200 GPUs, respectively, highlighting key optimization areas like kernel-launch overhead and domain-decomposed inference.

Conclusion: By coupling GROMACS with DeePMD-kit, AI deep potentials can deliver efficient, ab initio-quality molecular dynamics simulations, guiding future optimizations for GPU execution.

Abstract: State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.

</details>


### [632] [Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents](https://arxiv.org/abs/2602.02335)
*Weiming Sheng,Jinlang Wang,Manuel Barros,Aldrin Montana,Jacopo Tagliabue,Luca Bigon*

Main category: cs.DC

TL;DR: Bauplan introduces a code-first system for safer data handling in lakehouses by eliminating illegal states through typing, versioning, and atomicity.


<details>
  <summary>Details</summary>
Motivation: Lakehouses face safety issues when untrusted actors concurrently manipulate production data, leading to runtime errors and leaks in multi-table pipelines.

Method: The paper introduces Bauplan, a code-first approach incorporating typed table contracts, Git-like data versioning, and transactional runs.

Result: Preliminary results show promise in achieving safer pipelines using a lightweight formal transaction model.

Conclusion: Bauplan enhances lakehouse safety through software engineering principles, with opportunities for further improvement based on counterexamples and results.

Abstract: Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.

</details>


### [633] [LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340)
*Gustav Schmid*

Main category: cs.DC

TL;DR: This paper examines distributed time complexity of Locally Checkable Labelings (LCLs) on unbounded-degree trees, introducing Locally Finite Labelings (LFLs) to address the disappearance of polynomial gap results.


<details>
  <summary>Details</summary>
Motivation: To investigate the distributed time complexities of LCLs in the context of unbounded graph degrees and understand whether polynomial gap results persist.

Method: The authors allow LCLs to use infinitely many local configurations and demonstrate the removal of polynomial gaps. They then define a new class, Locally Finite Labelings (LFLs), imposing a restriction that node configurations must fall into finitely many local cases.

Result: They show that by using LFLs, the polynomial gap results on distributed time complexity are preserved even in unbounded-degree settings.

Conclusion: Introducing LFLs ensures that locally checkable problems on unbounded-degree trees retain clear deterministic LOCAL complexity bounds ($\Theta(n^{1/k})$ for integer $k\geq1$ or $O(\log n)$). The complexity class can be determined solely from the description of the LFL.

Abstract: The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of strong gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees.
  We focus on the polynomial regime ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting crucially depends on how LCLs are generalized beyond bounded degrees. We first demonstrate that if one allows LCLs to be defined using infinitely many local configurations, then the polynomial gaps disappear entirely: for every real exponent $0 < r \leq 1$, there exists a locally checkable problem on trees with deterministic LOCAL complexity $Θ(n^r)$.
  Rather than stopping at this negative result, we identify a natural class of problems for which polynomial gap results can still be recovered. We introduce Locally Finite Labelings (LFLs), which formalize the intuition that ''every node must fall into one of finitely many local cases'', even in the presence of unbounded degrees.
  Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either
  - $Θ(n^{1/k})$ for some integer $k \geq 1$, or
  - $O(\log n)$.
  Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.

</details>


### [634] [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355)
*Amirreza Kazemi,Seyed Mohammad Azimi-Abarghouyi,Gabor Fodor,Carlo Fischione*

Main category: cs.DC

TL;DR: Hierarchical federated learning (HFL) improves communication efficiency with a novel sign-based framework, HierSignSGD, while ensuring competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Address communication constraints in HFL environments caused by bandwidth and latency limits, particularly in large-scale wireless IoT systems.

Method: Developing and analyzing the HierSignSGD algorithm where signed gradients are majority-voted at edge servers, averaged at the cloud layer, and broadcast globally with downlink quantization.

Result: HierSignSGD achieves comparable accuracy to full-precision SGD at significantly reduced communication cost, performing well in both homogeneous and heterogeneous data setups.

Conclusion: The proposed algorithm efficiently balances extreme gradient compression and robust performance, bridging the gap in hierarchical federated learning settings.

Abstract: Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.

</details>


### [635] [sVIRGO: A Scalable Virtual Tree Hierarchical Framework for Distributed Systems](https://arxiv.org/abs/2602.02438)
*Lican Huang*

Main category: cs.DC

TL;DR: The proposed sVIRGO is a scalable virtual hierarchical tree structure, enhancing coordination in large distributed systems via role allocation on physical nodes without overlay networks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a scalable and resilient framework to efficiently manage coordination and communication in large-scale distributed systems while maintaining locality and robustness.

Method: Developed a virtual hierarchical tree framework (sVIRGO) with layered region configurations, dynamic re-selection, virtual roles, and mechanisms for locality-preserving communication.

Result: sVIRGO ensures efficient coordination with minimal recovery latency, bounded communication overhead, reduced failure probability, and robust operation in challenging conditions.

Conclusion: sVIRGO provides a scalable, reliable, and efficient framework for distributed systems, overcoming traditional challenges like failures and communication propagation.

Abstract: We propose sVIRGO, a scalable virtual tree hierarchical framework for large-scale distributed systems. sVIRGO constructs virtual hierarchical trees directly on physical nodes, allowing each node to assume multiple hierarchical roles without overlay networks. The hierarchy preserves locality and is organized into configurable layers within regions. Coordination across thousands of regions is achieved via virtual upper-layer roles dynamically mapped onto nodes up to the top layer.
  Each region maintains multiple active coordinators that monitor local health and perform dynamic re-selection if failures occur. Temporary drops below the minimum threshold do not compromise coordination, ensuring near-zero recovery latency, bounded communication overhead, and exponentially reduced failure probability while maintaining safety, liveness, and robustness under mobile, interference-prone, or adversarial conditions.
  Communication is decoupled from the hierarchy and may use multi-frequency wireless links. Two message hop strategies are supported: (i) with long-distance infrastructure-assisted channels, coordinators exploit the virtual tree to minimize hops; (ii) without such channels, messages propagate via adjacent regions.
  sVIRGO also supports Layer-Scoped Command Execution. Commands and coordination actions are executed within the scope of each hierarchical layer, enabling efficient local and regional decision-making while limiting unnecessary global propagation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [636] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO framework combines interpretable reasoning with combinatorial search for effective compiler auto-tuning.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in compiler auto-tuning methods such as black-box search and superficial pattern matching in LLM-based approaches.

Method: Reverse engineering Chain-of-Thought datasets to link code features with performance evidence and using collaborative inference combining LLM strategies with genetic algorithms.

Result: ECCO achieves superior performance, showing an average 24.44% reduction in cycles across seven datasets.

Conclusion: ECCO improves upon traditional methods by integrating interpretable reasoning and collaborative optimization, ensuring better compiler optimization outcomes.

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [637] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All is a framework utilizing Large Language Models (LLMs) to improve citizen interaction with geospatial Open Government Data by ensuring transparency, reproducibility, and reliability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for transparent, accessible, and accurate citizen interaction with Open Government Data (OGD), aiming to reduce the risks of misinterpretation or misinformation.

Method: OGD4All combines semantic data retrieval, agentic reasoning for code generation, and secure execution environments. It integrates with LLMs to provide explainable and multimodal outputs while being benchmarked on factual robustness and reliability.

Result: The framework demonstrated 98% analytical correctness and 94% recall in answering benchmark questions while reliably rejecting unsupported queries. It showed strong statistical and expert-verified robustness.

Conclusion: OGD4All showcases how LLMs can facilitate trustworthy, explainable, and multimodal public access to OGD, promoting open governance and advancing AI’s role in public trust.

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [638] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: The paper introduces a framework for measurement in challenging data environments by combining multi-source triangulation, interpretable machine learning, and theory-guided approaches to fill gaps in inaccessible measurement spaces.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measurement in high-stakes contexts where data are fragmented, indirect, or structurally missing, and conventional approaches are not viable.

Method: The authors propose a framework using multi-source triangulation and interpretable machine learning models to seek consistency across diverse data sources rather than relying on unattainable ideal ground truth data.

Result: The empirical analysis showcases the effective application of this framework in understanding organizational growth and pressure dynamics in a clandestine militant organization, yielding meaningful variations from fragmented observational data.

Conclusion: This framework allows for defensible conclusions in data-limited settings while explicitly acknowledging inferential limits, providing a robust basis for understanding dynamics in inaccessible contexts.

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [639] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: This paper introduces an advanced operational model and enhanced deep reinforcement learning (DRL) framework for hydrogen-based multi-energy systems (HMES), focusing on improving efficiency and handling system complexities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address operational challenges in HMES due to the nonlinear and coupled dynamics of hydrogen energy storage systems and uncertainties in supply and demand.

Method: A comprehensive model incorporating nonlinear dynamics and multi-physics processes for HMES is developed. Additionally, an enhanced DRL framework with representation learning techniques is proposed to optimize complex, coupled networks.

Result: Experimental studies demonstrate the proposed model improves HESS reliability and safety. The SR-DRL approach outperforms conventional DRL, reduces operation costs, and effectively handles system constraints.

Conclusion: Representation learning enhances DRL by reorganizing the state space into a structured geometric representation, resulting in smoother and more effective learning processes for HMES optimization.

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [640] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: This paper introduces COLT, a lightweight collaborative multi-LLM framework for compiler optimization, combining small and large LLMs in a Monte Carlo tree search-based process to reduce costs without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Model serving costs in AI systems are high, and existing compiler optimizations that rely solely on large language models are expensive, whereas smaller language models alone are less reliable.

Method: The authors propose using a collaborative multi-LLM framework (COLT) that integrates multiple LLMs of varying sizes. Central to the method is the use of a shared Monte Carlo tree search (MCTS) tree for efficient coordination, alongside mechanisms to select query models and escalate to large models in the presence of persistent regressions.

Result: By leveraging a shared MCTS tree and lightweight collaborative reasoning, the framework achieves optimization efficiency comparable to a solitary large language model.

Conclusion: The COLT framework successfully reduces the reliance on expensive large language models by combining their strengths with those of smaller models, showcasing cost efficiency and performance gains for compiler optimization tasks.

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [641] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: This paper introduces ELLMPEG, an edge-enabled framework for LLMs to generate video processing commands locally, avoiding cloud-based limitations and achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of cloud-based LLMs, such as high computational costs, privacy concerns, and recurring API expenses, by creating a more efficient edge-based solution.

Method: The paper proposes ELLMPEG, which combines Retrieval-Augmented Generation (RAG) and iterative self-reflection to generate executable video processing commands locally without relying on external cloud APIs.

Result: ELLMPEG achieves a generation accuracy of 78% using the Qwen2.5 LLM framework, outperforms other open-source models, and eliminates recurring API costs in handling FFmpeg and VVenC commands.

Conclusion: ELLMPEG proves to be an efficient agentic LLM framework that enables accurate, cost-effective local video processing, demonstrating the potential of edge-enabled AI solutions.

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [642] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: Introduces a RAG framework tailored for disaster response phases, utilizing multimodal grounding for efficient retrieval and situational adaptability.


<details>
  <summary>Details</summary>
Motivation: Improve situational understanding and decision-making in diverse disaster scenarios through enhanced retrieval and reasoning.

Method: Constructs a hierarchical multimodal knowledge base using disaster manuals, lessons, imagery, and implements adaptive retrieval and LoRA post-training for experiential insights.

Result: Experiments show improved situational grounding, task decomposition and usability in emergency operations leveraging multimodal RAG advancements.

Conclusion: Adaptive RAG with self-reasoning and multimodal chain-of-thought capabilities significantly enhance disaster response effectiveness.

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [643] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: This paper introduces MEG-XL, a brain-to-text interface model pre-trained on longer MEG contexts, showing enhanced generalization for word decoding with reduced data requirements.


<details>
  <summary>Details</summary>
Motivation: Designing clinical brain-to-text interfaces for paralyzed patients demands approaches that generalize efficiently across subjects, as extensive training data is unavailable for these patients.

Method: The authors propose MEG-XL, a model utilizing 2.5 minutes of MEG context per sample during pre-training, substantially longer than previous methods, to capture extended neural context.

Result: MEG-XL achieves supervised performance with significantly less training data, outperforming existing brain foundation models and demonstrating the value of long-context pre-training.

Conclusion: Long-context pre-training effectively leverages neural data context, improving transfer learning and word decoding performance, and represents a promising advance for brain-to-text interfaces.

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


### [644] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: LTSM-DIFF integrates large language models and diffusion models to address time series forecasting with limited data.


<details>
  <summary>Details</summary>
Motivation: Limited data availability in specialized domains makes accurately modeling temporal dynamics challenging.

Method: Combines a fine-tuned LTSM module for sequential memory extraction and a diffusion model for probabilistic guidance.

Result: Outperformed state-of-the-art methods in both data-rich and few-shot forecasting benchmarks.

Conclusion: LTSM-DIFF provides robust and generalizable time series forecasting, especially effective with scarce data.

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [645] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: This study examines sycophancy in language models across English and Hindi through a cultural and linguistic lens, finding that Hindi culturally-adapted prompts exhibit higher sycophancy rates.


<details>
  <summary>Details</summary>
Motivation: The research addresses the gap in understanding whether sycophancy diagnostics in language models, largely studied in English, apply across different languages and cultures.

Method: The authors extended an English-language sycophancy diagnostic (Beacon) to Hindi using three conditions: English original, Hindi literal translation, and Hindi culturally adapted prompts, evaluating four instruction-tuned models on 50 prompts per condition.

Result: Culturally adapted Hindi prompts showed consistently higher sycophancy rates (12.0–16.0 percentage points more than English). Cultural adaptation was the main factor driving this gap, with minimal contribution from language encoding.

Conclusion: The study concludes that alignment behaviors differ across languages and cultures. Culturally grounded prompts significantly affect sycophancy rates, suggesting non-uniform transfer of English evaluation results to other languages.

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [646] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: The paper introduces a data-centric optimization framework using dataset pruning to enhance resource efficiency in edge learning, achieving reduced training costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational and energy challenges in on-device training for edge learning, as current methods are hindered by processing large and redundant datasets.

Method: The paper proposes a lightweight on-device importance evaluation using average loss statistics derived from a truncated warm-up phase to rank and prune datasets dynamically, retaining only critical data points.

Result: Experiments show a near-linear reduction in training latency and energy consumption proportional to pruning ratio, with minimal loss in model accuracy.

Conclusion: Dataset pruning emerges as a significant method for improving sustainability and scalability in edge learning on resource-constrained mobile devices.

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [647] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: This paper introduces a distributional reinforcement learning method using QR-DQN for optimizing multi-equipment condition-based maintenance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inefficiencies and risks associated with traditional time-based maintenance strategies and leverage real-time data for better maintenance scheduling.

Method: It employs Quantile Regression Deep Q-Networks (QR-DQN) integrated with aging factors to manage multiple pump units under varying strategic scenarios such as safety-first, balanced, and cost-efficient approaches.

Result: Experimental validation over 3,000 episodes showed significant performance improvements, including 95.66% operational stability and ROI of 3.91 for the Safety-First strategy.

Conclusion: The approach is highly cost-efficient, stable, and directly applicable to industrial settings, providing superior benefits over traditional maintenance methods.

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [648] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: The paper proposes TextBFGS, a second-order method for discrete text optimization, outperforming first-order methods in convergence and cross-task scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing first-order gradient methods for text/code optimization show slow convergence and instability by neglecting the semantic curvature of the optimization landscape.

Method: TextBFGS, a second-order Quasi-Newton framework, approximates the inverse Hessian matrix using historical correction patterns stored in a pre-learned memory base, and applies a one-pass update utilizing these patterns.

Result: In empirical evaluations (e.g., HumanEval, MBPP), TextBFGS demonstrates better pass rates, fewer model calls, and strong transferability across tasks compared to first-order approaches.

Conclusion: TextBFGS introduces a mathematically grounded and efficient second-order optimization method for improving text/code optimization through memory-aware adjustments.

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [649] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: The paper proposes a novel training method named SCPL to improve efficiency in deep learning training by decoupling backpropagation. It shows superior results compared to existing methods and offers enhanced parallelism.


<details>
  <summary>Details</summary>
Motivation: High training costs and inefficiencies of backpropagation hinder adopting large-scale AI models in enterprises.

Method: SCPL decouples the backpropagation process and transforms long gradient flows into multiple shorter ones, enabling parameter gradients computation in parallel across layers.

Result: Experiments demonstrate SCPL's efficiency and effectiveness over BP, Early Exit, GPipe, and state-of-the-art methods for backpropagation decoupling.

Conclusion: SCPL enables practical, cost-effective development of enterprise AI systems and enhances agility. The experimental code is available for reproducibility.

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [650] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP framework optimizes LLM training with adaptive mixed-precision, outperforming baselines and reducing FLOPs while preserving model quality.


<details>
  <summary>Details</summary>
Motivation: Training large language models (LLMs) efficiently without compromising quality faces challenges with subbyte precision and existing mixed-precision approaches.

Method: SNIP collects activation, gradient, and optimizer state statistics, utilizes metrics on loss/weight divergence, and solves an ILP problem for layerwise precision optimization.

Result: SNIP achieves up to 80% reduction in FLOPs while maintaining model quality across various model sizes and training phases with minimal computational overhead.

Conclusion: SNIP demonstrates superior performance and efficiency, providing a scalable solution for LLM pretraining with adaptive precision.

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [651] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: Counterfactual explanations, used for interpreting machine learning models, are influenced by model and data uncertainty, making them unstable under real-world variability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of robustness in counterfactual explanations commonly used in machine learning, especially under model and data uncertainty.

Method: The study examined the sensitivity of counterfactual explanations to aleatoric and epistemic uncertainties through experiments on synthetic and real-world tabular datasets.

Result: Counterfactual explanations demonstrated significant sensitivity to model uncertainty, with variations in results driven by even minor accuracy reductions.

Conclusion: The study highlights the importance of developing uncertainty-aware explanation methods, particularly for critical domains like finance and social sciences.

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [652] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: The paper introduces SPGCL, a graph contrastive learning framework that makes structural perturbations more robust compared to current methods by using an SVD-guided refinement step and balanced edge operations.


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of Graph Neural Networks (GNNs) to structural noise caused by adverse factors and improve robustness and accuracy in graph contrastive learning.

Method: SPGCL employs stochastic edge removal combined with refinement via Singular Value Decomposition (SVD), ensuring recovery of important edges and adding meaningful links without densifying graphs. It uses a fusion module with global similarity constraints to align two graph views.

Result: SPGCL demonstrated consistent improvement in robustness and accuracy across ten benchmark datasets, surpassing state-of-the-art methods.

Conclusion: The framework successfully enhances structural robustness and semantic learning in GNNs, filling the gap between random and spectral structural perturbation approaches.

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [653] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: The paper proposes an efficient methodology using Tensor Train Decomposition for optimizing fully connected layers in deep neural networks, specifically targeting RISC-V architectures, achieving faster inference times and reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Deploying deep neural networks on resource-constrained devices like RISC-V platforms is difficult due to the computational and memory overhead of fully connected layers.

Method: The paper uses Tensor Train Decomposition (TTD) with a refined exploration methodology to efficiently factorize FC layers, applying compiler optimizations to enhance performance on RISC-V architectures.

Result: The decomposed layers achieve 3x faster execution compared to IREE and 8x faster compared to Pluto, demonstrating significant improvements in inference and computational efficiency.

Conclusion: The proposed approach provides a practical and efficient solution for deploying DNNs on embedded and edge devices powered by RISC-V, addressing challenges in memory and computation constraints.

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [654] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: NSG-MoE introduces a framework for multimodal graph learning, addressing modality confusion and achieving high performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multimodal graph learning caused by severe modality confusion.

Method: The paper proposes NSG-MoE, integrating node-splitting, graph-rewiring mechanisms, and a structured Mixture-of-Experts architecture.

Result: NSG-MoE outperforms baselines on multimodal benchmarks, achieves competitive training efficiency, and shows adaptive filtering and improved generalization.

Conclusion: NSG-MoE effectively preserves multimodal semantics, mitigates mixing effects, and improves generalization with computational efficiency.

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [655] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: This paper presents a generative transfer learning approach using a normalizing flow-based model to address data scarcity in multi-fidelity machine learning surrogates.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of using high-fidelity but scarce data alongside abundant but less accurate low-fidelity data for machine learning surrogate models.

Method: A probabilistic multi-fidelity surrogate framework using normalizing flow is developed, including pretraining on low-fidelity data and fine-tuning on high-fidelity data.

Result: The framework enables fast probabilistic predictions with quantified uncertainty and significantly improves accuracy while reducing the need for high-fidelity evaluations.

Conclusion: The proposed approach successfully combines low-fidelity and high-fidelity data to create efficient and accurate surrogates, making machine learning applications more practical for complex engineering systems.

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [656] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: This paper introduces a variance reduction method, called dimensional peeking, to improve gradient estimation in discrete optimization via simulation.


<details>
  <summary>Details</summary>
Motivation: To address slow convergence issues arising from variance in perturbation-based sampling of stochastic gradient estimators for high-dimensional discrete optimization.

Method: Dimensional peeking lifts the sampling granularity from scalar values to classes of values that follow the same control flow path, utilizing a custom numerical data type for implementation.

Result: Dimensional peeking achieves variance reductions by up to a factor of 7.9 across three benchmark problems and improves optimization progress compared to three meta-heuristics.

Conclusion: The proposed dimensional peeking method enhances gradient estimation through simulation, improving the competitiveness of zeroth-order optimization for complex, non-convex problems.

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [657] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: Explores the theory and modeling of Sheaf Neural Networks (SNN) and evaluates its effectiveness in biomedical applications compared to popular Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: To develop and assess Sheaf Neural Networks (SNN) as an improved algorithm for answering complex biomedical questions.

Method: The paper mathematically models SNNs and applies them in a biomedical case study, comparing performance against Graph Convolutional Networks, Graph Attention Networks, and GraphSage.

Result: SNN demonstrated superior performance compared to leading graph-based neural network models in addressing biomedical challenges.

Conclusion: Sheaf Neural Networks are a promising alternative to traditional GNNs, particularly in biomedical contexts, showcasing enhanced computational effectiveness and accuracy.

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [658] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: This paper introduces a methodology using regression trees and their ensembles for automated univariate time series forecasting, achieving accuracy comparable to ARIMA and exponential smoothing models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an alternative approach to traditional forecasting methods, leveraging regression trees to better handle autoregressive features, trends, and seasonality in time series.

Method: The authors employ regression trees, bagging, and random forests for recursive and autoregressive forecasting, addressing key challenges like trending or seasonal data.

Result: The proposed methodology achieves a similar accuracy level to established methods like ARIMA and exponential smoothing. It is also implemented in publicly available software.

Conclusion: The methodology provides a viable, automated alternative to traditional statistical forecasting techniques, with practical software support.

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [659] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: This paper reformulates Quality-Diversity (QD) optimization as a multi-objective optimization (MOO) problem, using established MOO methods to address QD challenges and achieve competitive performance with state-of-the-art QD algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the well-established theory and methods of MOO for solving the complex challenges of QD optimization in various domains such as robot control and creative design, avoiding the need to design a new QD algorithm.

Method: The authors recast QD optimization as a MOO problem with numerous objectives and use set-based scalarization techniques for a collaborative search process, supported by a theoretical and practical analysis.

Result: The proposed approach inherits theoretical guarantees from MOO and demonstrates desirable properties for QD optimization. Experiments across multiple domains validate that it performs competitively with state-of-the-art QD algorithms.

Conclusion: Casting QD optimization as a multi-objective problem enables the use of established MOO techniques, providing a powerful and theoretically sound alternative to traditional QD approaches.

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [660] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: The paper introduces a lossless compression method for unit-norm embeddings, achieving superior compression rates (1.5×) without training.


<details>
  <summary>Details</summary>
Motivation: To improve compression efficiency for high-dimensional unit-norm embeddings without compromising data fidelity.

Method: The approach uses the concentration of spherical coordinates of unit vectors around π/2 to simplify IEEE 754 exponent coding and entropy coding.

Result: Achieved a 1.5× compression rate, which is 25% better than previous methods, validated across 26 configurations of text, image, and multi-vector embeddings.

Conclusion: The method provides consistent, lossless compression improvements for unit-norm embeddings within float32 precision and requires no training.

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [661] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: The paper explores why LoRA (parameter-efficient fine-tuning method) naturally resists label noise and proposes Rank-Aware Curriculum Training (RACT) to leverage this trait.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inherent challenge of label noise in model training, particularly for large pretrained models adapted through LoRA.

Method: The authors use a theoretical framework to analyze LoRA's behavior under label noise and propose RACT, exploiting rank discrepancy for detecting noise.

Result: RACT achieves competitive performance, including 91.1% F1 score for noise detection on AG News and maintains 91.46% accuracy.

Conclusion: LoRA is resistant to label noise due to its rank constraints. This property is enhanced with RACT, providing a practical noise detection mechanism without sacrificing accuracy.

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [662] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe is a system for deploying Speech Language Models in streaming settings, enhancing performance with low latency and high throughput.


<details>
  <summary>Details</summary>
Motivation: Existing SpeechLM systems lack flexibility, efficiency, and streamability support for diverse models in streaming applications.

Method: VoxServe introduces a new model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines to decouple model architecture from system optimizations.

Result: Evaluations demonstrate that VoxServe delivers 10-20x higher throughput compared to existing methods while ensuring comparable latency and strong streaming capabilities.

Conclusion: VoxServe provides a unified framework for efficiently serving diverse SpeechLMs in streaming systems with significant performance improvements.

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [663] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: This paper introduces CARE-RFT, a new fine-tuning method for large language models, to balance reasoning capabilities and model trustworthiness effectively.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement finetuning methods either compromise trustworthiness to achieve reasoning performance or limit reasoning improvements to preserve trustworthiness.

Method: CARE-RFT uses a skew reverse KL divergence as a confidence-sensitive penalty, enabling an effective trade-off between reasoning and calibration by differentiating penalties based on confidence levels of explorations.

Result: CARE-RFT demonstrates superior performance, achieving reasoning capabilities comparable to unconstrained RFT while maintaining trustworthiness and calibration akin to the base model.

Conclusion: Confidence-aware regularization techniques, as implemented in CARE-RFT, are crucial for creating models that are both highly reasoning-capable and trustworthy.

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [664] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: This paper introduces a constant-cost-per-token method for self-attention in Transformers, significantly reducing memory, compute, and energy demands.


<details>
  <summary>Details</summary>
Motivation: The demand for storage, compute, and energy in self-attention models is exceeding society's ability to meet it, necessitating a more efficient approach.

Method: The authors propose a constant-cost-per-token technique derived from decomposing Taylor expansions into symmetric tensor products, using feed-forward transformations for efficient computation.

Result: The proposed approach achieves substantial reductions in memory and compute costs while enabling unbounded token generation with fixed costs.

Conclusion: The method allows for more efficient and sustainable Transformer models and introduces valuable mathematical techniques.

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [665] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: The paper introduces TAD-LoRA, a framework for decentralized federated learning that controls parameter misalignments and ensures stability in training under dynamic communication topologies.


<details>
  <summary>Details</summary>
Motivation: Decentralized federated learning (DFL) faces challenges due to misaligned parameter updates, especially under dynamic communication graphs, making stable and efficient training difficult.

Method: The paper proposes TAD-LoRA, which coordinates and manages updates of LoRA factors, focusing on mitigating errors caused by topology changes while ensuring a stable training process. The convergence is theoretically proven under non-convex objectives.

Result: Experiments demonstrate TAD-LoRA's robust performance across varying communication scenarios, showing competitive outcomes even in weakly connected topologies, with notable results on the MNLI dataset.

Conclusion: TAD-LoRA enhances decentralized federated learning by addressing topology-induced challenges, remaining competitive and stable under diverse communication conditions.

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [666] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: The study proposes a genetic programming framework for feature construction with enhanced generalization and reduced overfitting through minimizing the vicinal Jensen gap and controlling manifold intrusion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the overfitting challenge in genetic programming-based feature construction to improve generalization and broader applicability in automated machine learning.

Method: The method involves leveraging a vicinal risk decomposition to jointly optimize empirical risk and the vicinal Jensen gap, dynamically adjust regularization strength based on noise levels, and detect manifold intrusion caused by unrealistic sample generation in data augmentation.

Result: The framework showed improved effectiveness in 58 datasets when minimizing the Jensen gap against other complexity measures, and achieved superior performance compared to 15 machine learning algorithms.

Conclusion: Minimizing the vicinal Jensen gap and incorporating strategies to handle noise and manifold intrusion can effectively control overfitting, achieving better generalization and learning performance in genetic programming-based feature construction.

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [667] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: The paper introduces the Symbolic Transition Mechanism (STM), a framework that improves time series prediction using symbolic abstraction and language models, achieving significant accuracy improvements with negligible computational costs.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of deploying large language models for time series prediction, particularly their high computational and memory demands on lightweight platforms.

Method: The proposed STM transforms time series data into symbolic tokens using cognitive-based quantization and captures temporal dynamics through structured symbolic transformations, which are utilized in language models via prompt engineering.

Result: STM delivered error reductions of up to 69% in Mean Absolute Error (MAE) and 90% in Mean Squared Error (MSE) across multiple datasets, with minimal computational resource increases.

Conclusion: STM offers a highly efficient and adaptable layer for integrating symbolic representations into language models for time series prediction, making them more accessible for resource-constrained platforms.

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [668] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA enables more efficient and personalized federated learning by addressing systemic challenges in multi-objective alignment with heterogeneous rewards, showing better performance than traditional approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome memory limitations in on-device federated learning while improving reasoning capabilities in multi-objective reinforcement learning settings.

Method: The FedMOA framework combines critic-free architecture, adaptive weighting using hypergradient descent, and accuracy-aware aggregation to address federated learning challenges.

Result: Empirical results show accuracy improvement of up to 2.2% and enhanced personalization, global performance, and multi-objective balance in benchmarks like mathematical reasoning and code generation.

Conclusion: FedMOA proves to be a robust federated GRPO framework for efficiently aligning multi-objective reasoning tasks on heterogeneous devices.

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [669] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: This paper introduces a framework to understand and control the behavior of AI models using natural language summaries of prompt modifications and their impacts.


<details>
  <summary>Details</summary>
Motivation: The lack of interpretability and controllability in AI models necessitates a framework for understanding how changes to prompts influence model behaviors.

Method: The approach uses atomic concept edits (ACEs) to systematically modify prompts and observe causal effects on model behavior, learning generalizable constitutions.

Result: The framework demonstrated significant improvements in model behavior understanding and control, achieving a 1.86x boost in success rates across tasks like mathematical reasoning and text-to-image alignment.

Conclusion: The proposed interpretability framework proves effective in providing insights and improving model behavior control through learned constitutions across diverse tasks.

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [670] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: This paper explores achieving energy forecasting in Local Energy Communities using Federated Learning (FL) with LSTM networks, balancing privacy and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of achieving energy self-sufficiency in Local Energy Communities by using accurate forecasting models without compromising privacy regulations.

Method: The paper utilizes Federated Learning (FL) combined with long short-term memory (LSTM) networks to forecast energy consumption patterns, ensuring privacy preservation while generating accurate predictions.

Result: The study demonstrates that FL and LSTM networks effectively balance privacy concerns and forecasting accuracy, enabling reliable energy predictions in Local Energy Communities.

Conclusion: The conclusion emphasizes that Federated Learning is a viable solution to develop forecasting models that respect users’ privacy while managing energy demands effectively in sustainable development frameworks.

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [671] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: The paper introduces 'Dyadic Backpropagation', showing backpropagation as the result of a physical dynamical system's finite-time relaxation, with exact gradients in finite time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish backpropagation in neural networks as emerging from a physical process and provide a rigorous foundation for exact gradients in analog or neuromorphic systems.

Method: The authors reformulated feedforward inference using continuous-time processes and derived a saddle-point dynamics framework using Lagrangian theory of non-conservative systems.

Result: The framework recovers standard backpropagation exactly and guarantees exact gradients in finite 2L steps for L-layer networks, without relying on approximations or constraints like symmetric weights.

Conclusion: This work presents backpropagation as the digital optimization of a continuous physical system, enabling a deeper understanding and practical gradient computation in analog systems.

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [672] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: The paper surveys methods integrating Group Fairness and Individual Fairness into unified frameworks focusing on trade-offs and hybrid fairness approaches.


<details>
  <summary>Details</summary>
Motivation: Ethical and legal concerns drive the need for equitable outcomes in computational decision-making systems, prompting exploration of fairness across multiple dimensions.

Method: The paper systematically reviews hybrid fairness approaches, categorizing methods by mechanisms used, algorithmic strategies, and reconciliation of fairness principles.

Result: Insights from the literature are synthesized to structure existing methods, their foundations, optimization, empirical evaluations, and limitations.

Conclusion: The study identifies challenges and future research directions, serving as a guide for designing hybrid algorithms ensuring fairness for individuals and groups.

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [673] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: The paper demonstrates the advantages of using the Gauss-Newton method for optimization in shape learning applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges in shape learning such as ill-conditioning in constraints and optimization mismatches, improving speed and accuracy.

Method: Employing the Gauss-Newton method for optimization to enhance convergence and stability in shape learning processes.

Result: The method achieves faster, more stable convergence and better training outcomes compared to standard first-order methods.

Conclusion: The Gauss-Newton method enhances both training speed and accuracy, proving effective for benchmark shape optimization tasks.

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [674] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: The paper introduces THDC, improving HDC by reducing dimensionality and enhancing learning capacity through trainable embeddings and binary neural networks.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency in memory usage and limited learning capacity caused by the ultra-high dimensionality and static hypervectors in traditional HDC.

Method: Replacing random hypervector initialization with trainable embeddings and incorporating a one-layer binary neural network for class representation optimization.

Result: THDC achieves comparable or better accuracy than HDC across datasets like MNIST, Fashion-MNIST, and CIFAR-10, reducing dimensionality significantly (from 10,000 to 64).

Conclusion: THDC enhances HDC by significantly reducing dimensionality and improving learning performance through end-to-end optimization.

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [675] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: Machine learning models are evaluated for mortgage default prediction, addressing issues like labeling ambiguity, class imbalance, and information leakage. AutoML shows promising results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the evaluation and reliability of mortgage default predictions by tackling real-world challenges such as data labeling ambiguity, class imbalance, and temporal information leakage.

Method: The paper uses leakage-aware feature selection, a strict temporal dataset split, and controlled downsampling to validate machine learning approaches. It also employs AutoML (AutoGluon) for model performance evaluation.

Result: Performance across various positive-to-negative ratios is stable, with AutoML showing superior AUROC metrics among compared models.

Conclusion: Careful handling of leakage and imbalance enhances model reliability for mortgage default prediction. AutoML techniques are effective solutions and promise interpretability for risk management tasks.

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [676] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor is an open-source tensor library with a small install footprint, offering PyTorch-like APIs and Rust-based execution.


<details>
  <summary>Details</summary>
Motivation: To provide a lightweight, minimalistic tensor operations library suitable for research and development.

Method: MiniTensor integrates PyTorch-like Python API with efficient Rust backend, including features like broadcasting, differentiation, and memory management.

Result: Achieves a small package size (several MB), significantly smaller than PyTorch and TensorFlow, while maintaining essential functionalities for CPU-based R&D.

Conclusion: MiniTensor successfully delivers a high-performance, minimalistic tensor library suitable for neural network and computational research, with streamlined usability.

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [677] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: Federated Unlearning (FU) targets efficient data removal from models. FUPareto is proposed to address challenges in utility preservation, multi-client support, and Membership Inference Attack (MIA) vulnerability using Pareto optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenges of balancing unlearning efficacy with model utility, addressing MIA risks, and supporting concurrent unlearning for multiple clients.

Method: FUPareto employs a Pareto-augmented optimization framework with the Minimum Boundary Shift (MBS) Loss, Pareto improvement and expansion techniques, and Null-Space Projected MGDA for handling gradient conflicts.

Result: Experiments show FUPareto surpasses existing FU approaches in effective unlearning and retaining model utility across various scenarios.

Conclusion: FUPareto is a robust and efficient federated unlearning framework that improves security, retains model utility, and handles multi-client unlearning effectively.

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [678] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: The paper introduces ALIGN, a method improving complex reasoning in LLMs by inducing structured multi-agent interaction and guarantees superior performance over single-agent or ensemble methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning tasks due to inefficiencies in single-generation pipelines and limitations in current ensemble models.

Method: ALIGN formulates reasoning as a delegation game with multiple agents collaborating under aligned incentives, enhancing interaction and performance.

Result: ALIGN demonstrates better reasoning capabilities over benchmarks compared to single-agent and ensemble baselines, with theoretical guarantees.

Conclusion: ALIGN improves LLM reasoning quality and reliability, supported by theoretical assurances and empirical success across varied benchmarks.

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [679] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: The paper introduces a Quantum-Based Parallel Model (QBPM) for classifying Alzheimer's Disease (AD) stages using MRI data, showing higher accuracy and robustness compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: With rising life expectancy, Alzheimer's Disease represents a global health challenge. Current AI methods are limited by computational demands, creating the need for faster and more efficient diagnostic tools.

Method: The study presents QBPM, which uses two parallel quantum circuits on a quantum simulator. These circuits employ rotational and entanglement blocks to process MRI datasets for AD stage classification.

Result: The proposed QBPM model achieved high-accuracy classification across two datasets, showed robustness under noisy conditions, and outperformed classical transfer learning methods in accuracy and efficiency.

Conclusion: QBPM is shown to be a promising alternative to classical methods for AD diagnostics, demonstrating innovation, precision, and practical applicability in classifying complex disease stages.

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [680] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2 is a distributed reinforcement learning (RL) framework for post-training large language models (LLMs) that optimizes cost efficiency and rollout throughput, despite latency challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in efficient RL for LLMs, particularly in distributed setups with high latency, by enabling better coordination, dissemination, and resource utilization.

Method: ECHO-2 combines centralized learning with distributed rollouts and introduces a bounded policy staleness parameter. It incorporates an overlap-based capacity model, peer-assisted broadcast, and cost-aware worker activation to improve efficiency.

Result: Experiments with GRPO post-training of large LLMs under real-world bandwidth constraints demonstrated that ECHO-2 significantly enhances cost efficiency without compromising RL rewards when compared to robust baselines.

Conclusion: ECHO-2 proves effective in balancing cost, latency, and learning in distributed RL for LLMs, showcasing significant improvements in resource utilization while maintaining performance standards.

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [681] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot combines Monte Carlo Tree Search (MCTS) and large language models to address program repair challenges, achieving notable success on GitHub issues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in repository-level automated program repair caused by long-horizon reasoning and autoregressive decoding limitations.

Method: It introduces a hybrid framework, CodePilot, that integrates MCTS for hierarchical fault localization, explores patch trajectory diversity, and uses execution feedback for selective refinement.

Result: CodePilot achieves a 24.67% resolution rate on SWE-bench Lite, surpassing comparable baselines.

Conclusion: Combining symbolic search methods like MCTS with neural models improves execution-aware program repair and scalability in software engineering automation.

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [682] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: This paper presents that effective dimension, a geometric metric, strongly predicts neural network performance across various architectures and domains without using labeled data.


<details>
  <summary>Details</summary>
Motivation: To uncover a domain-agnostic method for predicting neural network performance using unsupervised geometric metrics without relying on labels.

Method: The study analyzes 52 pretrained ImageNet models, 13 architecture families, and extends to NLP tasks. It uses effective dimension, noise-induced geometry alterations, and PCA variations to examine causal and predictive relationships.

Result: Effective dimension showed strong correlation with accuracy (r=0.75) even after controlling for model capacity and across multiple domains. Noise degrading geometry led to performance loss, while PCA-maintained geometry preserved accuracy.

Conclusion: The paper concludes that effective dimension is a robust, unsupervised indicator for predicting and assessing neural network performance across domains.

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [683] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: The paper introduces RAPTOR, a logistic probe method to enhance concept vector estimation in LLMs with efficiency, accuracy, and stability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the need for accurate, directionally stable, and cost-effective methods for estimating concept vectors in LLM's layer representations.

Method: RAPTOR employs an L2-regularized logistic probe with validation-tuned ridge strength to extract concept vectors from normalized weights.

Result: RAPTOR demonstrates strong results in accuracy, competitive directional stability, and significant cost efficiency across tested LLMs and concept datasets.

Conclusion: RAPTOR is effective for probing and steering tasks, providing theoretical insights into ridge logistic regression and aligning trends with real-world LLM embeddings.

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [684] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: This paper introduces a method to compress large language models by removing transformer blocks through a binary optimization problem mapped to the Ising model, allowing efficient selection of block-removal configurations and improving performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of compressing large language models which involves identifying which transformer blocks to remove, as it presents a challenging combinatorial optimization problem.

Method: The paper formulates block removal as a constrained binary optimization problem, mapping it to the Ising model to efficiently rank and select block-removal configurations, requiring forward/backward passes for a small number of parameters and a solver for the Ising model.

Result: The proposed method surpasses existing block-removal techniques, with performance improvements of up to 6 points on the MMLU benchmark and demonstrating its efficiency even after short retraining. It is shown as generalizable to complex model architectures.

Conclusion: The proposed approach enables effective and efficient transformer block removal for large language models, achieving significant performance improvements and demonstrating its applicability to diverse architectures.

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [685] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: The paper introduces Benford-Quant, a non-uniform quantization method inspired by Benford's Law, claiming improved performance in weight quantization for language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of standard uniform quantizers, which assume evenly distributed parameters, despite real-world highly skewed distributions in LLM weights.

Method: Benford-Quant employs a logarithmically spaced codebook inspired by Benford's Law, aligning resolutions with the observed weight distributions in transformer layers.

Result: Benford-Quant improved perplexity metrics in small models (e.g., a 10% reduction in 4-bit perplexity for Gemma-270M) and displayed competitive performance in large models, with scope for further improvement through hybridization.

Conclusion: Incorporating Benford-inspired priors into quantization grids is a cost-effective way to enhance quantization accuracy in low-bit regimes and complements existing state-of-the-art methods.

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [686] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: This paper proposes DA-GRPO, a novel method for Small Language Models (SLMs) to efficiently collaborate with cloud-based Large Language Models (LLMs) under constraints, enhancing task performance and reducing forgetting issues.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of balancing local task handling by SLMs with cloud LLM assistance, particularly under strict computational and memory limitations, and issues like unstable offloading and catastrophic forgetting.

Method: It introduces DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization, which integrates cloud-usage constraints into advantage computation and naturally learns collaboration behavior within an assistance budget.

Result: DA-GRPO improves task accuracy after switches, reduces forgetting, and ensures stable cloud reliance through evaluations on tasks like mathematical reasoning and code generation benchmarks.

Conclusion: The model strikes an effective balance between local SLM performance and cloud LLM assistance, showing promise in enhancing learning stability and collaboration under resource constraints.

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [687] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: This paper demonstrates that weight-perturbation evolution strategies (ES) can fine-tune billion-parameter models with small populations due to the low-dimensionality in the training landscape's curvature, which drives optimization efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand why evolution strategies (ES) work efficiently in fine-tuning large language models despite the high-dimensional parameter space, challenging existing paradigms.

Method: The paper analyzes fine-tuning landscapes for billion-parameter models, combining theoretical geometric insights with experiments using ES optimization on tasks and models like GSM8K, ARC-C, and WinoGrande on Qwen2.5-Instruct models.

Result: The study uncovers that fine-tuning landscapes exhibit low-dimensional curvature, allowing ES to perform effectively with small populations and demonstrating consistent efficiency across model scales (0.5B to 7B parameters).

Conclusion: The findings suggest that due to the low-dimensionality of fine-tuning landscapes, scalable fine-tuning is achievable using a broader range of optimization strategies, debunking classical limitations posed by the curse of dimensionality.

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [688] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2, a novel feature selection method, is introduced to identify predictive covariates under challenging conditions like high correlation and low signal-to-noise ratio using efficient statistics and strict false discovery rate control.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying predictive covariates in nonlinear, highly correlated, and low-signal-to-noise regimes where current deep learning feature selection methods may fail.

Method: The authors propose GRIP2 which integrates first-layer feature activity over a 2D regularization surface and introduces block-stochastic sampling to estimate regularization statistics with finite-sample FDR control.

Result: GRIP2 displays improved robustness under high-feature correlation and low signal-to-noise conditions, showing high power and stability in synthetic, semi-real, and real-world datasets.

Conclusion: GRIP2 is a robust and effective method for feature selection, outperforming standard approaches, especially in challenging data environments with correlation and noise issues, as shown with real-world applications.

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [689] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: The paper introduces GASP, a robustness method for reinforcement learning that trains models to diagnose and repair errors caused by conditioning context failures, improving their recovery and robustness.


<details>
  <summary>Details</summary>
Motivation: RLVR models struggle when the conditioning context is fallible (e.g., corrupted reasoning paths or perturbed inputs). There is a need for robust methods to address such failures while maintaining overall performance and accuracy.

Method: The proposed GASP method trains robustness via adversarial self-play: a polluter generates corrupted contexts, while an agent learns to repair under adversarial conditions. An in-distribution repair guidance term helps the agent recover accurately despite failure scarcity during early training.

Result: GASP significantly enhances model robustness against misleading and perturbed contexts while often improving clean accuracy. The paper demonstrates its application across four model sizes (1.5B-8B).

Conclusion: This method successfully trains strong-but-brittle models to become consistently robust reasoners, even under challenging adversarial conditions, without external supervision or reliance on human-labeled data.

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [690] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: LatentTrack is a neural architecture designed for online probabilistic prediction in nonstationary environments, enabling fast adaptation and robust predictions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in online probabilistic prediction under nonstationary dynamics, where traditional latent-state models struggle with distribution shifts.

Method: LatentTrack uses causal Bayesian filtering in latent space, combined with a hypernetwork for constant-time online adaptation without gradient updates. Predictive frameworks consist of predict-->generate-->update cycles.

Result: The technique demonstrates superior performance in long-horizon regression tasks like Jena Climate benchmark, outperforming baseline methods in negative log-likelihood, mean squared error, and calibration.

Conclusion: LatentTrack provides an efficient and accurate alternative to traditional latent-state modeling under distribution shifts, supporting both structured and unstructured dynamics.

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [691] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: The paper reveals that unlearning-based defenses in diffusion models fail to effectively remove NSFW concepts as they leave 'dormant memories' intact. It proposes an attack framework, IVO, to reactivate these memories, exposing flaws in existing unlearning techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate and expose the shortcomings of unlearning-based defenses in diffusion models by highlighting their inability to fully remove undesirable NSFW concepts and exploring the remaining dormant knowledge.

Method: The proposed method, IVO, uses Initial Latent Variable Optimization to reconstruct disrupted symbol-knowledge mappings via techniques like Image Inversion, Adversarial Optimization, and Reused Attack, realigning the noise distribution to its unsafe state.

Result: IVO achieves higher attack success rates and better semantic consistency compared to existing unlearning methods, as demonstrated across 8 widely used techniques.

Conclusion: The findings reveal fundamental limitations in current unlearning defenses, demonstrating that NSFW concepts can still be reactivated by advanced attack methods like IVO, underlining the need for more robust defenses.

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [692] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: The paper proposes an efficient method to improve online reinforcement learning in finite-horizon Markov Decision Processes by introducing a K-step lookahead Q-function and a thresholding mechanism.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in online reinforcement learning for finite-horizon MDPs, where estimating returns to a fixed terminal time is difficult, and infinite-horizon methods aren't naturally suited for this structure.

Method: The method involves introducing a K-step lookahead Q-function for truncated planning and a thresholding mechanism to improve sample efficiency. An adaptive tabular learning algorithm is developed for efficient learning and minimax optimal regret performance.

Result: The developed algorithm achieves fast finite-sample convergence and superior cumulative rewards over existing state-of-the-art methods in synthetic and established RL environments.

Conclusion: The paper concludes that the proposed algorithm offers a balance between lookahead depth and estimation variance, performing well in finite-horizon settings compared to alternative state-of-the-art approaches.

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [693] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: The paper reassesses the critique of instability in local linear explanation methods for machine learning near decision boundaries, attributing such instability to forecast uncertainty instead.


<details>
  <summary>Details</summary>
Motivation: With machine learning increasingly applied to critical decisions, explainability has become essential. Existing methods like LIME and SHAP face criticism for instability near decision boundaries, which the paper argues stems from a misunderstanding.

Method: The paper investigates the relationship between forecast uncertainty and instability in explanations within local linear methods. It proposes changing the analytic sequence by first determining whether a usable forecast exists before seeking explanations.

Result: High explanatory instability is linked to high forecast uncertainty, particularly near decision boundaries. The paper identifies when local linear approximations are valid and critiques methods claiming universal explainability, such as ReLU networks.

Conclusion: Local linear explanations are suitable only where forecast uncertainty is low. Explaining predictions with high uncertainty is unproductive, requiring simpler models for decisions in such regions.

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [694] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: This paper investigates how Feature Learning Strength (FLS) affects generalization in neural networks and finds that an optimal FLS yields better results, contrary to prior intuition.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of Feature Learning Strength (FLS) in optimizing generalization performance under practical conditions, such as reaching a specific training target.

Method: The study involves empirical analysis to observe generalization performance trends and theoretical analysis using gradient flow dynamics on two-layer ReLU networks controlled by initialization scale.

Result: The findings reveal the existence of an optimal FLS that achieves better generalization by balancing the trade-off between over-alignment with data (when FLS is too large) and overfitting (when FLS is too small).

Conclusion: Feature Learning Strength does not universally improve generalization, and targeting an optimal range for FLS can significantly enhance model performance by avoiding over-alignment or overfitting issues.

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [695] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: The paper introduces GEPC, a method to measure consistency in learned score field transformations to detect out-of-distribution (OOD) data, leveraging group-equivariance breaking.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models overlook equivariances, focusing more on score magnitudes and local geometry for OOD detection. The authors aim to address this gap by utilizing group-equivariances to enhance OOD detection capabilities.

Method: The proposed Group-Equivariant Posterior Consistency (GEPC) quantifies how consistently a score transforms under a group $(\mathcal{G})$. A theoretical framework establishes ID upper bounds and OOD lower bounds using equivariance-residual functionals.

Result: GEPC achieves competitive or superior AUROC performance on OOD image benchmarks compared to recent diffusion-based baselines. It is also computationally efficient, offering visually interpretable maps for equivariance-breaking scenarios.

Conclusion: GEPC effectively combines theoretical and practical strategies to detect OOD scenarios by identifying equivariance-breaking, thereby improving on existing diffusion-based methods in both efficiency and interpretation.

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [696] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: This paper introduces the MinPV Principle to address performance issues in density ratio estimation methods caused by path variance, proposing a closed-form solution for optimization.


<details>
  <summary>Details</summary>
Motivation: Score-based density ratio estimation methods face a paradox: while theoretically path-independent, their performance heavily depends on the path schedule, resulting in inconsistent practical outcomes.

Method: The authors propose the MinPV Principle, which minimizes the path variance of the time score using a closed-form variance expression and a flexible Kumaraswamy Mixture Model to adaptively optimize this issue.

Result: Their approach establishes new state-of-the-art performance on challenging benchmarks by introducing a more accurate, stable path selection method for density ratio estimators.

Conclusion: By addressing the path variance issue with a principled method, the proposed technique improves the practical reliability and accuracy of density ratio estimation frameworks.

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [697] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: This paper develops a Bayesian approach to improve the balance between memorisation and generalisation in generative models, leveraging loss geometry and Riemannian metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the ongoing challenge of balancing memorisation and generalisation in modern generative models, which can produce realistic samples but struggle with this trade-off.

Method: The paper uses a Bayesian framework for flow matching and diffusion models, introduces a predictive posterior based on Riemannian geometry, and employs a flexible approximate posterior to adapt to the loss landscape.

Result: The approach successfully reduces memorisation while maintaining generalisation, and theoretical analysis supports these empirical findings.

Conclusion: Considering the geometry of the loss landscape enables more effective use of the parameter space in complex high-dimensional generative models.

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [698] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: The paper advocates for using Mixture Density Networks (MDNs) to address multimodal uncertainty in scientific machine learning (SciML). MDNs are shown to be data-efficient, interpretable, and generalize effectively in handling challenging physical phenomena.


<details>
  <summary>Details</summary>
Motivation: The authors want to improve multimodal uncertainty quantification in SciML, addressing limitations of current approaches like diffusion models and flow-based methods, which are resource-intensive and may not align with scientific problem structures.

Method: The paper introduces Mixture Density Networks (MDNs) as explicit parametric density estimators with tailored inductive biases for multimodal and low-dimensional physics. They establish a probabilistic framework comparing MDNs with implicit generative models.

Result: MDNs exhibit superior generalization, interpretability, and sample efficiency in tasks involving inverse problems, multistability, and chaotic dynamics compared to other methods.

Conclusion: MDNs are effective alternatives to implicit generative models for multimodal uncertainty in SciML, offering better alignment with structured scientific solution spaces and efficiency in data-scarce situations.

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [699] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: This paper introduces MR$^2$, a method for reducing classification disparities across classes by dynamically adjusting margins in logit and feature representation spaces.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often show significant class-wise accuracy disparities, even on class-balanced data. This compromise in reliability during deployment motivates the design of methods to address such imbalance, requiring theoretical principles.

Method: The authors propose Margin Regularization for Performance Disparity Reduction (MR$^2$), which adjusts margins dynamically based on per-class feature variability and generalization bounds. It optimizes logit margins proportional to feature spread and penalizes excessive intra-class representation margins to improve compactness.

Result: MR$^2$ was validated on seven datasets, including ImageNet, with diverse pre-trained models (e.g., MAE, MoCov2, CLIP). It improves overall accuracy while significantly boosting hard class performance without harming easy classes, thereby reducing disparities.

Conclusion: MR$^2$ effectively addresses class-wise disparities by leveraging theoretical insights, achieving better-balanced performance in neural networks and enhancing deployment reliability.

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [700] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: This paper explores how feature superposition impacts power-law training dynamics, finding it accelerates training speed and creates a universal power-law exponent.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the impact of feature superposition in training dynamics and its implications for neural networks like large language models.

Method: A teacher-student framework is used, developing an analytic theory to compare training with and without feature superposition.

Result: Superposition accelerates training by a factor of up to ten and induces a universal power-law exponent of approximately 1, independent of data and channel statistics.

Conclusion: Feature superposition enhances training speed while maintaining universal characteristics, potentially benefiting neural networks with superposition in real-world applications like large language models.

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [701] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: The paper proposes an explanation-driven method using SHapley Additive exPlanations to improve unsupervised anomaly detection by constructing more complementary and effective ensembles.


<details>
  <summary>Details</summary>
Motivation: Unsupervised anomaly detection is difficult due to the lack of labels and diverse data distributions. Ensemble methods address this partly, but often suffer from redundant detections due to similar decision-making cues across models.

Method: The paper uses SHapley Additive exPlanations to characterize the decision mechanisms of anomaly detectors, quantifying feature importance to evaluate detector similarity and improve ensemble selection.

Result: The study shows that detecting explanation divergence correlates with complementary detection behavior, leading to more effective anomaly detection ensembles when combined with good individual performance.

Conclusion: By combining explanation diversity and model quality, it is possible to construct ensembles that are both diverse and more effective for unsupervised anomaly detection.

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [702] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: This paper introduces MACI, a method reformulating conformal inference for better factuality scores in Large Language Models, ensuring safety in domains like medicine and law.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve factuality assurance in LLMs for high-risk fields by addressing limitations of existing conformal inference methods.

Method: The authors propose MACI, a multiplicative filtering mechanism leveraging ensemble methods and group-conditional calibration.

Result: MACI exhibits higher factuality retention, meets user-specified coverage, and operates with reduced computational costs compared to baseline methods.

Conclusion: MACI is a promising approach for ensuring LLM factuality while maintaining efficiency and validity, with open-source implementation provided to support future work.

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [703] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: The study observes a geometric phenomenon termed embedding condensation in token embeddings of small LLMs and proposes a dispersion loss to address the issue and improve model performance.


<details>
  <summary>Details</summary>
Motivation: Explore scalable methods to replicate representational qualities of larger LLMs in smaller models to avoid their computational cost.

Method: Analyzed embedding condensation in various Transformer models and developed a dispersion loss method to counteract the phenomenon.

Result: The dispersion loss mitigated embedding condensation and provided performance gains across 10 benchmarks while restoring properties seen in larger models.

Conclusion: Embedding condensation is a significant issue in small LLMs, but dispersion loss may improve their quality with reduced parameters efficiently.

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [704] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: This paper proposes efficient algorithms for diffusion model sampling and general log-concave distributions, significantly reducing sampling complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency of algorithms for diffusion model sampling and log-concave distributions by reducing their step complexity.

Method: The authors introduce algorithms that achieve δ-error in polylog(1/δ) steps based on accurate score estimates in L^2 norm.

Result: The algorithms demonstrate exponential improvement over previous methods, with complexities dependent on data dimension and other properties like Lipschitz conditions or intrinsic dimension.

Conclusion: These algorithms set a new benchmark in sampling efficiency for diffusion models and log-concave distributions, achieving polylog(1/δ) complexity with minimal assumptions.

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [705] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: The paper introduces AICD Bench, a comprehensive benchmark to detect AI-generated code, addressing limitations in current datasets.


<details>
  <summary>Details</summary>
Motivation: The rise of large language models generating functional source code raises challenges in authorship, accountability, and security, necessitating improved detection methods.

Method: AICD Bench includes 2M examples, 77 models across 11 families, 9 programming languages, and three realistic detection tasks: robust binary classification, model family attribution, and fine-grained human-machine classification.

Result: Tests with neural and classical detectors show significant performance gaps, especially in scenarios with distribution shifts and hybrid/adversarial code.

Conclusion: AICD Bench provides a challenging unified evaluation suite to encourage advancements in detecting AI-generated code.

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [706] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS is a neural architecture search framework optimized for accuracy and efficiency in weather forecasting, aiming to reduce computational energy costs and carbon footprints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop sustainable AI models, adhering to 'Green AI' principles, to minimize environmental impact and resource usage while maintaining high forecasting accuracy.

Method: The study applies multi-objective optimization to design lightweight models with efficient accuracies and utilizes transfer learning to improve results in scenarios with limited historical weather data.

Result: The Green-NAS-A model achieves strong weather forecasting performance (RMSE of 0.0988) with only 153k model parameters, significantly reducing resource usage compared to conventional models.

Conclusion: Green-NAS successfully demonstrates that lightweight, environmentally-friendly AI models can achieve high accuracy, highlighting its potential for sustainable and scalable deployment in weather forecasting.

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [707] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: The paper introduces Backward-on-Entropy (BoE) Steering for Masked Diffusion Models (MDMs), addressing trajectory lock-in in non-autoregressive generation through gradient-guided inference to improve generation quality.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current sampling methods in MDMs which rely on confidence-based heuristics and lead to issues like trajectory lock-in, while also addressing the computational inefficiencies of search-based methods.

Method: Developed BoE Steering, a framework that uses a Token Influence Score (TIS) and backward passes to guide inference based on entropy gradients, and introduced ActiveQueryAttention to handle scalability.

Result: BoE Steering achieved better scalability and quality for non-autoregressive generation, with a more efficient balance between computational cost and performance compared to existing methods.

Conclusion: Gradient-guided inference, as shown by BoE Steering, provides an efficient and mathematically principled solution for non-autoregressive generation. The framework significantly advances the state-of-the-art in MDM-based generative tasks.

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [708] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: The paper introduces OddSHAP, a novel estimator for Shapley value computation in machine learning by leveraging polynomial regression on the odd subspace, achieving state-of-the-art estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the intractable exact computation of the Shapley value and improve the theoretical understanding of paired sampling used in its approximation.

Method: Introduced OddSHAP, which isolates the odd subspace using the Fourier basis and incorporates polynomial regression and a proxy model to overcome high-order approximation challenges.

Result: OddSHAP outperforms existing methods in estimation accuracy, as demonstrated through extensive benchmarking.

Conclusion: OddSHAP provides a theoretically grounded, efficient, and accurate solution to compute Shapley values using paired sampling, with significant improvements over prior methods.

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [709] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: The paper introduces a decomposition-based framework for causal discovery in multivariate time series, improving performance under non-stationarity and autocorrelation.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods struggle with non-stationarity, seasonal patterns, and temporal autocorrelation in multivariate time series, leading to spurious associations and incorrect inferences.

Method: The framework decomposes time series into trend, seasonal, and residual components, performs component-specific causal analysis, and integrates results into a unified causal structure.

Result: The proposed method outperforms state-of-the-art causal discovery techniques, particularly with non-stationary and autocorrelated data, demonstrated through benchmarks and real-world applications.

Conclusion: Decomposition allows for isolating different causal effects, enhancing accuracy and interpretability in causal discovery for time series data.

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [710] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: This paper focuses on the theoretical analysis of bilevel reinforcement learning (RL), proposing a constrained bilevel RL framework. It presents new sample complexity results for the algorithm CBSO, which addresses non-smooth optimization challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap in theoretical studies of bilevel RL algorithms, which are crucial for settings such as hierarchical learning, meta-learning, and RL from human feedback.

Method: The proposed method, Constrained Bilevel Subgradient Optimization (CBSO), employs a penalty-based objective function and uses the Moreau envelope for handling non-smooth optimization challenges.

Result: The analysis yielded an iteration complexity of $O(ε^{-2})$ and sample complexity of $	ilde{O}(ε^{-4})$ for CBSO, advancing sample complexity bounds in bilevel RL.

Conclusion: This work provides foundational theoretical insights into constrained bilevel RL problems, leveraging penalty-based methods and non-smooth optimization techniques for improved algorithmic efficiency.

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [711] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: This paper explores Measure Consistency Regularization (MCR) as a method to address missing features and data in machine learning and offers theoretical insights, along with a novel early stopping protocol to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of corrupted data, missing features, or modalities in machine learning, improving model generalization through MCR.

Method: Analyzes MCR theoretically, identifies benefits under partial observability, and proposes a training protocol with an early stopping mechanism based on duality gap.

Result: Theoretical and empirical evidence highlights MCR’s effectiveness and its enhanced versatility through the proposed stopping condition across various data sources and architectures.

Conclusion: MCR improves imputation quality and generalization under specific conditions, emphasizing the importance of a well-defined stopping mechanism for optimal performance.

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [712] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: The paper explores the limitations and risks in Masked Diffusion Models (MDMs), particularly focusing on generation order and parallel processing biases.


<details>
  <summary>Details</summary>
Motivation: To systematically understand the errors and risks associated with order sensitivity and parallelization biases in MDMs, aiming to address incoherence and inefficiencies during generation.

Method: An information-theoretic framework is proposed to analyze failure sources like order sensitivity and parallelization bias in MDMs. Experiments are conducted on a controlled Block-HMM and large-scale MDMs (LLaDA).

Result: Three insights emerged: Easy-First decoding performs better with model errors, factorized decoding introduces significant incoherence risks, and verification can remove errors but is computationally expensive.

Conclusion: While certain heuristics like remasking can mitigate minor decoding issues, they are inefficient for ensuring correctness, especially under complex block correlations in MDMs.

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [713] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: The paper analyzes gated attention in Transformer models, explaining its benefits and theoretical underpinnings, particularly its efficiency and placement in architecture.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of the gated attention mechanism in Transformer architectures, despite its empirically observed benefits.

Method: The authors rigorously analyze gated attention theoretically and frame it as a hierarchical mixture of experts, comparing its sample efficiency to standard multi-head self-attention.

Result: Gated attention is theoretically proven to be more sample-efficient than standard multi-head self-attention and its optimal gating positions are identified.

Conclusion: Gated attention improves the efficiency and performance of attention mechanisms in Transformers, and this analysis provides theoretical insights into its advantages and placement for optimal performance.

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [714] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper introduces a novel paradigm, LatentTSF, to address the issue of temporally disordered latent representations, called Latent Chaos, in time series forecasting models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the issue of Latent Chaos in time series forecasting, where models generate temporally disordered latent representations due to point-wise error minimization in noisy and partial observations.

Method: The authors propose LatentTSF, which uses an AutoEncoder to encode observations into a high-dimensional latent state space and performs forecasting in the latent space to learn structured temporal dynamics. It implicitly optimizes mutual information between predicted latent states and the ground-truth states.

Result: LatentTSF effectively reduces latent chaos and achieves superior performance in experiments conducted on commonly-used benchmarks.

Conclusion: The research concludes that shifting the forecasting process to latent space can mitigate latent chaos, improving temporal structure and overall prediction performance. The code is made publicly available for further exploration.

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [715] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: The paper introduces "Rod Flow," an ODE approximation to better understand gradient descent dynamics, particularly in large step-size regimes. It offers improvements in accuracy and computational efficiency compared to the Central Flow method.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand gradient-based training in non-convex landscapes, especially how gradient descent diverges in scenarios with large step sizes and how these dynamics can be accurately approximated.

Method: The authors propose Rod Flow, an ODE derived from viewing GD iterates as a rod-like physical object, providing explicit and computationally efficient predictions for GD behavior.

Result: Rod Flow accurately predicts critical thresholds and explains stabilization phenomena in certain potential landscapes. It performs as well as or better than the Central Flow approach in experiments.

Conclusion: Rod Flow offers a theoretically sound and computationally accessible tool for analyzing gradient descent dynamics, improving accuracy and efficiency across examples and architectures.

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [716] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT is an automated framework to develop, refine, and verify epidemiological models for effective disease progression analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional epidemic modeling requires human redesign as situations change, making the process inefficient and prone to errors.

Method: Proposes EPIAGENT, which uses program synthesis to model disease progression via an intermediate epidemiological flow graph for initial verification, then generates mechanistic models for parameter learning.

Result: EPIAGENT effectively captures complex dynamics and consistent counterfactual projections in various case studies, accelerating model accuracy and convergence.

Conclusion: The framework mimics expert workflows, ensuring robust and interpretable epidemiological models, making it an impactful tool for public health planning.

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [717] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: This paper introduces a new, efficient test-time algorithm for improving the scaling behavior of Large Language Models (LLMs) by dynamically optimizing compute allocation through tail-guided search, leading to higher performance and lower compute requirements than existing methods like Best-of-$N$.


<details>
  <summary>Details</summary>
Motivation: To optimize reasoning capabilities and achieve better performance of LLMs during test time, and to address the inefficiencies and lack of guidance in current methods like Best-of-$N$.

Method: The paper proposes a tail-guided search method that estimates the tail distribution of rewards to predict LLM scaling laws. It develops a Scaling-Law Guided (SLG) algorithm that dynamically allocates compute to maximize intermediate state performance, reducing computational inefficiencies.

Result: The SLG algorithm achieves provable vanishing regret when compared to perfect-information oracles. It yields higher empirical rewards than Best-of-$N$ within the same compute budget across various LLMs and reward models.

Conclusion: The framework presented is both theoretically robust and empirically validated, showing that dynamic, tail-guided optimization can significantly outperform traditional approaches in efficiently boosting LLM reasoning capabilities.

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [718] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: A novel neural network-based heuristic is introduced for solving NP-hard Ising and Max-Cut problems via learned update rules, achieving competitive results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently solving NP-hard Ising and Max-Cut optimization problems by leveraging data-driven heuristics that can learn effective strategies for exploring non-convex energy landscapes.

Method: The method introduces a neural network parameterized Ising machine (NPIM), using a compact multilayer perceptron to learn node-wise update rules for spin updates. A zeroth-order optimizer is employed to stabilize training without backpropagation through extended iterations.

Result: NPIM demonstrates the capability to learn effective algorithmic structures such as momentum-based behavior and schedules, providing competitive performance in terms of solution quality and computational efficiency compared to recent learning-based models and classical heuristics.

Conclusion: NPIM provides an efficient, low-parameter approach for solving complex optimization tasks and highlights the potential of learned dynamics in exploring challenging optimization landscapes.

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [719] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: The paper improves the sample complexity to $O(ε^{-2})$ for actor-critic algorithms in discounted MDPs using variance-reduction techniques.


<details>
  <summary>Details</summary>
Motivation: Improve sample complexity for actor-critic algorithms in infinite-horizon discounted MDPs.

Method: Incorporates stochastic recursive momentum (STORM) and a sample buffer for variance reduction in critic updates.

Result: Achieved $O(ε^{-2})$ sample complexity for obtaining $ε$-optimal policies.

Conclusion: The method is efficient and maintains compatibility with practical deep learning architectures.

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [720] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: This paper proposes using class-conditional normalizing flows to serve as oracles for calculating exact posteriors in image benchmarks. Five key investigations are conducted to assess model limits, learning scalability, calibration, distribution shifts, and active learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks are inadequate for evaluating how close neural networks are to optimal performance as they lack access to the true posterior probability distributions.

Method: The authors utilize class-conditional normalizing flows as oracle models capable of providing exact posterior probabilities on image datasets, supporting detailed investigations across scaling laws, learning limits, and shifts.

Result: Using exact posteriors enables accurate decomposition of prediction error, reveals variations in architectural scaling, identifies learnable soft-label structures, assesses distribution shift sensitivity, and enhances active learning processes.

Conclusion: The framework demonstrates that standard metrics are insufficient. Leveraging posteriors from oracles offers profound insights into model scalability, calibration, distribution changes, and sample efficiency.

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [721] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: The paper addresses the limitations of current Graph Neural Network (GNN)-based social bot detection in real-world settings and introduces a method called BOCLOAK for robust testing under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the increasing risks posed by social media bots on public discourse, which current GNN-based detectors struggle to address effectively in dynamic and constrained real-world environments.

Method: BOCLOAK systematically evaluates GNN-based bot detection by implementing edge editing and node injection adversarial attacks using optimal transport to simulate realistic social bot behaviors within constraints.

Result: BOCLOAK demonstrated up to an 80.13% higher success rate in adversarial attacks and drastically reduced GPU memory usage by 99.80% under real-world constraints, outperforming existing attack baselines.

Conclusion: The study concludes that optimal transport provides a lightweight and principled approach for advancing adversarial attack frameworks and improving understanding of real-world social bot detection challenges.

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [722] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: This paper investigates Time Series Foundation Models (TSFMs), focusing on their redundancy and mechanisms using ablations, logit attribution, and theoretical approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze the efficiency and interpretability of transformer-based TSFMs, which show redundant components.

Method: The authors use mechanistic interpretability tools, layer and head ablations, and theoretical modeling to understand TSFM behaviors.

Result: They demonstrate robustness against layer ablations, propose strategies for ablation based on stable rank, and identify specific heads causing issues like parroting and seasonal bias.

Conclusion: This work provides insights into TSFM architectures' robustness and interpretability, advancing understanding in continuous-time sequence modeling.

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [723] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest optimizes GPU memory usage for large language model inference by utilizing idle peer GPU memory as a transient cache, reducing PCIe bandwidth latency and achieving over 2x throughput speedup.


<details>
  <summary>Details</summary>
Motivation: Large Language Model inference faces GPU memory constraints due to growing model sizes and KV cache overhead during autoregressive decoding.

Method: Harvest dynamically manages GPU memory by leveraging unused peer GPU memory through high-bandwidth interconnects, functioning as a transient cache to reduce data movement while ensuring correctness.

Result: Harvest achieves more than 2x throughput speedup for inference processes, especially for expert layer weights retrieval and KV cache handling.

Conclusion: Using peer GPU memory as a transient cache significantly alleviates memory pressure and improves inference efficiency, showcasing its utility in handling large-scale AI workloads.

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [724] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: The paper aims to improve data attribution methods for adaptive optimizers, particularly Adam, offering scalable and accurate solutions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable data attribution in machine learning, particularly for adaptive optimizers like Adam, which traditional methods fail to handle effectively.

Method: The authors propose Adam-Aware In-Run Data Shapley, a scalable closed-form approximation utilizing a novel Linearized Ghost Approximation technique.

Result: Their method achieves high fidelity to ground-truth contributions ($R > 0.99$) while maintaining training efficiency (~95% throughput).

Conclusion: Adam-aware attribution outperforms traditional SGD-based methods in downstream tasks, offering an optimizer-specific solution for effective data attribution.

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [725] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: The paper investigates how mini-batch noise interacts with Adam optimizer hyperparameters $(β_1, β_2)$ and batch size, influencing generalization in multi-epoch training. It proposes theoretical insights and experimental evidence for optimizing hyperparameter settings.


<details>
  <summary>Details</summary>
Motivation: Understanding the implicit biases of optimizer hyperparameters, specifically in Adam, towards sharper or flatter loss landscapes and their impact on generalization in scenarios with large and small batch sizes.

Method: The paper builds a theoretical framework analyzing the interaction between mini-batch noise and momentum parameters $(β_1, β_2)$ in Adam. It connects the batch size scale to critical batch size and conducts experiments to validate the findings.

Result: The study shows how batch size influences the behavior of $(β_1, β_2)$ in Adam, identifying better hyperparameter configurations for varying batch sizes in terms of validation accuracy.

Conclusion: The findings demonstrate a batch size-dependent shift in implicit bias due to Adam hyperparameters, providing actionable insights for optimizing these settings in multi-epoch training.

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [726] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: This paper presents a prototype-based XAI method for multi-channel geospatial data, enabling interpretable ML predictions with comparable performance to standard neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based XAI models are primarily designed for RGB image data and lack optimization for multi-channel geoscientific datasets with distinct variable-specific channels.

Method: The study develops a prototype-based XAI approach specific to multi-channel geospatial data, incorporating channel-specific prototypes and assessing their influence on predictions using geoscientific case studies.

Result: The proposed method achieves comparable performance to standard neural networks and produces local (instance-level) and global (model-level) explanations for model predictions.

Conclusion: By using channel-specific prototypes, this approach enhances the transparency and trustworthiness of ML models in geoscientific applications, providing insights into the relevance of distinct features.

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [727] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: The paper tackles online inverse optimization problems, achieving a regret bound of O(dlogd) for M-convex sets, resolving an open question on polynomial regret bounds.


<details>
  <summary>Details</summary>
Motivation: To address the open question of whether finite regret bounds polynomial in the dimension d are achievable in online inverse optimization problems.

Method: The approach combines structural characterization of M-convex sets with geometric volume arguments, extending to adversarial feedback scenarios using graph-based adaptation.

Result: Achieved finite regret bounds of O(dlogd) for M-convex sets and O((C+1)dlogd) in adversarial settings without prior knowledge of corruption rounds.

Conclusion: Results show that polynomial regret bounds are achievable under specific conditions, contributing significantly to contextual recommendation and inverse optimization research.

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [728] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: This paper introduces an attention-guided framework to improve the effectiveness and flexibility of steering methods for guiding LLM responses toward specific semantic concepts.


<details>
  <summary>Details</summary>
Motivation: Steering LLM responses toward desired semantic concepts is fragile, requiring better techniques to overcome brittleness and enhance understanding of semantic storage in LLMs.

Method: The attention-guided steering framework addresses token embedding selection, heterogeneous concept-related features, and identification of relevant LLM layers.

Result: The framework significantly improves steering performance, doubling the number of successfully steered concepts across various models (up to 70 billion parameters) and providing insights into concept-specific feature distributions.

Conclusion: This work advances steering capabilities for large LLMs, offering efficient methods and scalability to industry standards while fostering exploration of fine-tuning algorithms.

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [729] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: The paper addresses the suboptimality of standard stochastic gradients in high-dimensional settings by proposing a shrinkage-based gradient estimator, which improves performance in deep learning optimization.


<details>
  <summary>Details</summary>
Motivation: Stochastic gradient methods are commonly used in large-scale learning, but unbiased stochastic gradients are suboptimal under quadratic loss in high-dimensional settings. This reveals a need for a better gradient estimation approach.

Method: The authors use a decision-theoretic framework applying Stein-rule shrinkage, where a shrinkage gradient estimator is developed. Gradient noise is adaptively controlled based on second-moment statistics, building on methods like Adam. The estimator was integrated into the Adam optimizer and tested empirically.

Result: The proposed shrinkage gradient estimator dominates standard gradients under squared error loss and achieves minimax optimality. The modified Adam optimizer showed improvements in CIFAR10 and CIFAR100, especially in large-batch regimes.

Conclusion: Classical shrinkage techniques offer a robust and principled way to enhance stochastic gradient estimation in deep learning, with tangible benefits when applied selectively to certain high-dimensional parameters.

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [730] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: The paper introduces adaptive momentum coefficients in optimization schemes regulated by the kinetic energy of model parameters, improving convergence stability and speed.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in optimization methods like mSGD concerning stability and convergence speed, especially in large-scale, complex landscape tasks like training ViT, BERT, and GPT2.

Method: Continuous-time schemes were proposed with adaptive friction using cubic damping, augmenting existing methods like mSGD and Adam.

Result: The proposed methods demonstrated robustness in performance, outperforming Adam on key large-scale tasks and overcoming traditional challenges faced by mSGD.

Conclusion: Adaptive friction and cubic damping provide stable and efficient optimization, boosting the overall performance of large-scale training models.

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [731] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: The paper explores diffusion-based generative inference for indoor access point (AP) deployment, offering a more scalable and domain-agnostic solution compared to current LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: The deployment of APs in next-generation wireless networks is challenged by complex indoor environments and the inefficiencies of existing LLM-based optimization methods, which are computationally costly and not scalable.

Method: Using generative inference models guided by a unified reward function incorporating core AP deployment goals, the authors leverage diffusion samplers to optimize AP placement in diverse indoor floorplans.

Result: The diffusion-based method achieves superior performance compared to other generative approaches by optimizing the reward landscape effectively, particularly for non-convex and fragmented objectives. A large real-world dataset was developed to train and evaluate the models.

Conclusion: Diffusion-based generative models offer an effective, scalable, and flexible solution for AP deployment in complex indoor environments, demonstrating generalization robustness and the potential for broad application.

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [732] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: This paper introduces a novel transformer reinforcement learning approach for A/B testing in time series experiments, addressing limitations of traditional designs and improving performance.


<details>
  <summary>Details</summary>
Motivation: Despite A/B testing's widespread use in policy evaluation, its application to time series experiments is hindered by suboptimal treatment allocations and reliance on strong design assumptions.

Method: The approach combines transformers to use full historical data for treatment allocation and reinforcement learning to directly optimize mean squared error (MSE) without restrictive assumptions.

Result: Empirical evaluations on synthetic data, a dispatch simulator, and a real-world ridesharing dataset show that this method outperforms existing approaches.

Conclusion: Conditioning on the full history and optimizing MSE using transformer RL leads to better designs, setting a new standard for A/B testing in time series experiments.

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [733] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: This paper investigates multimodal sentiment analysis by combining image and text data using a novel method called TEMSA, showing improved results over individual analyses.


<details>
  <summary>Details</summary>
Motivation: The challenges in multimodal sentiment analysis arise from the dissimilarities between modalities (text and images), sentiment ambiguity, and contextual complexities, necessitating innovative methods.

Method: The authors introduce 'Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA), which utilizes object recognition to detect and extract object names from images, integrating these with associated text (TEMS).

Result: TEMS methodology yields better sentiment analysis results for multimodal data compared to analyzing image and text data separately.

Conclusion: The research presents TEMSA as an effective approach for improving multimodal sentiment analysis by strategically combining object detection in images with textual data.

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [734] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: The paper proposes a method for Bayesian active learning that improves Gaussian process surrogates by warping the input space based on observations, enhancing exploration and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian active learning methods using Gaussian processes have limitations as their posterior variance largely disregards observed outputs beyond hyperparameters, making exploration less adaptive.

Method: The authors introduce a learned monotone reparameterization that warps the input space based on observations. They also present a novel self-supervised training objective for these warps.

Result: The proposed method improves sample efficiency, especially in non-stationary regimes, across various active learning benchmarks.

Conclusion: Warping the input space dynamically in response to observed variability enhances active learning efficiency, addressing limitations in traditional Gaussian process-based methods.

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [735] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: This paper introduces Quantum Generator Kernels (QGKs), a novel approach for improving quantum kernel methods in Quantum Machine Learning by addressing data embedding challenges on limited quantum hardware.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of embedding and processing large-scale real-world data (e.g., images) into the constrained capacities of Noisy Intermediate-Scale Quantum (NISQ) hardware, achieving better performance with Quantum Machine Learning.

Method: The proposed Quantum Generator Kernels (QGKs) use a Variational Generator Groups (VGGs) framework, combining universal generators into a parameterizable operator. This allows scalable coverage of the quantum space and optimizes kernel alignment to specific target domains by training a weight vector for data-contextual projections.

Result: Empirical results show that QGK outperforms existing quantum and classical kernel methods in terms of projection and classification capabilities, highlighting its effectiveness and scalability.

Conclusion: QGKs represent a promising approach to enhance the utility of Quantum Machine Learning on current quantum hardware, providing a flexible and efficient framework for various applications in the field.

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [736] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: This paper develops algorithms for online episodic tabular MDPs with known transitions, achieving refined regret bounds in both adversarial and stochastic regimes through measures like first-order, second-order, and variance-based metrics.


<details>
  <summary>Details</summary>
Motivation: The need for adaptivity and improved regret bounds in episodic MDPs under both adversarial and stochastic conditions.

Method: Algorithms based on optimistic follow-the-regularized-leader with log-barrier regularization focused on global and policy optimization.

Result: Establishment of nearly optimal regret bounds with novel data-dependent and variance-dependent metrics in respective regimes.

Conclusion: The proposed algorithms provide robust performance across adversarial and stochastic settings, proving near-optimality of the global optimization approach.

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [737] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: The paper introduces Sparse Knowledge Distillation (SparseKD), a method to compress transformer models using self-referential distillation combined with structured pruning.


<details>
  <summary>Details</summary>
Motivation: Deploying large language models is costly, so there is a need to compress models while maintaining quality and minimizing deployment overhead.

Method: SparseKD combines structured Singular Value Decomposition (SVD) pruning with self-referential knowledge distillation, where the model teaches itself by matching its own pre-compression probability distribution.

Result: The method improves model performance by 39% post-training, achieves 15-65% parameter reduction with acceptable trade-offs, and demonstrates reproducibility across model sizes (0.6B and 3.8B parameters).

Conclusion: SparseKD is effective, does not require external teachers or infrastructure changes, achieves compression and quality retention simultaneously, and complements existing optimizations.

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [738] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: The paper presents MATRIX, a multimodal benchmark for evaluating reasoning in materials science using both text and visual data.


<details>
  <summary>Details</summary>
Motivation: To assess whether visual experimental data enhances mechanism-grounded scientific reasoning compared to text-only supervision.

Method: Developed MATRIX benchmark, comparing post-training effects of text-only and multimodal data integration with paired experimental images.

Result: Visual supervision improved experimental interpretation by 10-25% and scientific reasoning by 5-16%. Cross-modal transfer proved crucial for these improvements.

Conclusion: Multimodal post-training significantly enhances reasoning in materials science and positively impacts other domains like ScienceQA and PubMedQA.

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [739] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: The paper proposes a deep multivariate model to handle heterogeneous variable collections for multiple tasks by representing joint probabilities as conditional probabilities.


<details>
  <summary>Details</summary>
Motivation: Current multivariate models are typically tailored for specific tasks, limiting their adaptability for other downstream applications.

Method: The model learns by parameterizing Markov chain kernels and maximizing data likelihoods of their limiting distributions, enabling semi-supervised learning.

Result: The developed model demonstrates flexibility and utility across various downstream tasks.

Conclusion: Representing joint distributions via conditional probabilities offers versatility and adaptability in learning heterogeneous data structures.

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [740] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: This paper introduces a mask-based resampling framework built on a pre-trained DDPM model for generating parameter-constraint-aware engineering designs without retraining.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of conventional DDPM methods, particularly their inability to handle performance and parameter constraints in engineering design generation.

Method: The framework incorporates RePaint, which utilizes mask-based resampling during inference to efficiently and controllably modify partial designs while adhering to constraints.

Result: The approach was validated on ship hull and airfoil design tasks, showing it generates innovative designs with constrained performance accuracy similar to or better than pre-trained models.

Conclusion: This solution offers an efficient, training-free method to achieve controlled generative design, suitable for advanced engineering applications while maintaining performance constraints.

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [741] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP is a self-supervised framework for robust computation that assigns weights based on agreement levels, effectively suppressing outliers without supervision.


<details>
  <summary>Details</summary>
Motivation: To develop a robust computation method that emphasizes trustworthy data and minimizes the influence of outliers in high-dimensional settings.

Method: SNAP utilizes a self-supervised Agreement-Reliability Hypothesis to assign weights based on mutual agreement, ensuring outlier suppression.

Result: SNAP outperforms traditional methods like the Weiszfeld algorithm and multivariate median of means for tasks such as vector averaging and subspace estimation.

Conclusion: SNAP is a versatile and practical method for robust computation, proving effective in high-dimensional applications.

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [742] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: This paper analyzes the safety advantages of Diffusion large language models (D-LLMs) over autoregressive LLMs (AR-LLMs), identifies their vulnerabilities, and provides insights into their robustness and limitations against jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the intrinsic robustness of D-LLMs against jailbreak attacks and assess the mechanisms and limitations of their safety advantages compared to AR-LLMs.

Method: The authors conduct an analysis of D-LLMs' diffusion-style generation process to demonstrate a stepwise suppression of unsafe outputs. They also identify a failure mode called context nesting and test its effectiveness in bypassing D-LLMs' robustness.

Result: The study highlights that while D-LLMs demonstrate intrinsic robustness against jailbreak attacks through diffusion, they are vulnerable to context nesting. This failure mode achieves high attack success rates, including the first successful jailbreak of Gemini Diffusion, revealing critical vulnerabilities.

Conclusion: The paper concludes that D-LLMs possess unique safety features against unsafe generation but are not foolproof. It emphasizes the need for further research and red-teaming to bolster their robustness against adversarial attacks like context nesting.

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [743] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: This paper introduces a geographic location encoding method utilizing spherical Slepian functions to address the challenges of fine-grained resolutions in machine learning across geographically localized applications. It also presents a hybrid encoder combining Slepian functions and spherical harmonics for balancing global and local context.


<details>
  <summary>Details</summary>
Motivation: Machine learning models often fail at capturing fine-grained geographic patterns due to their tendency to distribute representational capacity uniformly across the globe. These limitations hinder applications requiring localized geographic precision.

Method: The authors propose a geographic encoder based on spherical Slepian functions, which concentrates representational capacity into specific regions of interest. They also develop a hybrid Slepian-Spherical Harmonic encoder for integrating global and local contexts effectively.

Result: The study demonstrates that Slepian-based encodings outperform baseline methods across five diverse tasks, including classification, regression, and image-augmented prediction. Performance advantages were observed across various neural network architectures.

Conclusion: Spherical Slepian-based encoding proves effective for localized applications, offering high-resolution performance and computational efficiency. The hybrid encoder successfully balances local-global tradeoffs while retaining core desirable properties like spherical-surface-distance preservation.

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [744] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: FastForward introduces a sparsity framework to enhance large language model inference by improving feed-forward network computations during the prefill stage, achieving up to 1.45× speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large language models face computational bottlenecks in the prefill stage during long-context processing. Existing methods for sparsifying feed-forward networks fail to leverage parallelism effectively and compromise accuracy.

Method: FastForward utilizes a framework with (1) expert predictors to identify important neurons, (2) error compensation networks for correcting sparsity-related errors, and (3) layer-wise sparsity scheduling based on token relevance.

Result: FastForward achieves up to 1.45× speedup with 50% sparsity in feed-forward networks and maintains <6% accuracy loss across language models like LLaMA and Qwen on the LongBench benchmark.

Conclusion: FastForward is a significant improvement in LLM inference, efficiently reducing computation time during the prefill stage without major accuracy trade-offs, and proves effective for long-context tasks on constrained hardware.

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [745] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: The paper develops a no-swap regret algorithm for combinatorial bandits which scales polylogarithmically in the number of actions (N), addressing a long-standing challenge.


<details>
  <summary>Details</summary>
Motivation: The idea is to develop an efficient no-swap-regret algorithm for combinatorial bandits where the action space is exponentially large, overcoming existing limitations in regret minimization methods.

Method: Introduces a no-swap-regret learning algorithm that achieves sublinear regret in time horizon T and polylogarithmic dependence on N. The design ensures efficiency for many applications.

Result: The developed algorithm achieves tight no-swap regret for combinatorial bandits with polylogarithmic dependence on the number of actions.

Conclusion: This work resolves the long-standing challenge of efficiently achieving no-swap regret in combinatorial bandits and provides practical implementation for various applications.

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [746] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: This paper introduces MemoryLLM to study decoupled feed-forward modules (FFNs) in transformers as token-wise memory and improve inference efficiency. It also proposes Flex-MemoryLLM to balance performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: The interpretability of feed-forward modules in transformers remains challenging. The research aims to understand FFNs' operations and enable token-wise neural retrieval memory.

Method: The authors proposed MemoryLLM, which separates FFNs from self-attention and uses token embeddings for isolated FFN training. They also introduced Flex-MemoryLLM, an architecture balancing standard transformers and MemoryLLM.

Result: MemoryLLM enables token-wise pre-computing for FFNs, enhancing efficiency. It advances the interpretability of FFNs as token-wise neural memory and provides insights into memory utilization across tasks.

Conclusion: The proposed MemoryLLM and Flex-MemoryLLM enhance FFN interpretability and inference efficiency, offering a blend of analytical clarity and performance optimization for transformers.

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [747] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: This paper develops a theory to study the geometric structure of neural network features by analyzing the spectra of weight-derived matrices, introducing the frame operator for understanding global feature interactions and their localization patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks represent features in smaller dimensions via superposition and how features share representational space, with a focus on the geometric structure often discarded by current methods.

Method: The authors introduced the frame operator ($F = WW^\top$) to study feature spectral measures and the global geometry of feature interactions through weight-derived matrices and their eigenspaces. They applied these methods in toy models and extended them to arbitrary settings.

Result: The study found that capacity saturation in neural networks leads to spectral localization, frames features into tight structures, and enables discrete classification of geometries. The approach classifies prior geometries and applies to real-world weight matrices.

Conclusion: The paper establishes a framework using operator theory for deeper interpretability of feature organization in neural networks and suggests potential for broader applications in examining neural network representations.

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [748] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper addresses computational limitations in eigenvector computation for reinforcement learning, proposing a neural network-based objective to approximate the principal eigenvector of the default representation.


<details>
  <summary>Details</summary>
Motivation: Eigenvectors of the DR are effective in various reinforcement learning applications but are computationally expensive to approximate using traditional eigendecomposition methods in high-dimensional environments.

Method: The authors derive an objective for directly approximating the principal eigenvector of the DR using neural networks instead of performing eigendecomposition on an approximated DR matrix.

Result: Empirical validation of the neural network objective shows effectiveness in multiple environments and demonstrates successful application of the learned eigenvectors for reward shaping.

Conclusion: The proposed method scales better computationally and advances reinforcement learning by simplifying the computation of useful eigenvectors for diverse applications.

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [749] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: This paper proposes a novel gradient-based attack, Fed-Listing, targeting Federated Graph Neural Networks (FedGNNs) to infer private label distributions without direct data access.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks in Federated Graph Neural Networks (FedGNNs), where gradients exchanged during training unintentionally expose sensitive label distribution information of local clients.

Method: Fed-Listing leverages final-layer gradients from training, combined with a shadow dataset for simulating client distributions, to extract label statistics secretly using a unique attack model.

Result: Fed-Listing achieves superior performance over existing baselines in inferring label distributions across four datasets and three architectures, even in non-i.i.d. settings, with defenses having limited effectiveness without reducing utility.

Conclusion: Fed-Listing demonstrates the vulnerability of FedGNNs to gradient-based label inference attacks, urging the need for enhanced privacy mechanisms to secure user data while maintaining model performance.

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [750] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: The paper addresses reinforcement learning (RL) in revenue management under delayed feedback by introducing a choice-model-assisted RL approach. Key experimental insights involve robustness and bias dynamics of this method under different scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to handle a common challenge in revenue management—delayed feedback due to customer booking cancellations/modifications—by leveraging discrete choice models to enhance RL performance.

Method: A calibrated discrete choice model is used as a fixed partial world model in RL to impute delayed feedback. Theoretical convergence for the proposed approach is analyzed, and extensive experiments using hotel booking data assess its performance.

Result: Key findings include (i) performance maintaining parity with baselines in stationary settings, (ii) improvements in certain parameter shift scenarios, and (iii) revenue degradation under structural misspecification of the model.

Conclusion: The choice-model-assisted RL method can enhance robustness and performance under specific conditions but may introduce bias if the underlying assumptions are violated.

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [751] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: The paper introduces the Variational Graph-to-Scheduler (VG2S) framework to efficiently solve the Job Shop Scheduling Problem (JSSP) via variational inference and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in conventional DRL techniques, such as non-stationarity during training and limited generalization for unseen scheduling problems.

Method: The VG2S framework uses variational inference and a variational graph encoder to decouple representation learning from policy optimization using an ELBO-based probabilistic objective.

Result: The VG2S method demonstrates superior zero-shot generalization capabilities compared to existing DRL methods and traditional dispatching rules, excelling on large-scale and complex JSSP benchmarks like DMU and SWV.

Conclusion: The proposed VG2S enhances training stability, generalization abilities, and performance on critical and challenging scheduling problems, making it a robust framework for JSSP optimization.

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [752] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: This paper introduces C-kNN-LSH, a framework for sequential causal inference focusing on longitudinal data, applied to an analysis of long COVID with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and decision-making in complex conditions, such as comorbidities and long COVID, by addressing challenges of high-dimensional, confounded longitudinal data.

Method: The authors propose C-kNN-LSH, which uses locality-sensitive hashing to efficiently find similar covariate histories ('clinical twins') and utilizes a doubly-robust correction to reduce bias from irregular sampling.

Result: C-kNN-LSH outperformed existing methods in capturing recovery heterogeneity and estimating policy values when tested on a real-world long COVID dataset consisting of 13,511 participants.

Conclusion: The method brings reliable and efficient causal effect estimation in longitudinal healthcare data, addressing key challenges such as bias and high-dimensionality while providing superior results in real-world applications.

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [753] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: This paper proposes a pipeline for transforming dirty categorical data into numerical format for integration with AutoML, benchmarking its performance against current methods.


<details>
  <summary>Details</summary>
Motivation: To improve AutoML handling of dirty categorical datasets, which are challenging due to high cardinality and issues such as lack of curation.

Method: Introduce a pipeline for translating categorical data into numerical using advanced encoding schemes, benchmark its performance across dirty categorical datasets, and analyze AutoML-built pipelines.

Result: Enhanced predictive performance and better insights into the functioning of AutoML pipelines when handling dirty categorical data.

Conclusion: Incorporating morphological encoders for categorical data improves AutoML robustness and predictive accuracy, offering insights beyond optimal model selection.

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [754] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: This paper proposes two Bayesian optimization methods to minimize the probability of failure in design optimization problems dealing with rare failures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of optimizing design reliability where the failure probabilities are extremely rare, which is crucial in manufacturing and engineering contexts.

Method: Two Bayesian optimization methods are introduced—one based on Thompson sampling and another on the knowledge gradient. Both incorporate importance sampling for targeting very small failure probabilities.

Result: The proposed methods outperform existing approaches in empirical evaluations across both extreme and non-extreme failure regimes.

Conclusion: These methods effectively tackle rare failure probabilities, improving the reliability optimization process in settings where failures are exceedingly unlikely.

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [755] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: The paper introduces scBatchProx, a method enabling improved batch correction for single-cell RNA sequencing data without requiring retraining or centralized data processing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle batch effects in evolving single-cell RNA sequencing datasets with a lightweight, decentralized approach that avoids issues with existing methods, such as insufficient correction or the need for centralized retraining.

Method: The authors propose scBatchProx, a post-hoc optimization technique inspired by federated learning. It learns batch-conditioned adapters with proximal regularization to correct batch variations directly in latent space, bypassing the need for raw expression data.

Result: scBatchProx achieves relative improvements of 3-8% in embedding quality, with better batch correction in 90% of cases and improved biological signal conservation in 85% of experiments.

Conclusion: The method provides a practical and adaptive solution for refining representations in dynamic and distributed single-cell datasets, advancing the field's ability to analyze evolving data efficiently.

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [756] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL) introduces an RL framework for crystal structure prediction using velocity fields instead of score, enhancing efficiency and maintaining diversity.


<details>
  <summary>Details</summary>
Motivation: While continuous-time generative models for crystalline materials enable stable structure predictions, integrating explicit target properties into generation remains difficult. A RL approach could align generative models with downstream objectives.

Method: OMatG-IRL employs policy-gradient RL on velocity fields with stochastic perturbations for exploration and inference-time policy-gradient estimation. It supports velocity annealing schedules for efficient generation.

Result: OMatG-IRL achieves effective reinforcement of energy-based objectives, maintains diversity, competes with score-based RL approaches, and improves sampling efficiency with reduced generation time.

Conclusion: OMatG-IRL successfully incorporates RL into crystal structure prediction, bridging gaps in computational efficiency, diversity preservation, and property alignment for generative models.

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [757] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: This paper presents a mathematical framework to explicitly describe the structure, training, and behaviors of large language models (LLMs), enabling theoretical and practical insights.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide researchers with a concise and explicit mathematical description of LLM training, alignment, and generation, filling a gap in understanding their underlying computational structure.

Method: It formulates LLMs as high-dimensional nonlinear autoregressive models with attention mechanisms, detailing pretraining, various alignment methods, and inference processes, with mathematical rigor.

Result: The paper explains how LLM operations and behaviors emerge from specific mathematical principles, facilitating a deeper understanding of alignment behaviors, inference phenomena, and model extensions.

Conclusion: This work serves as a mathematical reference to aid in interpreting LLMs, analyzing their behaviors, and inspiring further theoretical development.

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [758] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: The paper introduces a method to create foundation models that are resistant to unauthorized fine-tuning while maintaining performance in their original form.


<details>
  <summary>Details</summary>
Motivation: Open-source foundation models are at risk of misuse, particularly through unauthorized fine-tuning, which can undermine safety and economic benefits.

Method: The authors propose Private Mask Pre-Training (PMP), a training framework that integrates useful representations into a sparse subnetwork while keeping the binary mask of this subnetwork private, thus disrupting unauthorized fine-tuning due to mismatched training geometry.

Result: Theoretical analysis confirms the mismatch destabilizes gradient-based fine-tuning. Empirical evaluations demonstrate PMP maintains original model performance and effectively limits unauthorized fine-tuning, with tunability controlled by mask ratios.

Conclusion: PMP offers a solution to protect open-source foundation models from economic and safety risks by controlling fine-tuning adaptability while retaining usability through its innovative masking mechanism.

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [759] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: This paper introduces SIERL, a novel exploration method in reinforcement learning that sets sub-goals based on the agent's progress, outperforming existing methods in sparse-reward environments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in reinforcement learning where sparse rewards hinder efficient exploration.

Method: SIERL guides exploration using a sub-goal selection mechanism based on the agent's learning progress and prioritizing frontier states by cost-to-come and cost-to-go estimates.

Result: SIERL surpasses state-of-the-art baselines by effectively achieving main tasks and generalizing to diverse states in sparse-reward environments.

Conclusion: The SIERL mechanism provides a systematic and informed way of exploration, expanding the frontier efficiently and improving performance in sparse-reward environments.

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [760] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: The study introduces PAIR-Former, a pooling and relational transformer pipeline to address miRNA--mRNA targeting under budget constraints, demonstrating improved accuracy and controllable trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying functional miRNA--mRNA target sites under computational constraints, considering the heavy-tailed pool of candidate sites and pair-level observation limits.

Method: The authors propose the PAIR-Former framework, which scans candidate target sites, selects a subset for detailed processing, and applies a relational Set Transformer for the selected instances, optimizing performance under a compute budget.

Result: PAIR-Former achieves superior predictive performance compared to pooling baselines on miRAW, especially under a practical budget, and offers flexibility in managing trade-offs between accuracy and computation.

Conclusion: PAIR-Former effectively addresses the Budgeted Relational Multi-Instance Learning challenge, providing a theoretical basis for its budgeted selection and demonstrating performance improvements in functional targeting tasks.

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [761] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: Proposes GRASP, a stochastic, parallelizable planner utilizing differentiable world models for effective planning in long-horizon control tasks from visual inputs.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and success in long-horizon planning tasks using differentiable world models, tackling the challenges of unstructured search space and sensitive gradients.

Method: Introduces GRASP, a planner treating states as virtual optimization variables with soft dynamics constraints while incorporating stochasticity to improve optimization and mitigate local optima.

Result: GRASP outperforms existing planning algorithms like CEM and GD in success rate and time to convergence during long-horizon experiments on video-based world models.

Conclusion: GRASP is a promising, efficient solution for visual input-based long-horizon control tasks and alleviates traditional planning challenges associated with differentiable world models.

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [762] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: The paper proposes a method called CAL, which allows Diffusion Language Models (DLMs) to adaptively determine the correct infilling length without additional training, significantly improving performance in both code and text infilling tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of DLMs, which have constrained performance due to their reliance on pre-specified infilling lengths, hindering their full potential for accurate and efficient infilling tasks.

Method: This paper introduces CAL (Calibrated Adaptive Length), a training-free method that uses DLMs' inherent denoising confidence to identify the optimal infilling length. It leverages statistical signals, including an Oracle Peak and Length Bias, to calibrate and guide length discovery.

Result: CAL achieves a 47.7% improvement in Pass@1 for code infilling and boosts BLEU-2 and ROUGE-L scores by 8.5% and 9.9%, respectively, for text infilling tasks. It outperforms both fixed-length baselines and chat-based methods.

Conclusion: CAL enables DLMs to perform robust infilling by approximating the optimal length adaptively, showcasing significant performance improvements without requiring specialized training. The success of CAL broadens the capability of DLMs for infilling tasks.

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [763] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: This paper proposes AREAL-DTA, a scalable RL framework for large language models leveraging a DFS-based approach to optimize prefix sharing during rollouts, achieving significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks inefficiently handle repeated computations of shared token prefixes during training, leading to computational and memory inefficiencies.

Method: AREAL-DTA uses a dynamic DFS execution strategy to efficiently handle rollout prefix sharing, and incorporates distributed batching for scalability across GPUs.

Result: AREAL-DTA achieves up to 8.31× higher training throughput in RL post-training workloads.

Conclusion: AREAL-DTA addresses inefficiencies in RL for large language models, offering scalable and efficient training methods that exploit rollout prefix sharing.

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [764] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: This paper introduces OD-DEAL, a framework combining adversarial learning and hybrid methods for solving large-scale CVRP, achieving near-real-time high-quality results.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for capacitated vehicle routing problems struggle due to complexity and poor scalability on massive graphs, requiring better solutions.

Method: The framework integrates hybrid genetic search (HGS), barycenter clustering decomposition (BCC), and knowledge distillation into a graph attention network-based generative policy.

Result: OD-DEAL achieves state-of-the-art performance on real-time CVRP tasks, solving problems with up to 10000 nodes efficiently.

Conclusion: OD-DEAL offers a significant advance in CVRP solutions, enabling dynamic, high-quality inference for large-scale deployments.

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [765] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: This paper introduces a new neural network architecture, Partition of Unity Neural Networks (PUNN), which directly represents class probabilities without relying on softmax layers and improves interpretability.


<details>
  <summary>Details</summary>
Motivation: Current neural network classifiers lack interpretability as their class regions are implicitly defined through coupled inequalities. The authors aim to build an interpretable model where class probabilities are directly assigned and can be transparently analyzed.

Method: The authors propose PUNN, which constructs $k$ partition functions $h_i$ satisfying a unity condition, and directly defines class probabilities. These partitions use different parameterizations (e.g., MLPs or geometric shapes) to adapt to the data structure.

Result: Experiments demonstrate that PUNN achieves competitive accuracy (within 0.3%-0.6% of standard MLPs) with interpretable structures. Geometric priors enhanced performance with significantly fewer parameters.

Conclusion: PUNN offers an interpretable-by-design framework for classification tasks. It preserves accuracy while enabling transparent probability assignments, demonstrating its competitiveness against black-box models.

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [766] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: This paper explores reinforcement learning with verifiable rewards (RLVR) for improving structured Cyber Threat Intelligence (CTI) outputs from large language models (LLMs). It introduces Minerva, a dataset and training pipeline leveraging task-specific verifiers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing approaches for converting unstructured security artifacts into structured CTI outputs, particularly the brittleness of large language models under supervised fine-tuning.

Method: The authors proposed RLVR, using verifiable standards like canonical identifiers and schemas to ensure the correctness of structured outputs. They developed Minerva, a unified dataset with task-specific verifiers and introduced self-training mechanisms to handle reward sparsity during rollout.

Result: RLVR, combined with Minerva, improves accuracy and robustness in structured CTI tasks compared to supervised fine-tuning, demonstrated across multiple LLM backbones and benchmarks.

Conclusion: Reinforcement learning with verifiable rewards (RLVR) is a promising direction for generating reliable and automation-ready CTI outputs, outperforming traditional fine-tuning methods through task-specific verifiers and self-training techniques.

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [767] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: The paper reviews how contrastive learning can be used for privacy-preserving analytics in Industrial Internet of Things (IIoT) systems, highlighting challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Address the privacy and confidentiality risks associated with raw data sharing in IIoT environments, leveraging contrastive learning techniques for better data analytics.

Method: Comprehensive review of contrastive learning-based privacy-preserving techniques tailored to IIoT system characteristics, architectures, and applications.

Result: The paper identifies challenges, proposes solutions, and highlights unique features of industrial contexts for applying contrastive learning in IIoT.

Conclusion: Contrastive learning holds promise for improving privacy-preserving analytics in IIoT, but further research is needed to address open challenges and refine practices.

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [768] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: The paper introduces Nested Event Stream Transformer (NEST) for better handling hierarchical event stream data by preserving its structure, improving both computational efficiency and downstream performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and inaccuracies in current foundation models that flatten hierarchical event stream data, leading to suboptimal representations and computational inefficiency.

Method: The paper proposes NEST architecture, which preserves hierarchical structures of sequences of multisets and introduces Masked Set Modeling (MSM) to enhance set-level representation learning.

Result: NEST demonstrates improved pretraining efficiency, representation quality, and downstream performance on real-world multiset sequence data, capturing authentic dynamics.

Conclusion: Preserving hierarchical structure in event stream data is beneficial for efficient computational and representational outcomes, as shown by the proposed NEST model.

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [769] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: This paper proposes a method to synthesize sleep EEG signals from respiration signals using a novel generative framework.


<details>
  <summary>Details</summary>
Motivation: To enable alternatives for neurological assessment during sleep, especially in remote or contactless settings, by translating respiration signals into sleep EEG.

Method: The study uses a waveform-conditional generative framework with discrete tokenization to constrain EEG target space and preserve respiratory dynamics. It is trained on data from over 28,000 individuals.

Result: The model achieves a 7% MAE in EEG spectrogram reconstruction and supports downstream tasks like age estimation, sex detection, and sleep staging with comparable performance to ground truth EEG.

Conclusion: This proposed approach enables remote neurological assessment and achieves promising results for sleep analysis from respiration data, generalizing even to non-contact modalities like radio-frequency reflections.

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [770] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: The paper explores the geometry of neural representations through a controlled framework using tasks based on city coordinates, finding that while multitask training aligns representations, certain tasks harm integration and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the conditions governing the geometry of neural representations and their adaptability in downstream tasks.

Method: A framework is designed involving 5,075 city coordinates to represent a 'world' and 7 geometric tasks for autoregressive training to study representation geometry across multi-task and fine-tuning setups.

Result: Multi-task training leads to aligned world representations. However, some divergent tasks disrupt representation integration for new entities during fine-tuning, negatively affecting generalization.

Conclusion: While multi-task learning reliably aligns world representations, specific tasks can harm the adaptability of these representations to new data, emphasizing the need for careful task selection.

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [771] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: This paper introduces AIRE-Prune, a method for pruning state space models (SSMs) to reduce redundancy and computational cost, achieving high pruning rates with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs in state space models (SSMs) between capacity, stability, and computational efficiency by reducing redundant states while preserving performance.

Method: The AIRE-Prune method assigns an energy-based score to every state in an SSM, normalized across layers, and prunes states based on their asymptotic impulse-response energy contribution to minimize long-run distortion.

Result: AIRE-Prune achieves an average pruning rate of 60.8% with only a 0.29% average accuracy drop across various benchmarks, without requiring retraining, while significantly reducing computational costs.

Conclusion: AIRE-Prune effectively reduces redundancy in SSMs, preserving accuracy and stability while improving computational efficiency through energy-based pruning.

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [772] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: The paper presents IMFN, a method tackling long sequence compression using factorization instead of end-to-end learning. It simplifies compression tasks and achieves scalable inference.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and optimization challenges in compressing long sequences in neural memory architectures, focusing on scalability and accuracy.

Method: IMFN breaks long sequence compression into pairwise merge tasks using binary tree 'sweeper' modules and distills for online inference into a recurrent student model.

Result: Empirical tests on datasets like long MNIST sequences and UCF-101 videos showed the feasibility of compressing high-dimensional, long sequences effectively.

Conclusion: IMFN provides a scalable and robust solution for compressing long sequences, overcoming gradient and quadratic scaling issues in prior models through its factorization approach.

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [773] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: The paper introduces PolicyFlow, a reinforcement learning algorithm enhancing policy expressivity and stability using continuous normalizing flow (CNF) policies, while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges with Proximal Policy Optimization (PPO) when scaling to high-capacity policy models like CNFs, particularly the computational difficulties and instability in likelihood evaluation.

Method: PolicyFlow incorporates CNF-based policies into PPO objectives, using a simplified interpolation path for importance ratio approximation to reduce computational and numerical burden. Additionally, a Brownian Regularizer is introduced to enhance policy diversity and prevent mode collapse.

Result: PolicyFlow demonstrates competitive or superior performance to established methods across various environments, particularly excelling in scenarios requiring multimodal action distributions like MultiGoal.

Conclusion: PolicyFlow is an effective and computationally efficient solution for leveraging expressive CNF-based policies in on-policy reinforcement learning, providing stronger performance and richer behavior representations.

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [774] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: The paper introduces OpenDDI, a benchmark designed to streamline and enhance the prediction of drug-drug interactions by addressing data quality and evaluation standardization issues.


<details>
  <summary>Details</summary>
Motivation: The study aims to address significant challenges in drug-drug interaction prediction, namely the scarcity of high-quality data and the lack of standardized evaluation methods.

Method: OpenDDI is proposed as a benchmark, unifying 6 existing DDI datasets, contributing 3 new LLM-augmented datasets, expanding to multimodal drug representations, and standardizing evaluation protocols across 20 models and 3 tasks.

Result: A comprehensive evaluation conducted on OpenDDI produces 10 insights into current DDI prediction methodologies and highlights their limitations, providing directions for improvement.

Conclusion: OpenDDI offers a foundational resource for consistent and robust research in DDI prediction, paving the way for better drug interaction methodologies through improved data and standardized assessments.

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [775] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: Clinical data in EHR is irregularly sampled and includes mixed data types like events and measurements. ORA, a marked time-to-event model, improves representation by jointly modeling timing and values.


<details>
  <summary>Details</summary>
Motivation: Prior EHR Foundation Models fail to fully capture the structures of event timings and measurements, limiting their effectiveness and generalizability.

Method: ORA utilizes a marked time-to-event pretraining paradigm to integrate event timing and numerical measurements in EHR modeling.

Result: Consistent improvement across datasets, tasks, and architectures, enhancing classification, regression, and time-to-event predictions.

Conclusion: Pretraining objectives tailored to EHR structure are crucial for expanding model capabilities and generalizability.

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [776] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: The paper explores the eigenvalue distribution of the Hessian matrix in deep networks, showing that the 'bulk-and-spike' spectral structure arises from network architecture, not just data imbalance.


<details>
  <summary>Details</summary>
Motivation: To understand the factors driving the 'bulk-and-spike' eigenvalue spectrum of the Hessian matrix in deep neural networks and to challenge the assumption that this spectrum arises mainly from data imbalance.

Method: The authors analyze a deep linear network architecture and mathematically prove the presence of the spectral bifurcation structure in the Hessian matrix, even when the data covariance matrix is balanced.

Result: The analysis demonstrates that the spectral bifurcation, including dominant and bulk eigenvalue clusters, arises from the network's architecture. It also shows that the ratio between dominant and bulk eigenvalues scales linearly with network depth.

Conclusion: Network architecture significantly influences the spectral structure of the Hessian matrix, suggesting optimization strategies should account for both architectural and data-related factors.

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [777] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: This paper addresses the challenge of identifying molecules from mass spectrometry data and proposes a method to improve generalization by aligning spectral data with chemical structure embeddings.


<details>
  <summary>Details</summary>
Motivation: Molecule identification from MS data is challenging due to the semantic gap between spectral peaks and chemical structures, and current methods struggle to generalize to unseen molecular scaffolds.

Method: The researchers introduced a cross-modal alignment framework that maps mass spectra into the embedding space of a pretrained chemical language model, facilitating better recognition and generalization.

Result: The model achieved 42.2% Top-1 accuracy in a strict 256-way zero-shot retrieval benchmark and 95.4% accuracy in 5-way 5-shot molecular re-identification, showcasing strong generalization and chemical coherence.

Conclusion: Integrating physical spectral data with molecular structure embeddings provides a promising solution to the generalization challenges in identifying molecules from MS data.

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [778] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: This paper proposes a new method, AdaptNC, for improving uncertainty quantification in autonomous systems, addressing inefficiencies in existing online Conformal Prediction approaches during structural shifts.


<details>
  <summary>Details</summary>
Motivation: Current methods for uncertainty quantification in robotics rely on static nonconformity scores, leading to conservative and inefficient predictions under distribution shifts inherent in real-world scenarios.

Method: The authors propose AdaptNC, which jointly adapts the nonconformity score parameters and conformal thresholds using adaptive reweighting and a replay buffer to maintain stable coverage.

Result: AdaptNC reduces prediction region volume while maintaining coverage, as demonstrated using diverse robotic benchmarks under varying real-world scenarios such as policy/environmental changes and sensor degradation.

Conclusion: AdaptNC improves efficiency and reliability in predictive uncertainty for autonomous systems, outperforming existing methods in dynamic environments.

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [779] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: The paper introduces Clade-AHD, an efficient alternative to MCTS for automatic heuristic design that improves exploration using Bayesian beliefs, achieving better performance with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the over-exploitation issue in MCTS when applied to heuristic evaluation under tight computational budgets, which limits its effectiveness in designing heuristics using large language models.

Method: The proposed Clade-AHD replaces node-level point estimates in MCTS with clade-level Beta distribution-based Bayesian beliefs. It uses Thompson Sampling for better exploration and decision-making under uncertainty.

Result: Experimentation on combinatorial optimization problems demonstrates that Clade-AHD outperforms state-of-the-art methods and achieves this with significantly reduced computational cost.

Conclusion: Clade-AHD provides a more reliable and computationally efficient approach to heuristic design, overcoming MCTS limitations, and proves its effectiveness experimentally.

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [780] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: The paper introduces a three-axis taxonomy to integrate agentic reasoning and spatial intelligence for embodied agents, addressing their limitations in physical environments.


<details>
  <summary>Details</summary>
Motivation: There is a gap in connecting agentic reasoning and spatial domains, both crucial for embodied agents operating in the physical world.

Method: A review of over 2,000 papers with a unified three-axis taxonomy linking agentic capabilities to spatial tasks across scales, while distinguishing spatial and symbolic grounding.

Result: Findings include hierarchical memory for spatial tasks, promising GNN-LLM integration for spatial reasoning, and the necessity of world models for deployment across scales.

Conclusion: The taxonomy serves as a foundation to unify research, aid in standardized evaluations, and facilitate advancements in spatially-aware autonomous systems.

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [781] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: This paper introduces OEU, a novel framework addressing machine unlearning in quantized neural networks, ensuring better forgetting and utility preservation.


<details>
  <summary>Details</summary>
Motivation: The need for effective machine unlearning arises due to challenges in deploying quantized neural networks on privacy-compliant edge devices.

Method: OEU achieves unlearning by 1) maximizing prediction uncertainty on forgotten data and 2) using gradient orthogonal projection to separate forgetting from retention.

Result: Experimental results show OEU is superior in both forgetting effectiveness and retention accuracy compared to existing methods.

Conclusion: OEU provides a more reliable and effective approach to machine unlearning in quantized models, solving prior issues like conflated forgetting and gradient interference.

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [782] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: The paper introduces Stage-Aware CIL (Stage-CIL), focusing on handling intra-class evolution in Class-Incremental Learning, such as larva-butterfly morphological transformations.


<details>
  <summary>Details</summary>
Motivation: Existing Class-Incremental Learning (CIL) methods often overlook intra-class evolution and focus solely on preventing forgetting of past classes during class additions.

Method: The paper formalizes the Stage-CIL paradigm and introduces the STAGE framework, which learns and predicts evolution patterns using a compact memory pool. It also presents Stage-Bench, a dataset and evaluation protocol.

Result: STAGE consistently outperformed state-of-the-art methods in addressing both inter-class and intra-class challenges within Class-Incremental Learning.

Conclusion: STAGE is an effective approach to address inter-class discrimination and intra-class morphological transformations, advancing the field of Stage-Aware CIL.

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [783] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: The paper explores how modifying training data distribution can improve generalization in large language models (LLMs). It reveals that reducing simplicity bias in optimization leads to better generalization.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of enhancing generalization capabilities in large language models while minimizing computational costs associated with advanced optimizers like SAM.

Method: The authors theoretically analyze optimization dynamics in an in-context linear regression model with self-attention. They compare gradient descent (GD) with sharpness-aware minimization (SAM), and use training data distribution adjustments to replicate the beneficial properties of SAM.

Result: The approach reduces simplicity bias and significantly improves LLM accuracy, achieving up to 18% relative gains on mathematical reasoning tasks across various models.

Conclusion: Adjusting the training data distribution to focus on examples learned later in training can improve generalization in LLMs, offering a cost-effective alternative to SAM for optimizing model performance.

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [784] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: This study focuses on solving the challenge of unlearning sensitive information in sparse large language models (LLMs) using a new method called Sparsity-Aware Unlearning (SAU).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address privacy concerns in large language models, ensuring selective forgetting of sensitive information while considering model sparsification, which is crucial for efficient deployment.

Method: The authors propose Sparsity-Aware Unlearning (SAU), utilizing gradient masking to redirect updates to active weights and importance-aware redistribution to manage pruned parameters, enabling effective unlearning in sparse LLMs.

Result: SAU demonstrates superior performance over existing unlearning methods in sparse LLMs, achieving efficient forgetting without compromising the model's utility.

Conclusion: The paper concludes that SAU is an effective solution for the unlearning challenge in sparse LLMs, balancing both forgetting and preserving model performance.

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [785] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: The paper introduces TFMixer, a joint time-frequency framework for irregular multivariate time series forecasting, showcasing superior performance across real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Irregularities in multivariate time series complicate forecasting due to uneven sampling and asynchronous variables, which standard models fail to address effectively.

Method: Introduces TFMixer with a Global Frequency Module using NUDFT for spectral extraction, and a Local Time Module for adaptive temporal patch aggregation, combining them for forecasting and seasonal extrapolation.

Result: TFMixer outperforms previous methods, validated through extensive experiments on real-world datasets.

Conclusion: TFMixer successfully addresses IMTS forecasting challenges by leveraging time-frequency modeling and offers state-of-the-art results.

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [786] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: The study introduces SL-SAC, a safe reinforcement learning algorithm that balances reward and safety using parameter-space exploration and distributional risk control, achieving superior safety benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in constrained reinforcement learning related to poor generalization from sharp value minima and inadequate management of heavy-tailed risk distributions.

Method: The SL-SAC approach includes adaptive stochastic gradient Langevin dynamics, distributional risk estimation via IQN and CVaR, and a reactive Lagrangian relaxation scheme.

Result: SL-SAC achieved the lowest cost in 7 out of 10 tasks on Safety-Gymnasium benchmarks, with 19-63% cost reductions compared to state-of-the-art baselines, while maintaining competitive returns.

Conclusion: SL-SAC effectively balances safety and reward and offers a significant improvement in safety benchmarks for constrained reinforcement learning.

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [787] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: SEER proposes a robust time series forecasting approach improving patch quality by replacing low-quality data dynamically.


<details>
  <summary>Details</summary>
Motivation: Accurate time series forecasting is essential, but existing patch-based methods fail to address issues like low-quality data and anomalies.

Method: It introduces an Augmented Embedding Module and a Learnable Patch Replacement Module to refine patch representations and eliminate low-quality tokens.

Result: Experiments show that SEER achieves state-of-the-art performance by improving forecasting robustness and accuracy.

Conclusion: SEER successfully addresses low-quality issues in time series data by dynamically filtering and replacing patches to enhance prediction accuracy.

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [788] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: KEAT introduces a novel edge-focused attention mechanism for Temporal Graph Neural Networks (TGNNs), improving temporal relevance and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing TGNNs, which do not distinguish between slow-changing node embeddings and fast-changing edge features, resulting in poor temporal dependency capture and limited interpretability.

Method: The paper proposes KEAT (Kernelized Edge Attention for Temporal Graphs), which uses continuous-time kernels like Laplacian, RBF, and MLP to better model edge features while differentiating nodes and edges.

Result: KEAT outperformed benchmarks, achieving up to an 18% improvement in Mean Reciprocal Rank (MRR) on link prediction tasks compared to DyGFormer and 7% over TGN.

Conclusion: KEAT enhances temporal accuracy and interpretability in TGNNs by effectively separating and utilizing the temporal dynamics of nodes and edges.

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [789] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: This paper improves direct preference optimization (DPO) algorithms by incorporating rating gap information, achieving faster and robust performance on LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the ambiguity in DPO algorithms that rely on pairwise preferences, aiming for better model alignment and understanding feedback quality.

Method: Develop new algorithms that utilize rating gap information to enhance the optimization of foundation models.

Result: The proposed algorithms demonstrate faster statistical rates, robustness to rating gap inaccuracies, and superior performance compared to standard DPO algorithms across diverse models and benchmarks.

Conclusion: Incorporating rating gap improves DPO algorithms in alignment tasks, ensuring reliability and efficiency in handling limited pairwise feedback.

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [790] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: The paper introduces a new learning framework for stochastic games that achieves effective results using decentralized, payoff-based methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in designing decentralized algorithms that can efficiently find equilibria in stochastic games over an infinite horizon.

Method: A best-response-type actor-critic architecture with two critics: fast critic for quick payoff responses and slow critic for problem approximation, combined with smoothed best responses for adaptation.

Result: Convergence to approximate equilibria in two-agent zero-sum and multi-agent identical-interest games with theoretical assurance, and strong empirical performance.

Conclusion: The framework is robust, decentralized, and effective, marking significant progress in learning algorithms for stochastic game settings.

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [791] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: The authors propose TIC-FM, a zero-shot framework for time series classification that avoids classifier-dependent and training biases, achieving high accuracy, especially with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Address the issues of training-required biases and dependency on task-specific classifiers in zero-shot evaluation of Time Series Foundation Models (TSFMs).

Method: Introduced TIC-FM, an in-context learning framework with a time series encoder, projection adapter, and split-masked latent memory Transformer for training-free inference.

Result: TIC-FM demonstrated strong classification performance across 128 UCR datasets, with notable gains in scenarios involving extremely few labeled training samples.

Conclusion: TIC-FM enables effective and unbiased zero-shot time series classification, particularly thriving in low-label settings, showcasing its training-free transfer capability.

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [792] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: The paper introduces a layer sensitivity metric for understanding layer-specific behavior in multivariate long-term time series forecasting, and proposes MoDEx, a lightweight framework, that outperforms existing methods with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of individual layers' roles in long-term time series forecasting models, and to create a more efficient forecasting framework.

Method: The authors developed a gradient-based layer sensitivity metric inspired by GradCAM and effective receptive field theory. They used the insights to design MoDEx, a mixture of depth-specific MLP experts.

Result: MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78% of cases, while being more computationally efficient than existing methods. It also enhances the performance of transformer variants.

Conclusion: MoDEx shows superior efficiency and effectiveness in long-term time series forecasting, providing a robust and generalizable framework that reduces computational needs.

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [793] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: The study explores whether LLMs' internal hidden-state geometry can be inferred using psycholinguistic task behavior, finding that forced-choice tasks better predict these internal representations compared to free association.


<details>
  <summary>Details</summary>
Motivation: The paper aims to determine if Large Language Models' (LLMs) internal semantic representations can be analyzed and understood from their external behavioral responses during linguistic experiments.

Method: The researchers use instruction-tuned transformer models, perform two psycholinguistic task paradigms (similarity-based forced choice and free association), and build behavior-based similarity matrices with over 17.5M trials. Representational similarity analysis is used to compare behavioral and hidden-state geometries.

Result: The study shows that forced-choice behavior aligns more accurately with hidden-state geometry than free association. Behavioral similarities, particularly from forced choice, predict unseen hidden-state similarities beyond lexical baselines.

Conclusion: Behavioral tasks, especially forced-choice paradigms, can offer valuable insights into the hidden semantic geometry of LLMs, holding implications for uncovering cognitive states from behavior alone.

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [794] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: The paper introduces a novel framework (SEE) for safe exploration in reinforcement learning, addressing the challenge of identifying the maximum feasible exploration zone.


<details>
  <summary>Details</summary>
Motivation: To ensure safety in RL while exploring, by resolving how maximum feasible zones can be identified and expanded.

Method: Proposes the Safe Equilibrium Exploration (SEE) framework to iteratively find the largest feasible zone and refine uncertain environment models using graph formulations.

Result: Demonstrates theoretically and experimentally that SEE guarantees zero constraint violations, monotonically refines environment models, and converges to safe exploration equilibrium efficiently.

Conclusion: SEE successfully balances feasible zone expansion and model refinement, enabling safe RL environment exploration while ensuring safety constraints.

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [795] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: This paper introduces new Bayesian optimization (BO) methods for functions with tensor-output, addressing gaps in existing BO frameworks.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian optimization methods cannot handle tensor-output functions, a gap that limits BO application in certain complex scenarios.

Method: The authors develop a tensor-output Gaussian process (TOGP) and new kernels, coupled with an upper confidence bound (UCB) acquisition function. They extend this to a combinatorial bandit Bayesian optimization (CBBO) setting.

Result: Theoretical guarantees (sublinear regret bounds) are proven for the methods, and extensive experiments validate their effectiveness compared to alternatives.

Conclusion: The proposed tensor-output BO and CBBO frameworks expand the applicability of BO to handle tensor-output data and practical scenarios with partial observations.

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [796] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: The paper presents CoRe-Fed, a solution to fairness challenges in Federated Learning (FL), addressing representation and collaborative bias via embedding regularization and fairness-aware aggregation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses unfairness in conventional FL algorithms caused by disparities from heterogeneous data distributions and unequal participation across clients.

Method: The authors propose CoRe-Fed, featuring embedding-level regularization and a fair aggregation mechanism using alignment-driven mechanisms and dynamic reward-penalty-based strategies.

Result: CoRe-Fed outperforms existing algorithms, enhancing both fairness and model performance, as shown through extensive experiments.

Conclusion: CoRe-Fed unifies fairness challenges in FL, improving model performance and fairness by addressing representation bias and collaborative bias.

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [797] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: The paper introduces PHAT, a model for multivariate time series forecasting that accounts for periodic heterogeneity. It effectively captures distinct and dynamic periods in variates, performing better than other methods on real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing models overlook periodic heterogeneity in multivariate time series data, where variates exhibit different and changing periods, making accurate forecasting challenging.

Method: PHAT uses a "periodic bucket" tensor and a positive-negative attention mechanism to structure inputs based on periodic characteristics, align phases, restrict interactions across periods, and capture periodic dependencies with an attention mechanism modulated by periodic priors.

Result: PHAT was tested on 14 datasets against 18 baselines, demonstrating significant improvements and achieving highly competitive results in forecasting.

Conclusion: PHAT addresses periodic heterogeneity effectively and offers superior forecasting performance, offering valuable insights into real-world applications such as temporal data analysis.

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [798] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: The paper introduces DisRFM, a graph domain adaptation (GDA) framework utilizing Riemannian embedding and flow-based transport to resolve structural degeneration and adversarial training instability problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of structural degeneration and optimization instability in traditional GDA methods, which limit their effectiveness by causing entangled representations and unstable training dynamics.

Method: DisRFM uses Riemannian embedding to separate hierarchical structure from semantic representation, employs radial Wasserstein alignment and angular clustering for topology preservation, and utilizes Riemannian flow matching for stable feature alignment along geodesic paths.

Result: DisRFM achieves stable and disentangled graph alignment, showing superior performance over state-of-the-art methods in experiments.

Conclusion: The proposed framework addresses fundamental GDA challenges, proves its theoretical stability, and demonstrates its practical advantages through experiments.

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [799] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: The study uses machine learning (ML) models to classify EEG signals into three emotional states (Negative, Neutral, Positive). The random forest (RF) model showed the best performance among tested models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance the field of EEG-based emotion recognition using machine learning models to understand and optimize the classification of emotional states from brain activity data.

Method: The authors employed three ML models – logistic regression, support vector machine, and random forest – to classify emotions. The steps involved included data preprocessing, training, testing, and model performance evaluation based on accuracy and F1-score.

Result: Among the used models, random forest outperformed logistic regression and support vector machines, achieving the highest accuracy and F1-score for three-way emotion classification.

Conclusion: Machine learning models, particularly the random forest classifier, are effective for emotion recognition tasks involving EEG signals, outperforming current state-of-the-art models in specific metrics.

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [800] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: This paper illustrates that a simple linear autoregressive model using ordinary least squares (OLS) regression can match or exceed the performance of state-of-the-art deep models for time series anomaly detection (TSAD) while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To challenge the common assumption of using complex, resource-intensive deep learning architectures for time series anomaly detection by exploring the potential of simple linear models.

Method: The authors leverage a linear autoregressive model with the anomaly score derived from ordinary least squares (OLS) regression, providing theoretical analysis and extensive benchmarking on TSAD datasets.

Result: The proposed linear model consistently matches or outperforms deep learning methods in accuracy, while requiring significantly fewer computational resources, across both univariate and multivariate datasets.

Conclusion: Linear baselines should be included as standards in TSAD research to reinterpret the effectiveness of deep learning models and inspire richer benchmark datasets that highlight scenarios where deep models excel.

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [801] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: This paper addresses privacy concerns when fine-tuning large language models (LLMs), proposing a new algorithm (SCP-$Δ_r$) that enhances data protection with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs can expose confidential training data via training data extraction attacks. Current defenses either lack formal privacy guarantees or degrade utility significantly.

Method: The proposed SCP-$Δ_r$ algorithm applies a Near Access Freeness (NAF) approach, operating on relative probabilities and smoothing low-impact token deviations using a base model.

Result: SCP-$Δ_r$ demonstrates substantial improvement in theoretical protection bounds over existing NAF methods and empirically defends against TDE attacks effectively with minimal utility loss.

Conclusion: SCP-$Δ_r$ offers a practical solution to mitigate privacy risks in fine-tuning LLMs, balancing robust protection and maintaining model utility.

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [802] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: The paper investigates the parameter space of feed-forward ReLU networks, focusing on its connectedness and singularities, and their relation to network topology.


<details>
  <summary>Details</summary>
Motivation: To analyze and guide training dynamics in ReLU networks by understanding the parameter space properties, particularly connectedness and singularities.

Method: The authors studied the parameter space based on gradient flow, examining geometric properties like connectedness and singularities. They used mathematical characterizations related to DAG architectures and validated findings through numerical experiments.

Result: The study revealed that parameter space connectedness depends on bottleneck nodes and balance conditions in DAGs, and singularities are linked to network topology.

Conclusion: The topology of DAGs critically affects the training behavior and pruning mechanisms in ReLU networks.

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [803] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: This paper proposes LocalV, a multi-agent framework for automating RTL code generation, addressing scalability and debugging challenges in industrial digital hardware design, achieving significant advancements on RealBench benchmarks.


<details>
  <summary>Details</summary>
Motivation: Digital hardware design is labor-intensive and the manual effort required to translate specifications into RTL code is time-consuming. Existing LLM-based solutions struggle with scaling up for industrial-level tasks due to specific challenges like lengthy, detailed documents, long output code correctness, and debugging complexities.

Method: LocalV decomposes the task of generating RTL code by partitioning long documents into shorter, manageable tasks. It utilizes hierarchical partitioning, localized code generation, interface-consistent merging, and an AST-guided debugging process to improve scalability and reliability.

Result: LocalV demonstrates a significant improvement in generating RTL code compared to existing methods, achieving a 45.0% pass rate on the RealBench IP-level Verilog benchmark, surpassing the 21.6% rate achieved by state-of-the-art models.

Conclusion: LocalV offers a scalable and effective solution for RTL code generation by leveraging modularity and a multi-agent framework, addressing critical issues in scalability and debugging for industrial hardware design.

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [804] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: The paper introduces Kernelized Moment Balancing for Direct Forecasting (KMB-DF), which outperforms existing methods in deep time-series forecasting by addressing distribution imbalance issues.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge in time-series forecasting where existing methods fail to achieve true distribution balance, as they only enforce moment matching for limited preselected functions.

Method: Introduces KMB-DF, which utilizes kernelized moment balancing to adaptively select balancing functions from a reproducing kernel Hilbert space (RKHS), enabling efficient distribution alignment.

Result: Extensive experiments reveal that KMB-DF improves forecasting accuracy and achieves state-of-the-art results across multiple models and datasets.

Conclusion: KMB-DF overcomes limitations of current methods by achieving sufficient distribution balancing, enhancing forecasting performance, and is readily applicable in gradient-based training systems.

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [805] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: This paper surveys fairness in Federated Learning, classifying state-of-the-art fairness-aware approaches, identifying challenges, and exploring future research directions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical issue of fairness in Federated Learning, driven by heterogeneous clients and the need for balanced model performance.

Method: The authors classify fairness-aware approaches into performance- and capability-oriented categories, provide a framework for addressing fairness concerns, and discuss related evaluation metrics.

Result: A detailed classification, systematic framework, and significant evaluation metrics are presented for balancing equity and performance within FL frameworks.

Conclusion: The study lays a foundation for future research on fairness in FL by examining existing approaches, metrics, challenges, and potential solutions.

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [806] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: The study proposes a new continual learning strategy focusing on balancing task-specific updates in parameter adaptation using constrained optimization on the Stiefel manifold.


<details>
  <summary>Details</summary>
Motivation: The goal is to minimize forgetting in pre-trained models adapting to sequential tasks by analyzing critical properties of task-specific updates.

Method: The researchers use a constrained optimization formulation on a restricted Stiefel manifold, implemented via a projected first-order approach compatible with standard deep-learning optimizers.

Result: The method mitigates backward and forward forgetting while consistently outperforming existing continual learning baselines.

Conclusion: Balancing directional structure and update magnitude in task adaptation improves continual learning performance.

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [807] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: This paper introduces 'prompt multiplicity,' a framework for evaluating the consistency of Large Language Models (LLMs) to better assess hallucination-related harms.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs cause issues such as misinformation and erosion of trust, and current evaluation methods fail to address consistency as a key component.

Method: The authors propose 'prompt multiplicity' to measure consistency in LLM outputs and analyze its role in hallucination detection and mitigation strategies.

Result: The study shows significant inconsistencies (over 50% multiplicity) in benchmarks and finds that current techniques often fail to adequately address hallucinations.

Conclusion: Incorporating prompt multiplicity into evaluation frameworks improves understanding of hallucination harms and exposes limitations in current detection and mitigation strategies.

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [808] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: This paper introduces Pareto-Conditioned Diffusion (PCD) to solve offline multi-objective optimization challenges using conditional sampling strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Multi-objective optimization is essential in real-world scenarios, requiring trade-offs among objectives. Offline settings with static datasets pose a challenge for generalizing beyond observed data.

Method: The authors developed the Pareto-Conditioned Diffusion (PCD) framework, which uses a conditional sampling approach, focusing on desired trade-offs. It applies a reweighting strategy and reference-direction mechanism for exploring novel Pareto front regions.

Result: PCD demonstrates strong, competitive performance in standard offline MOO benchmarks, exhibiting greater consistency across diverse tasks compared to current approaches.

Conclusion: PCD is effective for offline multi-objective optimization, providing consistent results and a novel approach to discovering promising solutions beyond training data.

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [809] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: The paper discusses limitations of standard GNNs and explores interpolation-based methods like NNK for interpretable and potentially better predictions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the interpretability and generalization of GNNs by addressing the limitations of parametric classifiers commonly used in them.

Method: The method involves using interpolation-based approaches, specifically Non-Negative Kernel regression (NNK), to achieve predictions that are convex combinations of similar training examples.

Result: The approach offers theoretical results and enhances interpretability in the embedding space.

Conclusion: Utilizing NNK enhances the explainability and potentially the performance of GNNs by leveraging interpolation-based techniques.

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [810] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: The study addresses emergent misalignment in fine-tuned language models by targeting internal features that control misaligned behavior and constraining them during training, reducing misalignment without harming performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the issue of emergent misalignment in language models fine-tuned on specific tasks, which leads to undesirable out-of-domain behaviors despite achieving task objectives.

Method: The approach identifies and constrains internal features responsible for misaligned behavior, using multiple evaluation methods and extensive validation techniques to ensure effectiveness.

Result: The constrained approach reduced emergent misalignment by up to 95% across six domains without affecting model quality or task performance, along with further insights into issues like prolonged fine-tuning effects.

Conclusion: Constrained fine-tuning of internal features can effectively mitigate emergent misalignment in language models without sacrificing target-task performance, providing a robust mechanism for improving alignment.

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [811] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: The paper introduces Model Provenance Set (MPS) to tackle unauthorized model usage by formalizing provenance analysis with provable guarantees, ensuring rigorous and confident identification of model sources.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing problem of unauthorized model usage and misattribution by proposing a system that overcomes the limitations of heuristic fingerprint-matching methods, improving the reliability and accuracy of provenance analysis.

Method: The authors introduce the Model Provenance Set (MPS), which uses a sequential test-and-exclusion procedure to identify model sources with provable asymptotic guarantees at a user-specified confidence level.

Result: MPS demonstrates effective provenance coverage by rigorously including true model sources and limiting unrelated models through extensive experiments, showcasing its potential for attribution and auditing tasks.

Conclusion: MPS improves the reliability of model provenance analysis and provides practical applications for ensuring the integrity and attribution of models in cases involving multiple sources.

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [812] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: This paper investigates how equity balance affects greenwashing behavior in the mining industry, using innovative methods like Variational Autoencoder and Double Machine Learning models to identify causal relationships.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the urgent need to address greenwashing in the mining industry due to its resource-intensive nature and significant environmental impact, aligning with global green transformation goals.

Method: The research uses Variational Autoencoder (VAE) and Double Machine Learning (DML) to construct counterfactual scenarios, exploring the causal effects of equity balance on greenwashing.

Result: The study finds a negative causal link between equity balance and greenwashing. The effect varies by region, industry position, and environmental sensitivity, with notable temporal dynamics.

Conclusion: Equity balance effectively reduces greenwashing through mechanisms like reducing management pressure, stabilizing leadership, and increasing media oversight, contributing to better environmental governance in the industry.

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [813] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: The paper proposes a stable temporal prediction framework to address challenges in accurate enterprise carbon emission trend prediction due to significant data heterogeneity and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the accuracy and reliability of carbon emission predictions for enterprises to support energy optimization and low-carbon transformations amidst significant regional and industrial heterogeneity.

Method: The approach involves integrating causal inference, stable learning methodologies, and time-series modeling. It incorporates enterprise-level factors and uses a risk consistency-constrained framework alongside adaptive normalization and reweighting to handle distribution shifts.

Result: The proposed mechanism improves model generalization and explainability by extracting causally stable features and rectifying temporal non-stationarity in carbon emission data.

Conclusion: Stable learning methodologies combined with causal inference can effectively address the challenges of non-stationarity and distribution shifts in carbon emission predictions for better decision-making in energy and carbon management.

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [814] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: The paper proposes RESCUE, a new method using causal Bayesian optimization to improve efficiency in solving black-box optimization problems with multi-fidelity data.


<details>
  <summary>Details</summary>
Motivation: Existing MFBO methods struggle in scenarios where low-fidelity data is poorly aligned with high-fidelity targets, limiting their effectiveness in various applications.

Method: The authors introduce RESCUE, which employs a structural causal model to capture relationships between parameters, fidelities, and objectives, using causal calculus to enhance optimization. A "causal hypervolume knowledge-gradient" strategy ensures efficient input-fidelity pair selection.

Result: RESCUE demonstrates superior sample efficiency compared to other state-of-the-art MFBO methods, validated through experiments in robotics, AutoML, and healthcare domains.

Conclusion: RESCUE effectively progresses toward high-fidelity global optima by leveraging causal understanding, making it highly efficient for resource-sensitive optimization tasks.

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [815] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: The paper proposes Sporadic Gradient Tracking (Spod-GT), a novel decentralized federated learning algorithm addressing client-specific computation and communication frequencies over directed graphs, showcasing its efficiency in mitigating challenges like data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To solve challenges in decentralized federated learning related to data heterogeneity and clients' resource diversity, and unify existing approaches for better collaboration among clients.

Method: Developed Spod-GT algorithm for decentralized federated learning, enabling flexible computation and communication frequencies among clients, along with rigorous theoretical analysis of its convergence under relaxed assumptions.

Result: Demonstrated through experiments on image classification datasets that Spod-GT achieves improved model performance and efficiency compared to traditional gradient tracking methods.

Conclusion: Spod-GT effectively addresses key challenges of decentralized federated learning, providing robust theoretical guarantees and practical improvements in diverse client environments.

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [816] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: This paper introduces Masked Consistency Distillation (MCD), achieving significant inference speedup (16x) in masked discrete diffusion language modeling while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: To improve the inference efficiency of masked discrete diffusion models, which are hindered by the lack of deterministic sampling methods and reliance on stochastic distillation.

Method: The authors establish a Masked Diffusion Duality by showing masked processes as projections of continuous Gaussian processes using a maximum-value index preservation mechanism. They propose MCD for deterministic trajectory construction without needing numerical solvers.

Result: MCD achieves a 16x inference speedup over prior methods while maintaining the quality of generation, demonstrating theoretical and practical advancements.

Conclusion: This work provides a theoretical link between masked and continuous diffusion and demonstrates an effective, efficient framework (MCD) for discrete language generation, improving performance and speed.

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [817] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: This paper introduces token-indexed parameters (JTok and JTok-M) to improve LLM scalability with reduced computational costs and validation loss, achieving better performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the computational inefficiency of traditional dense and MoE architectures by introducing a novel scaling mechanism decoupling computational capacity from FLOPs.

Method: The proposed JTok and JTok-M architectures retrieve modulation vectors from embedding tables for token-level operations with minimal memory and computational overhead.

Result: Extensive experiments demonstrate reduced validation loss and improved downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval) with 35% less compute compared to vanilla MoE architectures.

Conclusion: Token-indexed parameters enhance model quality and scalability while maintaining efficiency, offering a new direction for LLM research.

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [818] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: The paper presents a proof-of-concept game that uses smartphone sensors and machine learning for human activity recognition and voice-based gameplay.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of smartphone sensors and machine learning in human activity recognition applications, specifically in gaming.

Method: The system integrates smartphone accelerometer, gyroscope, and magnetometer sensors for activity recognition and uses voice recognition to enhance gameplay complexity.

Result: The activity recognition system demonstrated high levels of accuracy, and combining movement-based and voice-based inputs improved gameplay immersion.

Conclusion: Machine learning techniques using smartphone sensors can effectively recognize human activities, and integrating voice recognition enriches user experience in gaming scenarios.

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [819] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow is an efficient multimodal generative model combining coarse MeanFlow and a noise-injection refinement step, achieving high-quality results across various generation tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of single-function evaluation (1-NFE) in MeanFlow for generating compelling outputs efficiently.

Method: Introduced RMFlow, an approach combining MeanFlow with a noise-refinement step, leveraging a neural network with a novel loss function optimizing both Wasserstein distance and sample likelihood.

Result: RMFlow produces near state-of-the-art results in text-to-image, context-to-molecule, and time-series generation using efficient 1-NFE computation.

Conclusion: RMFlow offers a computationally affordable solution for high-fidelity image generation and other tasks by addressing 1-NFE limitations of MeanFlow.

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [820] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: The paper explores knowledge distillation techniques, specifically focusing on challenges caused by datasets with spurious correlations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issues surrounding subtask distillation when relying on imperfect datasets which exhibit spurious correlations, as deployment often targets environments with limited resources.

Method: The authors evaluate established distillation methods such as SubDistill and baseline approaches under conditions of increasing spurious correlations in the dataset.

Result: SubDistill proves to be more robust under such conditions compared to baseline methods, which significantly deteriorate as spurious correlations intensify.

Conclusion: The study highlights the vulnerability of knowledge distillation to imperfect datasets, emphasizing the need for robust distillation methods like SubDistill in real-world scenarios.

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [821] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: The paper introduces a hierarchical graph-based learning framework for protein structures using multiscale GNNs, which improves accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods struggle with multiscale representations and efficiently modeling long-range dependencies in protein structures.

Method: The framework creates hierarchical graphs with fine-grained subgraphs for local structures (secondary motifs) and coarse-grained graphs capturing global spatial relationships. Two GNNs operate at different scales enhancing flexibility and expressiveness.

Result: The multiscale framework showed improved prediction accuracy and computational efficiency on various benchmarks.

Conclusion: This proposed framework is an efficient tool for protein structure learning, addressing limitations of existing GNN models and ensuring no loss of critical information.

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [822] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: This paper presents an improved objective function for flow-based generative models, enhancing their performance without reducing efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortcomings of Conditional Flow Matching (CFM) in accurately learning probability paths, despite its efficiency and strong performance in data generation.

Method: The authors introduce a partial differential equation framework to analyze errors in probability paths, derive bounds for total variation gaps, and propose a new objective function integrating flow and its divergence.

Result: The new approach demonstrates noticeable performance gains over standard CFM across tasks like modeling dynamical systems, DNA sequences, and videos.

Conclusion: The proposed method enhances flow-based generative models' accuracy and performance while maintaining computational efficiency, benefiting diverse generative modeling applications.

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [823] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: The paper introduces an SSV training framework for neural operators, demonstrating improved long-time accuracy and stability in predicting dynamics for heat-based equations.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and stability of neural networks in extrapolating long-time dynamics for heat-based equations.

Method: The authors developed an SSV training framework compatible with neural-operator training and tested it on two systems using fully connected architectures.

Result: SSV-trained networks showed enhanced extrapolation capabilities and better long-time dynamics predictions compared to models trained in physical coordinates.

Conclusion: Using self-similar coordinates introduces a beneficial inductive bias, improving the performance of neural networks for heat-based equations' long-time dynamics.

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [824] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: Dynamic Expert Sharing (DES) improves the integration of diffusion large language models (dLLMs) with Mixture-of-Experts (MoE) architectures, reducing memory bottlenecks while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The integration of diffusion large language models (dLLMs) with Mixture-of-Experts (MoE) faces challenges due to memory inefficiency, as increasing the number of tokens generated activates more experts, resulting in memory-bound inefficiencies.

Method: The authors propose Dynamic Expert Sharing (DES), a technique that leverages sequence-level coreset selection to optimize expert usage in parallel decoding. Two strategies—Intra-Sequence Sharing (DES-Seq) and Saliency-Aware Voting (DES-Vote)—are introduced to enhance expert reuse.

Result: DES reduces unique expert activations by over 55% and latency by up to 38%, while maintaining 99% of the original model's accuracy.

Conclusion: DES effectively addresses the memory bottlenecks of parallel decoding in dLLMs using MoE while preserving performance, enabling efficient scaling of these models.

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [825] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: The paper introduces a method enhancing zero-shot generalization of neural operators for PDEs by combining pretrained operators using a neural operator splitting strategy.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of neural operators in generalizing to unseen scenarios such as new initial conditions or physics without requiring large-scale fine-tuning.

Method: Proposes a neural operator splitting strategy that composes pretrained neural operators at test time to approximate new dynamics leveraging the DISCO framework.

Result: Demonstrates state-of-the-art zero-shot generalization performance in tasks involving parameter extrapolation and novel physics combinations.

Conclusion: Test-time computation strategies are crucial for developing flexible and generalizable neural operator models.

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [826] [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)](https://arxiv.org/abs/2211.11434)
*Lucas Lange,Maja Schneider,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: This study investigates improving machine learning models for COVID-19 while maintaining patient privacy using Differential Privacy (DP). It evaluates privacy-utility trade-offs and empirically tests the model's privacy strength using Membership Inference Attacks (MIAs).


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of strong privacy guarantees in existing private COVID-19 models while also tackling data limitations and practical threats posed by MIAs.

Method: The research applies Differential Privacy (DP) techniques to ML models for COVID-19 classification, evaluates class imbalances, tests privacy on stricter budgets, and employs Membership Inference Attacks to assess practical privacy.

Result: Empirical privacy leakage from MIAs only marginally decreases with stronger DP guarantees. Practical privacy is shown to vary based on task-specific threats. Utility-privacy trade-off improvements are identified.

Conclusion: DP provides limited practical defense against MIAs, but empirical privacy estimation tailored to specific attacks can enhance privacy tuning and achieve better utility-privacy trade-offs.

Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.

</details>


### [827] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: The paper introduces ProbDPP, a novel method for data selection in large language models under uncertain conditions, addressing inefficiencies in traditional methods.


<details>
  <summary>Details</summary>
Motivation: To develop a robust data selection approach for large language models that accommodates computational constraints, storage outages, and stochastic failures, avoiding the collapse of traditional methods.

Method: Introduced ProbDPP, a reliability-aware k-DPP variant, which includes a regularization term for probabilistic data access and uses a UCB-style algorithm for learning reliability online.

Result: ProbDPP ensures robust selection, decomposes the objective into diversity and unreliability cost, and provides theoretical regret bounds for its performance.

Conclusion: ProbDPP improves data subset selection under uncertainty, enabling efficient fine-tuning and deployment of language models while guaranteeing theoretical reliability.

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [828] [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](https://arxiv.org/abs/2401.13327)
*Lucas Lange,Nils Wenzlitschke,Erhard Rahm*

Main category: cs.LG

TL;DR: This study leverages Generative Adversarial Networks (GANs) with Differential Privacy (DP) safeguards to generate synthetic smartwatch health data for stress detection, improving data availability and protection.


<details>
  <summary>Details</summary>
Motivation: The growing use of sensitive smartwatch health data in applications like stress detection necessitates privacy preservation and overcoming data scarcity for research purposes.

Method: The authors generate synthetic multi-sensor smartwatch data using GANs and employ Differential Privacy safeguards to ensure the data's security and integrity. They test synthetic data in stress detection tasks with various GANs and enhancement strategies.

Result: GAN-based methods significantly enhanced stress detection model performance, with private training scenarios achieving 11.90-15.48% F1-score improvements and non-private scenarios achieving a 0.45% boost.

Conclusion: Differentially private synthetic data effectively balances privacy and utility, but stricter privacy constraints can impact data quality.

Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.

</details>


### [829] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: The paper introduces GAPNet, a framework for improving financial forecasting by dynamically adapting graph structures and representations in a task-specific manner. It outperforms state-of-the-art models on two datasets.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of predefined graphs in financial forecasting caused by noisy and asynchronous stock-related web signals and their poor alignment with downstream tasks.

Method: The proposed GAPNet is a plug-in network that learns task-specific graph structures and representations through a Spatial Perception Layer for short-term co-movements and a Temporal Perception Layer for long-term dependencies. It can be attached to existing graph or hypergraph models.

Result: GAPNet shows superior performance over state-of-the-art models, achieving annualised cumulative returns of up to 0.63 and peak Sharpe Ratios above 2.0 on real-world stock datasets, showcasing profitability and stability.

Conclusion: The study demonstrates the importance of jointly learning graph structures and task-specific representations, highlighting GAPNet's applicability to diverse graph-based architectures in financial forecasting.

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [830] [Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning](https://arxiv.org/abs/2409.01329)
*Lucas Lange,Maurice-Maximilian Heykeroth,Erhard Rahm*

Main category: cs.LG

TL;DR: This study examines the effects of image dataset characteristics on privacy-preserving CNN models, highlighting factors that affect utility and vulnerability while offering insights to optimize the utility-privacy trade-off.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to address security and privacy challenges when machine learning models are trained on sensitive data, especially in computer vision.

Method: The analysis focuses on examining various image datasets and privacy budgets, and studying how characteristics like class imbalance, entropy, and Fisher Discriminant Ratio (FDR) impact the utility and privacy of CNN models under Differential Privacy (DP).

Result: The study finds that datasets with imbalanced classes are more vulnerable, particularly for minority classes, but this can be mitigated by DP. Additionally, datasets with fewer classes improve model utility and privacy, while higher entropy or lower FDR negatively affects the utility-privacy balance.

Conclusion: The findings provide actionable recommendations for researchers and practitioners to enhance both utility and privacy in machine learning models by considering dataset characteristics, aiding in effective decision-making for privacy modifications.

Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.

</details>


### [831] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: The paper introduces a dense retrieval system for e-commerce recommendation focusing on natural-language intent instead of sparse keyword matching, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of sparse keyword matching in e-commerce recommendations, especially in handling vocabulary mismatches where user intent lacks lexical overlap with product metadata.

Method: The authors propose a dense retrieval system using a two-tower bi-encoder fine-tuned with supervised contrastive learning on Amazon Reviews 2023 (Fashion) data. Training pairs are constructed from review text and item metadata, and optimizations in model inference are achieved via FAISS HNSW indexing combined with INT8 dynamic quantization.

Result: The proposed system significantly improves Recall@10 from 0.26 (BM25) to 0.66 for a review-to-title benchmark while maintaining low latency (6.1 ms median CPU inference latency) and reducing model size by four times.

Conclusion: The paper provides a reproducible framework to implement and deploy domain-specific dense retrieval systems for e-commerce that are both scalable and efficient, demonstrating marked performance gains over traditional methods.

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [832] [Federated Learning With Individualized Privacy Through Client Sampling](https://arxiv.org/abs/2501.17634)
*Lucas Lange,Ole Borchardt,Erhard Rahm*

Main category: cs.LG

TL;DR: This paper addresses individualized privacy preferences in Federated Learning (FL) by introducing an adapted Individualized Differential Privacy (IDP) method with personalized sampling rates, showing improved privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: To accommodate diverse user privacy preferences rather than enforcing uniform privacy levels, and to improve the privacy-utility trade-off in FL systems.

Method: The authors extend the SAMPLE algorithm for FL by determining client-specific sampling rates based on privacy budgets and propose a modified IDP-FedAvg algorithm.

Result: The method outperforms uniform DP baselines and the SCALE method, enhancing privacy and utility trade-offs, but struggles with complex non-i.i.d. data in decentralized settings.

Conclusion: The proposed IDP method is promising for improving privacy-utility trade-offs in FL, though additional work is needed for non-i.i.d. and complex datasets in decentralized environments.

Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.

</details>


### [833] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: This paper formalizes and investigates the problem of LLMs hallucinating facts as a membership testing problem under limited capacity, explaining hallucinations as an inherent consequence of lossy compression.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the persistent issue of large language models (LLMs) confidently hallucinating random, unverifiable facts, which lack discernible patterns.

Method: The authors analyze hallucination through a theoretical framework based on rate-distortion theorem and membership testing, and validate their insights empirically with synthetic data.

Result: The analysis demonstrates that even under optimized conditions (perfect training, data, and idealized settings), hallucination arises inherently due to the trade-offs enforced by lossy compression in limited-capacity models.

Conclusion: Hallucination in LLMs is an inevitable consequence of optimal strategy under memory and capacity constraints, even in the most idealized conditions.

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [834] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: The paper introduces a method to handle random delays in reinforcement learning by reformulating the problem, enabling the use of existing algorithms designed for constant delays.


<details>
  <summary>Details</summary>
Motivation: Random delays in reinforcement learning break the Markov assumption, making decision-making challenging and less studied compared to constant delays.

Method: The proposed agent reformulates random-delay environments into constant-delay equivalents, allowing the use of state-of-the-art constant-delay methods directly.

Result: The method achieves improved asymptotic performance and sample efficiency in continuous control tasks compared to baseline algorithms.

Conclusion: Reformulating random delays as constant delays simplifies the use of existing methods while improving decision-making performance.

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [835] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX is a Python tool for geospatial analysis that integrates AutoML and XAI for analyzing spatial heterogeneity and improving interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance geospatial analysis by addressing spatial non-stationarity and making advanced machine learning methods accessible and interpretable.

Method: PyGALAX utilizes automated machine learning (AutoML) and SHAP for model selection, optimization, and interpretability. It introduces automatic bandwidth and flexible kernel function selection.

Result: PyGALAX improves upon the GALAX framework, providing greater flexibility and robustness, and outperforms traditional methods like geographically weighted regression (GWR).

Conclusion: PyGALAX is a comprehensive and accessible Python toolkit that enhances spatial modeling and interpretability, catering to diverse disciplines like geography and environmental science.

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [836] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: The paper reviews efficient and lightweight deep learning architectures for medical image analysis, focusing on improving deployment in clinical environments.


<details>
  <summary>Details</summary>
Motivation: Deploying large-scale deep learning models in real-world clinical settings faces challenges like high computational costs, latency, and patient data privacy concerns.

Method: The paper categorizes efficient deep learning architectures into three streams (CNNs, Lightweight Transformers, Linear Complexity Models) and discusses model compression techniques like pruning, quantization, knowledge distillation, and low-rank factorization.

Result: The review synthesizes strategies to reduce hardware requirements while maintaining diagnostic performance, paving the way for efficient deployment.

Conclusion: The review provides a roadmap for transitioning toward on-device intelligence in resource-constrained clinical environments, addressing current limitations.

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [837] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: The paper examines Early Classification of Time Series (ECTS) under scenarios where decision costs are uncertain and fluctuate over time. It integrates online learning approaches, including RL and bandit methods, to enhance the robustness of ECTS against cost non-stationarity.


<details>
  <summary>Details</summary>
Motivation: To tackle the practical challenge in ECTS where time-dependent decision costs are uncertain, change over time, and often mismatch between training and deployment objectives.

Method: The authors studied two types of cost non-stationarity in ECTS, adapted existing methods for an online learning setting by updating only the triggering model, proposed RL and bandit-based online strategies, and conducted controlled synthetic experiments.

Result: The proposed online learning methods, particularly RL-based strategies, demonstrated strong and stable performance under varying cost regimes, improving robustness to cost drift.

Conclusion: Introducing and adapting online learning approaches can effectively handle cost uncertainties in ECTS, ensuring robust and accurate early classification in practical settings.

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [838] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: This paper identifies a phenomenon where increasing training-time reasoning length improves out-of-distribution (OOD) performance even after in-distribution (ID) performance has saturated.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art models need improved reasoning length to solve complex problems. The motivation is to understand how training-time reasoning length impacts performance, especially for robustness and generalization.

Method: The study explores scaling training-time reasoning length through looped Transformers and RL fine-tuning, providing theoretical explanations and empirical validation for its impact.

Result: It was found that OOD performance can improve with increased reasoning length, even after ID performance saturates, due to stronger inductive biases and reduced reliance on shortcut solutions.

Conclusion: Robustness and generalization require embracing larger reasoning budgets beyond traditional ID validation methods.

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [839] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: The research introduces Continuous Utility Direct Preference Optimization (CU-DPO) to improve model alignment on reasoning tasks by training with continuous scores instead of binary preferences, leading to better reasoning quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of binary preference supervision in large language model reasoning, enabling more fine-grained evaluation and improvement of reasoning quality.

Method: The authors propose CU-DPO, which uses a two-stage training pipeline. First, strategy selection trains models to choose the best cognitive strategy for a problem. Second, execution refinement trains models to correctly implement the chosen strategy using margin-stratified pairs. Continuous scores replace binary labels to improve learning efficiency.

Result: The approach improves strategy selection accuracy significantly (from 35–46% to 68–78%) across seven base models on mathematical reasoning benchmarks. It also delivers consistent reasoning performance gains of up to 6.6 points on in-distribution datasets and adapts well to out-of-distribution tasks.

Conclusion: CU-DPO effectively enhances reasoning capabilities of large language models by leveraging fine-grained continuous scores, improving both strategy selection accuracy and downstream reasoning quality.

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [840] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: The paper presents SALAAD, a framework for sparsity and low-rank modeling, enabling efficient model scaling without retraining.


<details>
  <summary>Details</summary>
Motivation: To address memory and compute constraints in deploying large language models by introducing an adaptable and controlled framework for managing model capacity.

Method: Proposed SALAAD, a training framework combining sparse and low-rank structures with an augmented Lagrangian approach and adaptive controllers.

Result: SALAAD significantly reduces memory usage and achieves performance close to existing approaches, offering a spectrum of model capacities from a single training process.

Conclusion: SALAAD is a generic, efficient solution for dynamically managing model capacity during training, enhancing applicability across diverse constraints.

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [841] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: The paper proposes a method called Dynamic Prior Thompson Sampling (DPTS) that allows precise control over exploration in recommender systems, overcoming issues with current initialization methods in large-scale systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of inefficient exploration in cold-start scenarios for recommender systems. Existing methods, like Thompson Sampling with uniform priors, result in wasted resources and suboptimal recommendations when base success rates for new items are low.

Method: The authors develop Dynamic Prior Thompson Sampling, which adjusts priors based on a closed-form quadratic solution to control exploration. This ensures the probability of a new item outperforming the incumbent winner is predictable and tunable.

Result: The method was validated using Monte Carlo simulations, offline experiments, and a large-scale online test in a thumbnail personalization system, showing improved exploration efficiency and control compared to traditional methods.

Conclusion: Dynamic Prior Thompson Sampling offers a practical and tunable way to balance exploration and exploitation in recommender systems, particularly during cold-start scenarios, significantly enhancing system performance.

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [842] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: The paper proposes a framework for budget-aware supervised fine-tuning of large language models (LLMs), focusing on label efficiency and optimizing learning under supervision budget constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing downstream accuracy while considering the availability of labeled data in fine-tuning large language models.

Method: The framework uses a contextual Stackelberg game approach, where the learner commits to scoring and querying strategies, while the environment reacts adaptively. It introduces a finite supervision budget into the learning objective and proposes algorithms with regret bounds under specific assumptions.

Result: The algorithm achieves regret bounds of $\tilde{O}(d\sqrt{T})$ under linear contextual assumptions and $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ in budget-aware settings with selective label querying using the LLF confidence gate.

Conclusion: The framework effectively balances accuracy improvement and label efficiency within predefined budget constraints, providing a principled approach to supervised fine-tuning of LLMs.

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [843] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: This paper introduces SAGE, an AI system for computational pathology that identifies interpretable biomarkers through biological evidence and multimodal data.


<details>
  <summary>Details</summary>
Motivation: The difficulty in interpreting black-box AI models in pathology limits their clinical adoption, prompting the search for interpretability through biologically validated biomarkers.

Method: SAGE integrates literature-based reasoning with multimodal data to connect image-derived features to molecular biomarkers (e.g., gene expression) and clinically relevant outcomes.

Result: SAGE successfully facilitates the identification of transparent and biologically supported pathology biomarkers, enhancing the relevance of computational pathology in clinical settings.

Conclusion: SAGE offers a systematic, biologically grounded approach to hypothesis generation and validation, promoting the development of clinically interpretable AI models in pathology.

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [844] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: The paper examines transfer learning techniques for updating failed ML models under data drift, focusing on a case study from a thermal power plant. ETL and ALTL methods are compared for model accuracy across different batch sizes.


<details>
  <summary>Details</summary>
Motivation: Reliable MLOps requires effective updating strategies for ML models impacted by data drift in production environments.

Method: The paper compares three transfer learning methods—ETL, ALTL, and LLTL—to update feedforward ANN models using a case study from flue gas differential pressure monitoring in power plants.

Result: ETL demonstrated higher predictive accuracy for small batch sizes (5 days) while ALTL performed better for larger batch sizes (8 days). Computational requirements varied across methods and batch sizes.

Conclusion: The findings offer insights for MLOps practitioners in handling ML model failures due to data drifts, aiding in improving industrial process monitoring.

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [845] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: The paper presents a framework to systematically analyze the knowledge contained in large language models, revealing various patterns in knowledge scaling, exploration strategies, and performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: There is limited understanding of the scope and depth of knowledge contained within large language models, and existing tools for knowledge probing are mostly static and insufficient.

Method: The authors introduce an interactive agentic framework with four adaptive exploration policies and a three-stage knowledge processing pipeline for filtering duplicates, resolving semantic overlaps, and auditing relevance.

Result: The study finds that recursive taxonomy exploration works best, larger models extract more knowledge consistently, and discrepancies in domain-specific versus general-purpose models indicate trade-offs and knowledge scaling trends.

Conclusion: The proposed method demonstrates how these models' knowledge composition varies significantly depending on training data and model families, offering deeper insights into their structure and capabilities.

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [846] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: The paper analyzes why low precision (quantization) training harms Large Language Models' performance and proves its link to data spectral properties, with theoretical and empirical backing.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why ultra-low precision training of Large Language Models becomes unstable due to conflicts between quantization constraints and the natural spectral characteristics of linguistic data.

Method: The authors use theoretical derivations linking Zipfian statistics, random matrix theory, and power-law behavior in singular value spectra, alongside empirical tests on models like GPT-2 and TinyLlama.

Result: They show that quantization noise truncates spectral tails, flattens spectra, and raises stable ranks, leading to a degradation in semantic representations and model performance.

Conclusion: The paper concludes that preserving spectral fidelity is essential for stable low-bit optimization of Large Language Models and highlights the limitations of current quantization strategies.

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [847] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: The paper introduces FoSTA, a method to align multimodal datasets using forest geometry to address issues in current methods relying on Euclidean geometry.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods using Euclidean geometry struggle when features are weakly related to the task, leading to noisy and poor alignment.

Method: FoSTA applies forest-guided geometry for denoising and recovering task-relevant manifolds, then aligns representations using hierarchical semantic transport.

Result: FoSTA improves correspondence recovery and label transfer in synthetic benchmarks, excelling in biological tasks like batch correction and conservation.

Conclusion: FoSTA effectively enhances alignment quality in multimodal datasets by addressing Euclidean geometry limitations with forest-guided semantic transport.

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [848] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: The paper introduces Random Wavelet Features (RWF), a method for scalable, non-stationary kernel approximations focused on addressing challenges where statistical properties vary across domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of current scalable kernel methods, which are restricted by the assumption of stationarity and face trade-offs between computational complexity and expressiveness in modeling non-stationary processes.

Method: The authors propose a framework called Random Wavelet Features (RWF), which employs wavelet families to construct explicit feature maps, effectively capturing input-dependent patterns and extending Random Fourier Features to non-stationary settings.

Result: RWF achieves superior performance compared to traditional stationary random features and offers a favorable accuracy-efficiency balance over more complex models in both synthetic and real-world datasets.

Conclusion: RWF provides a scalable, expressive alternative for non-stationary kernel methods, backed by theoretical guarantees and practical results on diverse datasets.

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [849] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: Introduction of ESSAM framework to reduce GPU memory usage while improving mathematical reasoning accuracy in LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address high GPU memory usage in RL methods employed for fine-tuning LLMs for mathematical reasoning.

Method: Combines Evolution Strategies (ES) for zero-order parameter search with Sharpness-Aware Maximization (SAM) for improved generalization.

Result: ESSAM achieves 78.27% accuracy on GSM8K task, surpassing or matching PPO (77.72%) and GRPO (78.34%) while considerably reducing GPU memory usage.

Conclusion: ESSAM offers a promising alternative to RL methods for fine-tuning LLMs, balancing performance with resource efficiency.

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [850] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: The study evaluates childhood anemia prediction in Nepal using ML/DL models and identifies key features impacting anemia risk.


<details>
  <summary>Details</summary>
Motivation: The study addresses the persistent public health issue of childhood anemia in Nepal, highlighting its adverse effects on growth, cognition, and morbidity.

Method: Analyzing microdata from 1,855 children in Nepal (NDHS 2022), the study applied feature selection techniques (e.g., Chi-square, Boruta) to identify key factors and compared traditional ML and DL models for anemia prediction.

Result: Key predictive features like child age, maternal anemia, and deworming were found. Logistic regression achieved the best F1-score and recall, while SVM had the highest AUC, indicating effective anemia prediction.

Conclusion: ML and DL models are viable for anemia risk prediction and screening, with practical insights for public health strategies in Nepal.

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [851] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: The paper introduces the LASS-ODE model for dynamic physical systems, addressing scalability and efficiency issues in physics-informed learning by leveraging locally linear ODE representations and shared attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and knowledge-sharing efficiency in foundation models for dynamic physical systems, particularly addressing limitations in nonlinear ODE integration and localized attention mechanisms.

Method: The authors propose locally linear ODE token representations for scalable integration and a common structure hub for shared attention across systems, enhancing both computational efficiency and generalization.

Result: The LASS-ODE model, pretrained on a large ODE trajectory dataset, demonstrates strong in-domain performance, zero-shot generalization, and further fine-tuning improvements.

Conclusion: Utilizing locally linear ODE models combined with shared attention mechanisms leads to scalable, efficient, and generalizable models for dynamic physical systems.

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [852] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: This paper investigates the faithfulness of chain-of-thought (CoT) reasoning in large language models (LLMs) using synthetic arithmetic experiments, highlighting how training noise influences reasoning fidelity.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental mechanisms behind faithful chain-of-thought reasoning in language models and explore the impact of training noise and autoregressive dynamics.

Method: Synthetic experiments involving training small transformer models on modular arithmetic tasks with varying levels of training noise to evaluate the impact on reasoning fidelity.

Result: Models demonstrate faithful reasoning at low training noise levels but transition to unfaithful skip-step reasoning at higher noise levels, with an intermediate mixed mode showing increased prediction entropy.

Conclusion: Training noise critically influences whether LLMs produce faithful step-by-step reasoning or skip-steps in CoT reasoning, revealing implicit self-verification as a mechanism emerging from autoregressive training.

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [853] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: The paper introduces UltraBreak, a robust method for crafting transferable and universal adversarial patterns to jailbreak vision-language models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To address the limited transferability and generalization of existing gradient-based jailbreak methods in multimodal models.

Method: UltraBreak constrains adversarial patterns using vision-space transformations and regularization, while employing semantic loss in the textual embedding space to enhance generalization across diverse surrogate and black-box settings.

Result: UltraBreak reliably surpasses previous jailbreak methods, achieving higher robustness and transferability across varying models and objectives through smoothing the loss landscape and leveraging semantic-level objectives.

Conclusion: UltraBreak advances adversarial attack techniques by significantly improving the transferability of jailbreak patterns in vision-language models, pinpointing deficiencies in prior methods, and providing a publicly available implementation for broader use.

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [854] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: SFMP introduces a hardware-friendly and search-free mixed-precision quantization framework for large language models, addressing memory constraints efficiently.


<details>
  <summary>Details</summary>
Motivation: There is a need to compress large language models within tight memory budgets, avoiding inefficiencies in hardware and reliance on expensive optimization processes.

Method: SFMP utilizes four innovations: fractional bit-width allocation, block-wise mixed-precision, row-column weight reordering for efficient weight aggregation, and a unified matrix multiplication kernel to enable efficient mixed-precision at any bit-width.

Result: SFMP outperformed existing layer-wise mixed-precision methods under the same memory limitations, with lower quantization costs and higher inference efficiency.

Conclusion: SFMP provides an effective and efficient solution for mixed-precision quantization of large language models, maintaining both hardware efficiency and model performance.

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [855] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: This paper presents FLood, a federated learning framework addressing issues with non-IID data by using a dual-weighting mechanism for robust training and model aggregation, yielding superior accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To tackle the adverse effects of data heterogeneity in federated learning due to non-IID data, which negatively impacts convergence, generalization, and service quality.

Method: A dual-weighting mechanism: locally reweighting challenges samples to prioritize robust learning and globally weighting client contributions based on OOD confidence for improved aggregation.

Result: Extensive experiments show that FLood outperforms existing federated learning methods in accuracy and generalization, even under diverse data heterogeneity settings.

Conclusion: FLood offers a scalable and effective solution to enhance federated learning performance in heterogeneous environments, and it integrates seamlessly with existing algorithms.

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [856] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: Introduces a framework for T cell receptor analysis addressing sparsity, heterogeneity, and computational challenges, using a prototype-based parameterization for efficient task adaptation.


<details>
  <summary>Details</summary>
Motivation: To enable effective disease detection and immune monitoring by overcoming challenges such as sparse labels, cohort heterogeneity, and computational inefficiencies in T cell receptor analysis.

Method: Develops a framework utilizing lightweight task descriptors and pretrained backbone with small adapter modules, avoiding the need for full model fine-tuning.

Result: The framework enables rapid adaptation to new tasks using minimal labeled data, while maintaining model interpretable signals and calibrated motif discovery.

Conclusion: The proposed approach provides a practical, efficient, and interpretable solution for deploying repertoire-informed models in clinical and research settings facing data and computational constraints.

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [857] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent introduces an innovative KV cache sharing framework for multi-LoRA agents, reducing memory and compute overheads while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inefficiency in cache handling for multi-LoRA agents, where memory and compute overhead arise due to unshared KV caches despite shared pretrained backbone weights.

Method: LRAgent proposes decomposing the cache into shared base components (pretrained weights) and adapter-dependent components stored in low-rank form. It avoids redundant computations using shared-$A$ architectures and introduces Flash-LoRA-Attention for efficient runtime reconstruction of adapter contributions.

Result: LRAgent successfully reduces memory consumption and computational costs, maintaining near-baseline accuracy and achieving fast throughput and latency in agentic QA tasks.

Conclusion: With LRAgent, systems can achieve efficient resource utilization while delivering high performance in multi-LLM agent setups leveraging multi-LoRA.

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [858] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: The paper identifies a mismatch between supervised fine-tuning (SFT) and reinforcement learning (RL) in post-training of reasoning language models, proposing a new method, PEAR, to address the issue and improve overall performance.


<details>
  <summary>Details</summary>
Motivation: Current SFT-RL pipelines are not holistically aligned since SFT is optimized in isolation, leading to performance mismatches during RL. This can cause weaker SFT results to paradoxically perform better after RL.

Method: The authors introduce PEAR, a reweighting algorithm for SFT loss using importance sampling to mitigate the SFT-RL mismatch. PEAR operates at token, block, and sequence levels to better prepare models for RL training.

Result: PEAR consistently enhances post-RL performance across reasoning and mathematical datasets. Gains include significant improvement, such as a 14.6% 'pass at 8' increase on the AIME2025 dataset.

Conclusion: Designing SFT with downstream RL considerations improves holistic post-training. PEAR achieves this with minimal overhead, demonstrating the importance of aligning the pre-RL SFT stage with future policy optimization.

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [859] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: The paper presents a systematic theory for understanding the expressivity of weight-space networks, proving equivalence in expressive power across permutation-equivariant designs, universality, and its limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in studying the effectiveness of weight-space networks in leveraging the increasing availability of pretrained models, and addressing the need for a comprehensive theory that explains their expressivity.

Method: The paper provides theoretical insights by proving expressive power equivalences, establishing universality under assumptions, and identifying regimes where universality fails.

Result: It demonstrates that all prominent permutation-equivariant networks share equal expressive power and provides conditions for their universality in both weight- and function-space settings.

Conclusion: The research delivers a unified theoretical basis for understanding the expressivity of weight-space networks, advancing the field with foundational results.

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [860] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: The paper introduces a new optimizer that integrates directional orthogonalization and sign updates for efficient large-scale training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address implicit biases in norm-induced geometries and improve optimization in large-scale machine learning tasks.

Method: The proposed optimizer uses Lion-style momentum for direction, orthogonalizes it using Newton–Schulz iterations, and applies sign updates to maximize steps under specific constraints.

Result: The optimizer performs on par or better than AdamW and Muon in GPT-2, Llama pretraining, and image training, using less state.

Conclusion: This optimizer is efficient, effective, and shows improved compatibility across various training systems compared to existing methods.

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [861] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: The paper addresses the security risks posed by node-injection attacks in industrial graph neural network (GNN) applications and proposes a novel attack method called Single-Edge Graph Injection Attack (SEGIA), which is effective even under resource constraints.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of vulnerabilities in GNNs deployed in industrial systems such as IIoT, power grids, and manufacturing networks, where adversaries could compromise decisions by injecting malicious nodes.

Method: The authors develop the SEGIA framework combining pruned SGC models, multi-hop neighborhood sampling, reverse graph convolution-based feature synthesis, and a similarity-regularized objective to create effective node-injection attacks with minimal edge connectivity.

Result: Experiments demonstrate SEGIA achieves at least 25% higher attack success than existing methods while requiring significantly fewer edge resources, highlighting its efficiency and the potential risks to industrial GNN applications.

Conclusion: The study concludes that node-injection attacks pose serious risks to industrial GNN deployments and recommends mitigation strategies like admission validation and neighborhood-consistency monitoring to reduce vulnerabilities.

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [862] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: This paper introduces MarkovScale, a framework based on a two-state Markov process, to enhance sequential scaling performance in inference for LLMs, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the modest performance of sequential scaling methods by addressing the lack of clear, principled approaches that hinder optimality understanding and bounds.

Method: The method involves modeling sequential scaling as a two-state Markov process, deriving theoretical performance bounds, and creating the MarkovScale system to balance accuracy and efficiency based on principled criteria.

Result: MarkovScale outperforms state-of-the-art scaling methods in comprehensive experiments using 3 backbone LLMs, 5 benchmarks, and over 20 configurations.

Conclusion: The proposed MarkovScale framework is a significant advancement in resource-efficient and optimal inference for LLMs, bridging the gap between theory and practice.

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [863] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike, a novel adaptive spiking graph neural network, improves dynamic graph representation learning by overcoming trade-offs in attention-based and recurrent methods, achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations in existing dynamic graph learning methods, which struggle with trade-offs between expressiveness, computational cost, and effective gradient propagation.

Method: ChronoSpike integrates learnable LIF neurons, attention-based spatial aggregation, and a lightweight Transformer temporal encoder to achieve linear memory complexity while capturing both local and global temporal dependencies.

Result: ChronoSpike outperformed 12 state-of-the-art methods on large-scale benchmarks, achieving significant accuracy improvements (2.0% Macro-F1 and 2.4% Micro-F1), 3-10x faster training times, and maintaining a lightweight parameter budget.

Conclusion: ChronoSpike effectively balances efficiency, accuracy, and interpretability in dynamic graph representation learning, offering robust results and upholding theoretical guarantees for stability and performance.

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [864] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: WinFLoRA introduces a privacy-conscious federated learning approach with Low-Rank Adaptation (LoRA) that improves global accuracy and client utility in heterogeneous privacy settings without third-party involvement.


<details>
  <summary>Details</summary>
Motivation: The need arises from efficient specialization of large language models (LLMs) in privacy-sensitive environments, where differential privacy creates challenges in aligning individual and global performance objectives.

Method: WinFLoRA leverages aggregation weights informed by privacy noise levels, rewarding lower-noise contributions to improve the global model's accuracy while accommodating clients' privacy requirements.

Result: WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x increase in client utility compared to existing benchmarks across various LLMs and datasets.

Conclusion: The approach successfully aligns heterogeneous client privacy and performance requirements with global model optimization, offering an effective solution for privacy-sensitive federated learning applications.

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [865] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: This paper introduces Tangent-Space Direct Preference Optimization (TS-DPO) to enhance alignment of Large Language Models (LLMs) across multiple preference dimensions, like helpfulness and verbosity, allowing user-specified behaviors without additional optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing preference optimization methods, such as Direct Preference Optimization (DPO), which collapse multiple feedback objectives into a single scalar, restricting the flexibility in aligning LLMs with diverse user preferences.

Method: TS-DPO operates in the tangent space of LLMs, using linearized updates to learn per-objective directions that can later be linearly combined to produce desired behaviors, offering better control of preferences at inference.

Result: Through evaluation on helpfulness-verbosity trade-offs using HelpSteer and UltraFeedback datasets, TS-DPO demonstrates broader Pareto coverage, smoother control, and enhanced disentanglement of preferences compared to scalarized DPO.

Conclusion: TS-DPO improves preference alignment in LLMs, covering a broader range of optimal solutions for diverse preferences and achieving smoother control through tangent space-based methods.

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [866] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: The study introduces TRACE, a framework for causal discovery in single discrete event sequences, leveraging autoregressive models for scalable and robust mutual information analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in causal discovery from single observed sequences involving high dimensionality, limited samples, and temporal dependencies, common in fields like vehicle diagnostics and patient trajectories.

Method: TRACE uses autoregressive models as pretrained density estimators for estimating conditional mutual information, enabling scalable causal graph inference with support for delayed causal effects.

Result: Experiments demonstrate TRACE's performance across various setups, including applications in vehicle diagnostics with over 29,100 event types.

Conclusion: TRACE is a scalable, parallelizable approach for causal discovery, demonstrating effectiveness even with imperfect autoregressive models and offering robust analysis across different settings.

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [867] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: The paper introduces a matrix-spectral framework for analyzing stability in deep neural networks and highlights novel metrics and regularization approaches, backed by experiments.


<details>
  <summary>Details</summary>
Motivation: This paper aims to address the challenges of analyzing stability, interpretability, and robustness in deep neural networks by using spectral tools.

Method: The authors propose a matrix-spectral framework and Global Matrix Stability Index while incorporating spectral entropy to refine sensitivity measures. They use controlled experiments to validate their findings.

Result: The study demonstrates that spectral regularization improves attribution stability and establishes a link between spectral concentration and stability in deep networks.

Conclusion: The findings provide insights and tools for designing and training robust neural networks with stable and interpretable behaviors.

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [868] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: The paper introduces SGALM, a fine-tuning framework using adversarial learning within a single LLM to improve alignment and reduce dependence on external annotations or rewards.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods for tuning LLMs for alignment, such as high annotation costs and reliance on heuristic assumptions in synthetic data approaches.

Method: SGALM formulates alignment as a generative adversarial process within one LLM, evolving its generation and discrimination capabilities simultaneously, without requiring external reward models.

Result: SGALM achieves state-of-the-art performance in alignment tasks and can also serve as a robust synthetic data generation tool.

Conclusion: SGALM is an effective fine-tuning framework for aligning LLMs, eliminating reliance on costly annotations or external evaluative models, and offering improved alignment results.

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [869] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: This paper addresses the generalization, robustness, and representation learning challenges of Graph Neural Networks (GNNs) through three main contributions: advanced representation learning techniques, graph data augmentation, and orthonormalization-based defenses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome current limitations in the performance of GNNs, specifically their generalization weaknesses, vulnerability to adversarial attacks, and suboptimal representation learning capabilities.

Method: The methodology involves three key approaches: using Graph Shift Operators (GSOs) for improved representation learning, applying graph data augmentation for better generalization, and leveraging orthonormalization techniques with noise-based defenses to enhance robustness.

Result: New techniques provide enhanced GNN performance in terms of generalization, robustness, and representation learning, offering principled solutions to identified limitations.

Conclusion: The proposed methods contribute to a deeper understanding and address critical challenges in GNNs, enabling more effective usage across applications.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [870] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: GRIT-VQ improves vector quantization with a differentiable framework, enhancing stability, performance, and codebook utilization across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Vector quantization methods typically suffer from unstable gradients, severe codebook under-utilization, and optimization challenges due to non-differentiable hard nearest-neighbor assignments.

Method: GRIT-VQ introduces a radius-based update mechanism to move latents along quantization directions and incorporates an integrated transform to update codebooks through shared parameters, ensuring fully differentiable hard assignments.

Result: GRIT-VQ significantly enhances metrics like reconstruction error, generative quality, and recommendation accuracy across various benchmarks. It also achieves better codebook utilization compared to existing methods.

Conclusion: GRIT-VQ offers a robust solution to the limitations of traditional vector quantization, providing stability and improved performance across applications while preventing codebook collapse.

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [871] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: The paper critiques current Membership Inference Attack (MIA)-based auditing methods for machine unlearning, proposing Statistical Membership Inference Attack (SMIA) as a more reliable, training-free alternative with lower computational costs and quantified reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations and flaws of current MIA-based unlearning auditing methods, which often lead to overly optimistic results and high computational overhead, thus improving the reliability and efficiency of machine unlearning audits.

Method: The authors propose SMIA, which avoids the need for attack models by comparing member and non-member data distributions through statistical tests. SMIA also provides forgetting rates with confidence intervals, enhancing the auditing's reliability.

Result: The experiments demonstrate that SMIA is more reliable than existing MIA-based approaches and incurs significantly lower computational costs, providing both theoretical and empirical guarantees.

Conclusion: The study establishes SMIA as a promising new paradigm for performing reliable and efficient machine unlearning audits, addressing key limitations in current approaches and improving quantification of results.

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [872] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: The paper presents a new deep learning framework for multi-day electricity price forecasting, addressing challenges like volatility, spikes, and regime shifts in energy markets.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving electricity price forecasting, addressing gaps like limited multi-day predictions, underutilization of state-of-the-art DL models, and lack of granular evaluation for time-of-day variations.

Method: A novel forecasting framework is developed using advanced time series deep learning models, evaluated across intraday intervals and regions in the Australian National Electricity Market.

Result: Standard DL models often outperform in specific regions, while SOTA models show better robustness for extended horizons. Intraday evaluations reveal error patterns tied to diurnal and structural market dynamics.

Conclusion: The study highlights the need for advancing DL-based forecasting methods by improving feature representations for robust long-term predictions and capturing intraday volatility effectively.

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [873] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: This paper introduces MF-BPINN, a new multi-fidelity framework combining PINNs with Bayesian methods and adaptive learning to efficiently solve PDEs.


<details>
  <summary>Details</summary>
Motivation: High-fidelity PDEs are computationally intensive, especially when multiple parameter evaluations are needed, motivating the need for efficient, robust methods.

Method: The paper combines low-fidelity and high-fidelity simulations using a hierarchical neural network, adaptive residual learning, and Bayesian uncertainty quantification methods.

Result: MF-BPINN dynamically balances different fidelity levels, effectively enhancing computational efficiency and modeling accuracy in PDE solutions.

Conclusion: MF-BPINN offers a novel, adaptive, and computationally efficient approach to solving PDEs using a combination of neural networks and Bayesian frameworks.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [874] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper proposes a novel Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework for Gradual Domain Adaptation (GDA), addressing the limitations of flow-based models in constructing intermediate domains for domain adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of domain shift in GDA, where real intermediate domains are often unavailable or ineffective, and to overcome the performance degradation caused by flow-based models relying on log-likelihood estimation.

Method: The method involves reformulating flow-based GDA as a Lagrangian dual problem and deriving a semi-dual objective to avoid likelihood estimation. Entropy regularization is introduced to stabilize the training procedure, leading to a novel optimization-based GDA training framework.

Result: Extensive experiments validate the effectiveness of the E-SUOT framework in improving GDA performance through better stability and generalization.

Conclusion: The E-SUOT framework effectively constructs intermediate domains for GDA, stabilizes training, and improves domain adaptation outcomes compared to prior methods.

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [875] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper identifies limitations of diffusion models in time-series data imputation and proposes a novel SPIRIT framework to enhance performance in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address two key challenges in diffusion models for time-series imputation: non-stationary temporal dynamics causing biased imputations and inconsistencies between the imputation objective and model training.

Method: They analyze the diffusion model process through a proximal-operator perspective, introduce an entropy-induced Bregman divergence, derive the semi-proximal transport (SPT) discrepancy to mitigate non-stationarity, and propose the SPIRIT framework as a solution.

Result: The SPIRIT framework shows improved performance and robustness in experiments dealing with time-series data imputation.

Conclusion: SPIRIT effectively addresses the limitations of diffusion models in time-series data imputation by enhancing robustness against non-stationarity while maintaining fidelity.

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [876] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: This paper introduces GH-OFL, a one-shot federated learning method addressing privacy risks and high communication costs, leveraging Gaussian embeddings and synthetic data for robust and accurate model training.


<details>
  <summary>Details</summary>
Motivation: The current weaknesses in classical federated learning, such as communication overhead and privacy risks, alongside limitations in existing one-shot methods like reliance on public datasets and homogeneous client models.

Method: GH-OFL assumes class-conditional Gaussianity of pretrained embeddings. Clients send sufficient statistics, and the server constructs heads using Gaussian-based closed-form heads, synthetic Fisher subspaces, and lightweight residual refinement methods.

Result: GH-OFL demonstrates state-of-the-art robustness and accuracy in non-IID skewed datasets while maintaining data-free communication conditions.

Conclusion: GH-OFL effectively reduces communication overhead and risks by limiting exchanges to one round, showcasing practical deployability while addressing common constraints in federated learning setups.

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [877] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: This study reveals that recurrent neural policies in RL tasks develop stable cyclic structures resembling limit cycles, which enhance generalization, robustness, and skill adaptation in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why recurrent neural policies excel in generalization, robustness, and adaptation in partially observable control and meta-RL tasks.

Method: The researchers analyze hidden states of recurrent policies across various methods, architectures, and tasks, revealing the emergence of stable cyclic structures akin to limit cycles.

Result: Stable cyclic structures in recurrent policies stabilize memory and task-relevant environmental states, while the geometry of these cycles correlates with behavior, aiding skill adaptation in non-stationary environments.

Conclusion: The emergence and geometry of limit cycles explain the generalization, robustness, and adaptability of recurrent policies, providing new insights into their superior performance in dynamic scenarios.

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [878] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: This paper introduces a new normalization strategy, SimpleNorm, for stabilizing Transformer optimization by reducing the Hessian's spectral norm, allowing for significantly higher learning rates and better performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in Transformer optimization related to activation scale, Hessian properties, and limited tolerable learning rates.

Method: The authors propose SimpleNorm, which stabilizes activation scales, theoretically showing its impact on reducing the spectral norm of the Hessian and allowing higher learning rates. Regular experiments on GPT models validate this.

Result: SimpleNorm enables learning rates 3×-10× larger than the norm, improves training stability, and achieves better performance. Empirically, SimpleGPT reduces training loss by 0.08 compared to LLaMA2 with QKNorm in 7B-scale models.

Conclusion: SimpleNorm significantly enhances Transformer training efficiency and performance. SimpleGPT demonstrates its potential to outperform existing baselines, setting a new benchmark in optimization strategies for large GPT models.

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [879] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: This paper discusses strategies to handle anonymized data with mixed formats in machine learning, proposing novel methods to maintain data utility.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of using mixed-format anonymized data in machine learning while preserving its utility.

Method: Proposed novel data transformation strategies and evaluated them against standard imputation and LLM-based methods across diverse datasets and scenarios.

Result: The method successfully regained data utility, showing generalized values benefit over suppression and that consistent representations are key.

Conclusion: Appropriate handling of anonymized data is crucial for effective learning, with optimal approaches varying by context.

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [880] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: The paper introduces Mixture of Top-k Activations (MiTA), an efficient attention mechanism that compresses input tokens and uses deformable experts for scaling fast weights, showing promise in initial application to vision tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling fast weights for extremely long sequences in Transformers due to high computational cost, and to unify varying efficient attention methods under a common framework.

Method: Proposes a compress-and-route strategy by compressing the $N$-width MLP into a narrower one with landmark queries and constructing deformable experts using top-$k$ activated key-value pairs, termed as MiTA attention.

Result: Preliminary experiments on vision tasks show encouraging results, indicating MiTA attention's potential and prompting further exploration for broader and more complex applications.

Conclusion: MiTA attention effectively tackles fast-weight scaling challenges in efficient attention mechanisms and offers a promising direction for scalable and adaptable Transformer architectures.

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [881] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus optimizes training efficiency in large-scale models by reducing memory and training time while enhancing performance, overcoming limitations of prior methods like GaLore.


<details>
  <summary>Details</summary>
Motivation: To efficiently train large-scale models without compromising memory, time, or performance due to trade-offs typically observed in existing techniques.

Method: The paper introduces Lotus, which modifies the projection process and defines a gradient displacement criterion, enabling optimized transitions in low-rank gradient subspaces.

Result: Lotus achieves a 30% training time reduction and a 40% memory consumption decrease while outperforming the baseline in pre-training and fine-tuning.

Conclusion: Lotus effectively resolves efficiency trade-offs in large-scale training, offering substantial improvements in both computational metrics and model performance.

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [882] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: This study analyzes how brain-to-speech decoding models extract and utilize information across vocalized, mimed, and imagined speech modes using mechanistic interpretability techniques.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how neural speech decoders represent and transfer information across different speech modalities.

Method: The researchers used methods like cross-mode activation patching, tri-modal interpolation, coarse-to-fine causal tracing, causal scrubbing, and neuron-level investigations to examine the mechanisms of cross-mode transfer in speech decoding models.

Result: They found that cross-mode transfer depends on compact subspaces in neural layers rather than distributed activity, and speech modes exist on a shared continuous causal manifold.

Conclusion: Speech modality information in brain-to-speech decoding models is hierarchically and directionally organized, with localized and compact neural subspaces enabling the transfer of information across modalities.

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [883] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for Active Reinforcement Learning (ActiveRL), analyzing its sample efficiency and presenting an algorithm that leverages Gaussian Process (GP) uncertainty for learning an $ε$-optimal policy with improved efficiency compared to offline methods.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges of poor state-action space coverage and distributional shift in offline reinforcement learning (RL). It aims to address these by investigating theoretical aspects of ActiveRL, where limited online interactions refine uncertain value-function regions.

Method: The authors propose an algorithm for ActiveRL rooted in Gaussian Process (GP) uncertainty modeling, using GP concentration inequalities and information-gain bounds to derive high-probability guarantees for learning efficient policies.

Result: The proposed ActiveRL method achieves near-optimal information efficiency with sample complexity scaling as ${\mathcal{O}}(1/ε^2)$, outperforming the $Ω(1/ε^2(1-γ)^4)$ rate of offline methods. Experiments validate both the algorithm and theoretical findings.

Conclusion: ActiveRL coupled with guided uncertainty reduction offers accelerated value-function convergence with minimal online data, providing theoretical guarantees and practical improvements for reinforcement learning scenarios.

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [884] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: The paper presents BicKD, a new approach to knowledge distillation, which includes bilateral contrastive loss to improve sample-wise and class-wise predictions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of vanilla KD, which lacks class-wise comparison and structural constraints on probability space.

Method: They propose a bilateral contrastive loss that strengthens orthogonality among class generalization spaces while ensuring consistency within the same class.

Result: Experiments confirm BicKD's superior knowledge transfer and outperforming state-of-the-art distillation techniques in various benchmarks.

Conclusion: BicKD enhances knowledge transfer efficiency by introducing sample-wise and class-wise mechanisms, improving predictive distribution structure.

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [885] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: This paper presents Component Designed Kronecker Adapters (CDKA), optimizing the structure of Kronecker adapters for fine-tuning large-scale models and demonstrating their effectiveness in various NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Kronecker adapters, which treat component structures as fixed or heuristic and fail to explore their optimal dimensions and configurations.

Method: The authors analyze the dimensions and number of Kronecker adapter components, propose CDKA with parameter-budget-aware configuration guidelines, and introduce a training stabilization strategy for implementation.

Result: Experiments on natural language processing tasks indicate that CDKA improves the performance and efficiency of Kronecker adapters.

Conclusion: The study highlights the importance of component structure design in Kronecker adapters and presents CDKA as a solution to enhance their fine-tuning capacity with practical guidelines and effective results.

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [886] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: This paper introduces Mixture-of-World Models (MoW), a scalable and parameter-efficient architecture for sample-efficient multi-task reinforcement learning (MTRL) in diverse visual domains, achieving state-of-the-art performance on Atari 100k and Meta-World benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sample inefficiency in multi-task reinforcement learning (MTRL), especially in visually diverse domains where tasks differ in observations and dynamics.

Method: Proposes Mixture-of-World Models (MoW) consisting of modular variational autoencoders for task-adaptive visual compression, a Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy to optimize resource allocation.

Result: MoW achieves competitive human-normalized scores (110.4%) on Atari 100k while using 50% fewer parameters than STORM and reports a state-of-the-art success rate of 74.5% on Meta-World within 300k steps.

Conclusion: MoW demonstrates scalable and parameter-efficient solutions for MTRL, outperforming conventional methods while maintaining low parameter usage.

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [887] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: This paper presents an Agentic AI system enabling autonomous networks to interpret and act on diverse intents through three specialized agents.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for autonomous telecommunication networks to handle heterogeneous services with varying performance objectives and constraints, a task beyond the capabilities of current heuristic methods.

Method: The paper designs an Agentic AI system comprising three agents: a supervisory interpreter for intent parsing, an optimizer for deriving trade-offs, and a controller for multi-objective reinforcement learning to align network actions with intents.

Result: The proposed system allows networks to autonomously interpret, adapt to, and act upon different service intents and network conditions effectively and flexibly.

Conclusion: Agentic AI adds autonomy and scalability to networks, enabling intent-driven operations that meet diverse and conflicting service requirements in an adaptive and efficient way.

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [888] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: PolySAE introduces higher-order terms to sparse autoencoders (SAEs), achieving improved interpretability and performance in feature decomposition for neural network representations.


<details>
  <summary>Details</summary>
Motivation: SAEs struggle to decompose concepts into interpretable elements due to their reliance on linear reconstruction, which cannot account for compositional feature structures.

Method: PolySAE employs low-rank tensor factorization to extend the SAE decoder. It incorporates pairwise and triple feature interactions, keeping the linear encoder intact and adding minimal parameter overhead.

Result: PolySAE delivers an 8% average improvement in probing F1 across language models and SAE variants, maintains reconstruction error levels, and captures compositional structures effectively.

Conclusion: PolySAE enhances the interpretability of autoencoders by modeling feature interactions beyond co-occurrence statistics, enabling better compositional analysis and understanding.

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [889] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: This paper proposes an improved Bayesian Last Layers (BLLs) method for better uncertainty estimation in neural networks by leveraging Neural Tangent Kernel (NTK) features.


<details>
  <summary>Details</summary>
Motivation: BLLs are computationally efficient but underestimate epistemic uncertainty as they only Bayesianize the final layer, neglecting uncertainty from earlier layers.

Method: The paper leverages NTK feature projections to account for full network variability in BLLs while introducing a subsampling scheme to reduce computational burden.

Result: The proposed method demonstrates improved calibration and uncertainty estimates over standard BLLs across various tasks, including UCI regression and image classification.

Conclusion: The method effectively corrects epistemic uncertainty underestimation in BLLs while retaining computational efficiency, making it useful for uncertainty-aware neural network applications.

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [890] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: The paper introduces a metric called Entropy Dynamics Instability Score (EDIS) to evaluate large language model reasoning by analyzing entropy evolution over time rather than static aggregated confidence measures.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning accuracy in large language models by leveraging dynamic entropy evolution instead of static aggregated confidence measures.

Method: Analyzed entropy trajectories during text generation, identifying patterns correlated with reasoning failures. Developed EDIS, which quantifies instability in entropy evolution.

Result: Identified two key instability patterns in entropy evolution (burst spikes and peak-valley spikes) that indicate reasoning errors. Demonstrated the usefulness of EDIS for improving inference-time selection and training sample curation.

Conclusion: Entropy dynamics provide valuable insights into reasoning processes in LLMs, and the EDIS metric enhances reasoning accuracy and understanding of reasoning failures.

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [891] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: Group Relative Policy Optimization (GRPO) performance varies significantly across domain sequencing strategies in reasoning tasks, showing pronounced asymmetry, order sensitivity, and strategy dependence.


<details>
  <summary>Details</summary>
Motivation: Understanding GRPO behavior under different domain sequencing strategies is essential for optimizing reasoning abilities in large language models.

Method: The study systematically analyzed training-order effects in GRPO across math, science, logic, and puzzle reasoning tasks, comparing sequential and mixed-domain training approaches.

Result: Single-domain generalization showed asymmetric behavior, cross-domain interactions revealed high order-dependence, and no universally optimal strategy emerged for multi-domain training.

Conclusion: GRPO requires domain-aware and order-aware training designs due to its sensitivity to sequencing strategies and significant variability in performance across domains.

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [892] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: This paper introduces a novel post-training quantization framework for diffusion models, optimizing gradient alignment for better quantization results on timesteps.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel in image synthesis but suffer from slow inference and high computational demands, highlighting the need for efficient post-training quantization techniques.

Method: The paper proposes a PTQ method that learns optimal weights for calibration samples to align gradients across timesteps, addressing varying contributions and conflicts during the quantization process.

Result: Experiments conducted on CIFAR-10, LSUN-Bedrooms, and ImageNet datasets demonstrate superior performance of the proposed PTQ method compared to existing techniques.

Conclusion: The study effectively enhances diffusion models' efficiency by optimizing gradient alignment during post-training quantization to achieve higher-quality image synthesis and faster inference.

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [893] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: This paper explores the counterintuitive impact of removing 'high-gradient' versus 'low-gradient' components from neural networks and highlights the Gradient-Causal Gap.


<details>
  <summary>Details</summary>
Motivation: To understand why removing components with high gradient magnitudes sometimes improves generalization while removing low-gradient components often degrades it.

Method: Analyzing Transformers on algorithmic tasks, using metrics such as correlation between gradient magnitude and causal importance, as well as conducting pruning experiments.

Result: The relationship between gradient magnitude and causal importance varies with task complexity, sometimes collapsing or even inverting. Pruning shows unpredictable effects, with removing low-gradient components causing significant harm.

Conclusion: Gradient magnitude is an unreliable metric for pruning, suggesting that gradient-based methods may not consistently preserve model performance.

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [894] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: The paper introduces algorithms HT-FTRL-OM and HT-FTRL-UOB for Heavy-Tailed Markov Decision Processes that achieve improved regret bounds in both adversarial and stochastic environments.


<details>
  <summary>Details</summary>
Motivation: Current methods for HTMDPs lack adaptivity in adversarial environments and are overly cautious in stochastic contexts. This work aims to bridge that gap with versatile algorithms for varied regimes.

Method: 1. For known transitions: HT-FTRL-OM uses Occupancy Measure FTRL with skipping loss estimators for adversarial ($\widetilde{\mathcal{O}}(T^{1/α})$ regret) and stochastic ($\mathcal{O}(\log T)$ regret) settings. 2. For unknown transitions: HT-FTRL-UOB integrates pessimistic skipping loss estimators achieving $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret for adversarial and $\mathcal{O}(\log^2(T))$ for stochastic cases.

Result: 1. HT-FTRL-OM achieves logarithmic regret in stochastic regimes and polynomial regret in adversarial settings with known transitions. 2. HT-FTRL-UOB addresses unknown transitions with competitive adversarial and stochastic regime guarantees.

Conclusion: Both algorithms successfully achieve Best-of-Both-Worlds guarantees for HTMDPs, offering advanced adaptability across varied environments.

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [895] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: The paper proposes a relative-budget theory ($\xi$) to explain the varying effectiveness of reinforcement learning (RL) in large language models across tasks and compute budgets.


<details>
  <summary>Details</summary>
Motivation: Address the inconsistencies in the sample efficiency and reasoning performance of reinforcement learning applied to large language models.

Method: A relative-budget framework ($\xi := H/\mathbb{E}[T]$) is introduced, analyzing sample efficiency across deficient, balanced, and ample regimes based on token budget ($H$) and first-correct-solution token expectation ($T$).

Result: Three regimes of sample efficiency are identified: deficient ($\xi \to 0$), balanced ($\xi = \Theta(1)$), and ample ($\xi \to \infty$), with empirical findings showing optimal reasoning efficiency at $\xi \in [1.5, 2.0]$.

Conclusion: The relative-budget theory provides a unified explanation for RL performance variance in large language models and suggests optimal budget conditions to maximize reasoning efficiency.

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [896] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: The paper explores optimization instability in deep neural networks due to singularities in parameter space, proposing a method called Parametric Singularity Smoothing (PSS) which improves training stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand and address how singularities in parameter and representation spaces lead to instability and sharp loss explosions during the training of deep neural networks.

Method: Analyzing the emergence of singularities, their growth, and proposing PSS, a method to smooth singular spectra of weight matrices, enhancing training stability.

Result: PSS effectively mitigates instability, restores trainability after failures, improves training efficiency, and enhances generalization across datasets and architectures.

Conclusion: Parametric singularities pose a significant risk to training stability in deep neural networks, and PSS successfully alleviates these issues, improving overall training performance and robustness.

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [897] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: The paper provides a theoretical analysis of the widely used TRAK algorithm for data attribution, detailing its performance and errors.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the theoretical underpinnings of TRAK, a tool for interpreting AI models, is crucial for ensuring accurate and reliable data attribution.

Method: The study analyzes TRAK's approximations theoretically, quantifying errors and examining the correlation between estimated and original data influences.

Result: The errors in TRAK's approximations are significant, but it maintains a high correlation with original influence rankings, preserving the relative ranking of data points.

Conclusion: TRAK effectively retains the relative ranking of data points despite approximation-induced errors, supported by theoretical insights and empirical validations.

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [898] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: A new strategy, VIP, is introduced to improve sampling efficiency in reinforcement learning by optimizing rollout allocations through variance predictions.


<details>
  <summary>Details</summary>
Motivation: Current group-based policy optimization methods inefficiently allocate rollouts uniformly, treating all prompts equally informative and wasting computational resources.

Method: VIP uses a Gaussian process model to predict success probabilities per prompt, estimates variance, and solves a convex optimization problem for optimal allocation of rollouts under a fixed computational budget.

Result: VIP demonstrates consistent improvements in sampling efficiency and performance over traditional methods in several benchmarks.

Conclusion: Variance-Informed Predictive (VIP) allocation enhances training by reducing gradient variance through strategic rollout allocation, offering a more efficient alternative to existing methods.

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [899] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: The paper introduces AGT$^{AO}$, a framework to balance data erasure and model utility in LLMs, addressing privacy risks from sensitive data memorization.


<details>
  <summary>Details</summary>
Motivation: LLMs pose privacy risks by unintentionally memorizing sensitive data, necessitating effective machine unlearning methods.

Method: Proposes AGT$^{AO}$, incorporating Adaptive Orthogonality (AO) to reduce gradient conflicts and Adversarial Gating Training (AGT) for a latent-space min-max game.

Result: AGT$^{AO}$ achieves robust unlearning (KUR ≈ 0.01) while preserving model utility (MMLU 58.30).

Conclusion: This framework reconciles strong unlearning with maintained model utility, mitigating privacy risks and adversarial recovery effectively.

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [900] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: This paper introduces a novel method for finding differentially private second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in the literature by developing a framework for achieving SOSP in minimax optimization while ensuring differential privacy, expanding beyond existing work focused on first-order stationarity or classical minimization.

Method: A first-order method is proposed combining nested gradient descent--ascent, SPIDER-style variance reduction, and Gaussian noise to ensure privacy. A block-wise analysis manages stochastic variance and privacy noise without excessive accumulation across iterations.

Result: The method achieves high-probability guarantees for reaching approximate SOSP. For empirical-risk objectives, the rate is $\mathcal{O} ((\frac{\sqrt{d}}{n\varepsilon})^{2/3})$, and for population objectives, the rate is $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$, matching the best rates for private first-order stationarity.

Conclusion: This work provides a unified approach for private SOSP computation in both empirical and population risks, offering significant advancements in privacy-preserving stochastic minimax optimization.

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [901] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: The paper presents a theoretical foundation for self-play finetuning, likening it to adversarial imitation learning, and proposes an improved finetuning algorithm with better performance and stability.


<details>
  <summary>Details</summary>
Motivation: Self-play finetuning has proven effective for improving large language models without preference data, but its theoretical underpinnings are not well understood.

Method: The paper formulates self-play finetuning as a min-max game within adversarial imitation learning, proposes a game-theoretic analysis to show equilibrium convergence, and develops a $χ^2$-divergence based algorithm for improved stability.

Result: The new algorithm improves stability and consistently outperforms existing self-play methods in multiple language model finetuning tasks.

Conclusion: The theoretical analysis and new algorithm unify and extend self-play finetuning methods, providing a robust foundation and practical advances for future development.

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [902] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno is a new lightweight method for time-series anomaly detection that uses 1D convolutions and patch-based learning to achieve state-of-the-art performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Current approaches to time-series anomaly detection often rely on heavy neural network architectures like transformers, which are computationally costly and perform inconsistently compared to simpler methods.

Method: The proposed method, PaAno, uses temporal patches of time-series data and a 1D convolutional network to embed these patches. It trains the model with triplet loss and pretext loss to capture meaningful temporal representations.

Result: PaAno achieved state-of-the-art results on the TSB-AD benchmark, surpassing both lightweight and heavy architecture-based methods on univariate and multivariate anomaly detection.

Conclusion: PaAno demonstrates that lightweight architectures can outperform complex models in time-series anomaly detection, offering a practical and efficient solution for real-world applications.

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [903] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: This paper investigates outlier issues in 4-bit training of large language models, attributes them to particular architecture components, and proposes a solution (CHON) for minimizing loss gaps and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance throughput and memory efficiency in training large language models with 4-bit arithmetic, addressing challenges caused by outliers.

Method: Analyzing outlier dynamics during NVFP4 pretraining, identifying their causes and evolution, and proposing a mitigation method called Hot-Channel Patch (HCP) integrated into a training recipe called CHON.

Result: The proposed CHON method reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy in a GLA-1.3B model trained on 60B tokens.

Conclusion: Outliers in 4-bit training can be effectively managed with CHON, leading to improved training efficiency and minimized performance losses compared to BF16 arithmetic.

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [904] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE is a deep survival model utilizing variational autoencoders and contrastive learning to achieve both interpretability and high predictive performance in clinical risk stratification.


<details>
  <summary>Details</summary>
Motivation: Current deep survival models face a trade-off between predictive accuracy and interpretability, posing challenges for clinical acceptance.

Method: CONVERSE employs contrastive variational embeddings, cluster-specific survival predictions, and self-paced learning to provide interpretable and accurate patient risk stratification.

Result: Evaluation on four benchmark datasets shows that CONVERSE outperforms or matches other state-of-the-art deep survival models in prediction accuracy and meaningful risk stratification.

Conclusion: CONVERSE successfully bridges the gap between interpretability and performance, making it an effective tool for clinical decision-making.

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [905] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: This paper investigates how Large Language Models (LLMs) perform latent planning and how it impacts Chain-of-Thought (CoT) reasoning, finding ways to estimate uncertainty and bypass CoT processes without loss in performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the latent planning capabilities of LLMs to improve their reasoning and decision-making processes, especially in tasks requiring multi-step reasoning.

Method: The authors use a probing method called Tele-Lens to analyze hidden states of LLMs, investigating their planning strategies across various tasks and proposing a framework for estimating uncertainty in CoT paths.

Result: The study reveals that LLMs generally operate incrementally without global planning, and effectively validates a hypothesis that a small subset of CoT positions can represent the uncertainty of the entire reasoning path. It also demonstrates that CoT processes can be bypassed automatically without degrading model performance.

Conclusion: The research highlights the importance of understanding and utilizing CoT dynamics in LLMs, showing improved uncertainty estimation and suggesting that CoT can be streamlined efficiently for reasoning tasks.

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [906] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: This paper proposes masked diffusion models (MDMs) as an alternative to autoregressive models (ARMs) for language generation, introducing methods to improve generation order optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of prior work which either hard-coded generation orders or used suboptimal two-stage optimization processes for order learning in masked diffusion models.

Method: The paper introduces two approaches: (1) OeMDM—a unified framework for various generation orders, interpretive of MDM, ARM, and block diffusion models; and (2) LoMDM—a learnable-order diffusion model that jointly optimizes generation ordering and diffusion backbone from scratch.

Result: LoMDM achieves superior performance compared to other discrete diffusion models on multiple language modeling benchmarks.

Conclusion: The proposed models offer advancements in language generation by optimizing generation orders efficiently, confirming their effectiveness through empirical evaluation.

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [907] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: The paper presents a semi-supervised learning method to enhance transformer-based CAPP models, addressing low dataset availability in manufacturing.


<details>
  <summary>Details</summary>
Motivation: To overcome limited dataset availability in manufacturing, which hinders model generalization in CAPP, and to reduce the reliance on manually labeled data.

Method: A semi-supervised approach involves using an oracle trained on available transformer data to filter correct predictions for unseen parts, which are then leveraged for one-shot retraining of models.

Result: Experiments show consistent accuracy improvements over baseline methods with small-scale datasets, validating the approach's utility in data-scarce conditions.

Conclusion: The proposed method effectively enhances CAPP model performance in limited-data scenarios, demonstrating its practical potential for manufacturing industries.

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [908] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: The paper introduces EvoMU, an evolutionary approach to discover task-specific unlearning loss functions for machine unlearning, achieving state-of-the-art results efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in identifying suitable unlearning loss functions for machine unlearning, especially given the vast and variable nature of loss spaces and the lack of a universally optimal solution.

Method: EvoMU employs an evolutionary search approach to automatically explore and identify task-specific loss functions for machine unlearning, leveraging a small parameter model (Qwen3-4B-Thinking).

Result: The approach demonstrates superior performance over existing loss-based unlearning methods on datasets like TOFU-5%, TOFU-10%, MUSE, and WMDP, synthesizing new effective loss functions efficiently.

Conclusion: EvoMU showcases the potential of AI as a budget-conscious co-scientist, automating the discovery of dataset-specific unlearning losses and achieving state-of-the-art results with limited computational resources.

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [909] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: The paper explores the relationship between watermarking and speculative sampling in language models, proposing a quantified measure of watermark strength, resolving inefficiencies, and offering a mechanism to balance both effectively.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inference inefficiency when applying watermarking to large language models and improve their practical deployment.

Method: The authors introduce a quantified watermark strength measure, frame trade-offs as a constrained optimization problem, provide Pareto curves, and propose a mechanism injecting pseudorandomness into draft-token acceptance to maintain efficiency while enhancing detectability.

Result: The new approach improves statistical detectability of watermarking while retaining speculative sampling efficiency, demonstrated through experiments.

Conclusion: The study establishes a unified principle between watermarking and speculative sampling, enabling efficient and practical implementation in large language models.

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [910] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: This paper enhances the generative selection performance of small reasoning models using reinforcement learning, enabling higher efficiency and test-time scaling.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning in large language models (LLMs) through parallel sampling is commonly limited by selection quality. Generative selection methods like GenSelect help but typically require large models for good performance.

Method: The authors use reinforcement learning (via DAPO) to train small 1.7B-parameter models for effective generative output selection. They synthesize selection tasks from filtered math and code datasets with correct and incorrect candidate solutions.

Result: The trained models outperform traditional prompting and majority-voting approaches across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks. They also generalize well to outputs from stronger models, despite training on weaker model outputs.

Conclusion: Reinforcement learning is shown to be an effective way to enable small models to perform strong generative selection, allowing efficient computation and scaling during test time.

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [911] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: Adaptive rounding techniques for quantization in LLMs are expensive. VQRound proposes a compact and efficient reparameterization using codebooks, achieving scalability and fast-fitting with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To address the prohibitive cost of traditional adaptive rounding in large language models, enabling efficient post-training quantization with minimal parameters.

Method: Propose VQRound, reparametrize the rounding matrix into a compact codebook, minimize element-wise worst-case error under $L_\infty$ norm, and develop a lightweight finetuning pipeline using 128 samples.

Result: VQRound achieves better convergence than traditional adaptive rounding with minimal trainable parameters (0.2%), demonstrated on various LLMs like OPT, LLaMA, and Qwen3.

Conclusion: Adaptive rounding can be made scalable and efficient for LLMs using VQRound, offering a balance of accuracy and efficiency in post-training quantization.

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [912] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: The study investigates how two-layer neural networks identify low-dimensional latent spaces while learning under specific asymptotic conditions ($n/d\toδ$).


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the gradient descent dynamics and feature learning performance of two-layer neural networks in high-dimensional setups, focusing on identifying critical thresholds ($δ_{\text{NN}}$).

Method: The authors explore asymptotics where sample size $n$ and data dimension $d$ grow proportionally, keeping latent space $k$ and neuron count $m$ fixed. Mathematical derivations characterize phase transitions in gradient descent behavior.

Result: Feature learning is shown to depend on the threshold $δ_{\text{NN}}$, which is tied to changes in gradient and Hessian dynamics during training.

Conclusion: The findings enable the study of network performance and learning outcomes as functions of architecture and algorithm, grounded in theoretical thresholds.

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [913] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: This paper introduces Transformer Q-Learning (TQL), a method that stabilizes scaling in reinforcement learning value functions by controlling attention score entropy, enabling performance improvements with larger networks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the instability and performance degradation typically seen when naively scaling value functions in reinforcement learning, especially when using transformer architectures.

Method: They propose a technique that stabilizes training by controlling the entropy of attention scores, thus preventing the collapse of attention scores and enabling usage of larger models. This technique forms the basis of their Transformer Q-Learning (TQL) method.

Result: TQL achieves up to a 43% performance improvement when scaling from smaller to larger network sizes, overcoming issues faced by prior approaches.

Conclusion: The proposed TQL successfully unlocks the potential of transformers for scaling value functions in reinforcement learning, offering significant performance improvements while addressing the instability issue.

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [914] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: The paper proposes CurioSFT, a novel entropy-preserving supervised fine-tuning method for large reasoning models, improving their exploration capabilities and performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Standard supervised fine-tuning (SFT) methods for reasoning models can create overconfidence and limit diversity, reducing the solution space for reinforcement learning (RL) to explore. Traditional entropy regularization doesn't address this effectively.

Method: The paper introduces CurioSFT, consisting of: (a) Self-Exploratory Distillation for encouraging exploration through a self-generated teacher; and (b) Entropy-Guided Temperature Selection to balance exploration and factual stability. It aims to preserve exploration capabilities while minimizing knowledge forgetting.

Result: CurioSFT improves supervised fine-tuning performance by 2.5 points on in-distribution and 2.9 points on out-of-distribution mathematical reasoning tasks. It also leads to a 5.0-point average improvement during the RL fine-tuning stage.

Conclusion: CurioSFT enhances the exploration capabilities of reasoning models during SFT, translating into better performance in both SFT and RL stages, making it a promising approach for improving reasoning model pipelines.

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [915] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: The study introduces LLM-AutoOpt, a hybrid framework integrating Bayesian Optimization (BO) with Large Language Model (LLM)-based reasoning for context-aware hyperparameter optimization in time-series forecasting.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter optimization is crucial yet computationally challenging for deep learning models, particularly in time-series forecasting. Existing methods like Bayesian Optimization lack interpretability and context-awareness.

Method: LLM-AutoOpt combines BO with contextual reasoning from LLMs. It integrates meta-features, model descriptions, historical outcomes, and objectives as structured meta-knowledge in LLM prompts, using BO to mitigate cold-start effects.

Result: Experiments on multivariate time-series forecasting showed LLM-AutoOpt outperforms traditional BO and LLM methods, offering better predictive performance and enhanced interpretability.

Conclusion: The framework offers a robust solution to hyperparameter optimization challenges by incorporating structured meta-knowledge and improving the interpretability of optimization processes.

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [916] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: This paper studies reward-free exploration in cooperative multi-agent reinforcement learning for tabular finite-horizon MDPs, focusing on the tradeoff between the number of exploration phases and agents required.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the understanding of how cooperative exploration can be optimized in terms of both the number of agents and learning phases, especially under constraints like small learning phases.

Method: The authors adopt a phased learning framework where agents independently interact with the environment to explore the dynamics without rewards, analyzing the relationship between learning phases and agents.

Result: The paper identifies a sharp phase-agent tradeoff. It presents an efficient algorithm requiring $\tilde{O}(S^6 H^6 A / ε^2)$ agents when the phases equal the horizon $H$, and lower bounds showing exponential agent requirements when phases are less than $H$.

Conclusion: A linear relationship between learning phases and the horizon $H$ is essential for efficient exploration, especially when limiting agents to a polynomial number.

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [917] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: The paper introduces a Lean 4 formalization for statistical learning theory based on empirical process theory, filling gaps in Lean's Mathlib library with contributions like Gaussian Lipschitz concentration and Dudley's entropy integral theorem.


<details>
  <summary>Details</summary>
Motivation: To create a rigorous, reusable formal infrastructure for statistical learning theory in Lean 4, addressing gaps in existing libraries and improving theoretical understanding.

Method: A human-AI collaborative process was used, where humans devise proof strategies and AI agents handle tactical proof execution.

Result: Formal tools verifying statistical learning theory were developed, exposing implicit assumptions in standard texts and enhancing understanding.

Conclusion: The work establishes a reusable foundation for developing machine learning theory, encouraging future implementations using Lean 4.

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [918] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: This paper explores the influence of graph topology on node attribute distributions using a novel algebraic approach.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the interaction between graph topology and node attributes to better capture topological effects on attribute distributions.

Method: The authors introduce a categorical framework to quantify topology from a node's perspective and integrate it with attribute distributions to derive topology-conditioned probabilities. They propose a test model for evaluation.

Result: The approach accurately reflects the original attribute distribution in non-informative topology scenarios like complete graphs, and demonstrates potential in graph anomaly detection tasks.

Conclusion: The study provides a principled method to incorporate topology effects into node attribute distributions, offering insights for analyzing attributed graphs.

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [919] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: Introduces RDMReg for JEPA to enforce sparsity, offering improved representation learning and competitive image classification performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of existing JEPA methods that favor dense over sparse representations, missing efficiency in representation learning.

Method: Develop Rectified Distribution Matching Regularization (RDMReg), aligning JEPA representations to Rectified Generalized Gaussian distributions for sparsity.

Result: Empirical results show sparse, non-negative representations with competitive image classification performance using RDMReg.

Conclusion: RDMReg enhances JEPA by balancing sparsity and performance, preserving task-relevant information efficiently.

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [920] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: The paper introduces P-EAGLE, a method for faster multi-token parallel prediction in LLMs through scalable, long-context training optimizations.


<details>
  <summary>Details</summary>
Motivation: To address the latency and training complexity in long-output reasoning LLMs when utilizing speculative decoding and parallel drafting techniques.

Method: The authors transform EAGLE into P-EAGLE by enabling parallel multi-token prediction with a shared learnable hidden state, supported by attention mask pre-computation and sequence partitioning techniques for scalable training.

Result: P-EAGLE achieves 1.10-1.36x speed improvements over autoregressive EAGLE-3 in LLMs such as GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

Conclusion: P-EAGLE demonstrates that parallel multi-token prediction in LLMs can be implemented efficiently for long-sequence contexts, offering latency and training advantages over traditional autoregressive methods.

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [921] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: The paper introduces SPARKLING, a method addressing challenges of mid-stage width expansion in Progressive Learning to enhance computational efficiency and decrease training costs.


<details>
  <summary>Details</summary>
Motivation: Mid-stage width expansion in Progressive Learning is crucial for computational savings but is underexplored, primarily due to instability issues.

Method: SPARKLING ensures signal preservation via RMS-scale consistency and symmetry breaking through asymmetric optimizer state resetting and learning rate re-warmup.

Result: The approach stabilizes training, outperforms baseline methods, and reduces training costs by up to 35% for $2\times$ width expansion.

Conclusion: SPARKLING effectively addresses mid-stage width expansion challenges, reducing computational costs while improving model performance.

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [922] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything is a reinforcement learning framework enhancing RL systems through dynamic environment adaptation, joint policy-reward model optimization, and feedback integration.


<details>
  <summary>Details</summary>
Motivation: To improve reinforcement learning systems for LLM scenarios by optimizing environment, policy, and reward components in a closed-loop setup.

Method: Uses dynamic adaptation of environments, joint optimization of reward models through consistency feedback, and integrated step-wise/outcome signals for policy training.

Result: RLAnything consistently enhances performance across various LLM and agent tasks, achieving significant accuracy increases in benchmarks like OSWorld and AlfWorld.

Conclusion: The framework successfully integrates automatic adjustments and feedback mechanisms, outperforming human-labeled outcomes for robust RL improvement; code available for replication and application.

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [923] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian approach for causal discovery, incorporating expert input to improve accuracy in identifying directed acyclic graphs (DAGs).


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods struggle to efficiently integrate expert knowledge to refine directed acyclic graph (DAG) inference. This paper aims to enhance this through a novel Bayesian framework.

Method: The approach uses a Bayesian framework, modeling expert judgments with a three-way likelihood over edges, employing particle approximations for posterior inference, and selecting queries via an information gain criterion.

Result: The proposed method demonstrates faster posterior concentration and superior directed effect recovery on synthetic, protein signaling, and human gene data within limited expert query budgets.

Conclusion: Actively eliciting expert input with this Bayesian framework leads to more efficient and accurate causal discovery, especially under tight resource constraints.

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [924] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: The paper introduces multi-scale wavelet transformers (MSWTs) to improve the representation of high-frequency components in data-driven surrogates for dynamical systems, which is crucial for tasks like weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Data-driven models, especially those using machine learning, suffer from spectral bias, leading to attenuation of high-frequency components crucial for accurate representation of dynamical systems, such as in weather forecasting.

Method: The MSWT model operates in the wavelet domain, leveraging a wavelet-preserving downsampling scheme to retain high-frequency features and using wavelet-based attention mechanisms to capture multi-scale dependencies in data.

Result: The proposed MSWT model achieves significant reductions in error and better long-horizon spectral fidelity in experiments on chaotic dynamics. On real-world ERA5 climate data, it demonstrates reduced climatological bias.

Conclusion: MSWTs effectively address the challenge of spectral bias, showcasing their ability to retain and utilize high-frequency components, improving the accuracy and stability of data-driven forecasting applications.

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [925] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: A proposed framework using operator inference combined with large language models enhances PDE solving with improved accuracy, generalization, and ease of use.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in solving diverse PDEs using LLMs, focusing on generalization to unseen parameters and boundary conditions while maintaining accuracy.

Method: The paper proposes OpInf-LLM, which integrates operator inference with LLMs for accurate and generalizable PDE solutions, utilizing small solution datasets and natural language interfaces.

Result: The developed framework shows high execution success rates, accurate predictions of unseen PDE instances, and provides seamless natural language integrations.

Conclusion: OpInf-LLM demonstrates potential in advancing LLM-based PDE solving by enhancing generalization, computational efficiency, and usability for diverse scenarios.

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [926] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: This paper introduces a white-box adaptive NMPC architecture for vehicular adaptation which uses a Modular Sovereignty paradigm and symbolic graph for transparency and auditability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses vehicular plasticity, enabling adaptation to varying operating regimes without needing model retraining, and aims to ensure high transparency in adaptive methods.

Method: The approach employs frozen, regime-specific neural specialists with a Modular Sovereignty paradigm. It uses CasADi to maintain dynamics in symbolic graph form for full auditability.

Result: Simulation shows rapid adaptation (~7.3ms) and excellent tracking under shifting regimes, outperforming non-adaptive baselines. However, transparency costs increased solver latency by 72-102X compared to traditional compiled parametric models.

Conclusion: White-box adaptive control proves effective for rapid regime adaptations but comes at a significant computational cost for maintaining transparency, highlighting the trade-off between efficiency and auditability.

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [927] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: The paper addresses inefficiencies in Large Language Model (LLM) caching by introducing Position-Independent Caching (PIC) through a redesigned system, COMB, which significantly boosts processing speed and maintains accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Key-Value cache systems in LLMs are inefficient when handling contexts retrieved out of order. Current PIC methods often lead to accuracy loss, hindering their practical use.

Method: The authors reintroduce the encoder in decoder-only LLMs and explicitly train it to support PIC. They develop a PIC-aware caching system, COMB, which integrates seamlessly with inference workflows.

Result: COMB reduces the Time-to-First-Token (TTFT) by 51-94% and improves throughput by three times while maintaining comparable accuracy. It also demonstrates broader applicability across other LLMs like DeepSeek-V2-Lite-Chat.

Conclusion: The proposed COMB system successfully enhances inference efficiency and accuracy in LLMs by addressing the limitations of existing PIC approaches, showcasing practical utility for broader applications.

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [928] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: The paper addresses instability issues in extremely low-rank settings (rank-1 LoRA) for parameter-efficient fine-tuning (PEFT) due to mismatched vision and text features, and proposes Gap-Init, an initialization method that enhances stability and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve instability issues in rank-1 LoRA during adaptation of multimodal large language models, caused by sensitivity to update direction and mismatched pretrained feature spaces.

Method: The paper proposes Gap-Init, a geometry-aware initialization that aligns rank-1 LoRA updates with an estimated modality-gap vector derived from a small calibration set, ensuring better alignment while keeping initial updates at zero.

Result: Experiments across multiple vision-language tasks and backbones show that Gap-Init stabilizes rank-1 training and can even outperform stronger rank-8 baselines.

Conclusion: The findings reveal that proper initialization, such as Gap-Init, is crucial in achieving stability and performance at extremely low-rank limits, potentially mitigating the need for higher-rank configurations.

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [929] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: This paper addresses a bottleneck in Implicit Neural Representations (INRs) called 'Inlet Rank Collapse', which limits fine detail recovery, introducing a structural diagnostic framework and a new initialization method to improve performance.


<details>
  <summary>Details</summary>
Motivation: Implicit Neural Representations (INRs) excel in continuous signal modeling but struggle with fine-grained detail recovery due to expressive bottlenecks, like the 'Inlet Rank Collapse'. Current solutions like positional encoding (PE) and normalization techniques lack unified theoretical foundations.

Method: The authors propose a layer-wise decomposition of the Neural Tangent Kernel (NTK) to identify rank deficiencies in INRs and their remedies. They introduce a 'Rank-Expanding Initialization' to structurally eliminate rank collapse without additional computational loads or architectural changes.

Result: The proposed Rank-Expanding Initialization enables standard MLPs using INRs to reconstruct signals with high fidelity, overcoming limitations of prior methods and optimizing initial rank propagation.

Conclusion: Improving the rank propagation at initialization is key to enhancing the expressive power of INRs, unifying the understanding of previous methods like PE and SIREN, and enabling their broader applicability.

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [930] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: The paper presents PENCIL, a Transformer-based model for link prediction in graphs, which outperforms traditional methods without relying on complex engineering or excessive computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current link prediction models, such as GNNs and Graph Transformers, struggle either with scalability, generalization, or excessive computational costs. A simple yet effective framework is required to address these issues.

Method: The authors propose PENCIL, an encoder-only plain Transformer that leverages attention mechanisms over sampled local subgraphs instead of relying on structural heuristics or resource-intensive embeddings.

Result: PENCIL demonstrates superior structural expressivity compared to traditional GNNs, achieves parameter efficiency, and delivers competitive performance even without node features across various benchmarks.

Conclusion: The study highlights that simple model designs, like PENCIL, can achieve performance levels similar to or better than complex and computationally demanding frameworks, challenging the need for intricate engineering.

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [931] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: The paper introduces 'InfoTok', a visual tokenization mechanism based on the Information Bottleneck principle to improve multimodal large language models (MLLMs) for both understanding and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack explicit criteria for designing visual tokenizers, leading to inefficiencies. The paper aims to create a principled approach for tokenization that balances compression and task relevance.

Method: The authors propose InfoTok, which uses mutual-information regularization to control the information flow from images to shared tokens for multimodal outputs.

Result: Integrating InfoTok into unified MLLMs improves outcomes on both image understanding and generation tasks without additional training data.

Conclusion: InfoTok offers a principled mechanism for learning shared token spaces in unified multimodal models, effectively balancing information compression and relevance.

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [932] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: Long-term memory mechanisms in LLMs amplify implicit biases, which intensify over time and propagate across domains. The Dynamic Memory Tagging (DMT) method mitigates this issue effectively.


<details>
  <summary>Details</summary>
Motivation: To explore how long-term memory in LLMs can escalate implicit biases and assess methods to curb these fairness risks.

Method: Developed the Decision-based Implicit Bias (DIB) Benchmark to measure bias in long-term interaction scenarios and introduced Dynamic Memory Tagging (DMT) to enforce fairness constraints during memory storage.

Result: Implicit biases were found to grow over time and extend across unrelated domains within LLMs using existing memory architectures. DMT significantly reduced bias accumulation and propagation compared to other methods.

Conclusion: While existing mitigation strategies offer limited effectiveness, enforcing fairness at the memory write stage through DMT can address the growing risk of implicit bias in memory-equipped LLMs.

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [933] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: The study explores the mean-field Langevin descent-ascent (MFL-DA) for entropically regularized zero-sum games. It focuses on proving local exponential stability of the equilibrium near initialization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address an open question concerning the stability and rate of convergence of MFL-DA for nonconvex-nonconcave payoffs, where the long-time behavior remains unclear.

Method: The authors use spectral analysis of a linearized operator to establish a coercivity estimate for entropy near equilibrium and reveal a local displacement convex-concave structure driving contraction.

Result: It is proven that the equilibrium of MFL-DA dynamics is locally exponentially stable when initialized close to equilibrium in Wasserstein metric.

Conclusion: The findings provide partial resolution to the stability question posed by Wang and Chizat, establishing local stability and exponential convergence while leaving global convergence as a challenge.

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [934] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: The paper introduces gWorld, a new approach to visual world modeling for mobile graphical user interfaces by generating executable web code instead of direct pixel rendering, achieving high performance and overcoming limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the trade-offs in current mobile GUI world models: balancing high visual fidelity and precise text rendering without relying on slow, complex pipelines or external resources.

Method: The proposed method uses a single Vision-Language Model (VLM) to predict executable web code that renders graphical user interface states, combining the benefits of linguistic accuracy and structured visual data representation.

Result: The gWorld model demonstrates superior performance, outperforming models over 50 times larger on multiple benchmarks, and scales effectively in training data, enhancing both data quality and mobile GUI policy modeling.

Conclusion: The research successfully establishes a novel and efficient paradigm for mobile GUI world modeling, showing scalability, robustness, and practical enhancements in both performance and visual-textual fidelity.

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [935] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: This paper proposes two active learning algorithms to improve sample efficiency in collecting human preference labels for aligning large language models.


<details>
  <summary>Details</summary>
Motivation: The process of obtaining human preference labels to align large language models is costly, and existing methods using classical design criteria are not well-suited for preference learning.

Method: Two methods are presented: one offering instance-dependent label complexity guarantees, and another practical greedy algorithm. Both are motivated by insights specific to preference learning.

Result: The proposed algorithms demonstrate improved sample efficiency over existing methods when evaluated on real-world preference datasets.

Conclusion: The paper highlights the limitations of classical approaches in preference learning and provides novel, tailored methods that enhance the learning process in terms of efficiency.

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [936] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: The paper introduces LSINet, a lightweight model improving long-term time-series forecasting (TSF) by leveraging sparse temporal interactions instead of traditional self-attention.


<details>
  <summary>Details</summary>
Motivation: Current linear models for TSF underperform in capturing complex temporal dependencies despite outperforming some transformer models, indicating room for improvement.

Method: LSINet employs a proposed Multihead Sparse Interaction Mechanism (MSIM) using sparsity-induced Bernoulli distribution for temporal dependencies, coupled with self-adaptive regularization loss and Shared Interaction Learning (SIL) for efficiency.

Result: The model outperforms advanced linear and transformer models in terms of accuracy and efficiency according to experiments on public datasets.

Conclusion: LSINet is a superior TSF model with a lightweight MLP structure, offering explicit temporal interaction mechanisms for enhanced performance in forecasting.

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [937] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: This paper introduces SpecTF, a framework for multimodal time series forecasting that combines textual and numerical data in the frequency domain to improve predictions.


<details>
  <summary>Details</summary>
Motivation: Current methods for multimodal time series forecasting often fail to consider the global context of textual data and its multiscale influences on time series data.

Method: SpecTF projects textual embeddings into the frequency domain, integrates them with the spectral components of time series via a cross-attention mechanism, and converts the results back to the temporal domain for predictions.

Result: SpecTF surpasses state-of-the-art models on numerous multimodal datasets with fewer parameters, as shown by experimental results.

Conclusion: The proposed method effectively leverages frequency domain analysis to integrate textual data into time series forecasting, achieving significant performance improvements.

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [938] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: The study explores parameter redundancy in machine learning models, finding that training only 1% of parameters can match or exceed full-parameter performance in reinforcement learning tasks.


<details>
  <summary>Details</summary>
Motivation: The research investigates how parameter redundancy in machine learning, specifically in reinforcement learning, can be effectively exploited to reduce computational overhead and enhance efficiency.

Method: The authors train randomly selected subsets of parameters at extreme sparsity levels and analyze their performance across models and tasks, introducing the concept of multiple viable sparse subnetworks (Multiple Ticket Hypothesis).

Result: Training just 1% of parameters demonstrated performance comparable to full-parameter fine-tuning, with different random masks achieving success and minimal overlap among subnetworks.

Conclusion: Pretrained models contain many viable sparse subnetworks (not just one), and reinforcement learning constraints enable sparse parameter updates to succeed, supporting the Multiple Ticket Hypothesis.

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [939] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: The paper introduces FLAME, a framework to enable efficient one-step generation in Flow Matching-based Maximum Entropy Reinforcement Learning, overcoming challenges such as intractable policies and discretization bias.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address high inference latency in expressive diffusion policies and the integration challenges in Flow Matching within MaxEnt RL, optimizing exploration-exploitation trade-offs.

Method: The paper proposes FLAME, which includes a Q-Reweighted FM objective for bypassing partition function estimation, a decoupled entropy estimator to correct bias, and MeanFlow for efficient one-step control.

Result: Empirical results show FLAME's superior performance on MuJoCo benchmarks, outperforming Gaussian baselines and matching multi-step diffusion policies at lower inference cost.

Conclusion: FLAME provides a principled solution for effective and efficient RL policy generation, combining expressiveness and reduced inference latency.

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [940] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: The paper introduces PIPE, a protocol for evaluating and diagnosing interface reliance in agents, and highlights issues with interface shortcutting in standard evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the conflation of task success in agents due to semantic tool-use versus memorization of interface-specific patterns, and to improve the evaluation of environment-invariant agent capabilities.

Method: Introduces PIPE, which rewrites environment interfaces minimally while preserving semantics to diagnose reliance. It also develops the Interface Reliance (IR) metric to measure an agent's dependency on the training interface.

Result: PIPE applied to 16 environments reveals trajectory-SFT amplifies reliance on specific interfaces, with significant degradation under interface changes. Non-trajectory-trained agents show better stability.

Conclusion: Agents trained using trajectory-SFT overly depend on training interfaces, leading to non-robust performance. Environment-invariant evaluation practices need improvement for consistent assessments.

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [941] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima compresses large language models for GPU memory and latency efficient use, achieving significant performance improvements and supporting speculative decoding.


<details>
  <summary>Details</summary>
Motivation: Address the deployment limitations of large language models caused by high GPU memory requirements and inference latency.

Method: Minima learns structural compression using sensitivity predictors, applies tensor decompositions to less sensitive areas, fine-tunes the model briefly, and optimizes execution with custom kernels.

Result: Achieved significant memory reduction (from 64 GiB to 40 GiB VRAM), improved throughput (from 40 to 75 tokens per second for individual requests), and sustained improvements under high concurrency.

Conclusion: Minima offers a practical, effective solution for compressing Transformers, enabling more efficient deployment and setting the stage for future innovations in structural compression.

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [942] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: The paper introduces a benchmark dataset integrating physics-based simulations and real-world data to facilitate AI-driven agroecosystem modeling for greenhouse gas prediction.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate tools and datasets necessary for quantifying carbon, nutrient, and water dynamics in agroecosystems, which are critical for mitigating greenhouse gas emissions and advancing sustainability.

Method: Developed a spatial-temporal agroecosystem GHG benchmark dataset combining physics-based model simulations with real-world observations, and evaluated sequential deep learning models and transfer learning approaches for flux prediction.

Result: Various deep learning models, including LSTM, temporal CNN, and Transformers, were tested for their performance on this dataset. Transfer learning helped improve model generalization when applied to real-world observations.

Conclusion: The dataset and evaluation framework enable the development of better AI-models, significantly enhancing the ability to study ecosystem-climate interactions and contributing to environmental sustainability.

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [943] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: The paper introduces SUSD, a novel framework for discovering diverse and dynamic skills without supervision by leveraging compositional structures in environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing USD methods like MI and DSD, which fail to discover dynamic and comprehensive skills in complex environments.

Method: SUSD factorizes the state space into independent components and allocates skill variables to these factors. It tracks learning dynamically to promote exploration of underexplored areas, resulting in more diverse skill discovery.

Result: SUSD outperforms existing USD methods in discovering complex and diverse skills across environments with varying factors, enabling efficient downstream task training.

Conclusion: The study shows that SUSD provides a structured, fine-grained approach for skill discovery, enhancing skill diversity and enabling better control over entity-level interactions in complex environments.

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [944] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: FedMuscle is a federated multi-task learning algorithm focused on heterogeneous tasks and models, using a novel Muscle loss to align shared representation spaces instead of parameters.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing FMTL frameworks which require homogeneous models across users, making them unsuitable for diverse real-world settings.

Method: Muscle loss is introduced as a contrastive learning objective to align representations across tasks by maximizing mutual information among models' outputs.

Result: FedMuscle achieves superior performance compared to state-of-the-art methods on various image and language tasks, demonstrating robustness in heterogeneous conditions.

Conclusion: FedMuscle effectively tackles both model and task heterogeneity, providing an efficient and practical approach to federated multi-task learning.

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [945] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: The paper proposes COMET for time-series anomaly detection, using multi-scale patch encoding, vector-quantized coreset, and online codebook adaptation to improve detection across various temporal ranges.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with limited exploration of temporal and multivariate correlations, reliance on single-scale patterns, and vulnerability to distribution shifts in normal data representations during inference.

Method: COMET utilizes multi-scale patch encoding to capture temporal dependencies, a vector-quantized coreset to learn normal patterns and detect anomalies, and online codebook adaptation with contrastive learning for dynamic model adjustment at inference.

Result: Experiments on five benchmark datasets show COMET achieves top performance in 36 out of 45 evaluation metrics, demonstrating its effectiveness across diverse scenarios.

Conclusion: COMET overcomes major limitations of current models, proving its capability in adaptive and robust anomaly detection across multivariate and temporal ranges in time-series data.

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [946] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: The paper introduces "chance-constrained inference," a method to mitigate factual hallucinations in large language models by bounding the likelihood of errors through a sequential, adaptive procedure.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to address the lack of explicit probabilistic guarantees in existing mitigation strategies for hallucinations in large language models, which pose risks during deployment.

Method: The method involves formulating inference as a risk control problem by modeling hallucinations as stochastic constraint violations. It introduces chance-constrained inference that uses a sequential, anytime-valid adaptive procedure to efficiently maintain feasibility bounds.

Result: Experiments show that the proposed approach ensures reliable risk control, identifies infeasible inputs early, and supports safe repeated use, outperforming confidence-based baselines in providing probabilistic guarantees.

Conclusion: Chance-constrained inference can effectively manage the risks of factual hallucinations in language models, improving safety and reliability during real-world applications.

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [947] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen leverages many-body interactions to improve molecular structure generation from mass spectrometry.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current methods where higher-order edge interactions and many-body characteristics are not effectively modeled, leading to reduced accuracy in molecular generation and isomer differentiation.

Method: A many-body enhanced diffusion framework named MBGen is introduced, incorporating a many-body attention mechanism and higher-order edge modeling to better utilize MS/MS spectral data.

Result: MBGen outperformed state-of-the-art methods on NPLIB1 and MassSpecGym benchmarks, achieving performance improvements of up to 230%.

Conclusion: MBGen's incorporation of many-body modeling significantly enhances its ability to capture complex molecular interactions, accurately generate structures, and differentiate isomers, advancing the field of computational mass spectrometry.

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [948] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: The paper explores why neural networks fail to generalize tasks like addition for larger inputs and introduces a physics-driven neural architecture, SEAD, to address this challenge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and resolve the failure of neural networks to generalize tasks (e.g., addition) for larger-scale inputs, unlike humans who can generalize logically.

Method: The authors identify three physics-based constraints (locality, symmetry, stability) necessary for generalization and derive the SEAD architecture, a neural cellular automaton.

Result: SEAD achieves remarkable results, such as 100% accuracy in scale-invariant addition up to 1 million digits, perfect length generalization in parity, and accurate learning of Turing-complete cellular automata.

Conclusion: SEAD demonstrates that respecting physical principles in computation can bridge gaps between statistical learning and logical reasoning, overcoming traditional generalization failures of neural networks.

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [949] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: This paper investigates the adversarial vulnerability of offline bandit algorithms when the reward model is perturbed, revealing that small, targeted changes can significantly manipulate the evaluation outcomes.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored vulnerability of offline bandit algorithms to adversarial perturbations in reward models, which could compromise their reliability in machine learning evaluations.

Method: The authors introduce a novel threat model and analyze the vulnerability of bandit algorithms through theoretical proofs and extensive experiments using reward functions, including linear and nonlinear models like ReLU neural networks. They study attacks on generative model evaluators hosted on Hugging Face, focusing on image aesthetics and compositional alignment.

Result: The findings reveal that adversarial perturbations to reward model weights, even small and imperceptible ones, can drastically alter bandit behavior, with increasing vulnerability in high-dimensional settings. Random perturbations fail, but targeted attacks achieve high success rates.

Conclusion: Offline evaluations using bandit algorithms are highly susceptible to adversarial attacks on reward models, emphasizing the need for robust defenses, especially for high-dimensional applications like image evaluations.

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [950] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: The paper studies epistemic predictive uncertainty within conformal prediction, proposing a measure based on Maximum Mean Imprecision to improve uncertainty assessments.


<details>
  <summary>Details</summary>
Motivation: Quantifying epistemic predictive uncertainty in the conformal prediction framework, looking beyond conformal prediction region size for a more detailed analysis.

Method: Utilizes the concept of credal sets derived from CP, develops Maximum Mean Imprecision, and validates through experiments on active learning and selective classification.

Result: The novel uncertainty measure provides more informative assessments than relying solely on conformal prediction region size.

Conclusion: The work demonstrates conformal prediction's capability to act as a robust foundation for decision-making under epistemic uncertainty.

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [951] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: The paper introduces ASGMamba, an efficient forecasting framework for long-term multivariate time series, addressing scalability and noise challenges in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Existing models for long-term time series forecasting either face scalability issues (Transformers) or struggle to handle noisy signals (SSMs), which limits their effectiveness in high-performance computing applications.

Method: The proposed ASGMamba integrates an Adaptive Spectral Gating mechanism to filter noise dynamically and employs a hierarchical multi-scale architecture with Node Embeddings for diverse physical properties.

Result: ASGMamba achieves state-of-the-art accuracy on nine benchmarks while significantly reducing memory usage and maintaining linear complexity.

Conclusion: ASGMamba offers a scalable and accurate solution for time series forecasting in resource-limited environments, addressing both noise filtering and computational efficiency.

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [952] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: The paper introduces Wasserstein Policy Regularization (WPR), a semantic-aware approach for aligning large language models (LLMs) using reinforcement learning from human feedback (RLHF).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of conventional KL divergence-based regularization, which fails to capture semantic similarities in token distributions during human preference alignment of LLMs.

Method: The proposed Wasserstein Policy Regularization (WPR) leverages the entropy-regularized Wasserstein distance, incorporating token space geometry. It uses dual formulations that integrate into standard reinforcement learning algorithms.

Result: Experimental results show WPR surpassing baseline methods (KL and $f$-divergence) in performance, highlighting improved model alignment using semantic-aware token distances.

Conclusion: WPR provides a promising alternative to traditional KL-regularization in RLHF, improving alignment efficiency by incorporating semantic token relationships. The code is publicly available for further exploration.

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [953] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: This paper introduces Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a method utilizing continuous latent space with guided diffusion for improved LLM reasoning and exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning methods in token space face challenges like diversity collapse during Chain-of-Thought generation due to policy entropy reduction.

Method: LaDi-RL explores discrete Chain-of-Thought reasoning in continuous latent space using guided diffusion, distributing stochasticity across multi-step denoising while maintaining semantic trajectories.

Result: Experiments on code generation and mathematical reasoning benchmarks show significant performance improvements over discrete RL methods, with pass@1 gains of +9.4% for code and +5.7% for math.

Conclusion: Diffusion-based latent RL offers a principled and effective alternative to discrete token-level RL for semantic reasoning tasks with improved exploration and performance outcomes.

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [954] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: This paper evaluates the robustness of generalization measures in deep learning, specifically focusing on prediction beyond training distributions, across various experimental settings and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generalization in deep learning models by predicting their performance in non-IID (independent and identically distributed) settings using metrics available before test time.

Method: The authors trained models using 10,000 hyperparameter configurations, evaluated over 40 generalization measures, and benchmarked these across IID and non-IID settings, multiple architectures, and training recipes.

Result: The study finds that distribution shifts adversely impact the predictive power of many generalization measures, although a small subset of these measures remains robust across diverse settings.

Conclusion: Generalization measures should be carefully chosen as most are sensitive to distribution shifts, though some robust measures can serve as reliable indicators across various scenarios.

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [955] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: This paper tackles training instability in large language models by identifying key phenomena causing gradient explosions and proposes a novel optimizer, MSign, to prevent such failures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of training instability in large language models, which results in sudden gradient explosions and wastes computational resources.

Method: The study identifies two instability indicators: decline in weight matrix stable rank and increasing Jacobian alignment. It introduces MSign, an optimizer that periodically applies matrix sign operations to maintain stable rank and prevent instability.

Result: MSign effectively prevents training failures in models ranging from 5M to 3B parameters, with a computational overhead of less than 7.0%.

Conclusion: By addressing the underlying causes of training failures, MSign improves the robustness of LLM pretraining with minimal computational cost.

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [956] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: This paper critiques the effectiveness and scalability of neural network architectures in time series forecasting, highlighting their limitations and offering a call to refocus research efforts.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of neural network architectures in achieving both generalizability across time series domains and domain-specific state-of-the-art results. They aim to provide insights into the stagnation of progress in this area and propose alternative directions.

Method: The paper performs a critical analysis of current neural network designs for time series forecasting, scrutinizing their inability to balance general domain performance with practical domain-specific applicability.

Result: The authors find that neural network architectures rarely succeed in both generalizability and domain-specific performance, and the field has reached a performance plateau with overly complex models.

Conclusion: The paper advocates for a shift in focus from general-domain time series neural network architectures to either domain-specific deep learning methods or meta-learning approaches for broader applicability.

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [957] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: The paper introduces Softmax Linear Attention (SLA), which enhances linear Transformers by restoring global competition, improving focus and robustness without losing efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of expressivity in linear attention mechanisms by reintroducing global competition for better focus on relevant information amidst noise.

Method: Proposed SLA framework shifts softmax normalization from the token to the head level, using attention heads as semantic slots and a gating mechanism for precise selection.

Result: SLA improves performance across state-of-the-art linear models (RetNet, GLA, GDN) in language modeling and long-context tasks, particularly in retrieval under noise.

Conclusion: SLA effectively enhances linear Transformers by combining efficiency with precise retrieval and robustness, leveraging multi-head structure for better performance.

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [958] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: The paper introduces RankTuner, a token-level reweighting method that integrates probability and entropy to better calibrate fine-tuning objectives, penalizing under-learned tokens while avoiding penalizing intrinsically uncertain ones.


<details>
  <summary>Details</summary>
Motivation: Current token-level reweighting mechanisms overly rely on one-dimensional indicators, like probability or entropy individually, failing to effectively manage noisy or uncertain tokens while aligning with task-specific targets.

Method: RankTuner uses the Relative Rank Indicator, comparing the rank of the ground-truth token relative to expected rank under predictions, and employs an inverse indicator as Relative Scale for token-wise fine-tuning reweighting.

Result: Experiments demonstrate improvement in mathematical reasoning benchmarks, better transfer to out-of-distribution tasks, and superior code generation performance compared to baseline methods.

Conclusion: RankTuner offers a nuanced token-weighting mechanism that achieves enhanced fine-tuning by addressing joint uncertainty and alignment, serving as a consistent improvement over one-dimensional approaches.

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [959] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: The paper introduces FedGaLore to address LoRA's underperformance in non-IID federated settings by resolving update-space and optimizer-state mismatches.


<details>
  <summary>Details</summary>
Motivation: LoRA underperforms in federated fine-tuning environments with non-IID data due to mismatches during client updates and server aggregation.

Method: FedGaLore uses GaLore-style gradient subspace optimization on clients and server drift-resistant synchronization via spectral shared-signal extraction.

Result: FedGaLore improves robustness and accuracy on benchmarks for natural language understanding, vision, and natural language generation tasks.

Conclusion: Combining subspace optimization and drift-resistant synchronization resolves mismatches, enhancing model performance in federated non-IID settings.

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [960] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: The paper introduces MGKAN, a graph-based model aiming to better predict drug-drug interactions (DDIs) using learnable asymmetric and nonlinear features. It outperforms competitive baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the prediction of DDIs, which is critical for safe drug treatments. Existing models suffer from limitations like linear aggregation and symmetry assumptions, which fail to capture complex drug relationships.

Method: The proposed MGKAN integrates learnable Kolmogorov-Arnold-based functions, replacing traditional MLP layers for expressive and nonlinear relationship modeling. It uses three network views (DDI, co-interaction, biochemical similarity) with embeddings to preserve directional semantics and blends these via a specialized fusion module.

Result: MGKAN outperforms seven state-of-the-art models on two benchmark datasets. Ablation and case studies confirm its superior predictive accuracy and effective handling of drug interaction directionality.

Conclusion: MGKAN successfully enhances drug-drug interaction prediction, demonstrating its ability to model nonlinear, heterogeneous, and directional relationships effectively.

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [961] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: The paper investigates and offers theoretical insights on the expressive power of different attention mechanisms in transformers, contrasting full attention with linear and hybrid variants.


<details>
  <summary>Details</summary>
Motivation: To fill the theoretical gap in understanding the expressive power differences among attention mechanisms like full, linear, and hybrid attention in large language models.

Method: The study establishes an expressiveness hierarchy and proves performance differences by analyzing sequential function composition, comparing models with distinct attention mechanisms.

Result: It reveals a hierarchy of expressiveness, showing that full attention outperforms hybrid variants for certain tasks like sequential function composition, proving the limitations of hybrid mechanisms.

Conclusion: Full attention possesses a superior expressive power compared to hybrid and linear attention mechanisms, and the findings provide a foundational theoretical perspective on the capabilities of attention types in transformers.

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [962] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: This paper presents a novel architecture (CoMeT) that enables long-context processing for large language models with constant memory usage and linear complexity.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers have quadratic complexity and their indefinitely growing KV cache limits their ability to efficiently process long contexts.

Method: CoMeT uses a dual-memory system consisting of temporary (FIFO queue) and global memory (gated update rule), fine-tuned with layer-level pipeline parallelism to efficiently process sequences. It acts as a dynamic soft prompt for next data chunks.

Result: Models integrated with CoMeT successfully retrieve information from sequences up to 1M tokens, excel on SCROLLS benchmarks, and demonstrate strong performance in real-world tasks.

Conclusion: CoMeT provides an efficient and scalable solution for long-context processing, surpassing other methods and matching full-attention baselines in performance on certain tasks.

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [963] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: The paper introduces IRIS, a method addressing hallucination in Multimodal Large Language Models (MLLMs) by leveraging implicit rewards and self-generated preference pairs without external feedback.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue of hallucinations in Multimodal Large Language Models, which stem from fine-grained conflicts between modalities during generation.

Method: IRIS employs an on-policy approach, utilizing implicit rewards in the native log-probability space to resolve modal conflicts with self-generated preference pairs.

Result: Experiments show IRIS performs competitively on hallucination benchmarks, achieving success with only 5.7k samples and without external feedback reliance.

Conclusion: IRIS presents an efficient and principle-driven solution to mitigating hallucinations in MLLMs, eliminating reliance on costly external evaluations.

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [964] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: The paper introduces DIA-CLIP, a pre-trained model for high-precision, zero-shot peptide-spectrum match (PSM) inference in mass spectrometry, outperforming current tools and enhancing proteomic research applications.


<details>
  <summary>Details</summary>
Motivation: Existing Data-independent acquisition mass spectrometry frameworks rely on semi-supervised training for peptide-spectrum match re-scoring, which is prone to overfitting and struggles with generalizability across species and conditions.

Method: The authors propose DIA-CLIP, integrating a dual-encoder contrastive learning framework with an encoder-decoder architecture, enabling universal cross-modal representation learning for peptides and spectral features.

Result: DIA-CLIP outperforms state-of-the-art methods, achieving up to a 45% increase in protein identification and a 12% reduction in entrapment identifications across benchmarks.

Conclusion: DIA-CLIP shifts the paradigm of DIA-MS analysis with its universal cross-modal learning approach, paving the way for improved biomarker discovery and insight into cellular processes in various applications like single-cell and spatial proteomics.

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [965] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: This paper introduces the concept of agentic time series forecasting (ATSF), proposing a shift from model-centric prediction to agentic workflows that leverage features like perception, planning, feedback, and experience.


<details>
  <summary>Details</summary>
Motivation: Traditional time series forecasting models are limited in adaptive, iterative, and tool-interacting settings required for dynamic applications.

Method: The paper proposes the ATSF paradigm based on perception, planning, action, reflection, and memory, supported by three paradigms: workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow.

Result: The concept promotes the development of forecasting systems that go beyond static predictions, effectively integrating reasoning and adaptability.

Conclusion: Agentic forecasting is positioned as a transformative approach for advancing time series forecasting research, emphasizing adaptability and iterative learning.

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [966] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: The Grad2Reward framework enhances reinforcement learning in open-ended tasks by deriving dense rewards using gradient-based attribution from a Judge's inference process, allowing finer supervision and improved reasoning for tasks like mathematics and programming.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of sparse rewards and black-box treatment in current RL approaches, which hinder the learning of complex, long-form trajectories in open-ended tasks.

Method: Grad2Reward extracts dense, process-level rewards from the Judge's inference process using gradient-based attribution for token-level credit assignment and introduces a self-judging mechanism for autonomous improvement.

Result: Experiments show that Grad2Reward significantly improves the efficiency and reasoning quality of policies, achieving high performance in a variety of open-ended tasks.

Conclusion: Grad2Reward is effective in providing precise rewards and enhancing training processes in RL for open-ended tasks, showing broad applicability and better learning outcomes.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [967] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: This paper addresses the instability in Reinforcement Learning for training Large Language Models by proposing a dynamic LR scheduler to counteract training-inference mismatch and gradient noise.


<details>
  <summary>Details</summary>
Motivation: To address the notorious instability of Reinforcement Learning in training Large Language Models, which worsens as training progresses due to increased gradient noise and mismatch between training and inference.

Method: The study proposes a dynamic Learning Rate scheduler that decays learning rates based on response length, which acts as an early-warning signal for instability, to mitigate gradient noise and mismatch issues.

Result: The proposed Learning Rate adjustment method effectively stabilizes RL training by suppressing gradient noise and keeping the training-inference mismatch manageable.

Conclusion: Dynamic LR scheduling based on response length provides a simple and effective way to stabilize Reinforcement Learning for Large Language Models, addressing optimization issues linked to training-inference mismatch and noise escalation.

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [968] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: The paper questions the common adoption of Hyperbolic Graph Neural Networks (HGNNs), introducing a geometry-task alignment condition to evaluate their suitability for certain graph-related tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the applicability and limitations of HGNNs by investigating whether geometry-task alignment plays a role in their efficacy for different graph-based learning tasks.

Method: The methodology involves both theoretical and empirical analysis, including synthetic regression problems and evaluations of HGNNs on link prediction and node classification tasks while measuring predictive performance and embedding distortion.

Result: The paper found that HGNNs recover low-distortion representations when metric structure preservation is required, with performance advantages visible only in tasks aligned with hyperbolic geometry, such as link prediction.

Conclusion: The work emphasizes the need to evaluate the alignment between geometry and task requirements, showing that HGNNs perform better than Euclidean models under proper alignment but lose their advantage otherwise.

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [969] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: Modern single-cell transcriptomics requires improvements in data representation rather than model complexity. DOGMA offers a new data-centric framework that enhances raw data with biological priors, improving efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Current methods for analyzing single-cell transcriptomics either oversimplify cell relationships or neglect biological knowledge, leading to inefficiencies and inaccurate representations.

Method: DOGMA integrates multi-level biological prior knowledge, using ontologies and phylogenetic trees for graph construction and functional priors for better data representation.

Result: DOGMA delivers state-of-the-art (SOTA) performance in zero-shot tasks, cross-species analysis, and sample efficiency, with reduced computational requirements.

Conclusion: DOGMA provides significant improvements in single-cell transcriptomics analysis by employing deterministic graph construction and semantic data enhancement, outperforming existing methods.

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [970] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: The paper introduces Prism, an efficient test-time scaling (TTS) framework for discrete diffusion language models (dLLMs), using hierarchical search, remasking, and self-verification methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of inference-time test-time scaling (TTS) methods for dLLMs due to their unique parallel decoding process, aiming to unlock their full generative capabilities.

Method: The proposed method, Prism, includes: (i) Hierarchical Trajectory Search (HTS) for dynamic compute allocation, (ii) Local branching with partial remasking to maintain diversity and token confidence, and (iii) self-verification using Self-Verified Feedback (SVF) instead of external verifiers.

Result: Prism demonstrates a robust performance-efficiency trade-off across multiple benchmarks, matching state-of-the-art performance with significantly fewer function evaluations (NFE).

Conclusion: Prism enhances the efficiency and effectiveness of dLLMs in reasoning tasks by solving issues with traditional TTS methods, and its success is validated through experiments on mathematical and code generation tasks.

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [971] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust, a 309M-parameter causal protein language model (PLM), combines predictive and generative functionalities using architectural innovations. It achieves competitive performance across multiple benchmarks while remaining computationally efficient.


<details>
  <summary>Details</summary>
Motivation: The dichotomy in PLMs between predictive (MLMs) and generative (causal models) capabilities often forces researchers to use separate models for different tasks, leading to inefficiencies.

Method: Proust integrates grouped-query attention, cross-layer value residuals, and depthwise causal convolutions, leveraging architectural innovations to unify predictive and generative functionalities in one model. It was trained on 33B tokens using a computationally efficient method.

Result: Proust achieves competitive predictive performance compared to MLMs requiring much higher computational resources. It outperforms larger models on indel tasks and approaches the performance of structure-aware methods on viral fitness benchmarks.

Conclusion: Proust offers a unified and computationally efficient PLM that handles both predictive and generative tasks, making it versatile for protein-related tasks while providing insights for further scaling and enhancements.

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [972] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: The paper introduces self-rewarding sequential Monte Carlo (SMC) as an inference-time algorithm for improving sampling in masked diffusion language models by enabling more diverse and high-quality outputs.


<details>
  <summary>Details</summary>
Motivation: Current masked diffusion language models (MDLMs) suffer from limited diversity in generation due to their reliance on confidence-based, greedy sampling approaches.

Method: The authors propose running multiple parallel diffusion processes, or particles, and use trajectory-level confidence as a self-reward signal to guide resampling and weighting of particles.

Result: The method improves sampling quality across various MDLMs and benchmarks without requiring additional training or reward modifications.

Conclusion: Self-rewarding SMC converts existing parallel inference capabilities into better sampling outcomes, enhancing both diversity and quality of generated samples.

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [973] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: This paper introduces an efficient deep learning framework for controlling myoelectric prostheses using minimal sensors, achieving high accuracy with novel techniques like Time2Vec embeddings and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To make myoelectric prosthesis control more accessible by reducing the need for dense multi-sensor arrays while maintaining accurate functionality.

Method: The framework utilizes a hybrid Transformer architecture with Time2Vec learnable embeddings, normalized additive fusion, and curriculum learning, leveraging sparse sEMG data from 8 subjects.

Result: Achieved a state-of-the-art F1-score of 95.7% for multi-class movement prediction and recovered high performance with rapid calibration for unseen subjects.

Conclusion: The study demonstrates that minimal sensor hardware combined with optimized temporal embeddings can achieve reliable and accurate control, offering a cost-effective solution for personalized prosthetic interfaces.

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [974] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: The paper introduces the autocorrelated Optimize-via-Estimate (A-OVE) model for decision-making under autocorrelated uncertainties, showing superior performance over traditional methods in portfolio optimization.


<details>
  <summary>Details</summary>
Motivation: Compare models in data-driven optimization under conditions of autocorrelated uncertainties and seek to improve decision accuracy while reducing regret in real-world applications.

Method: Develop the A-OVE model that computes out-of-sample optimal solutions using recursive sufficient statistics under VARMA(p,q) processes, tested in a portfolio optimization scenario.

Result: A-OVE demonstrated lower regret compared to a perfect information oracle and outperformed predict-then-optimize benchmarks, maintaining robust performance even under slight mis-specifications.

Conclusion: Autocorrelated uncertainties, handled effectively through A-OVE, provide improved decision-making outcomes, suggesting traditional estimates may mislead decision quality despite high prediction accuracy.

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [975] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: This paper proposes 'internal flow signatures' as a lightweight method for LLMs to conduct self-checking and refinement without relying on external validation.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate fluent yet unfaithful responses, requiring new methods for internal validation and refinement.

Method: Internal flow signatures monitor token-wise motion, summarize trajectories through compact subspaces, and train a GRU validator for self-checking and anomaly detection.

Result: The pipeline enables self-checking, localization of depth events causing errors, and dynamic refinement through rollback and clamping abnormal steps.

Conclusion: This approach offers a low-overhead internal validation strategy that enhances LLM decision-making reliability and localization capabilities.

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [976] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace is introduced as a fast and faithful token attribution mechanism to address inefficiencies and faithfulness drops in modern long-context and reasoning tasks using language models.


<details>
  <summary>Details</summary>
Motivation: Token attribution methods currently encounter challenges of inefficiency and faithfulness in working with language models, particularly for long-context inputs and extended reasoning chains.

Method: The paper proposes FlashTrace, a method employing span-wise aggregation for efficient multi-token attribution and a recursive attribution mechanism to improve faithfulness through reasoning chains.

Result: FlashTrace outperformed existing methods, showing over 130x speedup and improved faithfulness in tasks like long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA). A single recursive attribution hop already enhances faithfulness.

Conclusion: FlashTrace effectively addresses both key challenges: it is significantly more efficient and capable of maintaining faithfulness even in complex reasoning tasks.

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [977] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: This paper proposes a novel method to enhance reinforcement learning by using vision-language models (VLMs) to prioritize experiences in the replay buffer, improving success rates and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve sample efficiency, high-level planning, and interpretability in reinforcement learning through efficient usage of the replay buffer by leveraging advanced vision-language models (VLMs).

Method: The method uses a frozen, pre-trained VLM, without fine-tuning, to evaluate and prioritize promising sub-trajectories in the replay buffer based on the agent’s experiences.

Result: Experiments demonstrate that the proposed prioritization method leads to 11-52% higher average success rates and 19-45% improved sample efficiency across various tasks, including discrete and continuous domains.

Conclusion: This approach demonstrates that incorporating VLMs into the replay buffer of reinforcement learning can significantly enhance performance and efficiency without requiring fine-tuning of the VLM.

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [978] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: PIMPC-GNN tackles imbalanced node classification in graph neural networks by integrating physics-informed models for improved minority-class recall and balanced accuracy.


<details>
  <summary>Details</summary>
Motivation: GNNs face challenges in class-imbalanced scenarios, where predictions are biased towards majority classes, necessitating an approach that performs well in these settings.

Method: PIMPC-GNN integrates thermodynamic diffusion for long-range dependency propagation, Kuramoto synchronization for node alignment, and spectral embedding for structural class separation, along with class-adaptive ensemble weighting and imbalance-aware loss.

Result: Tested on five benchmark datasets with imbalance ratios between 5-100, PIMPC-GNN outperformed 16 state-of-the-art methods, with up to +12.7% improvement in minority-class recall and +8.3% in balanced accuracy.

Conclusion: PIMPC-GNN advances imbalanced graph classification with both empirical gains and interpretable insights, demonstrating superior performance in minority class prediction.

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [979] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: This review focuses on the challenges of embedding learning for multiplex networks in link prediction tasks, proposing new taxonomies, evaluation procedures, and guidelines.


<details>
  <summary>Details</summary>
Motivation: Embedding learning is essential for capturing complex information in networks and advancing link prediction, but challenges increase with network complexity.

Method: The paper introduces refined taxonomies for organizing models, addresses reproducible evaluation practices, and develops a novel testing procedure for directed multiplex networks.

Result: The review enhances understanding of multiplex network embedding methods, proposes fair evaluation guidelines, and informs on tools and challenges in downstream analyses.

Conclusion: This work is a step towards improving embedding learning techniques for better link prediction on multiplex networks while ensuring fair evaluation and reproducibility.

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [980] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC is a probabilistic framework that effectively integrates multimodal clinical data with missing elements for better modeling, interpretation, and prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in analyzing multimodal clinical data characterized by high dimensionality, heterogeneous representations, and structured missingness while improving predictive modeling, interpretability, and data integration.

Method: BIONIC integrates heterogeneous multimodal data into a joint generative-discriminative latent model, leveraging pretrained embeddings for complex data types and directly incorporating structured clinical variables within a Bayesian framework.

Result: The framework demonstrates strong predictive performance and robustness on three multimodal clinical datasets, especially under incomplete data scenarios, outperforming baseline models.

Conclusion: BIONIC provides a powerful tool for multimodal data analysis by delivering robust predictions and intrinsic interpretability, aiding in clinically relevant insights and population-level analysis.

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [981] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: The paper introduces MCPST, a framework for improved few-shot traffic flow forecasting in cross-domain and sparse-data contexts, achieving high accuracy through multi-phase consensus learning.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in traffic flow prediction, particularly in scenarios lacking sufficient data and involving non-linear urban mobility dynamics.

Method: The MCPST framework employs a multi-phase engine for traffic modelling, an adaptive consensus mechanism for phase fusion, and a meta-learning strategy for adaptation to new cities with minimal data.

Result: MCPST significantly outperforms fourteen state-of-the-art methods across four datasets, achieving better accuracy, reducing data requirements, and providing insights.

Conclusion: The proposed MCPST framework effectively tackles complex traffic prediction issues, offering robust and adaptive solutions with broad applications in intelligent transportation systems.

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [982] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: The paper proposes T-LLM, a framework equipping large language models with time series forecasting abilities using temporal distillation from a lightweight teacher model.


<details>
  <summary>Details</summary>
Motivation: Existing approaches face challenges in teaching forecasting to LLMs because of time-bound constraints in time series data, rendering traditional pretraining insufficient.

Method: The proposed method involves transferring predictive behavior from a lightweight teacher model that uses trend modeling and frequency-domain analysis during training, allowing the LLM to forecast autonomously at inference.

Result: Experiments demonstrate that T-LLM outperforms existing LLM-based methods across full-shot, few-shot, and zero-shot settings on benchmarks and infectious disease forecasting tasks.

Conclusion: T-LLM enhances LLMs with efficient, high-performance forecasting capabilities while maintaining a simple deployment process.

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [983] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: The study innovates automated floorplan generation by introducing Diversity Score for layout diversity and Boundary Cross-Attention for better boundary adherence, highlighting a trade-off between realism and diversity.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for floorplan generation optimize perceptual metrics like FID but lack design diversity.

Method: The paper introduces Diversity Score (DS) as a measure for layout diversity and builds a Boundary Cross-Attention (BCA) module to condition on building boundaries.

Result: Experiments show BCA enhances geometric consistency and FID fails to diagnose diversity loss during prolonged training.

Conclusion: The research emphasizes balancing fidelity, diversity, and generalization in architectural design tasks by exposing models' dependency on dataset priors.

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [984] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: The paper introduces a method to efficiently estimate epistemic uncertainty (EU) in Large Language Models (LLMs) using small draft models, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational prohibitive nature of estimating epistemic uncertainty in LLMs through deep ensembles and to enable risk-aware deployment in safety-critical tasks.

Method: The proposed method uses draft models to estimate EU via a Bias-Variance Decomposition approach. It also employs Online Stochastic Distillation (OSD) and Data-Diverse Drafts (DDD) to improve accuracy and diversity in EU estimation.

Result: The approach reduces estimation error (RMSE) by up to 37% compared to baselines and shows competitive hallucination detection performance compared to perturbation-based methods, with negligible inference costs.

Conclusion: This method provides an efficient and effective solution to epistemic uncertainty estimation for LLMs, supporting uncertainty-aware and practical deployment in various applications.

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [985] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: The proposed method, GVP-WM, addresses the issue of temporal inconsistency and physical constraint violations in video-generated plans by grounding them into feasible action sequences using a learned world model.


<details>
  <summary>Details</summary>
Motivation: Large-scale video generative models face issues with temporal consistency and physical constraints, which affect their applicability in creating executable action plans.

Method: The method involves generating a video plan, then optimizing for feasible latent trajectories using video-guided latent collocation and a learned action-conditioned world model to preserve semantic alignment and feasibility.

Result: GVP-WM successfully recovers feasible long-horizon plans from problematic video inputs in navigation and manipulation simulation tasks.

Conclusion: Grounding video-generated plans into feasible action sequences with a learned world model enhances their practical utility in zero-shot visual planning for complex tasks.

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [986] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: The paper addresses off-policy learning in zero-shot RL settings using successor measures for better adaptation.


<details>
  <summary>Details</summary>
Motivation: To mitigate challenges in off-policy RL, such as distributional shift and value function bias, especially in zero-shot scenarios.

Method: Derives theoretical connections between successor measures and stationary density ratios for optimal importance sampling and stationary distribution correction without retraining.

Result: Benchmarks demonstrate improved performance in motion tracking, continuous control, and long-horizon tasks.

Conclusion: Bridges off-policy learning and zero-shot adaptation, showcasing effective adaptation and benefits for both fields.

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [987] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: This paper introduces a self-evolving framework for LLM agents to learn from both successes and failures, while mitigating issues of retrieval inefficiency and noisy textual experiences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing LLM systems, which do not evolve over time and fail to leverage learning from failed experiences, leading to inefficiencies in retrieval and noise management.

Method: The authors introduce two key mechanisms: (1) a contrastive reflection strategy for summarizing errors and extracting insights, and (2) a self-consolidation mechanism that distills textual experiences into compact learnable parameters for better internalization within the model.

Result: Experiments show that the proposed self-evolving framework effectively enhances the long-term performance and learning capabilities of LLM agents.

Conclusion: The self-evolving framework improves on existing limitations of static LLM models by enabling them to learn from failures and success, internalizing experiences efficiently within their latent space.

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [988] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: IntraSlice introduces a block-wise intra-Transformer PCA compression pruning method to improve LLM deployment efficiency. It eliminates extra parameters and ensures minimal performance disruption.


<details>
  <summary>Details</summary>
Motivation: Large Language Models face deployment challenges due to their massive sizes. Structured pruning degrades performance, and PCA-based methods applied previously only partially address the issue while introducing extra parameters and disrupting activation distributions.

Method: The method involves a block-wise module-intra PCA compression pruning, an approximate PCA technique to fuse transformation matrices without extra parameters, and a PCA-based global pruning ratio estimator to optimize activation compression.

Result: The proposed approach was validated on Llama2, Llama3, and Phi models across language benchmarks, demonstrating superior compression performance compared to baselines at equivalent compression ratios or inference speeds.

Conclusion: IntraSlice achieves enhanced compression for LLMs, balancing performance, inference speed, and efficiency without introducing additional parameters.

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [989] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: The paper introduces FlyPrompt, a brain-inspired framework addressing challenges in General Continual Learning (GCL), achieving state-of-the-art performance across several benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve key challenges in General Continual Learning (GCL), particularly in parameter-efficient tuning of pretrained models without task boundaries, under single-pass, non-stationary data streams.

Method: FlyPrompt introduces a hierarchical design inspired by the fruit fly's memory system. It includes an analytic router for instance-level expert activation and temporal ensemble output heads to adapt decision boundaries over time.

Result: FlyPrompt demonstrates superior performance on multiple benchmarks, achieving notable improvements of up to 11.23% on CIFAR-100, 12.43% on ImageNet-R, and 7.62% on CUB-200 over existing state-of-the-art methods.

Conclusion: FlyPrompt effectively addresses expert parameter allocation and competence improvement in GCL, offering a promising framework for tackling non-stationary learning tasks in intelligent systems.

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [990] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: The paper addresses challenges in Multimodal Continual Instruction Tuning (MCIT) and proposes the StAbilized Mixture-of-Experts (SAME) method to stabilize expert routing and mitigate expert drift.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve Multimodal Large Language Models (MLLMs) for real-world deployment by addressing issues like router drift and expert drift in instructional tuning for continually expanding abilities.

Method: The SAME method stabilizes expert selection by decomposing routing dynamics, scales updates using historical input covariance, and introduces adaptive expert activation to freeze selected experts during training.

Result: Extensive experiments show that SAME achieves state-of-the-art performance in addressing the challenges of router drift and expert drift.

Conclusion: SAME effectively stabilizes the expert routing and reduces cross-task interference, offering a robust solution for MCIT in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [991] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: Layer pruning can compress LLMs effectively for classification tasks but struggles with generative reasoning, especially under constraints. A simple supervised fine-tuning strategy improves performance significantly but has limitations for deep reasoning.


<details>
  <summary>Details</summary>
Motivation: Explore the performance degradation in generative reasoning tasks caused by layer pruning in LLMs and determine strategies for mitigation under realistic constraints.

Method: Systematic evaluation across different model families and tasks, followed by supervised fine-tuning using self-generated responses to recover performance after pruning.

Result: Recovered up to 90% of baseline on classification tasks. On generative benchmarks, achieved improvements of 20-30 percentage points over prior techniques but still limited effectiveness for reasoning at higher pruning ratios.

Conclusion: Layer pruning is viable for tasks with fewer reasoning demands but has limited applicability for deep reasoning tasks, especially under constrained post-training conditions.

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [992] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: The paper introduces a method called Structured Residual Reconstruction (SRR) that optimizes weight quantization in Post-Training Quantization (PTQ) by preserving intrinsic weight structure and reducing quantization error, leading to better model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address suboptimal quantization accuracy caused by traditional methods that fail to consider the low-rank structure of weight matrices, leading to significant losses in quantization-damaged dominant directions.

Method: The proposed method, Structured Residual Reconstruction (SRR), allocates rank between preserving key singular subspaces of weights pre-quantization and reconstructing residual quantization error. A theoretical approach is presented to balance energy considerations and irrecoverable errors, guiding the selection of rank. Additionally, SRR aids quantized parameter-efficient fine-tuning (QPEFT).

Result: The method demonstrates consistent reductions in perplexity across models and quantization setups in PTQ. Moreover, a notable 5.9 percentage-point improvement on GLUE tasks was observed under 2-bit QPEFT settings.

Conclusion: SRR effectively balances weight structure preservation and error correction within fixed rank constraints, improving quantization and fine-tuning performance significantly.

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [993] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: This paper investigates the effects of post-training quantization (PTQ) on world models, revealing unique challenges and providing practical guidance for efficient deployment under computational constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of research on how post-training quantization impacts world models, which are computationally and memory intensive, to enable efficient deployment.

Method: The researchers systematically studied PTQ on world models using DINO-WM as a case study. They evaluated multiple PTQ methods under diverse settings of bit-widths, quantization granularities, and planning horizons across various visual planning tasks.

Result: The study found that world models exhibit unique vulnerabilities to quantization, such as asymmetric sensitivity between modules and degradation in task-objective alignment with aggressive quantization, leading to distinct failure modes.

Conclusion: The paper highlights the significant impact of PTQ on world models and provides insights and practical guidelines for their efficient deployment. This work opens avenues for further exploration of quantization effects in world model-based planning.

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [994] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: The paper presents Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework for flow matching generative models that incorporates symbolic constraints for improved accuracy and feasibility in constrained generation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of mechanisms in generative models for enforcing declarative symbolic constraints during generation, combining the strengths of symbolic logic and neural learning.

Method: LGVF operates with two mechanisms: a training-time logic loss that minimizes constraint violations and an inference-time adjustment to guide sampling using constraint gradients, ensuring logic-informed corrections.

Result: The proposed LGVF framework reduces constraint violations by 59-82% and achieves high-performance metrics with improved distribution fidelity in linear and ring settings while balancing trade-offs between feasibility and fidelity in the multi-obstacle case.

Conclusion: LGVF demonstrates the ability to enforce constraints effectively, offering emergent behaviors like obstacle avoidance without explicit path planning, showing potential for broader constrained generation applications.

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [995] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: The paper introduces a framework addressing domain generalization under compound distribution shifts involving both marginal and conditional changes, achieving high performance even in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization approaches primarily address conditional shifts in data distribution, but real-world scenarios often face compound shifts in both marginal and conditional distributions, necessitating a robust approach.

Method: The paper derives a new risk bound by decomposing the data distribution into marginal and conditional components, and proposes a meta-learning algorithm to minimize this bound across domains.

Result: Empirical results show the proposed method outperforms existing techniques on both standard DG benchmarks and in multi-domain long-tailed recognition settings.

Conclusion: The proposed method effectively generalizes to unseen domains by addressing compound distribution shifts, establishing its strength for diverse and challenging real-world scenarios.

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [996] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: Latent Action Models (LAMs) face challenges with action-correlated noise. MaskLAM solves this by incorporating visual segmentation masks, improving reinforcement learning performance and latent action quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Latent Action Models (LAMs) struggling to disentangle action-relevant features from action-correlated noise, such as background motion, which hampers performance in reinforcement learning.

Method: Proposed a lightweight modification, MaskLAM, that integrates visual agent segmentation using segmentation masks from pretrained models into the LAM training process to prioritize agent-specific features over noisy backgrounds, without architectural changes.

Result: Compared to baselines, MaskLAM achieves up to a 4x increase in reward and provides 3x improvement in latent action quality across continuous-control MuJoCo tasks with action-correlated noise.

Conclusion: MaskLAM effectively addresses the limitations of LAMs by using segmentation masks to enhance learning of action-relevant features, resulting in superior task performance and latent action spaces.

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [997] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: This paper introduces DASH, a faster implementation of Distributed Shampoo, which improves computational speed through better GPU utilization and novel approaches for matrix root inversion.


<details>
  <summary>Details</summary>
Motivation: To address the computational slowdown of the Shampoo optimizer, making it more practical for large-scale applications.

Method: The authors enhance Shampoo's implementation by stacking preconditioner blocks into 3D tensors to improve GPU efficiency and use techniques like Newton-DB iteration and Chebyshev polynomial approximations for faster matrix root inversions.

Result: DASH achieves up to 4.83× faster optimizer steps compared to Distributed Shampoo and provides the lowest validation perplexity per iteration.

Conclusion: DASH significantly improves the efficiency of Shampoo optimizations while maintaining effectiveness, making it more suitable for practical ML training tasks.

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [998] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: The paper addresses robustness and stability in diffusion-based solvers for Bayesian inverse problems (BIPs) and proposes a novel robust diffusion posterior sampling technique.


<details>
  <summary>Details</summary>
Motivation: To investigate the stability and robustness of diffusion-based solvers for BIPs, especially under mismatched likelihood assumptions, and improve their performance.

Method: The authors characterize posterior approximation error and develop robust diffusion posterior sampling, which is compatible with existing gradient-based posterior samplers.

Result: Empirical validation on scientific inverse problems and natural image tasks demonstrates improved performance and robustness, particularly under likelihood mismatches.

Conclusion: The proposed robust diffusion posterior sampling method enhances the robustness and general applicability of diffusion-based solvers for BIPs, addressing performance degradation due to mismatched likelihoods.

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [999] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: The study proposes FORLER, a method to enhance offline federated reinforcement learning (FRL) in IoT systems, aiming to prevent performance degradation caused by low-quality, heterogeneous data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in offline FRL like policy pollution and local optima due to poor or diverse data quality, enabling safe and cost-effective reinforcement learning in resource-constrained, privacy-sensitive IoT devices.

Method: The paper introduces FORLER, combining Q-ensemble aggregation on a server to merge device Q-functions robustly, with actor rectification on devices for enhanced local policy learning. A $δ$-periodic strategy is also employed to reduce computation while ensuring privacy.

Result: FORLER demonstrates superior performance compared to strong baseline approaches, consistently excelling under diverse data quality and heterogeneity scenarios.

Conclusion: The proposed FORLER method effectively mitigates policy pollution and enhances learning in offline FRL, ensuring safe policy improvements with theoretical performance guarantees.

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [1000] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA is a proposed adaptation framework that controls internal feature reliance in multimodal models using instructions, improving robustness and feature modulation.


<details>
  <summary>Details</summary>
Motivation: To address the issue of uncontrolled reliance on specific feature groups within multimodal foundation models and offer a mechanism to deliberately manage this reliance without changing task semantics.

Method: FiLoRA utilizes feature-aligned LoRA modules and instruction-conditioned gating to selectively manage internal feature reliance without altering the predictive objective, label space, or training semantics.

Result: Experiments across benchmarks show that FiLoRA effectively modulates feature reliance, improving robustness under interventions and shifting internal computations causally.

Conclusion: FiLoRA provides a principled mechanism to regulate feature reliance by enabling explicit control using natural language instructions, offering improvements in robustness and understanding of computation.

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [1001] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: The paper develops a joint routing and scheduling algorithm for large language model (LLM) services using retrial behaviors as implicit feedback, improving efficiency and maintaining queue stability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from challenges in efficiently managing user queries for LLM services, such as retrying unsatisfied queries increasing backlog and explicit feedback degrading user experience.

Method: The study proposes the contextual queueing bandits with multinomial logit feedback (CQB-MNL) framework and introduces an algorithm named anytime CQB (ACQB) that combines Thompson sampling and forced exploration.

Result: ACQB achieves efficient routing and scheduling with cumulative regret of $\widetilde{O}(\sqrt{t})$ for routing and queue length regret of $\widetilde{O}(t^{-1/4})$ for large $t$. Experimental results outperform baseline methods across datasets.

Conclusion: The proposed ACQB algorithm effectively harnesses implicit feedback to optimize query-LLM matching and prioritization, enhancing efficiency and maintaining system stability.

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [1002] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: The paper addresses the Transformer inference bottleneck caused by the softmax operation and proposes a low-precision workflow using an 8-bit floating-point format (HiF8) and precision rescaling to maintain performance while improving hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: The softmax operation limits the performance of Transformer inference due to hardware constraints like data bandwidth and the high area cost of high-precision exponentiation units. There is a need to optimize this step to sustain hardware and model efficiency.

Method: The authors propose a low-precision workflow utilizing an 8-bit floating-point format (HiF8) combined with block-aware precision rescaling. This minimizes bandwidth demands and reduces the space required for exponentiation units by computing in low-precision formats.

Result: The new method doubles inference throughput without increasing chip area while avoiding significant model accuracy loss. This is validated through extensive testing on language and multi-modal models.

Conclusion: The proposed low-precision softmax workflow addresses key bottlenecks in Transformer inference, offering a practical path for hardware/software co-design to improve performance and efficiency while maintaining accuracy.

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [1003] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: The study implements an Adaptive Smoothing Method (ASM) in Python for traffic state reconstruction, with calibration using real-world ground truth data and evaluation on multiple freeways.


<details>
  <summary>Details</summary>
Motivation: To provide a reproducible, Python-based implementation of ASM for accurate traffic state reconstruction, enabling integration with deep learning and addressing existing challenges in model calibration.

Method: The paper formulates ASM as a parameterized kernel optimization problem, calibrates it with real-world radar sensor data, and implements it using PyTorch for compatibility with deep learning tools.

Result: The implementation successfully reconstructs traffic states, evaluates its performance using various error and distribution metrics, and demonstrates its application across multiple freeways.

Conclusion: The study highlights the feasibility of using the ASM implementation as a benchmark for freeway traffic analysis while addressing reproducibility and calibration challenges.

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [1004] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: The paper investigates learning with a modified contrastive example oracle, introducing a mechanism for perturbation to contrastive examples and analyzing its influence on sample complexity in various scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore a more realistic setting for learning with contrastive examples by introducing noise to the ideal contrastive examples used in previous research. The motivation is to understand the effect of contrastive examples under non-ideal conditions.

Method: The authors introduce a noise-parameterized mechanism for generating contrastive examples and analyze its impact on sample complexity in fixed and stochastic perturbation settings using specific statistical models.

Result: The study provides a characterization of active and passive sample complexity under the introduced noise mechanism. It also demonstrates that well-conditioned noise functions can significantly reduce query complexity in learning.

Conclusion: Incorporating perturbed contrastive examples can improve the efficiency of learning processes under specific conditions, albeit with some dependence on the noise parameterization.

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [1005] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: The paper addresses active PU learning, a scenario in which only positive examples are labeled and the learner can query instances, but reveals labels under certain probabilistic conditions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in practical applications like advertising and anomaly detection where obtaining explicit binary labels can be difficult or costly.

Method: Active PU learning strategy is investigated with querying instances from an unlabeled pool and probabilistic mechanisms determining label revelation.

Result: Presents a theoretical analysis of the label complexity in active PU learning.

Conclusion: Provides foundational understanding of active PU learning and its label complexity, aiding its applications in weakly supervised learning scenarios.

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [1006] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: The paper proposes a method to provide high-confidence performance guarantees for multi-task RL policies on unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task RL methods lack formal performance guarantees, critical for safety-critical applications.

Method: The approach combines per-task lower confidence bounds from limited rollouts with task-level generalization to create guarantees for new tasks from the same distribution.

Result: The guarantees are theoretically sound and informative, validated on state-of-the-art multi-task RL methods with realistic sample sizes.

Conclusion: The proposed method enables reliable performance guarantees for multi-task RL policies in unseen settings, addressing a key limitation in current approaches.

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [1007] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: This paper presents a game-theoretic framework for Von Neumann entropy (VNE) maximization, illustrating its utility in modern machine learning tasks like kernel representation selection and matrix completion.


<details>
  <summary>Details</summary>
Motivation: To extend the maximum entropy principle to von Neumann entropy (VNE) and establish a game-theoretic and decision-theoretic foundation for its use in data-driven contexts.

Method: The authors adapt a game-theoretic maximum entropy principle (Grünwald-Dawid) to the VNE setting, maximizing VNE over density matrices and trace-normalized positive semidefinite operators.

Result: Development of a robust interpretation of VNE maximization for spectral diversity with practical applications in machine learning, such as kernel representation selection and kernel matrix completion.

Conclusion: The proposed Maximum VNE principle provides a unifying information-theoretic framework for optimizing spectral diversity in VNE-based machine learning applications.

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [1008] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: The paper proposes a two-stage optimization framework to improve group-wise quantization of large language models by minimizing layer-wise reconstruction loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods, like GPTQ, lead to mismatched group scales because they neglect input statistics and inter-group correlations, impacting accuracy in low-bit quantization of LLMs.

Method: The framework has two stages: 1) Pre-GPTQ initialization of group scales based on input statistics and group-wise reconstruction loss. 2) Refinement of group scales using coordinate descent and a closed-form update rule while freezing integer weights from GPTQ.

Result: The method enhances group-wise quantization accuracy consistently, with negligible computational overhead.

Conclusion: The proposed method improves accuracy in group-wise quantization by addressing limitations of GPTQ through a two-stage optimization that incorporates input statistics explicitly and prevents error accumulation.

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [1009] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD is a diffusion model optimized for generating microsecond-scale protein dynamics with high accuracy, overcoming constraints faced by traditional molecular dynamics simulations and generative models.


<details>
  <summary>Details</summary>
Motivation: The need for computationally efficient methods to simulate biologically relevant protein dynamics beyond traditional approaches.

Method: STAR-MD uses a SE(3)-equivariant diffusion model with a causal transformer that incorporates joint spatio-temporal attention to address challenges in long-horizon trajectory generation.

Result: STAR-MD outperformed other generative methods on the ATLAS benchmark, producing stable trajectories and successfully capturing protein dynamics at the microsecond level.

Conclusion: STAR-MD addresses key limitations in current generative approaches and offers a scalable, robust solution for modeling protein dynamics, enabling advancements in studying protein functions at extended timescales.

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [1010] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: The paper presents DCoPilot, a hybrid framework combining a large language model and hypernetwork to adaptively generate control policies for dynamic data center operations, ensuring minimal constraint violations and enhanced performance in varying conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the specification-to-policy lag in dynamic data center operations, where manual design of reinforcement learning agents cannot cope with rapidly changing workloads and service-level agreements, leading to potential service outages.

Method: The method involves combining a large language model for generating structured reward forms and a hypernetwork for producing policy weights. DCoPilot operates in three phases: simulation-scale testing of reward candidates, training a meta-policy hypernetwork, and online adaptation for zero-shot policy generation.

Result: DCoPilot demonstrated effective control across five data center-related tasks, achieving near-zero constraint violations and superior performance compared to all baseline methods. LLM-based reward generation also ensured hypernetwork stability.

Conclusion: DCoPilot leverages generative AI techniques to deliver adaptive, efficient control policies for dynamic and complex data centers, addressing the limitations of manual DRL agent design.

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [1011] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: The paper proposes BTTF, a forecasting framework leveraging look-ahead augmentation and model refinement to improve long-term time series forecasting accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Current methods for long-term time series forecasting suffer from a trade-off between maintaining temporal consistency (IMS methods, slower but accurate) and fast parallel predictions (DMS methods, less consistent). This inspired a need for a method that balances these challenges.

Method: The BTTF framework involves refining a base model through an ensemble of second-stage models augmented using initial forecasts and applying look-ahead augmentation and self-corrective strategies, without relying on complex architectures.

Result: BTTF demonstrated accuracy improvements by up to 58% over linear forecasting models, ensuring long-horizon stability even under suboptimal conditions of the first-stage model.

Conclusion: BTTF showcases that leveraging model-generated forecasts as augmentation is a simple and robust way to improve long-term forecasting beyond traditional methods or complex architectures.

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [1012] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: ECHO improves test-time reinforcement learning by addressing branching inefficiencies and biases, employing adaptable entropy-confidence mechanisms for better reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Previous methods for test-time reinforcement learning face issues like inefficient sampling and bias from noisy pseudo-labels, limiting their effectiveness.

Method: ECHO uses adaptive entropy-confidence controls in both rollouts and policy updates, employing confidence-based pruning and hybrid shaping to manage branching and reduce noise.

Result: ECHO achieves better performance across multiple reasoning tasks while being more efficient under limited computational budgets.

Conclusion: ECHO's improvements in adaptive branching and policy robustness lead to enhanced exploration, reasoning accuracy, and overall efficiency in reinforcement learning tasks.

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [1013] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: The paper introduces a new method enhancing Neural CDEs with Kernel and Gaussian Process smoothing for efficient path construction, coupled with MV-CDEs using attention-based queries to minimize computational overhead while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Neural CDEs often suffer from computational inefficiency due to the roughness of control paths and excessive NFEs caused by standard spline interpolation.

Method: The paper employs Kernel and Gaussian Process smoothing for trajectory regularity and introduces Multi-View CDE with attention-based queries to optimize path reconstruction and capture diverse temporal patterns.

Result: The proposed method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and inference time compared to traditional spline-based approaches.

Conclusion: The approach successfully improves Neural CDE efficiency and performance, demonstrating the potential of Kernel GP smoothing and attention-based path reconstruction in sequence modeling.

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [1014] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: The paper introduces a framework for counterfactual validation of temporal link prediction (TLP) models to assess their ability to capture causal mechanisms in temporal interactions using causal temporal interaction graphs (CTIGs).


<details>
  <summary>Details</summary>
Motivation: Current evaluations of TLP models primarily focus on predictive accuracy and fail to assess whether these models accurately capture causal mechanisms behind temporal interactions. The paper aims to address this gap by proposing a framework for causally-grounded validation.

Method: The authors developed a structural equation model for continuous-time event sequences with excitatory and inhibitory effects, extended this model to temporal interaction graphs, proposed a distance metric based on cross-model predictive error, and instantiated counterfactual evaluation using controlled causal shifts and timestamp shuffling.

Result: The proposed framework demonstrates that predictors trained on one causal model significantly degrade on sufficiently distant models, validating the utility of the framework for assessing causal mechanisms.

Conclusion: The paper provides a new causality-aware benchmarking approach for evaluating TLP models, offering insights into their ability to generalize under causal shifts and distortions.

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [1015] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: KernelICL enhances tabular model interpretability with kernel-based layer replacement for transparent predictions, achieving competitive performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Tabular foundation models lack interpretability due to opaque architectures, motivating a transparent mechanism for understanding predictions.

Method: Replaces the prediction layer with kernel functions for interpreting predictions and introduces a unified taxonomy of kernel and attention mechanisms.

Result: KernelICL matches state-of-the-art performance on 55 TALENT benchmark datasets, while enhancing model interpretability.

Conclusion: Explicit kernel constraints offer transparent, interpretable predictions without compromising model performance.

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [1016] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: The paper introduces Co-RedTeam, a multi-agent framework to improve vulnerability discovery and exploitation by enabling structured interaction, execution feedback, and reuse of prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in using large language models (LLMs) for automatic vulnerability discovery and exploitation face challenges due to limitations in interaction, execution grounding, and experience reuse.

Method: The authors propose Co-RedTeam, a security-aware multi-agent framework that integrates security knowledge, code analysis, iterative reasoning, execution validation, and long-term memory. It decomposes tasks into discovery and exploitation stages with feedback loops.

Result: Extensive evaluations on security benchmarks show Co-RedTeam's performance surpassing strong baselines, achieving over 60% success in exploitation and over 10% improvement in vulnerability detection.

Conclusion: The paper demonstrates the importance of execution feedback, structured agent interaction, and memory in creating robust, generalizable cybersecurity agents for vulnerability discovery and exploitation.

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [1017] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: The paper introduces a MIP-based framework for optimally learning classification trees under nonlinear metrics, such as F1-score, addressing class imbalance while improving scalability and predictive performance.


<details>
  <summary>Details</summary>
Motivation: The need to address class imbalance and improve decision tree optimization for interpretable machine learning has driven this research.

Method: The study employs mixed-integer programming, with enhancements like tailored branch-and-cut algorithms, instance-reduction, and warm-start strategies to scale optimal classification tree learning under nonlinear metrics.

Result: Experiments on 50 datasets show efficient optimization of nonlinear metrics, achieving high predictive performance and faster solution times compared to existing methods.

Conclusion: The proposed framework demonstrates its ability to efficiently optimize nonlinear metrics and improve scalability and accuracy, making it superior to traditional approaches.

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [1018] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: The study presents SurvKAN, a novel survival prediction model that overcomes limitations of previous methods, offering both improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing survival models, such as the restrictive assumptions of Cox models and the lack of interpretability in deep learning approaches, to improve clinical decision-making.

Method: This paper proposes SurvKAN, a fully parametric survival model using Kolmogorov-Arnold Networks (KANs), which predicts log-hazard functions based on input directly without relying on proportional hazards assumptions.

Result: SurvKAN outperforms classical and modern baselines in concordance and calibration metrics on survival prediction benchmarks, maintaining interpretability and showing alignment with medical knowledge.

Conclusion: SurvKAN successfully combines high prediction accuracy and interpretability, making it a viable solution for clinical adoption in survival analysis.

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [1019] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: STILL introduces advanced techniques to efficiently linearize large language models (LLMs) by addressing token routing and feature map distribution shift issues, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome the inefficiencies of current linear attention methods in LLMs, such as position-based token routing and distribution shifts from learnable feature maps.

Method: STILL uses a Self-Saliency Score for token selection, Norm-Preserved Feature Maps to retain pretrained feature representations, and implements chunk-wise parallelization and delayed selection for hardware efficiency.

Result: STILL outperforms existing linear attention methods, with significant improvements on long-context benchmarks (up to 86.2%) and matching original pretrained model performance on reasoning tasks.

Conclusion: STILL proves to be a robust framework for LLM linearization, enhancing both computational efficiency and model accuracy.

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [1020] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: The paper investigates the internal dynamics of state-of-the-art Linear Attention LLMs during runtime, discovering consistent rank stratification among attention heads, and proposes a pruning method to optimize efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding the opaque runtime dynamics of Linear Attention models' compressed state matrix and optimizing memory efficiency without compromising performance.

Method: Analyzed runtime state dynamics of linear attention heads, identified intrinsic structural properties via experiments, and developed a pruning strategy called Joint Rank-Norm Pruning.

Result: Discovered consistent bifurcation into high-rank and low-rank attention heads. Low-rank heads are critical for reasoning, while high-rank heads exhibit redundancy. Achieved 38.9% KV-cache reduction.

Conclusion: State rank stratification is an inherent property acquired during training. The pruning strategy enhances computational efficiency with minimal impact on model accuracy.

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [1021] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: The paper introduces Hierarchical Adaptive Eviction (HAE) to optimize memory use and efficiency in multimodal large language models (MLLMs), addressing inefficiencies in handling visual and text tokens.


<details>
  <summary>Details</summary>
Motivation: Current KV cache eviction strategies in transformer architectures fail to efficiently manage the varying attention distributions between visual and text tokens in MLLMs, leading to inefficiencies or degraded performance.

Method: The authors propose HAE, a KV cache eviction framework consisting of Dual-Attention Pruning during the pre-filling phase (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during the decoding phase.

Result: Experimentally, HAE reduces KV cache memory usage by 41% with an accuracy drop of only 0.3% in image understanding tasks and speeds up story generation inference by 1.5 times while maintaining output quality using the Phi3.5-Vision-Instruct model.

Conclusion: HAE significantly improves memory efficiency, computational overhead, and task performance in MLLMs while preserving output quality, making it a valuable framework for multimodal language models.

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [1022] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: The paper focuses on improving molecular property prediction using a new graph transformer model called CardinalGraphFormer, which demonstrates enhanced performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of molecular property prediction with limited labeled data in the vast chemical space, estimated to contain ~10^60 drug-like molecules.

Method: CardinalGraphFormer is a graph transformer incorporating structural biases, sparse attention mechanisms, and cardinality-preserving aggregation. Self-supervised pretraining combines contrastive alignment with masked attribute reconstruction.

Result: The model outperforms strong baselines on 10 of 11 benchmarks across MoleculeNet, OGB, and TDC ADMET tasks, setting new performance standards.

Conclusion: CardinalGraphFormer demonstrates superior data efficiency and molecular representation learning, making it a robust method for drug discovery and molecular prediction.

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [1023] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: This paper introduces Fat-Cat, a document-driven architecture improving LLM agent performance by replacing rigid syntax representations like JSON with semantically-driven Markdown documents.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks suffer from efficiency limitations due to rigid, syntax-heavy state representations that distract models with syntactic processing instead of semantic reasoning.

Method: Fat-Cat employs (1) a Semantic File System using Markdown to align with pre-training corpora, (2) Textual Strategy Evolution for knowledge accumulation without parameter updates, and (3) a Closed-Loop Watcher to monitor and correct reasoning trajectories.

Result: Fat-Cat improves performance in reasoning, retrieval, and coding tasks. It allows the Kimi-k2 model to surpass GPT-4o on HotPotQA and demonstrates the necessity of document-driven state modeling through performance drop with JSON-based states.

Conclusion: The document-driven architecture enhances agent outcomes by addressing inefficiencies in state management. Fat-Cat aids in balancing semantic reasoning with computational attention, proving its efficacy over rigid syntactic methods.

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [1024] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: The paper introduces TIDES, an approach combining text-based design with structural-visual optimization and physical constraints, evaluated through simulations and experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge textual design instructions with physically sound structural optimization, catering to engineering design needs.

Method: TIDES uses pre-trained text-image models for visual-text alignment and a differentiable physics simulator for evaluating physical performance, optimizing topology and visual properties together.

Result: TIDES performs effectively across various structural conditions, resolutions, and real-world experiments, meeting both engineering and aesthetic requirements.

Conclusion: TIDES successfully integrates textual descriptions into structural optimization, demonstrating physically viable and visually aligned designs under engineering constraints.

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [1025] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: This paper proposes the concept of a Scientific Theory of a Black Box (SToBB) to consolidate explanations of black-box AI models into persistent, auditable artifacts, ensuring adequacy, adaptability, and auditability over their life cycle.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of a systematic approach for compiling interpretive insights about black-box models into a lifecycle-accompanying, auditable framework.

Method: They introduce the Scientific Theory of a Black Box (SToBB), grounded in Constructive Empiricism, operationalized as a framework with a maintained observation base, hypothesis class, construction/revision algorithms, and transparent documentation. They also create the Constructive Box Theoriser (CoBoT) algorithm for maintaining empirically adequate surrogate models.

Result: They demonstrate SToBB by implementing and maintaining a rule-based surrogate for a neural-network classifier, showcasing the feasibility and benefits of their approach.

Conclusion: SToBBs present a robust framework for scalable, auditable lifecycle explanations of black-box models, ensuring consistent analysis and external scrutiny while evolving with new observations.

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [1026] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: The paper proposes a monitoring framework, Prediction-Powered Risk Monitoring (PPRM), which uses limited real labels combined with synthetic labels to provide valid risk assessments in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: There is a challenge in monitoring model performance when the environment changes dynamically and labeled data are scarce.

Method: PPRM employs semi-supervised prediction-powered inference to construct valid risk bounds using a mix of true and synthetic labels, incorporating a threshold-based comparison to detect harmful shifts.

Result: PPRM is demonstrated as effective through experiments on diverse domains like image classification, large language models, and telecommunications monitoring.

Conclusion: PPRM provides a robust solution for monitoring risks in evolving contexts with finite-sample guarantees, addressing false alarm probabilities.

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [1027] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: The paper introduces SEDformer, a Spiking Neural Network-based approach utilizing the Sparsity-Event Duality (SED) property for forecasting irregular multivariate time series (IMTS) from telemetry data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for forecasting IMTS do not align well with its Sparsity-Event Duality (SED), failing to capture the sparse and event-driven nature of the data due to artificial pre-alignment.

Method: The authors propose SEDformer, combining a SED-based Spike Encoder, an Event-Preserving Temporal Downsampling module, and Spike Transformer blocks with linear attention to more naturally and efficiently handle SED characteristics of IMTS.

Result: SEDformer demonstrates state-of-the-art accuracy in IMTS forecasting on public datasets while reducing energy and memory consumption.

Conclusion: SEDformer effectively addresses limitations in current methods by utilizing SED-based features and Spiking Neural Networks, offering accurate, efficient, and natural modeling for IMTS forecasting.

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [1028] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: TopoDiff enhances EEG spatial super-resolution by integrating geometry-aware embeddings and dynamic inter-electrode relationship modeling, achieving improved generation fidelity and task performance.


<details>
  <summary>Details</summary>
Motivation: Existing EEG spatial super-resolution methods lack consideration of physiological spatial structure, limiting spatial generation quality.

Method: TopoDiff combines topology-aware image embeddings with a dynamic channel-relation graph, leveraging EEG topographic representations and temporal dynamics.

Result: TopoDiff demonstrates consistent improvements in EEG spatial generation across diverse datasets and downstream tasks like emotion recognition and seizure detection.

Conclusion: The integration of geometric and relational awareness in EEG spatial super-resolution leads to increased performance in generation accuracy and task-specific applications.

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [1029] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: The paper proposes a shift in interpretability methods for deep time series models towards 'semantic alignment' with meaningful variables for end-users and outlines a blueprint for achieving this.


<details>
  <summary>Details</summary>
Motivation: Deep time series models offer superior predictive capabilities, but their limited interpretability hampers deployment. Existing approaches focus heavily on internal computations rather than meaningful user-centric reasoning.

Method: The authors formalize 'semantic alignment' for deep time series models and introduce a blueprint for ensuring predictions are expressed in meaningful variables. They address constraints unique to temporal evolution.

Result: The paper identifies properties fostering trust in semantically aligned models and delves into implications for their design.

Conclusion: Semantic alignment in deep time series modeling is essential for aligning predictions and mechanisms with human understanding, facilitating trustful model deployment in dynamic scenarios.

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [1030] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: The paper introduces Variational Entropic Optimal Transport (VarEOT), a novel method to optimize the entropic optimal transport objective in a computationally efficient manner.


<details>
  <summary>Details</summary>
Motivation: To overcome computational inefficiencies in solving the entropic optimal transport (EOT) problem due to the intractable log-partition term.

Method: The proposed method, VarEOT, reformulates the log-partition term as a variational minimization with an auxiliary positive normalizer, which enables differentiable learning without simulation-based training.

Result: VarEOT achieves competitive or better results in synthetic data experiments and unpaired image-to-image translation, demonstrating its effectiveness.

Conclusion: VarEOT offers a practical and theoretically sound approach to improving EOT optimization by eliminating the need for simulation-based procedures, enhancing efficiency and accuracy.

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [1031] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: This paper introduces an alignment-aware fine-tuning framework that integrates feedback on alignment through policy-gradient regularization and dynamically balances gradients, reducing harmful outputs and improving alignment without degrading task performance.


<details>
  <summary>Details</summary>
Motivation: Traditional fine-tuning methods focus solely on downstream task objectives, ignoring critical alignment factors like safety and hallucination prevention, which may degrade or fail to correct misaligned behavior.

Method: The proposed framework uses policy-gradient-based regularization and an adaptive gating mechanism. It prioritizes uncertain or misaligned inputs while maintaining supervised updates for well-aligned examples. It also incorporates abstention behaviors for misaligned data, ensuring conservative responses.

Result: The method achieved consistent reductions in harmful and hallucinated outputs and maintained downstream task performance across general and domain-specific instruction-tuning benchmarks. It also exhibited robustness to adversarial fine-tuning and attacks.

Conclusion: The adaptively gated alignment optimization is effective for preserving and recovering alignment in fine-tuned models, balancing task performance with critical alignment objectives.

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [1032] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: The paper introduces a novel learning algorithm for reinforcement learning under a restrictive 'fully bandit' feedback model and achieves theoretical guarantees on regret.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of reinforcement learning in scenarios where the agent receives only aggregate rewards (without state-action pair feedback), overcoming the limitations of traditional detailed feedback models.

Method: The study develops an efficient bandit learning algorithm tailored for episodic Markov Decision Processes (MDPs) under fully bandit feedback and also extends results to 'ordered' MDP models for applications like $k$-item prophet inequalities.

Result: The algorithm achieves $Tilde{}(TildeTildeTilde(\u007Finitely bounds on regret with scalable empirical performance.

Conclusion: Despite highly restrictive feedback, the proposed algorithm achieves competitive performance, bridging gaps in learning efficiency across varied reinforcement learning scenarios.

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [1033] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: The paper unifies and explores the connection between Conditional Flow Matching (CFM) and Interaction Field Matching (IFM), finding that they align under a subclass and that IFM is more expressive overall.


<details>
  <summary>Details</summary>
Motivation: To understand whether Conditional Flow Matching (CFM) and Interaction Field Matching (IFM), which originate from different principles, represent distinct frameworks or descriptions of the same dynamics.

Method: Construct a bijection to demonstrate equivalence between CFM and a subclass of IFM (forward-only IFM), and analyze the expressiveness of general IFM.

Result: CFM and forward-only IFM are equivalent under a specific subclass, but general IFM is more expressive, surpassing the standard CFM framework.

Conclusion: This duality between CFM and IFM provides a probabilistic perspective on IFM and introduces new techniques for improving CFM based on insights from IFM.

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [1034] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: This paper addresses challenges in solving partial differential equations using machine learning by developing a novel unsupervised training approach and proposing a new physics-informed neural operator model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of existing methods in solving partial differential equations, specifically issues with unstable convergence, limited generalization, and reliance on supervised data.

Method: The authors introduce a multi-stage physics-informed training strategy that enforces boundary conditions first and gradually incorporates interior residuals during training. They also propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO) with spline-based kernels.

Result: PhIS-FNO demonstrates a level of accuracy comparable to supervised learning methods, while using minimal labeled data, applied mainly to the boundary region.

Conclusion: The proposed staged training strategy and PhIS-FNO model improve both stability and accuracy in physics-informed operator learning, offering a new robust paradigm in this field.

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [1035] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: This paper introduces HopFormer, a graph Transformer that eliminates the need for explicit positional encodings and dense global attention by using n-hop masked sparse attention.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and assumptions in current graph Transformer designs that rely on positional encodings and dense global attention for graph topology.

Method: The authors developed HopFormer, a graph Transformer structure that employs head-specific n-hop masked sparse attention instead of dense global attention or positional encodings.

Result: HopFormer achieves competitive or superior results on node-level and graph-level benchmarks and demonstrates that localized attention often outperforms dense global attention on graphs with strong small-world properties.

Conclusion: The findings challenge the necessity of dense global attention and positional encodings in graph Transformers while promoting sparsity-controlled attention as a principled and efficient alternative design choice.

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [1036] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: The paper presents MoLF, a generative model enhancing histogenomic prediction across various cancers, outperforming existing models and generalizing across species.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scalability and data scarcity in spatial transcriptomics by combining shared biological principles across cancers through pan-cancer training.

Method: The authors propose MoLF, a generative model utilizing a conditional Flow Matching objective and a Mixture-of-Experts velocity field, dynamically routing inputs to specialized sub-networks for efficient learning.

Result: MoLF achieves state-of-the-art results in pan-cancer benchmarks, surpassing specialized and foundation models, and can generalize to cross-species data.

Conclusion: MoLF effectively leverages diverse tissue patterns and conserved mechanisms for robust and scalable histogenomic prediction, potentially transforming ST analyses.

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [1037] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: The paper introduces a training method for time-series forecasting models that focuses on enforcing autoregressive prediction errors and enabling concatenation for long-term forecasts, showing improved accuracy across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current time-series methods rely heavily on scaling model size while neglecting temporal causality principles, leading to inefficiencies in forecasting accuracy.

Method: The method integrates principles like penalty for random guessing when AR errors violate prediction patterns and concatenates short-term AR predictions for flexible long-term forecasting.

Result: The novel approach reduces MSE by over 10% compared to top benchmarks like iTransformer and supports reliable long-term forecasts at extended horizons.

Conclusion: Enforcing AR prediction errors and concatenation principles shows significant improvement in forecasting capability, setting a new standard for model performance.

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [1038] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: EvalQReason is a framework for evaluating reasoning quality in large language models via step-level probability analysis, emphasizing domain-specific differences.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of systematically evaluating the reasoning processes within large language models, which are critical for reasoning-heavy applications.

Method: The authors propose two algorithms, Consecutive Step Divergence (CSD) and Step-to-Final Convergence (SFC), using statistical metrics to assess local coherence and global alignment in reasoning.

Result: Experiments show that CSD-based features effectively classify correctness, with strong predictive performance (F1=0.88, ROC-AUC=0.97) for sequential models and highlighting domain-specific reasoning differences.

Conclusion: EvalQReason provides a scalable evaluation framework for reasoning quality, advocating probability-based divergence methods as integral to trustworthy AI deployment.

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [1039] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: This paper proposes a principle to balance privacy preservation and model generalizability in deep neural networks by addressing their distinct regional risks.


<details>
  <summary>Details</summary>
Motivation: Deep learning models face a trade-off between utilities, like generalizability, and privacy preservation due to differing loss structures.

Method: The authors investigate regions of privacy risk within deep neural network architectures and propose Privacy-Preserving Training Principle (PPTP) that targets these regions for optimization.

Result: The proposed approach improves model generalizability while enhancing privacy preservation, as demonstrated through extensive evaluations.

Conclusion: The study offers a novel training principle to effectively decouple and optimize privacy risks and generalization trade-offs, achieving a significant enhancement in both aspects.

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [1040] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: The paper introduces ReasonCACHE, a method using Prefix Tuning to enable LLMs to learn reasoning without weight updates and context overload, overcoming ICL limitations.


<details>
  <summary>Details</summary>
Motivation: The work aims to improve large language models' reasoning capabilities without relying on costly in-weight learning (IWL), addressing challenges in scaling in-context learning (ICL).

Method: This study proposes ReasonCACHE, which uses Prefix Tuning to distill demonstrations into a key-value cache for reasoning tasks, injecting them into attention mechanisms.

Result: ReasonCACHE outperforms standard ICL and matches or surpasses IWL in reasoning benchmarks while being more data, inference, and parameter efficient.

Conclusion: ReasonCACHE serves as a scalable reasoning solution, balancing between ICL and IWL, bypassing context window limits without modifying LLM parameters.

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [1041] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: The paper addresses the shortcomings of current self-supervised learning in handling one-to-many mappings, proposing AdaSSL, a method incorporating uncertainty estimation to improve performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised learning methods struggle with the one-to-many mapping problem, arising when data pairs come from generative processes like sequential video frames, due to their inability to handle conditional uncertainty.

Method: The authors introduce a latent variable to capture the conditional uncertainty in one-to-many mappings and derive a variational lower bound on mutual information, resulting in a new regularization term for self-supervised learning objectives.

Result: The proposed AdaSSL method is applicable to contrastive and distillation-based SSL objectives, improving performance in tasks such as causal representation learning, fine-grained image analysis, and video-based world modeling.

Conclusion: AdaSSL effectively addresses the one-to-many mapping challenge in self-supervised learning by incorporating conditional uncertainty, making it a versatile and improved method for representation learning.

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [1042] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME is a new objective for aligning LLMs that addresses issues in existing methods like "unlearning" and "formatting collapse" by combining anchoring, stabilization, and dual-margin mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning LLMs often suffer from issues like unlearning and formatting collapse due to a mismatch between the optimization objective and generation quality.

Method: SLIME is a reference-free objective that includes anchoring preferred responses, stabilizing penalties for rejected tokens, and dual-margin constraints to balance hard and soft constraints.

Result: SLIME outperforms state-of-the-art baselines in alignment tasks while maintaining higher generation stability.

Conclusion: SLIME effectively addresses critical limitations in current preference optimization methods and offers a more stable and efficient alignment approach for LLMs.

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [1043] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: The paper investigates why transformers decompose their internal representations into orthogonal subspaces, exploring the tradeoff between dimensional efficiency and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding how and why transformers represent their modeled world as orthogonal subspaces can clarify their efficiency and interpretability in tasks.

Method: The authors derived hypotheses on representations—product space versus factored—and tested these on transformers trained on synthetic datasets with known latent structures.

Result: Transformers preferentially learn factored (orthogonal subspace) representations under conditions of conditional independence, enforcing an inductive bias even with added noise or dependency.

Conclusion: Interpretable, low-dimensional structures emerge naturally in transformers, hinting at inherent biases toward decomposable representations and suggesting their persistence in complex datasets.

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [1044] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: The paper introduces 'Tag-Along Attacks,' highlighting vulnerabilities in tool-augmented large language models by validating attacks through a new reinforcement learning approach called Slingshot.


<details>
  <summary>Details</summary>
Motivation: To address the security risks of adversarial manipulations in autonomous large language models operating in tool-augmented environments.

Method: The authors propose Slingshot, a reinforcement learning framework, to discover emergent attack vectors and evaluate their efficacy across language models in various tool-use scenarios.

Result: Slingshot achieves high attack success rates (e.g., 67% versus a 1.7% baseline) and demonstrates its transferability across different model families and defensive settings.

Conclusion: The work identifies 'Tag-Along Attacks' as a critical security threat, emphasizing that agentic attacks can emerge even from open-weight models, necessitating new safety strategies.

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [1045] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: This study examines the impact of noisy data on loss divergence during large language model (LLM) pretraining, using controlled noise injection to analyze training dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how noisy data in large-scale pretraining datasets impacts training instabilities and loss divergence in large language models.

Method: Synthetic uniformly random noise was introduced into clean datasets, and training dynamics were analyzed across models of varying sizes (480M-5.2B parameters) to identify divergence patterns.

Result: Noisy data was shown to induce loss divergence, with the likelihood dependent on the noise type, amount, and model scale. Distinct activation patterns separate noise-induced divergences from failures caused by high learning rates.

Conclusion: Noisy data significantly affects LLM pretraining stability, and diagnostics can differentiate between noise-induced divergence and other failure modes.

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [1046] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: The paper introduces DAIL (Distribution Aligned Imitation Learning), a method to improve reasoning in large language models by aligning expert solutions with model-compatible reasoning traces and optimizing learning.


<details>
  <summary>Details</summary>
Motivation: Reasoning in LLMs requires effective training data, but difficult problems are often insoluble for frontier models and expert-human solutions are expensive and out of model compatibility.

Method: DAIL uses a two-step approach: transforming expert solutions into detailed reasoning traces, and applying a contrastive objective that focuses learning on expert methodologies.

Result: DAIL achieves improvements of 10-25% pass@k, doubles reasoning efficiency, and enables generalization to out-of-domain tasks using under 1000 expert solutions.

Conclusion: DAIL is an efficient, scalable, and effective method to leverage expert solutions for reasoning improvement in LLMs, overcoming distribution gaps and enabling better performance gains.

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [1047] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: The paper proposes ATBagging, a novel method for selecting seed datasets in active learning to reduce labeling costs by leveraging approximate datasets and enhancing early performance.


<details>
  <summary>Details</summary>
Motivation: Active learning often relies on initial seed sets chosen randomly, which can lead to suboptimal early performance despite the availability of related datasets.

Method: ATBagging estimates informativeness using Bayesian interpretations of bagged models with an information-gain proxy and ensures diversity via determinantal point processes with Random Fourier Features.

Result: ATBagging outperforms alternative seed selection methods in improving early active learning and increases learning curve performance across various real-world datasets, especially in low-data settings.

Conclusion: ATBagging offers an efficient and high-impact approach for initiating active learning, reducing costs and optimizing dataset selection for enhanced results.

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [1048] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: This paper introduces trust region continual learning, a hybrid approach combining generative replay and Fisher-metric trust region to tackle catastrophic forgetting for sequential tasks. It shows superior results empirically.


<details>
  <summary>Details</summary>
Motivation: The study aims to address catastrophic forgetting in continual learning by tackling the tradeoff between overconstrained updates in regularization-based methods and performance drift in replay-based methods.

Method: The paper proposes a hybrid approach combining generative replay with a Fisher-metric trust region constraint. This creates a MAML-style update where replay supplies a gradient signal, and the Fisher metric shapes the curvature, yielding rapid re-convergence to prior task optima.

Result: The method achieves superior outcomes in task-incremental diffusion image generation and continual diffusion-policy control. It also surpasses methods like EWC, replay, and continual meta-learning in terms of final performance retention and recovery speed of prior task performance.

Conclusion: Trust region continual learning offers an effective solution in continual learning, enabling rapid re-convergence to prior tasks without bilevel optimization, thus balancing performance and retention effectively.

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [1049] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: This paper introduces poly-attention mechanisms as generalizations of self-attention, capable of modeling more complex token interactions. It analyzes these mechanisms' computational complexity, representational power, and provides a new mechanism with improved efficiency and capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing self-attention mechanisms in Transformer models struggle with tasks requiring interactions among multiple tokens (e.g., triples or function compositions). The paper seeks to address these limitations and improve computational efficiency.

Method: The authors define poly-attention mechanisms integrating higher-order computations and relationship structures. They analyze complexity, representational strength, and provide new algorithms with tight computational bounds, including an efficient quadratic-time attention mechanism.

Result: The proposed poly-attention class encompasses higher-dimensional alternatives like higher-order and Strassen attention. They demonstrate improved computational efficiency and representational ability, introducing a mechanism that performs multidimensional tasks in quadratic time.

Conclusion: By establishing trade-offs between expressiveness and efficiency, this work advances attention mechanisms and sets new theoretical bounds. It enables efficient computation for complex tasks previously unattainable by standard or other advanced self-attention models.

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [1050] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: This paper introduces CHASE, a compact and efficient framework to optimize protein fitness using evolutionary insights from pretrained protein language models, achieving superior performance without predictor-guidance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of protein fitness optimization in a combinatorially vast landscape, where current methods are either ineffective or computationally expensive.

Method: CHASE uses pretrained protein language model embeddings, compresses them into a latent space, and applies conditional flow-matching with classifier-free guidance to generate high-fitness protein variants.

Result: CHASE demonstrates state-of-the-art results on protein design benchmarks for AAV and GFP and shows performance scalability with synthetic data in limited-data scenarios.

Conclusion: The framework efficiently generates high-fitness protein variants, and its bootstrapping ability with synthetic data makes it robust in constrained environments.

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [1051] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: This paper proposes a perturbation-driven approach for Uncertainty Quantification (UQ) in Large Language Models (LLMs), focusing on their reasoning processes.


<details>
  <summary>Details</summary>
Motivation: Large language models can produce unreliable outputs, especially during reasoning tasks. There is a need for techniques to quantify uncertainty, not only in final answers but also in intermediate reasoning steps.

Method: The authors introduce a perturbation-driven metric that identifies uncertainty in reasoning steps by analyzing token sensitivity to preceding token embeddings' perturbations.

Result: The perturbation-based metric outperforms traditional baseline methods like token probability and token entropy in quantifying uncertainty and provides simplicity and efficiency without relying on multiple sampling.

Conclusion: The new perturbation-based approach is more effective and efficient for detecting uncertain reasoning steps in LLMs, aiding in fine-grained interventions for responsible AI deployment.

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [1052] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: The paper addresses limitations in token-level sampling for model performance and proposes Expert-Sample, a training-free method for diverse generation in fine-grained Mixture of Experts (MoE) models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of token-level sampling in generating diverse and accurate solutions, especially trade-offs between diversity and stability during test-time scaling.

Method: The authors utilize fine-grained MoE models and propose the Expert-Sample method. It captures high-confidence expert selections while injecting stochasticity in uncertain areas, without requiring additional training.

Result: The approach was evaluated across tasks like math, logic, and coding, leading to significant improvements in pass@n and accuracy. For example, pass@32 increased from 85.4% to 91.9% and accuracy from 59.1% to 62.6% on GPQA-Diamond.

Conclusion: Expert-Sample effectively enhances both diversity and accuracy in MoE models, offering a practical, training-free solution for improving performance across varied reasoning tasks.

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [1053] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: This paper provides finite-sample error bounds for nonlinear stochastic approximation algorithms using Wasserstein distance, emphasizing last iterate behavior and Polyak-Ruppert averaging.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit finite-sample guarantees for nonlinear stochastic approximation algorithms, particularly focusing on convergence rates in Wasserstein distance.

Method: A coupling argument comparing discrete-time processes to Ornstein-Uhlenbeck processes and direct analysis under noise conditions, with exploration of central limit properties and high-probability bounds.

Result: Explicit convergence rates for both last iterates and the Polyak-Ruppert average were derived in Wasserstein distance, improving over moment-based inequalities and confirming asymptotic distributional behaviors.

Conclusion: These findings advance the understanding of stochastic approximation with improved finite-sample error bounds and applications to stochastic gradient descent and linear approximation models.

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [1054] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: The paper introduces ACE, a method for learning experimental design policies for causal discovery, which improves outcomes significantly over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of traditional methods like random sampling and greedy approaches in sequential experimental decision-making for causal discovery.

Method: The study proposes ACE, a model that uses Direct Preference Optimization to learn from pairwise intervention comparisons rather than unstable reward magnitudes, to iteratively improve experimental design.

Result: ACE achieved a 70-71% improvement in performance over baseline methods across multiple settings, demonstrating its capability to autonomously identify effective experimental strategies.

Conclusion: Preference-based learning, as demonstrated by ACE, is effective in discovering principled experimental strategies, showing potential to complement existing theoretical methods by enabling adaptive learning in complex domains.

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [1055] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: This paper introduces RL-CRP, a decentralized reinforcement learning framework aimed at improving multi-server federated learning systems by reducing inter-server conflicts and enhancing training efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional single-server federated learning suffers from high communication latency, while multi-server FL introduces challenges like resource contention and bandwidth conflicts due to uncoordinated client selections.

Method: The RL-CRP system applies reinforcement learning with conflict risk prediction using categorical hidden Markov models for estimating client selection conflicts and incorporates a fairness-aware reward mechanism to enhance long-term client participation.

Result: RL-CRP significantly reduces inter-server conflicts and improves training convergence speed and communication costs, as proven through extensive experiments.

Conclusion: RL-CRP successfully optimizes client selection in multi-server FL systems, addressing issues like resource contention and promoting efficiency in collaborative model training.

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [1056] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: The paper introduces RLTF (RL from Text Feedback), which uses textual feedback for LLM training, offering improvement over traditional scalar rewards and distillation methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between low-information scalar rewards and costly demonstrations by leveraging text feedback, which is cheaper and naturally abundant.

Method: Two methods are proposed: RLTF-SD (Self Distillation) and RLTF-FM (Feedback Modeling), which train LLMs by using intermediate supervision from text feedback.

Result: Both methods consistently outperform baselines across various tasks like reasoning puzzles, competition math, and creative writing.

Conclusion: Text feedback provides an effective and scalable intermediate signal for enhancing RL in LLMs, demonstrating the potential of richer supervision.

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [1057] [Repair Brain Damage: Real-Numbered Error Correction Code for Neural Network](https://arxiv.org/abs/2602.00076)
*Ziqing Li,Myung Cho,Qiutong Jin,Weiyu Xu*

Main category: cs.NE

TL;DR: This paper proposes a real-number-based error correction code for neural networks to handle memory and computational errors effectively while maintaining classification performance.


<details>
  <summary>Details</summary>
Motivation: Addressing memory and computational errors in neural networks to improve reliability without compromising performance.

Method: Introduced real-number-based linear constraints on NN weights for error detection and correction, avoiding parameter increase.

Result: The ECC design efficiently detects and corrects errors while preserving the neural network's classification accuracy.

Conclusion: The proposed method successfully enhances neural network reliability by mitigating errors without added complexity in parameters.

Abstract: We consider a neural network (NN) that may experience memory faults and computational errors. In this paper, we propose a novel real-number-based error correction code (ECC) capable of detecting and correcting both memory errors and computational errors. The proposed approach introduces structures in the form of real-number-based linear constraints on the NN weights to enable error detection and correction, without sacrificing classification performance or increasing the number of real-valued NN parameters.

</details>


### [1058] [MO-ELA: Rigorously Expanding Exploratory Landscape Features for Automated Algorithm Selection in Continuous Multi-Objective Optimisation](https://arxiv.org/abs/2602.00098)
*Oliver Preuß,Jeroen Rook,Jakob Bossek,Heike Trautmann*

Main category: cs.NE

TL;DR: The paper proposes a novel set of exploratory landscape features (MO-ELA) for multi-objective optimization, enhancing problem characterization and contributing to improved automated algorithm selection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in the availability of exploratory landscape features for multi-objective optimization, which limits advancements in automated algorithm selection in this domain.

Method: A new set of features (MO-ELA) is introduced, based on random sampling across decision and objective spaces, categorized into five groups: non-dominated sorting, descriptive statistics, principal component analysis, graph structures, and gradient information. An AAS study on established benchmarks evaluates their effectiveness.

Result: The newly developed features effectively distinguish algorithm performance, adequately capture problem hardness, and lead models close to the virtual best solver. They frequently rank among the top contributors after feature selection.

Conclusion: The proposed MO-ELA features are valuable additions to exploratory landscape analysis for multi-objective optimization, improving algorithm selection and problem characterization.

Abstract: Automated Algorithm Selection (AAS) is a popular meta-algorithmic approach and has demonstrated to work well for single-objective optimisation in combination with exploratory landscape features (ELA), i.e., (numerical) descriptive features derived from sampling the black-box (continuous) optimisation problem. In contrast to the abundance of features that describe single-objective optimisation problems, only a few features have been proposed for multi-objective optimisation so far. Building upon recent work on exploratory landscape features for box-constrained continuous multi-objective optimization problems, we propose a novel and complementary set of additional features (MO-ELA). These features are based on a random sample of points considering both the decision and objective space. The features are divided into 5 feature groups depending on how they are being calculated: non-dominated-sorting, descriptive statistics, principal component analysis, graph structures and gradient information. An AAS study conducted on well-established multi-objective benchmarks demonstrates that the proposed features contribute to successfully distinguishing between algorithm performance and thus adequately capture problem hardness resulting in models that come very close to the virtual best solver. After feature selection, the newly proposed features are frequently among the top contributors, underscoring their value in algorithm selection and problem characterisation.

</details>


### [1059] [Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive Optimization](https://arxiv.org/abs/2602.00532)
*Qianhao Zhu,Sijie Ma,Zeyuan Ma,Hongshu Guo,Yue-Jiao Gong*

Main category: cs.NE

TL;DR: This paper develops an adaptive and generalizable constraint handling policy using reinforcement learning to address challenges in constrained optimization.


<details>
  <summary>Details</summary>
Motivation: Existing constraint handling techniques are largely designed by human experts, which lack utility in general cases. Automated algorithm design in Meta-Black-Box Optimization offers potential for improved optimization performance.

Method: A tailored Markov Decision Process is formulated, utilizing a deep Q-network-based policy to control the constraint relaxation level during optimization. The approach is trained on CEC 2017 Constrained Optimization benchmarks.

Result: The proposed method demonstrates competitive or superior performance compared to strong baselines, validated through Leave-one-out cross-validation and train-test split validation.

Conclusion: The technique offers effective and adaptive constraint handling via reinforcement learning, showing promise for enhancing performance in constrained optimization tasks.

Abstract: Constraint handling plays a key role in solving realistic complex optimization problems. Though intensively discussed in the last few decades, existing constraint handling techniques predominantly rely on human experts' designs, which more or less fall short in utility towards general cases. Motivated by recent progress in Meta-Black-Box Optimization where automated algorithm design can be learned to boost optimization performance, in this paper, we propose learning effective, adaptive and generalizable constraint handling policy through reinforcement learning. Specifically, a tailored Markov Decision Process is first formulated, where given optimization dynamics features, a deep Q-network-based policy controls the constraint relaxation level along the underlying optimization process. Such adaptive constraint handling provides flexible tradeoff between objective-oriented exploitation and feasible-region-oriented exploration, and hence leads to promising optimization performance. We train our approach on CEC 2017 Constrained Optimization benchmark with limited evaluation budget condition (expensive cases) and compare the trained constraint handling policy to strong baselines such as recent winners in CEC/GECCO competitions. Extensive experimental results show that our approach performs competitively or even surpasses the compared baselines under either Leave-one-out cross-validation or ordinary train-test split validation. Further analysis and ablation studies reveal key insights in our designs.

</details>


### [1060] [Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning](https://arxiv.org/abs/2602.00540)
*Yuxin Wu,Hongshu Guo,Ting Huang,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.NE

TL;DR: This paper introduces SEEMOO, a framework that dynamically selects surrogate models in multi-objective optimization tasks, using reinforcement learning to improve overall performance.


<details>
  <summary>Details</summary>
Motivation: To address the bias introduced by human-dependent surrogate model selection in Surrogate-assisted Evolutionary Algorithms (SAEAs), which can hinder their performance on diverse and complex optimization tasks.

Method: SEEMOO uses a reinforcement learning-based approach. It incorporates a pre-collected pool of surrogate models, an attention-based state extractor for optimization state representation, and a deep Q-network to dynamically schedule surrogate models during optimization.

Result: SEEMOO outperforms single-surrogate baselines, demonstrating improvements in optimization performance across benchmarks. Ablation studies highlight the significance of its key components.

Conclusion: SEEMOO showcases the potential of reinforcement learning for adaptive surrogate model selection, providing a robust method to enhance the efficiency of multi-objective optimization tasks.

Abstract: Surrogate-assisted Evolutionary Algorithms~(SAEAs) have shown promising robustness in solving expensive optimization problems. A key aspect that impacts SAEAs' effectiveness is surrogate model selection, which in existing works is predominantly decided by human developer. Such human-made design choice introduces strong bias into SAEAs and may hurt their expected performance on out-of-scope tasks. In this paper, we propose a reinforcement learning-assisted ensemble framework, termed as SEEMOO, which is capable of scheduling different surrogate models within a single optimization process, hence boosting the overall optimization performance in a cooperative paradigm. Specifically, we focus on expensive multi-objective optimization problems, where multiple objective functions shape a compositional landscape and hence challenge surrogate selection. SEEMOO comprises following core designs: 1) A pre-collected model pool that maintains different surrogate models; 2) An attention-based state-extractor supports universal optimization state representation of problems with varied objective numbers; 3) a deep Q-network serves as dynamic surrogate selector: Given the optimization state, it selects desired surrogate model for current-step evaluation. SEEMOO is trained to maximize the overall optimization performance under a training problem distribution. Extensive benchmark results demonstrate SEEMOO's surrogate ensemble paradigm boosts the optimization performance of single-surrogate baselines. Further ablation studies underscore the importance of SEEMOO's design components.

</details>


### [1061] [NegaBent, No Regrets: Evolving Spectrally Flat Boolean Functions](https://arxiv.org/abs/2602.00843)
*Claude Carlet,Marko Ðurasevic,Ermes Franch,Domagoj Jakobovic,Luca Mariot,Stjepan Picek*

Main category: cs.NE

TL;DR: The paper investigates the use of evolutionary algorithms, particularly genetic programming, to evolve negabent Boolean functions with desirable spectral properties.


<details>
  <summary>Details</summary>
Motivation: Negabent Boolean functions exhibit optimal nega-Hadamard spectral properties, and their subclass (bent-negabent functions) holds practical interest. The paper seeks methods to evolve these functions efficiently.

Method: Evolutionary algorithms, with a focus on genetic programming, are employed to evolve negabent Boolean functions in varying dimensions.

Result: Experimental findings showcase the suitability of evolutionary algorithms for evolving negabent Boolean functions across all considered dimensions.

Conclusion: Evolutionary algorithms are effective tools for producing negabent Boolean functions, particularly genetic programming proving highly capable in this task.

Abstract: Negabent Boolean functions are defined by having a flat magnitude spectrum under the nega-Hadamard transform. They exist in both even and odd dimensions, and the subclass of functions that are simultaneously bent and negabent (bent-negabent) has attracted interest due to the combined optimal periodic and negaperiodic spectral properties. In this work, we investigate how evolutionary algorithms can be used to evolve (bent-)negabent Boolean functions. Our experimental results indicate that evolutionary algorithms, especially genetic programming, are a suitable approach for evolving negabent Boolean functions, and we successfully evolve such functions in all dimensions we consider.

</details>


### [1062] [Organismal Agency and Rapid Adaptation: The Phenopoiesis Algorithm for Phenotype-First Evolution](https://arxiv.org/abs/2602.00978)
*Nam H. Le*

Main category: cs.NE

TL;DR: This paper introduces the Phenopoiesis Algorithm, which enables organisms to inherit not just genes but learned phenotypic patterns, enhancing adaptation efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of the gene-centric evolutionary paradigm by demonstrating that organismal agency can be computationally implemented.

Method: Developed the Phenopoiesis Algorithm allowing inheritance of phenotypic patterns and conducted experiments showing faster adaptation in changing environments.

Result: Organisms using this algorithm adapted 3.4 times faster than those relying solely on gene-centric models, emphasizing the impact of cross-generational inheritance of learned patterns.

Conclusion: Organismal agency is a measurable and algorithmic mechanism that boosts adaptation, integrating both fast phenotypic and slow genetic inheritance.

Abstract: Evolutionary success depends on the capacity to adapt: organisms must respond to environmental challenges through both genetic innovation and lifetime learning. The gene-centric paradigm attributes evolutionary causality exclusively to genes, while Denis Noble's phenotype-first framework argues that organisms are active agents capable of interpreting genetic resources, learning from experience, and shaping their own development. However, this framework has remained philosophically intuitive but algorithmically opaque.
  We show for the first time that organismal agency can be implemented as a concrete computational process through heritable phenotypic patterns. We introduce the Phenopoiesis Algorithm, where organisms inherit not just genes but also successful phenotypic patterns discovered during lifetime learning. Through experiments in changing environments, these pattern-inheriting organisms achieve 3.4 times faster adaptation compared to gene-centric models. Critically, these gains require cross-generational inheritance of learned patterns rather than within-lifetime learning alone.
  We conclude that organismal agency is not a philosophical abstraction but an algorithmic mechanism with measurable adaptive value. The mechanism works through compositional reuse: organisms discover how to compose primitive elements into solutions, encode those compositional recipes, and transmit them to offspring. Evolution operates across multiple timescales -- fast, reversible phenotypic inheritance and slow, permanent genetic inheritance -- providing adaptive flexibility that single-channel mechanisms cannot achieve.

</details>


### [1063] [The Stacked Autoencoder Evolution Hypothesis](https://arxiv.org/abs/2602.01026)
*Hiroyuki Iizuka*

Main category: cs.NE

TL;DR: The paper introduces the 'Stacked Autoencoder Evolution Hypothesis,' linking layered autoencoder dynamics to biological evolutionary systems.


<details>
  <summary>Details</summary>
Motivation: To provide a new perspective on evolutionary processes, focusing on hierarchical layers of genetic encoding and decoding, rather than traditional mutation and selection.

Method: Proposes a theoretical framework and validates it through artificial chemistry simulations illustrating hierarchical autoencoder structures.

Result: Simulations showed spontaneous emergence of autoencoder-like structures, supporting the hypothesis.

Conclusion: The framework suggests evolution operates on multi-layered abstraction levels, potentially explaining abrupt evolutionary changes and offering insights into discontinuous evolution.

Abstract: This study introduces a novel theoretical framework, the Stacked Autoencoder Evolution Hypothesis, which proposes that biological evolutionary systems operate through multi-layered self-encoding and decoding processes, analogous to stacked autoencoders in deep learning. Rather than viewing evolution solely as gradual changes driven by mutation and selection, this hypothesis suggests that self-replication inherently compresses and reconstructs genetic information across hierarchical layers of abstraction. This layered structure enables evolutionary systems to explore diverse possibilities not only at the sequence level but also across progressively more abstract layers of representation, making it possible for even simple mutations to navigate these higher-order spaces.Such a mechanism may explain punctuated evolutionary patterns and changes that can appear as if they are goal-directed in natural evolution, by allowing mutations at deeper latent layers to trigger sudden, large-scale phenotypic shifts. To illustrate the plausibility of this mechanism, artificial chemistry simulations were conducted, demonstrating the spontaneous emergence of hierarchical autoencoder structures. This framework offers a new perspective on the informational dynamics underlying both continuous and discontinuous evolutionary change.

</details>


### [1064] [Parallel Training in Spiking Neural Networks](https://arxiv.org/abs/2602.01133)
*Yanbin Huang,Man Yao,Yuqi Pan,Changze Lv,Siyuan Xu,Xiaoqing Zheng,Bo Xu,Guoqi Li*

Main category: cs.NE

TL;DR: This paper suggests a novel approach to improve spiking neurons in Spiking Neural Networks (SNNs) by removing their conventional reset mechanism for enhanced parallelism on GPUs, proposing a dynamic decay model tested across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational inefficiencies of spiking neurons for modern, large-scale tasks on GPUs, focusing on their inability to efficiently support parallel computation due to the inherent reset mechanism.

Method: The authors propose removing the traditional reset mechanism in spiking neurons and introduce a dynamic decay spiking neuron model that retains efficient serial inference while enabling parallel training. They define how reset functions and parallelism can work together.

Result: The dynamic decay spiking neuron achieved a 25.6x speedup in training compared to prior methods on long sequences and demonstrated robustness for sequences significantly longer than trained ones. It showed consistent effectiveness across diverse tasks, network architectures, activation types, and reduced energy consumption.

Conclusion: The proposed dynamic decay neuron successfully supports parallel computation and efficient inference, making SNNs more scalable and energy-efficient for modern applications. It is versatile across tasks, energy-friendly, and capable of handling extended sequence lengths robustly.

Abstract: The bio-inspired integrate-fire-reset mechanism of spiking neurons constitutes the foundation for efficient processing in Spiking Neural Networks (SNNs). Recent progress in large models demands that spiking neurons support highly parallel computation to scale efficiently on modern GPUs. This work proposes a novel functional perspective that provides general guidance for designing parallel spiking neurons. We argue that the reset mechanism, which induces complex temporal dependencies and hinders parallel training, should be removed. However, any such modification should satisfy two principles: 1) preserving the functions of reset as a core biological mechanism; and 2) enabling parallel training without sacrificing the serial inference ability of spiking neurons, which underpins their efficiency at test time. To this end, we identify the functions of the reset and analyze how to reconcile parallel training with serial inference, upon which we propose a dynamic decay spiking neuron. We conduct comprehensive testing of our method in terms of: 1) Training efficiency and extrapolation capability. On 16k-length sequences, we achieve a 25.6x training speedup over the pioneering parallel spiking neuron, and our models trained on 2k-length can stably perform inference on sequences as long as 30k. 2) Generality. We demonstrate the consistent effectiveness of the proposed method across five task categories (image classification, neuromorphic event processing, time-series forecasting, language modeling, and reinforcement learning), three network architectures (spiking CNN/Transformer/SSMs), and two spike activation modes (spike/integer activation). 3) Energy consumption. The spiking firing of our neuron is lower than that of vanilla and existing parallel spiking neurons.

</details>


### [1065] [Unleashing the Potential of Differential Evolution through Individual-Level Strategy Diversity](https://arxiv.org/abs/2602.01147)
*Chenchen Feng,Minyang Chen,Zhuozhao Li,Ran Cheng*

Main category: cs.NE

TL;DR: This paper introduces iStratDE, a Differential Evolution (DE) variant that uses fixed, individual-level strategy diversity to improve performance without relying on adaptation or feedback.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the largely unexamined structural benefits of static strategy diversity in DE, as most variants focus on adaptive mechanisms rather than persistent behavioral heterogeneity.

Method: iStratDE assigns individualized mutation and crossover strategies during initialization, maintains them fixed throughout, and ensures efficient scalability through intrinsic concurrency for GPU computing.

Result: Experiments on the CEC2022 benchmark suite and robotic control tasks show iStratDE performs as well as or better than adaptive DE variants, verifying the effectiveness of individual-level strategy diversity.

Conclusion: Individual-level strategy assignments offer a simple yet effective approach to enhancing DE, with the additional benefits of ease of implementation and scalability for parallel computing.

Abstract: Since Differential Evolution (DE) is sensitive to strategy choice, most existing variants pursue performance through adaptive mechanisms or intricate designs. While these approaches focus on adjusting strategies over time, the structural benefits that static strategy diversity may bring remain largely unexplored. To bridge this gap, we study the impact of individual-level strategy diversity on DE's search dynamics and performance, and introduce iStratDE (DE with individual-level strategies), a minimalist variant that assigns mutation and crossover strategies independently to each individual at initialization and keeps them fixed throughout the evolutionary process. By injecting diversity at the individual level without adaptation or feedback, iStratDE cultivates persistent behavioral heterogeneity that is especially effective with large populations. Moreover, its communication-free construction possesses intrinsic concurrency, thereby enabling efficient parallel execution and straightforward scaling for GPU computing. We further provide a convergence analysis of iStratDE under standard reachability assumptions, which establishes the almost-sure convergence of the best-so-far fitness. Extensive experiments on the CEC2022 benchmark suite and robotic control tasks demonstrate that iStratDE matches or surpasses established adaptive DE variants. These results highlight individual-level strategy assignment as a straightforward yet effective mechanism for enhancing DE's performance. The source code of iStratDE is publicly accessible at: https://github.com/EMI-Group/istratde.

</details>


### [1066] [Dynamic Heuristic Neuromorphic Solver for the Edge User Allocation Problem with Bayesian Confidence Propagation Neural Network](https://arxiv.org/abs/2602.01294)
*Kecheng Zhang,Anders Lansner,Ahsan Javed Awan,Naresh Balaji Ravichandran,Pawel Herman*

Main category: cs.NE

TL;DR: The paper introduces a neuromorphic solver for the NP-hard Edge User Allocation problem using a novel attractor network that combines WTA and heuristic biasing.


<details>
  <summary>Details</summary>
Motivation: To address the NP-hard Edge User Allocation problem and improve computational efficiency by leveraging neuromorphic architectures and novel network mechanisms.

Method: A dynamic heuristic biasing attractor network implementing the Bayesian Confidence Propagation Neural Network (BCPNN) framework with a novel 'no allocation' state in each WTA motif.

Result: The proposed method achieves near-optimal performance with empirically bounded time steps and offers compatibility with energy-efficient neuromorphic systems.

Conclusion: This neuromorphic approach provides a competitive solution to the Edge User Allocation problem, promising practical energy efficiency and broader application potential in neuromorphic computing.

Abstract: We propose a neuromorphic solver for the NP-hard Edge User Allocation problem using an attractor network with Winner-Takes-All (WTA) mechanism implemented with the Bayesian Confidence Propagation Neural Network (BCPNN) framework. Unlike previous energy-based attractor networks, our solver uses dynamic heuristic biasing to guide allocations in real time and introduces a "no allocation" state to each WTA motif, achieving near-optimal performance with an empirically upper-bounded number of time steps. The approach is compatible with neuromorphic architectures and may offer improvements in energy efficiency.

</details>


### [1067] [SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays](https://arxiv.org/abs/2602.01978)
*Roel Koopman,Sebastian Otte,Sander Bohté*

Main category: cs.NE

TL;DR: The paper proposes a novel SpikingGamma model to address challenges in training Spiking Neural Networks (SNNs), enhancing efficiency, scalability, and hardware compatibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of training SNNs with fine temporal discretization for energy-efficient and low-latency computation, while aligning software-trained SNNs with efficient neuromorphic hardware implementation.

Method: The proposed method involves spiking neurons with internal recursive memory structures combined with sigma-delta spike-coding. The SpikingGamma model avoids the need for surrogate gradients and supports direct error backpropagation.

Result: The SpikingGamma model shows capability to learn fine temporal patterns with minimal spiking, works effectively in an online manner, handles complex tasks with competitive accuracy, and is insensitive to temporal resolution.

Conclusion: This approach provides an efficient alternative to current recurrent SNNs using surrogate gradients and facilitates the direct mapping of SNNs to neuromorphic hardware.

Abstract: Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.

</details>


### [1068] [Scale-covariant spiking wavelets](https://arxiv.org/abs/2602.02020)
*Jens Egholm Pedersen,Tony Lindeberg,Peter Gerstoft*

Main category: cs.NE

TL;DR: This paper explores the link between wavelet transforms and spiking neural networks using scale-space theory for energy-efficient signal processing.


<details>
  <summary>Details</summary>
Motivation: The authors aim to connect wavelet transforms with spiking neural networks to enable energy-efficient signal processing.

Method: Utilize leaky integrate-and-fire neurons for implementing discrete mother wavelets approximating continuous wavelets and conduct a reconstruction experiment.

Result: The approach demonstrates feasibility but indicates approximation errors requiring further analysis.

Conclusion: A novel spiking signal representation is introduced, suggesting potential for energy-efficient signal processing algorithms.

Abstract: We establish a theoretical connection between wavelet transforms and spiking neural networks through scale-space theory. We rely on the scale-covariant guarantees in the leaky integrate-and-fire neurons to implement discrete mother wavelets that approximate continuous wavelets. A reconstruction experiment demonstrates the feasibility of the approach and warrants further analysis to mitigate current approximation errors. Our work suggests a novel spiking signal representation that could enable more energy-efficient signal processing algorithms.

</details>


### [1069] [Spark: Modular Spiking Neural Networks](https://arxiv.org/abs/2602.02306)
*Mario Franco,Carlos Gershenson*

Main category: cs.NE

TL;DR: This paper introduces Spark, a modular framework for spiking neural networks designed for efficient and streamlined simulation, demonstrating success in solving a sparse-reward cartpole problem.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from inefficiencies in current neural network models concerning data and energy usage. Spiking neural networks are seen as a promising alternative, but effective learning mechanisms for them are still lacking.

Method: The authors developed the Spark framework, emphasizing modular design. It utilizes basic components and integrates them into models compatible with established machine learning pipelines.

Result: The framework successfully addressed the sparse-reward cartpole problem by implementing simple plasticity mechanisms within spiking networks.

Conclusion: Spark aims to accelerate research in spiking neural networks, particularly targeting continuous and unbatched learning mechanisms inspired by biological systems.

Abstract: Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.

</details>


### [1070] [Introns and Templates Matter: Rethinking Linkage in GP-GOMEA](https://arxiv.org/abs/2602.02311)
*Johannes Koch,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.NE

TL;DR: This work proposes new methods for improving GP-GOMEA, a tool for symbolic regression, by addressing issues with introns affecting dependency learning using novel linkage measures.


<details>
  <summary>Details</summary>
Motivation: To address the issue of introns in GP-GOMEA, which hinder the accurate learning of dependencies, thereby affecting performance and interpretability.

Method: The authors propose two approaches: one measure explicitly considers introns in mutual information calculations, and the other applies a grey-box perspective deriving linkage without requiring population-based learning.

Result: Across five symbolic regression benchmarks, both new measures improve performance, and directly using the template-derived linkage measure results in the best outcomes.

Conclusion: New linkage learning approaches significantly enhance GP-GOMEA's efficiency and interpretability by addressing intron-related challenges and leveraging template-derived structures.

Abstract: GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.

</details>


### [1071] [Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization](https://arxiv.org/abs/2602.02439)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz,Duygu Erisken,Rana Irem Turhan*

Main category: cs.NE

TL;DR: NeuEdge is a framework for ultra-low-power and low-latency edge AI, using adaptive spiking neural networks (SNNs) and hardware-aware optimization to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deploying spiking neural networks (SNNs) on resource-constrained edge devices, such as training difficulties, hardware-mapping inefficiencies, and sensitivity to temporal dynamics.

Method: NeuEdge integrates adaptive SNN models with a temporal coding scheme blending rate and spike-timing patterns. It employs hardware-aware training to co-optimize network structure and chip placement, alongside an adaptive threshold mechanism for reduced energy consumption.

Result: NeuEdge achieves 91-96% accuracy with up to 2.3 ms latency, a 847 GOp/s/W energy efficiency, and demonstrates up to 312x energy savings in a drone-based workload compared to traditional deep neural networks.

Conclusion: NeuEdge provides a highly efficient and practical framework for deploying neuromorphic AI on edge devices, balancing accuracy, latency, and energy efficiency.

Abstract: Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1072] [Phoenix: A Modular and Versatile Framework for C/C++ Pointer Analysis](https://arxiv.org/abs/2602.01720)
*Peisen Yao,Zinan Gu,Qingkai Shi*

Main category: cs.PL

TL;DR: Phoenix is a unified, modular framework for pointer analysis in C/C++ that simplifies analysis comparison and usage, while exposing clear trade-offs between precision and performance.


<details>
  <summary>Details</summary>
Motivation: The C/C++ pointer analysis ecosystem is fragmented, lacking unified interfaces that hinder easy comparison and integration of state-of-the-art alias analysis methods.

Method: Phoenix integrates IR construction, constraint generation, and solver backends within a modular design, enabling easy swapping and composition of analysis components. It evaluates performance against SVF under various precision settings.

Result: Phoenix demonstrates up to 2.88x speedup in flow- and context-insensitive scenarios and competitive performance in flow- and context-sensitive precisions (up to 2.91x) when analyzed using GNU coreutils.

Conclusion: Phoenix improves the modularity and performance of pointer analysis frameworks for C/C++, supporting industrial toolchains that have identified thousands of bugs and improving developer workflows.

Abstract: We present Phoenix, a modular pointer analysis framework for C/C++ that unifies multiple state-of-the-art alias analysis algorithms behind a single, stable interface. Phoenix addresses the fragmentation of today's C/C++ pointer analysis ecosystem by cleanly separating IR construction, constraint generation, solver backends, and client-facing queries, making analyses easy to compare, swap, and compose while exposing explicit precision-performance trade-offs. We evaluate Phoenix against SVF under two representative configurations: a flow- and context-insensitive setting and a more precise flow- and context-sensitive setting, on 28 GNU coreutils programs. Phoenix delivers robust speedups in the baseline configuration (up to 2.88x) and remains competitive, and often faster, even in the stronger precision regime (up to 2.91x), without a systematic runtime penalty. In production, Phoenix serves as the analysis substrate for static analysis and fuzzing tools that have uncovered hundreds of new bugs and enabled deployments reporting more than 1000 bugs found in an industrial toolchain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1073] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: The paper introduces MapDream, a method for vision-language navigation that combines map construction with navigation objectives, leading to state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional map representations for navigation rely on pre-defined structures and lack adaptability to navigation objectives. The authors propose a learned representation to optimize performance.

Method: The paper introduces MapDream, a framework that generates autoregressive bird's-eye-view maps directly tied to navigation policies. It uses supervised pre-training and reinforcement fine-tuning for optimization.

Result: The proposed method delivers state-of-the-art results on benchmarks like R2R-CE and RxR-CE for monocular performance.

Conclusion: Task-driven generative map learning significantly improves navigation by focusing on critical navigation affordances rather than detailed reconstruction.

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [1074] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: ZEST is a framework for training humanoid robots using reinforced learning from diverse motion data sources, enabling robust, zero-shot deployment of dynamic and human-like skills across platforms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving robust and agile whole-body control for humanoid robots in contact-rich environments without extensive engineering and tuning.

Method: ZEST trains policies using reinforcement learning from diverse motion data like motion capture, videos, and animations. Techniques include adaptive sampling, model-based curriculum, and joint-gain selection for simulations.

Result: The ZEST framework demonstrates success in transferring skills from motion sources to robots like Atlas and Spot, achieving complex maneuvers, expressive dance, and acrobatics.

Conclusion: ZEST proves to be a scalable and generalizable tool for translating biological movements into robotic control systems, enabling robust performance across different robots and motion data.

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [1075] [FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control](https://arxiv.org/abs/2602.00480)
*Mohini Priya Kolluri,Ammar Waheed,Zohaib Hasnain*

Main category: cs.RO

TL;DR: A decentralized method inspired by fluid dynamics is proposed to control large robot swarms without relying on inter-agent communication, validated via simulations and demonstrating comparable results to Computational Fluid Dynamics.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current swarm strategies reliant on inter-agent communication, such as latency, bandwidth constraints, and susceptibility to failure, by proposing a scalable and decentralized control model.

Method: Developed a decentralized approach mimicking fluid dynamics, linking fluidic element properties to robot agent states, enabling collective evolution without direct communication. Validated through simulations of thousands of quadcopters and comparison with CFD solutions using RMSE metrics.

Result: The swarm model achieved normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, and 0-0.937 for pressure when compared to CFD, showing quantitative agreement and feasibility of the fluid-inspired model.

Conclusion: The study demonstrates the potential for treating large robotic swarms as fluid-inspired systems, offering scalable, coherent, and communication-independent control suitable for large-scale multi-agent applications.

Abstract: Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm "flows" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.

</details>


### [1076] [Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning](https://arxiv.org/abs/2602.00500)
*Jianyi Zhou,Yujie Wei,Ruichen Zhen,Bo Zhao,Xiaobo Xia,Rui Shao,Xiu Su,Shuo Yang*

Main category: cs.RO

TL;DR: This paper introduces INFUSE, a backdoor attack framework for Vision-Language-Action (VLA) models, effectively maintaining backdoors even after user fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore the security of VLA models, particularly against backdoor attacks, which are overlooked yet critical threats in real-world deployments and adaptation processes.

Method: INFUSE identifies fine-tune-insensitive modules in VLA models, injects backdoors into these modules, and freezes the remaining, ensuring the persistence of malicious behavior after fine-tuning.

Result: INFUSE achieves mean attack success rates of 91.0% in simulation environments and 79.8% in real-world tasks, greatly outperforming existing methods like BadVLA.

Conclusion: INFUSE reveals a critical security vulnerability: backdoors in VLA base models can persist through adaptation, emphasizing the need for improved safeguards against such threats.

Abstract: Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.

</details>


### [1077] [A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation](https://arxiv.org/abs/2602.00514)
*Yaohua Liu,Binkai Ou,Zicheng Qiu,Ce Hao,Yemin Wang,Hengjun Zhang*

Main category: cs.RO

TL;DR: The paper introduces LVTG, a cost-effective visuo-tactile robotic gripper with improved durability, sensing area, and modular design, enhancing manipulation performance through cross-modal representation.


<details>
  <summary>Details</summary>
Motivation: Manipulation in contact-rich environments is constrained by limitations in current tactile sensors, such as sensing range, reliability, and cost efficiency.

Method: A visuo-tactile gripper (LVTG) is proposed, leveraging a modular design, wear-resistant materials, and vision-tactile feedback. It integrates contrastive learning (CLIP-inspired) for aligning tactile and visual data.

Result: LVTG improves tactile sensing, perception, durability, and policy learning efficiency. Success rates in manipulation tasks substantially improve compared to the original ACT method.

Conclusion: LVTG offers a robust solution for contact-rich manipulation challenges by enabling enhanced perception capabilities, efficient learning, and operational durability in robotic systems.

Abstract: Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.

</details>


### [1078] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

TL;DR: This paper focuses on improving UAV navigation in complex environments by introducing a hierarchical agent (APEX) for better exploration and target identification.


<details>
  <summary>Details</summary>
Motivation: The aim is to address limitations in UAV navigation, such as poor handling of spatial representations, decision-making, and exploration inefficiency.

Method: The proposed method, APEX, includes three parts: dynamic mapping memory, a decision module based on reinforcement learning, and a target grounding module. These operate in a hierarchical and parallel framework.

Result: APEX outperforms the state of the art by +4.2% success rate and +2.8% SPL on UAV-ON benchmarks.

Conclusion: APEX demonstrates superior efficiency and effectiveness in UAV navigation and sets a new standard in aerial object goal navigation.

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [1079] [ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation](https://arxiv.org/abs/2602.00557)
*Weisheng Dai,Kai Lan,Jianyi Zhou,Bo Zhao,Xiu Su,Junwen Tong,Weili Guan,Shuo Yang*

Main category: cs.RO

TL;DR: The paper introduces ConLA, an unsupervised pretraining framework that learns robotic policies from human demonstration videos, outperforming prior approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of costly robot teleoperation datasets and unsupervised learning frameworks relying on visual cues.

Method: ConLA employs a contrastive disentanglement mechanism using action category priors and temporal cues to isolate motion dynamics from visual content.

Result: ConLA outperforms benchmarks, surpassing prior methods using real robot trajectory pretraining, demonstrating scalability and effective latent action representations.

Conclusion: ConLA extracts semantically consistent action representations from scalable human videos, improving robot learning beyond existing approaches.

Abstract: Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.

</details>


### [1080] [UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning](https://arxiv.org/abs/2602.00566)
*Nan Song,Junzhe Jiang,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.RO

TL;DR: UniMotion is a unified Transformer-based framework for motion simulation, prediction, and planning in autonomous driving that promotes cross-task generalization and achieves state-of-the-art performance after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of isolated approaches for motion tasks in autonomous driving by proposing a unified framework that captures shared structures and enables mutual benefits across tasks.

Method: Developed UniMotion, a decoder-only Transformer framework with dedicated interaction modes and tailored training strategies for joint motion tasks while allowing specialized fine-tuning.

Result: Extensive experiments on the Waymo Open Motion Dataset show that UniMotion ensures robust generalization, effective task integration, and achieves state-of-the-art performance with fine-tuning.

Conclusion: UniMotion provides a versatile and scalable solution for motion tasks in autonomous driving by leveraging joint optimization, representation sharing, and task fine-tuning.

Abstract: Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.

</details>


### [1081] [Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction](https://arxiv.org/abs/2602.00575)
*Chaoqun Cui,Jing Huang,Shijing Wang,Liming Zheng,Qingchao Kong,Zhixiong Zeng*

Main category: cs.RO

TL;DR: This paper presents VAGEN, a framework addressing limitations in reinforcement learning evaluation for GUI agents through interactive verification methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation practices in reinforcement learning for GUI agents struggle with scalability and partial state observability, limiting their effectiveness in verifying task completion.

Method: The proposed VAGEN framework uses a verifier agent with interaction tools to actively plan verification strategies and gather evidence of task completion by interacting with the environment.

Result: VAGEN outperformed LLM-as-a-Judge baselines on OSWorld-Verified and AndroidWorld, improving evaluation accuracy and benefiting from test-time scaling strategies.

Conclusion: Interactive verification using VAGEN is a more accurate and effective approach for evaluating tasks in reinforcement learning for GUI agents compared to traditional passive methods.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

</details>


### [1082] [Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction](https://arxiv.org/abs/2602.00675)
*Valerio Belcamino,Mariya Kilina,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: The paper introduces JANUS, a cognitive architecture for robots in dialogue-based interactions, emphasizing modular design, persistent user context, recovery from vague requests, and evidence-grounded responses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable robots in dialogue-based human-robot interactions to maintain context, make auditable and evidence-based decisions over extended interactions, and handle underspecified user requests effectively.

Method: The authors conceptualize JANUS as a partially observable Markov decision process with a factored controller model. It includes modularized components (e.g., scope detection, memory, query generation) and exposes policies guiding its decision-making.

Result: JANUS is evaluated through unit tests in a dietary assistance scenario, showing strong agreement with curated references and practical latency, demonstrating the feasibility of its modular, evidence-driven design.

Conclusion: Factored reasoning, as demonstrated in JANUS, is a scalable and auditable approach for building cognitive architectures in robot assistance, enabling effective and verifiable interaction over extended durations.

Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

</details>


### [1083] [Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion](https://arxiv.org/abs/2602.00678)
*Tianyang Wu,Hanwei Guo,Yuhang Wang,Junshu Yang,Xinyang Sui,Jiayi Xie,Xingyu Chen,Zeyang Liu,Xuguang Lan*

Main category: cs.RO

TL;DR: The paper introduces a robust framework using Mixture-of-Experts (MoE) locomotion policy and RoboGauge for quadrupedal robots to overcome sim-to-real challenges, achieving strong performance in challenging terrains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in sim-to-real gap and reward overfitting for quadrupedal agile locomotion, while minimizing the risks and inefficiencies associated with physical trials.

Method: The proposed approach employs Mixture-of-Experts (MoE) for terrain decomposition using proprioception inputs only and introduces RoboGauge for predictive sim-to-real transfer assessments.

Result: Experiments on a Unitree Go2 showed reliable performance in unseen terrains like snow, sand, stairs, slopes, obstacles, and achieving high-speed stability at 4 m/s with emergent gait adaptations.

Conclusion: The framework successfully enables robust and generalized locomotion in multi-terrain environments for quadrupedal robots, with reduced reliance on physical testing and improved transferability.

Abstract: Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.

</details>


### [1084] [Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching](https://arxiv.org/abs/2602.00686)
*Yujie Wei,Jiahan Fan,Jiyu Guo,Ruichen Zhen,Rui Shao,Xiu Su,Zeke Xie,Shuo Yang*

Main category: cs.RO

TL;DR: The paper proposes a dynamic, task-aware framework for accelerating Vision-Language-Action (VLA) model inference, improving speed and effectiveness in robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models excel in robotic manipulation but suffer from computational overhead, limiting real-world usability. Efficient inference methods are crucial to practical deployment.

Method: The authors propose a learnable policy optimization framework with two cooperative, lightweight modules: Cached Token Selector and Cache Ratio Predictor, trained using a differentiable relaxation for end-to-end optimization.

Result: The method achieves a 1.76x inference speedup and improves average success rate by 1.9% on LIBERO benchmarks and 5.0% on real-world tasks, surpassing existing approaches.

Conclusion: Task-aware computational allocation policies can significantly improve both efficiency and effectiveness of VLA models, enabling better robotic applications.

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.

</details>


### [1085] [USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation](https://arxiv.org/abs/2602.00708)
*Weiqi Gai,Yuman Gao,Yuan Zhou,Yufan Xie,Zhiyang Liu,Yuze Wu,Xin Zhou,Fei Gao,Zhijun Meng*

Main category: cs.RO

TL;DR: USS-Nav introduces a lightweight framework for zero-shot object navigation by UAVs in unknown environments, emphasizing computational efficiency and real-time capabilities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in zero-shot object navigation for UAVs where semantic reasoning conflicts with onboard computational limitations.

Method: The paper builds USS-Nav with a Unified Spatio-Semantic scene graph using spatial graph generation, clustering semantic regions, and a coarse-to-fine exploration strategy employing LLM guidance.

Result: Experimentally validated USS-Nav improves computational efficiency, updates at 15 Hz, and surpasses state-of-the-art navigation methods in SPL.

Conclusion: USS-Nav effectively balances computational demands and semantic reasoning for zero-shot navigation in UAVs, fostering future research through open-source code availability.

Abstract: Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.

</details>


### [1086] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: The paper presents SA-VLA, a framework that enhances spatial generalization in Vision-Language-Action models during reinforcement learning fine-tuning, overcoming robustness degradation in robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Address the degradation of robustness in spatial generalization that commonly occurs in Vision-Language-Action models during reinforcement learning fine-tuning due to sparse rewards and spatially agnostic exploration.

Method: Developed SA-VLA, a spatially-aware reinforcement learning framework. It integrates spatial representation with visual tokens, provides dense rewards based on geometric progress, and introduces SCAN, a spatially-conditioned annealed exploration strategy.

Result: SA-VLA achieved stable RL fine-tuning and improved spatial generalization across multi-object and cluttered manipulation scenarios, leading to robust and transferable robotic behaviors.

Conclusion: SA-VLA aligns spatial grounding with policy optimization, enabling superior performance and generalization in robotic manipulation tasks when compared to other methodologies.

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [1087] [Physics-informed Diffusion Mamba Transformer for Real-world Driving](https://arxiv.org/abs/2602.00808)
*Hang Zhou,Qiang Zhang,Peiran Liu,Yihao Qin,Zhaoxu Yan,Yiding Ji*

Main category: cs.RO

TL;DR: This paper introduces innovations in trajectory planning for autonomous driving by presenting a novel Diffusion Mamba Transformer and Port-Hamiltonian Neural Network module.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop trajectory planners that can effectively model uncertainty in motion prediction and integrate dependencies with domain-specific physical constraints.

Method: They propose a Diffusion Mamba Transformer with mamba and attention mechanisms for context aggregation, and a Port-Hamiltonian Neural Network for embedding energy-based physical constraints.

Result: The proposed framework outperforms state-of-the-art baselines in accuracy, physical plausibility, and robustness in autonomous driving benchmarks.

Conclusion: The study advances motion planning for autonomous vehicles by ensuring better predictive accuracy, physical consistency, and reliability, representing a significant improvement in safety and reliability.

Abstract: Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.

</details>


### [1088] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

TL;DR: This paper introduces a method to create synthetic negative data to improve autonomous robots' ability to recognize non-traversable regions, enhancing navigational safety.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised learning for traversability lacks explicit negative examples, hindering a robot's ability to identify non-traversable areas accurately.

Method: The authors propose creating synthetic negative data for traversability learning, which integrates into Positive-Unlabeled and Positive-Negative frameworks without altering inference architectures.

Result: Extensive experiments on public and private datasets demonstrate improved robustness and generalization for traversability estimation in diverse environments.

Conclusion: The new method successfully addresses the absence of explicit negative data, improving non-traversable region identification, and enhances traversability estimation robustness. Public code and resources are available for further research.

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [1089] [Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation](https://arxiv.org/abs/2602.00823)
*Spyridon Syntakas,Kostas Vlachos*

Main category: cs.RO

TL;DR: The paper proposes a method to make Autonomous Underwater Vehicles (AUVs) more energy-efficient by harnessing ocean currents via a novel cost-reduction strategy integrated into control algorithms.


<details>
  <summary>Details</summary>
Motivation: AUV deployment faces challenges of limited energy efficiency and endurance, restricting their practical utility for extended or energy-intensive underwater operations.

Method: The study introduces the Current-Harnessing Stage-Gated MPC approach. It uses per-stage scalars to evaluate the effectiveness of ocean currents for the control goal and incorporates this into two lightweight cost terms—Monotone Cost Shaping (MCS) and Speed-to-Fly (STF)—which are C1 and work as plug-and-play in existing predictive control designs.

Result: Simulations of the BlueROV2 model under realistic current conditions reveal that this method significantly reduces energy usage without sacrificing arrival time or constraints satisfaction compared to traditional control approaches.

Conclusion: The proposed method improves energy efficiency in AUVs, paving the way for longer missions and more sustainable underwater operations.

Abstract: Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the "helpfulness" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative "gliding". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

</details>


### [1090] [Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects](https://arxiv.org/abs/2602.00868)
*Nikhil Uday Shinde,Dylan Hirsch,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: This paper introduces S.S.Explorer, a framework for safe exploration and interaction in stochastic environments, emphasizing the reduction of safety uncertainty and information gathering.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods that ensure safe robot operation in unstructured, stochastic environments where system dynamics are unknown.

Method: The S.S.Explorer framework combines Gaussian Processes for online learning of safety functions with scalable approaches for extending these methods to continuous state spaces.

Result: The approach effectively balances safety and exploration, verified through simulation and hardware experiments, demonstrating reliable autonomy for robots in uncertain environments.

Conclusion: S.S.Explorer improves robot autonomy in dynamic and unknown environments, advancing reliable and safe decision-making systems for complex scenarios.

Abstract: Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.

</details>


### [1091] [Learning When to Jump for Off-road Navigation](https://arxiv.org/abs/2602.00877)
*Zhipeng Zhao,Taimeng Fu,Shaoshu Su,Qiwei Du,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury,Chen Wang*

Main category: cs.RO

TL;DR: This paper presents a Motion-aware Traversability (MAT) model for agile off-road navigation, enabling dynamic path planning by incorporating motion-dependent terrain costs.


<details>
  <summary>Details</summary>
Motivation: The existing off-road driving path planning approaches overly simplify by neglecting dynamic motion influences, risking safety and performance in traversing challenging terrains.

Method: The MAT framework models terrain traversability as a Gaussian function of velocity, predicting parameters via perception and efficiently computing costs influenced by motion in real-time.

Result: MAT achieves real-time performance, demonstrated through experiments in simulated and real-world environments, reducing path detours by 75% while ensuring safety.

Conclusion: The MAT system enhances the efficiency and safety of off-road navigation with agility and dynamic consideration, showing its applicability for complex terrains.

Abstract: Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.

</details>


### [1092] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: The paper introduces RoDiF, a method to fine-tune robotic control diffusion policies using human preferences despite corrupted labels, showing superior performance over state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in fine-tuning diffusion policies for robotic control due to the multi-step denoising structure and corrupted human preferences.

Method: The paper formulates a Unified MDP integrating diffusion denoising with environmental dynamics and proposes RoDiF, which uses a conservative geometric cutting strategy to robustly handle corrupted human preferences.

Result: RoDiF significantly outperforms current methods in long-horizon manipulation tasks, demonstrating effective fine-tuning of diffusion policies with robustness against up to 30% corrupted preference labels.

Conclusion: RoDiF enables robust and effective fine-tuning of pretrained diffusion policies to align with human preferences, even under challenging conditions like high noise in preference labels.

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [1093] [UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation](https://arxiv.org/abs/2602.00915)
*Zhiyuan Wu,Xiangyu Zhang,Zhuo Chen,Jiankang Deng,Rolandos Alexandros Potamias,Shan Luo*

Main category: cs.RO

TL;DR: UniMorphGrasp is a diffusion-based framework that uses hand morphological information to enable state-of-the-art, cross-embodiment dexterous grasping for robotic hands with diverse structures.


<details>
  <summary>Details</summary>
Motivation: Existing grasping methods often fail to generalize to robotics hands with unseen kinematic structures, necessitating a framework for scalable and generalizable cross-embodiment grasp synthesis.

Method: The approach maps grasps into a unified canonical human-like pose representation, conditions grasp generation on structured hand kinematics represented as graphs along with object geometry, and uses hierarchical loss for joint-level supervision.

Result: Experimental evaluations show UniMorphGrasp achieves state-of-the-art performance on benchmarks and demonstrates strong zero-shot adaptation to unseen hand structures.

Conclusion: UniMorphGrasp offers a practical and scalable solution for cross-embodiment dexterous grasp deployment by generalizing well to diverse robotic hand morphologies.

Abstract: Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.

</details>


### [1094] [Green-VLA: Staged Vision-Language-Action Model for Generalist Robots](https://arxiv.org/abs/2602.00919)
*I. Apanasevich,M. Artemyev,R. Babakyan,P. Fedotova,D. Grankin,E. Kupryashin,A. Misailidi,D. Nerus,A. Nutalapati,G. Sidorov,I. Efremov,M. Gerasyov,D. Pikurov,Y. Senchenko,S. Davidenko,D. Kulikov,M. Sultankin,K. Askarbek,O. Shamanin,D. Statovoy,E. Zalyaev,I. Zorin,A. Letkin,E. Rusakov,A. Silchenko,V. Vorobyov,S. Sobolnikov,A. Postnikov*

Main category: cs.RO

TL;DR: Green-VLA is a framework for deploying Vision-Language-Action models on a humanoid robot, generalizing across diverse robotics embodiments. It uses a five-stage learning process and includes robust safety enhancements.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a unified VLA framework that can generalize across different robotics embodiments while efficiently transferring vision-language understanding into robotic actions.

Method: Green-VLA uses a five-stage curriculum for training: foundational VLMs, multimodal grounding, multi-embodiment pretraining, embodiment-specific adaptation, and reinforcement-learning policy alignment. The system adopts a scalable data-processing pipeline and a unified action interface.

Result: Green-VLA demonstrated strong generalization and performance improvements in long-horizon robotic tasks, achieving robustness and efficiency in both simulated and real-world evaluations.

Conclusion: A staged VLA approach with RL alignment provides a scalable and robust solution for enabling humanoid robots to generalize across diverse embodiments and perform complex tasks efficiently.

Abstract: We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.

</details>


### [1095] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: The paper demonstrates how Real-Time Recurrent Reinforcement Learning (RTRRL) helps fine-tune pretrained policies for autonomous systems in dynamic tasks, validated using simulations and real-world examples.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of fixed policies degrading in performance when autonomous systems face changing dynamics, sensor drift, or new objectives.

Method: The paper introduces RTRRL, a biologically inspired online adaptation algorithm, coupled with a Liquid-Resistance Liquid-Capacitance RNN to fine-tune pretrained policies.

Result: The approach was validated through experiments in a simulated CarRacing environment and a real-world RoboRacer car task using event cameras, showing improved performance.

Conclusion: RTRRL and the recurrent model effectively enhance adaptability of pretrained policies, enabling robust performance in variable conditions for autonomous systems.

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [1096] [SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation](https://arxiv.org/abs/2602.00923)
*Jincheng Wang,Lingfan Bao,Tong Yang,Diego Martinez Plasencia,Jianhao Jiao,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: SanD-Planner, a sample-efficient diffusion-based local planner, improves performance in cluttered and dynamic environments using depth image-based imitation learning and ESDF-based safety checking.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating reliable local plans in dynamic and cluttered environments, hindered by limited expert demonstrations and inefficient learning.

Method: SanD-Planner uses depth image-based imitation learning in clamped B-spline space and integrates ESDF-based safety checks with metrics to reduce training burden.

Result: With only 500 episodes of training, SanD-Planner achieves state-of-the-art success rates: 90.1% in simulated cluttered environments and 72.0% in indoor simulations, demonstrating zero-shot transferability to real-world experiments.

Conclusion: The proposed planner effectively balances learning efficiency and performance, showcasing scalability and practicality in various environments. Dataset and pre-trained models will be open-sourced.

Abstract: The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\%$ in simulated cluttered environments and $72.0\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.

</details>


### [1097] [Minimal Footprint Grasping Inspired by Ants](https://arxiv.org/abs/2602.00935)
*Mohamed Sorour,Barbara Webb*

Main category: cs.RO

TL;DR: This paper investigates the design of a gripper inspired by ant forelegs and their fine manipulation abilities, achieving high success rates for various objects, especially in clutter.


<details>
  <summary>Details</summary>
Motivation: To improve robotic grippers by studying the functional advantages of ant forelegs' microstructures and flexibility while enabling robust manipulation in cluttered environments.

Method: The researchers abstracted the functional features of ant forelegs (setal pads, hairs, tarsal flexibility) and implemented these into a slim gripper design suitable for bin-picking tasks.

Result: The experimental evaluation showed the gripper was highly successful (100% grasp attempts) at grasping consumer objects and effectively picking individual items from clutter.

Conclusion: The study opens avenues for enhancing grasping technology by demonstrating the mechanical significance of hairy structures and tarsal flexibility, as observed in ants.

Abstract: Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.

</details>


### [1098] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: This paper introduces CLAMP, a 3D pre-training framework for robotic manipulation using point clouds, contrastive learning, and multi-view data, outperforming state-of-the-art methods in task efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of 2D pre-trained image representations, which fail to capture critical 3D spatial information necessary for precise robotic manipulation.

Method: The authors proposed CLAMP, where they merge point clouds to re-render multi-view observations with depth and 3D coordinates, apply contrastive learning on large-scale simulated robot trajectories, and pre-train a Diffusion Policy to enhance fine-tuning efficiency.

Result: CLAMP significantly improves learning efficiency and outperforms state-of-the-art methods across six simulated and five real-world tasks.

Conclusion: Using 3D pre-training with dynamic multi-view data and contrastive learning ensures better learning performance and task generalization in robotic manipulation tasks.

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [1099] [Meanshift Shape Formation Control Using Discrete Mass Distribution](https://arxiv.org/abs/2602.00980)
*Yichen Cai,Yuan Gao,Pengpeng Li,Wei Wang,Guibin Sun,Jinhu Lü*

Main category: cs.RO

TL;DR: This paper introduces a decentralized control strategy for swarms to form complex shapes and adapt to variations in swarm size using a discrete mass-distribution model and a mean-shift control law.


<details>
  <summary>Details</summary>
Motivation: Existing density-distribution methods struggle with representing complex shapes and implementing decentralized systems, driving the need for an improved, adaptive approach.

Method: The paper utilizes a discrete mass-distribution function to model swarm formation and a decentralized mean-shift control law to adjust swarm distribution to sample points using feedback from mass estimations.

Result: Simulations and experiments demonstrate that the proposed method effectively forms complex shapes and accommodates variations in swarm size.

Conclusion: The presented approach offers an efficient, fully decentralized solution for swarm control, addressing previous challenges in adaptability and complex shape formation.

Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.

</details>


### [1100] [Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds](https://arxiv.org/abs/2602.00992)
*Phone Thiha Kyaw,Jonathan Kelly*

Main category: cs.RO

TL;DR: This paper introduces a novel method for motion planning on Riemannian manifolds, improving trajectory cost efficiency for high-dimensional systems.


<details>
  <summary>Details</summary>
Motivation: Current motion planners often rely on Euclidean distances, which fail to adequately account for the configuration space's task-specific, non-Euclidean geometry.

Method: The authors propose a sampling-based motion planning framework leveraging a midpoint-based Riemannian geodesic distance approximation and local planning using first-order retractions and Riemannian natural gradients.

Result: The proposed method shows improved trajectory cost compared to Euclidean-based planners and traditional geodesic solvers in experiments with robotic systems like a two-link arm, a Franka manipulator, and rigid-body planning in $
SE(2)$ under non-holonomic constraints.

Conclusion: The approach successfully balances geometric fidelity and scalability in high-dimensional motion planning by operating directly on Riemannian manifolds.

Abstract: In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

</details>


### [1101] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: The study introduces HERMES, a multimodal autonomous driving framework that incorporates detailed risk cues for improved trajectory planning.


<details>
  <summary>Details</summary>
Motivation: To address challenges faced by autonomous vehicles in long-tail mixed-traffic scenarios by capturing and integrating explicit risk and contextual cues.

Method: HERMES employs a foundation-model-assisted annotation pipeline and a Tri-Modal Driving Module to integrate long-tail risk cues, multi-view perceptions, historical motion data, and semantic guidance for trajectory planning.

Result: HERMES outperforms standard end-to-end and vision-language model-driven baselines in long-tail mixed-traffic conditions, with ablation studies backing up its efficiency.

Conclusion: HERMES offers enhanced safety and accuracy for autonomous driving in complex and uncertain environments through its multimodal and risk-aware approach.

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [1102] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: The paper introduces LOKI, a hierarchical imitation learning framework for discovering reusable skills from complex robotic tasks using an offline dataset without explicit rewards or annotations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning reusable skills for robots from offline, long-horizon, multi-task data, especially in the absence of explicit rewards or subtask annotations.

Method: Proposes a three-stage framework: (1) Task-aware macro-segmentation of data using a Vector Quantized VAE; (2) Micro-refinement of segments with a self-supervised model and iterative clustering; (3) Constructs hierarchical policies with learned skill termination conditions.

Result: LOKI achieves high success on the D4RL Kitchen benchmark, surpassing standard hierarchical imitation learning baselines and demonstrating meaningful, composable skills.

Conclusion: LOKI provides a powerful, structured way to discover and hierarchically utilize skills from complex offline robotic data, enabling task generalization and intuitive skill serialization.

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [1103] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: The paper introduces ContrAstive Prompt Orchestration (CAPO), a new method for adaptive visuomotor policy learning that uses contrastive prompt learning and adaptive prompt orchestration to improve sample efficiency and domain adaptability.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of training visuomotor policies for agents that can adapt to diverse sensor configurations and environmental variations, as traditional methods fail to separate task-relevant features from domain-specific variations, leading to inefficiencies and failures in unseen environments.

Method: CAPO utilizes a hybrid contrastive learning strategy to create a pool of learnable visual, temporal, and text prompts that capture domain features. An adaptive mechanism dynamically orchestrates these prompts based on current observations, shielding policy optimization from irrelevant domain factors.

Result: Experiments show that CAPO significantly outperforms existing baselines in both sample efficiency and terminal performance. Additionally, it excels in zero-shot adaptation to unseen domains with severe environmental and physical variations.

Conclusion: CAPO proves to be an effective strategy for improving visuomotor policy adaptation in cross-embodiment scenarios by dynamically addressing domain-specific challenges, ensuring better efficiency and robustness.

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [1104] [LLM-Based Behavior Tree Generation for Construction Machinery](https://arxiv.org/abs/2602.01041)
*Akinosuke Tsutsumi,Tomoya Itsuka,Yuichiro Kasahara,Tomoya Kouno,Kota Akinari,Genki Yamauchi,Daisuke Endo,Taro Abe,Takeshi Hashimoto,Keiji Nagatani,Ryo Kurazume*

Main category: cs.RO

TL;DR: This paper presents an LLM-based workflow for Behavior Tree (BT) generation to automate earthwork operations, improving safety and coordination among heterogeneous construction machines.


<details>
  <summary>Details</summary>
Motivation: Automation is necessary in construction due to workforce aging, skill loss, and increasing operational demand. Manual Behavior Trees limit scalability, particularly in complex and cooperative machine tasks.

Method: The proposed workflow utilizes large language models (LLMs) for high-level planning with synchronization flags and structured templates for Behavior Tree (BT) generation, ensuring safety via system database parameters.

Result: The approach was validated in both simulations and real-world experiments, demonstrating its efficacy in enabling synchronized and automated construction machinery operations.

Conclusion: The developed LLM-based BT generation framework shows promise for improving automation and scalability in civil engineering, especially in coordinating heterogeneous machine operations.

Abstract: Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.

</details>


### [1105] [A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation](https://arxiv.org/abs/2602.01067)
*Fanqi Lin,Kushal Arora,Jean Mercat,Haruki Nishimura,Paarth Shah,Chen Xu,Mengchao Zhang,Mark Zolotas,Maya Angeles,Owen Pfannenstiehl,Andrew Beaulieu,Jose Barreiros*

Main category: cs.RO

TL;DR: The paper examines how co-training with different data modalities affects dexterous robot manipulation policies and concludes that certain modalities enhance generalization while others do not offer significant benefits.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in dexterous robot policy generalization caused by insufficient robot data coverage and explore how co-training with diverse data can improve performance.

Method: This research studies five data modalities—vision-language data, robot trajectory annotations, cross-embodiment robot data, human videos, and discrete action tokens—across single- and multi-phase training, leveraging 4,000 hours of robot/human data and 50M vision-language samples. The study evaluates these on 58,000 simulation and 2,835 real-world rollouts.

Result: Co-training with vision-language and cross-embodiment data improves generalization, while discrete action tokens show no benefits. Combining effective modalities produces cumulative improvements for unseen tasks.

Conclusion: Effective co-training with certain modalities restores model capabilities, enhances generalization, and facilitates fine-tuning for long-horizon tasks, offering guidance for scalable robot policy development.

Abstract: Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.

</details>


### [1106] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: The paper proposes a method to detect and estimate forces on deformable linear objects using observed shapes from a depth camera, without external sensors.


<details>
  <summary>Details</summary>
Motivation: To improve robot-wire interaction tasks where contacts happen along the robot body, enabling safer and efficient trajectory planning while avoiding wire damage and risks.

Method: The method uses wire shape data with the assumption of static equilibrium. By enforcing consistency conditions and solving a linear force-torque balance system, it estimates force location and magnitude.

Result: Validated through simulations with high accuracy and real-world experiments, achieving reliable force estimation in relevant scenarios.

Conclusion: The proposed technique effectively estimates forces on wires without external sensors, aiding robot interaction tasks and avoiding hardware complexities.

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [1107] [Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance](https://arxiv.org/abs/2602.01092)
*Peng Zhou,Zhongxuan Li,Jinsong Wu,Jiaming Qi,Jun Hu,David Navarro-Alarcon,Jia Pan,Lihua Xie,Shiyao Zhang,Zeqing Zhang*

Main category: cs.RO

TL;DR: This paper introduces a failure-aware framework for teleoperation of high-precision bimanual tasks, offering compliant haptic guidance while maintaining human authority.


<details>
  <summary>Details</summary>
Motivation: Human operators face challenges in teleoperation due to limited ability to anticipate failures under tight tolerances and complex dynamics.

Method: The proposed framework uses Conservative Value Learning trained from offline teleoperation data, integrating risk-sensitive estimates and corrective motion via joint-space impedance interfaces.

Result: Experiments show improved success rates and reduced operator workload compared to conventional techniques.

Conclusion: Conservative Value Learning effectively embeds failure awareness and improves teleoperation performance during precise, contact-rich tasks.

Abstract: Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE

</details>


### [1108] [StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating](https://arxiv.org/abs/2602.01100)
*Hang Wu,Tongqing Chen,Jiasen Wang,Xiaotao Li,Lu Fang*

Main category: cs.RO

TL;DR: StreamVLA proposes a dual-system architecture for robotic manipulation, balancing high-level and low-level processes to reduce latency and improve goal stability.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action models struggle with high latency and instability due to excessive multimodal reasoning at every timestep.

Method: StreamVLA integrates task decomposition, visual end-state imagination, and action generation using a "Lock-and-Gated" mechanism to selectively trigger slow thinking for sub-task transitions.

Result: StreamVLA achieves a 98.5% success rate on the LIBERO benchmark and reduces inference latency by 48% compared to existing methods.

Conclusion: StreamVLA successfully bridges high-level planning and low-level control in robotic manipulation, ensuring efficient execution and resilience in dynamic environments.

Abstract: Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a "Lock-and-Gated" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.

</details>


### [1109] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

TL;DR: The paper introduces KAN-We-Flow, a lightweight flow-matching visuomotor policy using RWKV and KAN for efficient 3D manipulation, significantly reducing computational costs while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To enhance inference efficiency and deployment viability of visuomotor policies on resource-constrained robots, addressing the limitations of diffusion-based methods and bulky architectures.

Method: Introducing KAN-We-Flow, utilizing RWKV for efficient task context mixing and GroupKAN for nonlinear feature calibration, alongside Action Consistency Regularization to stabilize training and refine policy accuracy.

Result: The approach reduces parameters by 86.8%, improves runtime efficiency, and achieves state-of-the-art success on benchmarks like Adroit, Meta-World, and DexArt.

Conclusion: KAN-We-Flow offers a computationally efficient, robust, and effective solution for visuomotor policy deployment, particularly on resource-limited robotic systems.

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [1110] [UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors](https://arxiv.org/abs/2602.01153)
*Zhuo Chen,Fei Ni,Kaiyao Luo,Zhiyuan Wu,Xuyang Zhang,Emmanouil Spyrakos-Papastavridis,Lorenzo Jamone,Nathan F. Lepora,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: UniForce develops a unified tactile representation across different sensors to enable scalable force-aware robotic manipulation, showing consistent improvements in force estimation and cross-sensor coordination.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of heterogeneous tactile sensors which hinders scalable learning of force-aware robotic manipulation policies, aiming to overcome the limitations due to sensor-specific requirements.

Method: UniForce learns a shared latent force space by jointly modeling inverse and forward dynamics while leveraging equilibrium constraints and image reconstruction to align diverse sensors without external force/torque sensors.

Result: UniForce consistently enhances force estimation across various tactile sensors and enables zero-shot transfer to manipulation tasks, as validated through experiments.

Conclusion: The framework supports scalable and generalized force-aware policy learning, proving effective for robotic tasks across heterogeneous tactile sensors and will release code and datasets to encourage further research.

Abstract: Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.

</details>


### [1111] [Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models](https://arxiv.org/abs/2602.01166)
*Shuanghao Bai,Jing Lyu,Wanqi Zhou,Zhe Li,Dakai Wang,Lei Xing,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Cheng Chi,Badong Chen,Shanghang Zhang*

Main category: cs.RO

TL;DR: The paper proposes LaRA-VLA, a Vision-Language-Action (VLA) framework leveraging latent continuous representations for efficient embodied action, avoiding explicit chain-of-thought (CoT) reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action (VLA) models suffer from high inference times and rely on discrete reasoning representations that do not align well with continuous perception and control.

Method: The authors introduce LaRA-VLA, a framework that internalizes multi-modal reasoning into continuous latent representations. It uses a curriculum-based training method that transitions from explicit CoT supervision to latent reasoning, optimizing for action generation while eliminating explicit CoT reasoning during inference.

Result: LaRA-VLA achieves superior performance over state-of-the-art VLA methods, reducing inference latency by up to 90%. It performs well on both simulation benchmarks and real-world robotic manipulation tasks.

Conclusion: Latent reasoning via LaRA-VLA provides an efficient, effective paradigm for real-time embodied control, demonstrating that continuous latent reasoning outperforms explicit CoT reasoning in terms of performance and efficiency.

Abstract: Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.

</details>


### [1112] [SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment](https://arxiv.org/abs/2602.01189)
*Astik Srivastava,Thomas J Chackenkulam. Bitla Bhanu Teja,Antony Thomas,Madhava Krishna*

Main category: cs.RO

TL;DR: The paper presents a mapless approach to reactive motion planning for quadrotors, leveraging spatio-temporal planning and real-time vision-based perception, to navigate unknown and dynamic environments while avoiding collisions.


<details>
  <summary>Details</summary>
Motivation: Traditional motion planning methods for quadrotors rely on pre-fused maps, which are computationally intensive and may struggle in unknown, dynamic environments. This paper aims to improve collision-avoidance performance by developing a mapless framework.

Method: The authors propose a 4D spatio-temporal planner integrated with vision-based Safe Flight Corridor generation, trajectory optimization, and a perceptual pipeline for detecting and tracking dynamic obstacles. Additionally, a backup module for deadlock scenarios is incorporated.

Result: Extensive validation in simulation and real-world experiments demonstrates robust navigation and collision avoidance, outperforming state-of-the-art methods in dynamic, unknown environments.

Conclusion: The proposed mapless, vision-integrated framework offers significant performance benefits for reactive quadrotor motion planning in challenging, real-time scenarios.

Abstract: We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.

</details>


### [1113] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: SkySim is a framework enabling non-experts to control UAV swarms via natural language while ensuring safety with real-time adjustments.


<details>
  <summary>Details</summary>
Motivation: The need to simplify UAV swarm control for non-experts while maintaining safety and adaptability, overcoming the limitations of static and ungrounded LLM controls.

Method: A ROS2-based framework in Gazebo decoupling high-level LLM planning from low-level safety enforcement using Gemini 3.5 Pro for natural language input and an Artificial Potential Field (APF) for safety filtering.

Result: Validated on swarms of 3, 10, and 30 Crazyflie drones, demonstrating 100% spatial reasoning accuracy for tested geometric primitives and real-time collision avoidance.

Conclusion: SkySim bridges natural language processing and robotic safety, making UAV swarm management accessible to non-experts, with future plans for hardware integration.

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [1114] [Reinforcement Learning for Active Perception in Autonomous Navigation](https://arxiv.org/abs/2602.01266)
*Grzegorz Malczyk,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper introduces a reinforcement learning framework enabling active camera control and safe navigation for robots in unknown environments.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous robots' ability to navigate safely and effectively in complex environments by integrating active camera control with navigation.

Method: The authors propose reinforcement learning where robots receive state, depth, and local geometry inputs and optimize a reward combining navigation and voxel-based information metrics.

Result: The proposed framework enables safer navigation and induces exploratory behaviors compared to fixed, non-actuated camera setups.

Conclusion: Active perception through coupled camera control and navigation enhances situational awareness and safety for autonomous robots.

Abstract: This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.

</details>


### [1115] [TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design](https://arxiv.org/abs/2602.01385)
*Xiangyu Li,Mingwei Lai,Mengke Zhang,Junxiao Lin,Tiancheng Lai,Junping Zhi,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: This paper presents a novel triphibious robot capable of motion in aerial, terrestrial, and aquatic domains, using a minimalist quadcopter design with passive wheels and without extra actuators.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of existing dual-mode platforms and improve propulsion efficiency and adaptability in multi-domain environments.

Method: The paper introduces an eccentric Center of Gravity design for better efficiency, a unified Field-Oriented Control propulsion system for different fluids, and a Hybrid Nonlinear Model Predictive Control for stable and seamless operations.

Result: Experimental results demonstrate the robot's capability for multi-domain motion, cross-domain transitions, and the efficiency of the proposed design and control systems.

Conclusion: The novel design and control system enable efficient and adaptable triphibious motion with potential for handling complex tasks in diverse environments.

Abstract: Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.

</details>


### [1116] [Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation](https://arxiv.org/abs/2602.01389)
*Michele Antonazzi,Lorenzo Signorelli,Matteo Luperto,Nicola Basilico*

Main category: cs.RO

TL;DR: The paper addresses performance degradation in semantic segmentation networks caused by domain shifts by proposing a method to improve unsupervised domain adaptation (UDA) for robots using a volumetric 3D map and foundation model-based refinement.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to improve the performance of semantic segmentation networks in robotic perception, which often declines when deployed in environments with visual distributions differing from their training datasets.

Method: The proposed method combines volumetric 3D maps to generate multi-view consistent pseudo-labels, which are then refined using zero-shot instance segmentation capabilities of a foundation model to enforce instance-level coherence. These refined labels are used for self-supervised fine-tuning of the robot's perception model.

Result: Experiments showed that the proposed approach outperforms state-of-the-art UDA baselines in improving unsupervised domain adaptation performance without requiring ground-truth labels in the target domain.

Conclusion: The study demonstrates the effectiveness of the proposed method in enabling robotic vision systems to adapt to new deployment environments in a self-supervised fashion, addressing domain shifts and improving perception performance.

Abstract: Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.

</details>


### [1117] [Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors](https://arxiv.org/abs/2602.01429)
*Gonzalo Olguin,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: This paper introduces a mapless global navigation system for outdoor environments, combining trajectory generation via conditional variational autoencoders (CVAEs) and selection based on segmentation from a lightweight visual language model (VLM).


<details>
  <summary>Details</summary>
Motivation: The goal is to improve outdoor navigation by generating diverse trajectories and enabling real-time selection and execution, addressing the limitations of traditional map-based planning.

Method: The approach integrates CVAEs for generating trajectories, lightweight VLM for segmentation and selection using natural language, and a local planner for executing velocity commands.

Result: The system demonstrated superior performance in real-world outdoor experiments compared to existing methods.

Conclusion: This method offers an effective solution for real-time, mapless outdoor navigation, leveraging trajectory variability and advanced selection techniques validated through experimentation.

Abstract: This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.

</details>


### [1118] [Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression](https://arxiv.org/abs/2602.01448)
*Harshith Jella,Pejman Kheradmand,Joseph Klein,Behnam Moradkhani,Yash Chitalia*

Main category: cs.RO

TL;DR: The paper presents a robotic system with a shape-adjustable mechanism to manage severe bleeding in emergency scenarios, tested on simulated casualty kits.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively managing severe bleeding in diverse environments, including space stations and complex anatomical regions.

Method: Development of a novel robotic system with shape-adjustable ring mechanisms, inflatable airbags, and extensive experiments to evaluate flexibility, pressure application, and bleeding control.

Result: The system showed adaptability to various anatomical regions and successfully controlled simulated bleeding, with limitations in complex anatomical coverage identified.

Conclusion: The robotic system demonstrates promising potential in managing bleeding effectively, though improvements are needed for better anatomical adaptability.

Abstract: This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable "ring mechanism", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.

</details>


### [1119] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

TL;DR: TreeLoc is a LiDAR-based localization framework for forests addressing GPS degradation and complex LiDAR measurements through unique tree-based representations and efficient pose estimation.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of GPS degradation and the shortcomings of urban-centric localization methods in forest environments, which require forest-specific solutions for reliable navigation.

Method: TreeLoc represents forest scenes using tree stems, aligned according to Diameter at Breast Height (DBH), employs tree distribution histograms for coarse matching, and fine-matches using a 2D triangle descriptor, followed by geometric verification for pose estimation.

Result: On forest benchmarks, TreeLoc achieves superior localization accuracy compared to traditional methods. Ablation studies confirm the significance of individual framework components.

Conclusion: TreeLoc demonstrates its effectiveness for forest-based localization and presents applications for forest management, encouraging further iteration within the robotics community with open-sourced resources.

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [1120] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: The paper introduces RAPT, a self-supervised tool for high-rate humanoid control. It detects out-of-distribution conditions during robot deployment and explains failures with actionable insights to prevent hardware damage.


<details>
  <summary>Details</summary>
Motivation: Deploying control policies in real-world humanoid robots post-Sim-to-Real transfer can lead to silent failures due to out-of-distribution states. Current detection methods fail to meet the requirements of high-rate operations, strict low false-positive rates, and interpretable diagnostics, necessitating improvements.

Method: The authors developed RAPT, which learns a probabilistic spatio-temporal manifold of nominal simulated robot behavior. Using this, execution-time deviations are evaluated to detect OOD states with calibrated precision. Additionally, temporal saliency and a post-hoc semantic diagnostic using LLMs identify and explain failures.

Result: RAPT improved detection performance significantly, surpassing baselines by 37% TPR in simulations and 12.5% TPR on physical robots at strict false-positive rates. Its root-cause analysis reached 75% accuracy using proprioceptive data in real-world testing.

Conclusion: RAPT represents a foundational improvement for safe and interpretable humanoid robot deployment, addressing silent failures effectively and providing failure diagnostics in dynamic operating conditions.

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [1121] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: This paper proposes a Bayesian optimization framework to co-design wheel geometry and steering controller parameters for rovers using high-fidelity simulation on deformable terrain. It focuses on improving traversal speed, tracking accuracy, and energy efficiency while avoiding expensive DEM methods.


<details>
  <summary>Details</summary>
Motivation: Developing effective and efficient optimization of robotic systems operating on deformable terrain, particularly addressing the limitations of traditional DEM approaches.

Method: Uses Bayesian optimization in combination with continuum-representation model (CRM) for terramechanics to optimize wheel geometry and steering controller parameters through full-vehicle closed-loop simulations.

Result: Campaigns achieve co-optimization of wheel and controller parameters across 3,000 simulations in significantly reduced durations (5-9 days) compared to DEM-based workflows.

Conclusion: High-fidelity, scalable simulations can effectively enable co-optimization for off-road vehicle design on deformable terrain at reduced costs and practical scales. Open-source infrastructure is provided for future research.

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [1122] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

TL;DR: UniDWM is a unified driving world model designed to enable efficient and reliable autonomous driving by leveraging multifaceted representation learning for perception, prediction, and planning.


<details>
  <summary>Details</summary>
Motivation: The need for a reliable and efficient planning system in complex driving scenarios that can reason over scene geometry, dynamics, and appearance motivated this work.

Method: UniDWM uses a structure- and dynamic-aware latent world representation with a joint reconstruction pathway for recovering scene attributes and a conditional diffusion transformer to predict future evolution.

Result: UniDWM excelled in tasks like trajectory planning, 4D reconstruction, and scene generation, showcasing its effectiveness in improving autonomous driving capabilities.

Conclusion: UniDWM provides a robust, theoretically grounded framework for unified autonomous driving intelligence and sets a new benchmark in multifaceted world representation.

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [1123] [A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation](https://arxiv.org/abs/2602.01632)
*Chuizheng Kong,Yunho Cho,Wonsuhk Jung,Idris Wibowo,Parth Shinde,Sundhar Vinodh-Sangeetha,Long Kiu Chung,Zhenyang Chen,Andrew Mattei,Advaith Nidumukkala,Alexander Elias,Danfei Xu,Taylor Higgins,Shreyas Kousik*

Main category: cs.RO

TL;DR: This paper proposes SEW-Mimic, a novel orientation alignment approach that optimizes retargeting human motion to robot poses, achieving fast computation, improved accuracy, and broader applicability in robotics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing human-to-robot motion retargeting methods, which suffer from undesirable motion and latency due to workspace constraints and suboptimal optimization approaches.

Method: The paper introduces SEW-Mimic, which aligns robot arm movements to human arm orientations based on shoulder, elbow, and wrist keypoints. This geometric solution enables fast inference and efficient retargeting for humanoid robots.

Result: SEW-Mimic achieves superior computational efficiency at 3 kHz, better retargeting accuracy, smoother data for policy learning, and improvements in teleoperation task success.

Conclusion: SEW-Mimic is validated as an effective, practical tool for bimanual humanoid robots and teleoperation tasks, presenting potential for broader usage in robotics applications.

Abstract: Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.

</details>


### [1124] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: $multipanda_ros2$ is a novel open-source ROS2 framework for real-time multi-robot control with Franka Robotics. It focuses on high-frequency control, accurate modeling, and simulation-to-reality consistency.


<details>
  <summary>Details</summary>
Motivation: Facilitate advanced multi-robot control and interaction for robust robotics research by addressing torque control, real-time frequency, and simulation-to-reality challenges.

Method: Developed $multipanda_ros2$ leveraging ROS2 for real-time control (1kHz frequency), introduced a novel controller-switching design, and integrated high-fidelity MuJoCo simulation with inertial parameter identification techniques.

Result: Achieved low controller-switching delays (≤ 2 ms), improved sim2real accuracy with iterative physics refinement, and applied techniques to complex dual-arm, contact-rich tasks.

Conclusion: $multipanda_ros2$ presents a scalable, accurate, and robust platform for multi-robot systems, reducing the sim2real gap and enabling advanced robotics research.

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [1125] [AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act](https://arxiv.org/abs/2602.01662)
*Pengyuan Guo,Zhonghao Mai,Zhengtong Xu,Kaidi Zhang,Heng Zhang,Zichen Miao,Arash Ajoudani,Zachary Kingston,Qiang Qiu,Yu She*

Main category: cs.RO

TL;DR: AgenticLab introduces a platform for benchmarking vision-language models' robot manipulation in real-world settings, revealing their limitations in tasks requiring consistent multi-step grounding and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the unclear manipulation capability of large vision-language models in real-robot tasks under unstructured environments.

Method: AgenticLab provides a model-agnostic robot platform for perception, task decomposition, online verification, and replanning, benchmarking VLM-based agents on real-robot tasks.

Result: The benchmark highlights failure modes in VLM-based manipulation, such as issues with multi-step grounding, object grounding under occlusion, scene changes, and spatial reasoning limitations.

Conclusion: AgenticLab fosters reproducible evaluation and supports research in developing general-purpose robot agents, bridging gaps in VLM-based capabilities for real-world manipulation tasks.

Abstract: Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

</details>


### [1126] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: This study introduces a robotic system to automate the assembly stage of surgical instrument packing in sterile trays, significantly improving efficiency and reducing errors compared to manual processes.


<details>
  <summary>Details</summary>
Motivation: Manual assembly of surgical trays is time-consuming, error-prone, and susceptible to contamination and breakage, necessitating automation in Sterile Processing and Distribution (SPD).

Method: The robotic system involves a hybrid perception pipeline using YOLO12 and cascaded ResNet models, a calibrated vision module, robotic arm with custom gripper, and a packing algorithm utilizing 3D printed dividers to prevent collisions.

Result: The system demonstrates high perception accuracy and significantly reduces instrument collisions compared to human-assembled trays.

Conclusion: This automated system improves the safety, consistency, and efficiency of surgical instrument preparation, and serves as a scalable foundation for SPD workflow automation.

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [1127] [GSR: Learning Structured Reasoning for Embodied Manipulation](https://arxiv.org/abs/2602.01693)
*Kewei Hu,Michael Zhang,Wei Ying,Tianhao Liu,Guoqiang Hao,Zimeng Li,Wanchan Yu,Jiajian Jing,Fangwen Chen,Hanwen Kang*

Main category: cs.RO

TL;DR: The paper proposes Grounded Scene-graph Reasoning (GSR), a method for improving long-horizon manipulation tasks using explicit scene graph-based reasoning and introduces a related dataset and benchmarks, achieving improved generalization and task completion.


<details>
  <summary>Details</summary>
Motivation: Embodied agents face challenges with long-horizon tasks due to difficulty in maintaining spatial, causal, and goal constraints within high-dimensional latent task structures.

Method: The authors introduce GSR, which uses semantically grounded scene graphs to represent world states and transitions, paired with stepwise reasoning of object states and spatial relations. They also provide a new dataset, Manip-Cognition-1.6M, to train these reasoning capabilities.

Result: GSR demonstrated significant improvements in zero-shot generalization and task completion across multiple benchmarks and real-world robotic applications, outperforming prompting-based approaches.

Conclusion: Explicit world-state representations via scene graph reasoning are essential for scalable and effective embodied reasoning systems, especially in long-horizon manipulation tasks.

Abstract: Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.

</details>


### [1128] [Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels](https://arxiv.org/abs/2602.01700)
*Ruoyu Wang,Xuchen Liu,Zongzhou Wu,Zixuan Guo,Wendi Ding,Ben M. Chen*

Main category: cs.RO

TL;DR: The paper introduces Tilt-Ropter, a hybrid aerial-terrestrial vehicle combining tilt rotors and passive wheels to optimize energy-efficient locomotion across air and ground modes.


<details>
  <summary>Details</summary>
Motivation: To develop an energy-efficient and multi-mode vehicle capable of operating seamlessly in both airborne and terrestrial environments.

Method: Tilt-Ropter combines a fully actuated design with a nonlinear model predictive controller (NMPC) for superior mobility. A control allocation module optimizes energy efficiency, and an external wrench estimation algorithm manages real-time interaction forces and torques.

Result: Tilt-Ropter achieved low tracking errors in air and ground modes. Ground locomotion showed a 92.8% reduction in power consumption, validated through simulations and experiments.

Conclusion: Tilt-Ropter significantly enhances energy efficiency, adaptability, and robustness, showing potential for prolonged missions in large, energy-constrained environments.

Abstract: In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.

</details>


### [1129] [Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion](https://arxiv.org/abs/2602.01731)
*Jiwoo Hwang,Taegeun Yang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: This paper presents CURA-PPO, a reinforcement learning framework addressing challenges of occluded sensor views in non-prehensile manipulation by modeling uncertainty and enabling active perception.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of sensor occlusion caused by manipulated objects during robotic non-prehensile manipulation, which risks collisions under limited field of view.

Method: CURA-PPO predicts collision possibility as a distribution to extract risk and uncertainty, encourages active perception, and uses confidence maps to ensure safe navigation under sensor occlusion.

Result: Experiments showed CURA-PPO achieved up to 3X higher success rates compared to baselines across various object sizes and obstacle setups.

Conclusion: CURA-PPO offers an effective and robust solution for autonomous robotic manipulation in cluttered environments using onboard sensing.

Abstract: Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.

</details>


### [1130] [RFS: Reinforcement learning with Residual flow steering for dexterous manipulation](https://arxiv.org/abs/2602.01789)
*Entong Su,Tyler Westenbroek,Anusha Nagabandi,Abhishek Gupta*

Main category: cs.RO

TL;DR: The paper presents Residual Flow Steering (RFS), a reinforcement learning framework to efficiently adapt pretrained generative policies for robotic tasks. RFS combines residual action refinement and latent space modulation to improve adaptability.


<details>
  <summary>Details</summary>
Motivation: Pretrained policies in robotics often struggle with generalization and require fine-tuning for robust deployment. The need is to combine global exploration from pretraining with methods for rapid correction of local execution errors.

Method: RFS optimizes a combination of residual actions and latent noise distribution to refine policies. It leverages local adjustments and global exploration capabilities of pretrained generative policies (like flow-matching).

Result: RFS achieves efficient fine-tuning in dexterous manipulation tasks, showing strong results both in simulations and real-world experiments when adapting pretrained policies.

Conclusion: RFS provides a data-efficient and effective framework for adapting pretrained robotic policies, enhancing their operational performance during deployment.

Abstract: Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.

</details>


### [1131] [From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models](https://arxiv.org/abs/2602.01811)
*Wentao Zhang,Aolan Sun,Wentao Mo,Xiaoyang Qu,Yuxin Zheng,Jianzong Wang*

Main category: cs.RO

TL;DR: The paper introduces VLA-SCT, a training-free, self-correcting framework for vision-language-action models addressing grasping errors and task completion failures.


<details>
  <summary>Details</summary>
Motivation: To address the issues of spatial deviations in grasping actions and unreliable task completion recognition, limiting the performance of VLA models.

Method: Proposes VLA-SCT framework using a self-correcting control loop combining data-driven action refinement and conditional termination logic.

Result: Achieves improved success rates and task completeness in manipulation tasks compared to baseline methods in the LIBERO benchmark.

Conclusion: VLA-SCT enhances the robustness and reliability of VLA agents in complex, unstructured environments, addressing key limitations of current systems.

Abstract: While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.

</details>


### [1132] [Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models](https://arxiv.org/abs/2602.01834)
*Siqi Wen,Shu Yang,Shaopeng Fu,Jingfeng Zhang,Lijie Hu,Di Wang*

Main category: cs.RO

TL;DR: The paper presents a concept-based dictionary learning framework to enhance the safety of Vision Language Action (VLA) models by suppressing unsafe behaviors. It cuts attack success rates by over 70% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address safety risks in VLA models, which can cause harmful physical actions in embodied systems if exploited, unlike textual jailbreaks in LLMs.

Method: The method constructs interpretable, sparse dictionaries from hidden activations in VLA models, identifies harmful concept directions, and applies thresholds to suppress unsafe activations during inference.

Result: The framework achieves state-of-the-art defense performance in experiments across multiple benchmarks, reducing attack success rates by over 70% without impacting task success.

Conclusion: This plug-in, model-agnostic framework improves safety and interpretability in deploying embodied VLA systems, offering the first inference-time concept-based defense mechanism in this domain.

Abstract: Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.

</details>


### [1133] [Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach](https://arxiv.org/abs/2602.01860)
*Filip Novák,Matěj Petrlík,Matej Novosad,Parakh M. Gupta,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: This paper proposes a new method for precise state estimation in high-speed UAVs using a monocular camera and IMU, focusing on compensating VIO drift for better performance in cluttered and GNSS-denied environments.


<details>
  <summary>Details</summary>
Motivation: High-speed UAVs in GNSS-denied and cluttered environments face challenges in state estimation due to hardware complexity and the unreliability of VIO alone. The motivation is to enable aggressive UAV maneuvers accurately and efficiently using simpler hardware.

Method: The paper introduces a fusion of monocular RGB camera and IMU data with Visual-Inertial Odometry (VIO) and a landmark-based measurement system. A novel mathematical drift correction model is applied to improve the accuracy of all VIO states.

Result: The proposed method was validated by 1600 simulations and numerous real-world experiments. It performed effectively, achieving accurate state estimation in both simulated and real-world conditions. The method was also implemented successfully in a competitive drone racing challenge.

Conclusion: The approach provides reliable, accurate UAV state estimation during fast maneuvers, outperforming existing methods with its simplicity and innovative drift correction model. It demonstrates practical applicability through competition success.

Abstract: Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.

</details>


### [1134] [BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models](https://arxiv.org/abs/2602.01870)
*Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.RO

TL;DR: The paper introduces BTGenBot-2, an open-source, 1B-parameter lightweight small language model that converts task descriptions into behavior trees for robots with enhanced efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of accessible, efficient, and universally applicable methods for integrating LLM-based task planning in robotics. Existing models are either closed-source, computationally demanding, or not optimized for real-world robotics deployment.

Method: The authors developed BTGenBot-2, which directly generates behavior trees in XML from natural language task descriptions and robot action primitives. It supports zero-shot generation and error recovery, emphasizing lightweight operability.

Result: BTGenBot-2 showed superior performance compared to GPT-5, Claude Opus 4.1, and similarly sized models. It achieved 90.38% success in zero-shot and 98.07% in one-shot tasks, with up to 16x faster inference than its predecessor, BTGenBot, in benchmark tests.

Conclusion: BTGenBot-2 provides an effective and accessible solution for robotic task planning by utilizing lightweight, open-source language modeling, outperforming larger and more resource-intensive models across key metrics.

Abstract: Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.

</details>


### [1135] [Multimodal Large Language Models for Real-Time Situated Reasoning](https://arxiv.org/abs/2602.01880)
*Giulio Antonio Abbo,Senne Lenaerts,Tony Belpaeme*

Main category: cs.RO

TL;DR: This study integrates a multimodal large language model with a TurtleBot 4 to create a smart home cleaning robot capable of context-aware and value-driven decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the potential of multimodal large language models to enhance robotic decision-making by incorporating reasoning about domestic activities, social norms, and user preferences.

Method: A GPT-4o language model was combined with a TurtleBot 4, simulating a smart vacuum cleaning robot in a home environment, using vision input for evaluating the cleaning context.

Result: The robot demonstrated context and value-based reasoning in a home environment but also revealed challenges related to consistency, bias, and real-time performance.

Conclusion: Multimodal large language models hold promise for improving robotic autonomy and situational awareness, though challenges like bias and consistency need to be addressed.

Abstract: In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.

</details>


### [1136] [Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study](https://arxiv.org/abs/2602.01892)
*Alexandre Lombard,Florent Perronnet,Nicolas Gaud,Abdeljalil Abbas-Turki*

Main category: cs.RO

TL;DR: This study introduces an adaptable path-tracking framework for autonomous vehicles, integrating dynamic lateral control and curvature-aware longitudinal control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve tracking stability and adaptability in autonomous vehicles during varied driving contexts, including speed changes and reverse motion.

Method: The approach introduces a dynamic control mechanism along the wheelbase, using barycentric blended lateral steering controls and a curvature-aware speed regulation strategy.

Result: Through simulation and real-vehicle tests, the solution demonstrated enhanced trajectory accuracy, smoother steering, and better adaptability compared to conventional methods.

Conclusion: The proposed framework effectively improves tracking stability and usability in autonomous vehicles, offering a unified solution adaptable to diverse driving conditions.

Abstract: This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.

</details>


### [1137] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

TL;DR: The paper investigates solutions for multi-task problem solving under data imbalance and limited ground truth labels, focusing on robots' resource limitations.


<details>
  <summary>Details</summary>
Motivation: Robots face challenges due to imbalanced data and the difficulty of labeling in various environments. The study aims to address these issues to enhance task learning with limited labels.

Method: The proposed method learns tasks even without ground truth labels for some tasks. It analyzes which tasks enhance performance of others by training a teacher network using task outputs (e.g., depth).

Result: The method is evaluated using semantic segmentation and depth estimation tasks on datasets such as NYUDv2 and Cityscapes, especially with limited data.

Conclusion: The research highlights interactions among tasks and demonstrates how task performance can be improved in scenarios with imbalanced and limited labeling resources.

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [1138] [ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning](https://arxiv.org/abs/2602.01916)
*Keyu Chen,Wenchao Sun,Hao Cheng,Zheng Fu,Sifa Zheng*

Main category: cs.RO

TL;DR: The paper introduces ForSim, a closed-loop simulation system addressing existing issues in traffic simulation for autonomous driving, enhancing fidelity and reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current traffic simulation systems in autonomous driving, like covariate shift and lack of realistic agent interactions.

Method: ForSim employs a stepwise closed-loop simulation where agents simulate trajectories based on physically grounded motion dynamics and stepwise predictions, enhancing multimodal and interaction-aware simulations.

Result: ForSim, when integrated with RIFT, enhances safety, efficiency, realism, and comfort in traffic simulations, validated through extensive experiments.

Conclusion: Modeling closed-loop and multimodal agent interactions significantly improves traffic simulation fidelity, aiding autonomous driving research.

Abstract: As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/

</details>


### [1139] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

TL;DR: The paper focuses on using Vision-Language Foundation Models (VLFMs) combined with 3D Semantic Scene Graphs to enable robots to explore and reason in unknown environments beyond predefined labels.


<details>
  <summary>Details</summary>
Motivation: Traditional mapping methods limit robots to pre-designed object classes, restricting their adaptability to new or out-of-distribution knowledge. This paper aims to overcome this by integrating VLFMs for more flexible and open-set mapping.

Method: The paper introduces LIEREx, a system that combines Vision-Language Foundation Models with 3D Semantic Scene Graphs to provide actionable representations for robots and enable target-directed exploration in partially unknown environments.

Result: The proposed integration allows robots to perform target-based exploration without being restricted to a fixed symbolic vocabulary.

Conclusion: By leveraging VLFMs and 3D Semantic Scene Graphs, the approach enhances a robot's ability to explore and adapt in real-world scenarios involving unknown or out-of-distribution object categories.

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [1140] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: This paper introduces the Exploratory and Focused Manipulation (EFM) problem and proposes the EFM-10 benchmark, alongside a Bimanual Active Perception (BAP) strategy, to address visual occlusion in manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the frequent visual occlusion experienced in robotic manipulation due to main cameras being mounted on robot heads, necessitating active vision for effective completion of tasks.

Method: The authors propose the EFM-10 benchmark consisting of diverse tasks related to exploratory and focused manipulation, and a Bimanual Active Perception (BAP) strategy using one robotic arm for active vision and another for force sensing.

Result: Using the custom BAPData dataset for EFM-10 tasks, the effectiveness of the BAP strategy was demonstrated in an imitation learning setup.

Conclusion: The EFM-10 benchmark and BAP strategy are expected to advance research in active vision and manipulation, addressing challenges in visually-occluded tasks more effectively.

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [1141] [A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications](https://arxiv.org/abs/2602.01948)
*Patrick Frank,Christian Friedrich*

Main category: cs.RO

TL;DR: The paper introduces a novel control architecture for macro-micro manipulators aiming to improve interaction control bandwidth.


<details>
  <summary>Details</summary>
Motivation: Traditional architectures for macro-micro manipulators limit interaction control bandwidth due to the division of roles between macro and micro levels. The study aims to address this limitation.

Method: The study proposes a new control architecture that actively incorporates the macro manipulator in interaction control. Additionally, surrogate models are introduced for efficient controller design.

Result: The new architecture increases control bandwidth by a factor of 2.1 compared to a leader-follower design, and 12.5 times compared to traditional robot-based force control. Validation experiments demonstrated improved performance across tasks like collision response, trajectory tracking, and industrial assembly.

Conclusion: The proposed approach significantly enhances the interaction control bandwidth and offers adaptable and efficient controller designs, making it more effective for practical applications.

Abstract: Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.

</details>


### [1142] [Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements](https://arxiv.org/abs/2602.02006)
*Thomas Jantos,Giulio Delama,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: The paper explores a method to improve state estimation for mobile robots by fusing AI-derived object-relative pose measurements in an EKF, while accounting for DNN uncertainty.


<details>
  <summary>Details</summary>
Motivation: To enhance mobile robots' capability to localize precisely with respect to objects in their environment by leveraging DNNs for real-time pose estimation.

Method: The authors reformulate the EKF measurement equation to decouple position and rotation measurements, utilize DNNs for deriving object-relative poses, and incorporate their predicted uncertainty into the estimation process.

Result: The proposed approach improved the performance and consistency of state estimators by mitigating the impact of erroneous rotation measurements and enhancing measurement rejection capabilities.

Conclusion: Using AI-derived measurements with uncertainty estimation in an EKF provides significant advantages for robust and accurate robot localization.

Abstract: Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.

</details>


### [1143] [Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp](https://arxiv.org/abs/2602.02026)
*Zhenwei Niu,Xiaoyi Chen,Jiayu Hu,Zhaoyang Liu,Xiaozu Ju*

Main category: cs.RO

TL;DR: A framework for gentle robotic grasping integrates friction estimation and adaptive grasp control, ensuring stability and sensitivity.


<details>
  <summary>Details</summary>
Motivation: Achieving stable and adaptive robotic grasping requires real-time feedback mechanisms to adjust to varying friction conditions.

Method: A particle filter-based method estimates friction coefficients in real-time using tactile sensors, integrating with a controller to dynamically tune grasp force in a closed-loop process.

Result: Experiments confirm the framework's robust and efficient sensorimotor cycle for real-time grasp adjustment.

Conclusion: The proposed solution effectively enhances robotic grasping, emphasizing its adaptability and responsiveness to tactile input.

Abstract: We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.

</details>


### [1144] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: The paper introduces a framework integrating information bottleneck theory with vector quantization for bandwidth-efficient multi-agent communication.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-agent reinforcement learning systems face communication constraints that reduce coordination effectiveness in applications like robotics.

Method: The method uses information bottleneck theory and vector quantization for compressing and discretizing communication messages while dynamically determining communication needs based on environmental and agent states.

Result: The proposed method achieves 181.8% performance improvement over no-communication baselines and reduces bandwidth usage by 41.4%.

Conclusion: This framework outperforms existing strategies and offers a robust approach for multi-agent systems operating in bandwidth-constrained settings such as robotics and autonomous networks.

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [1145] [Frictional Contact Solving for Material Point Method](https://arxiv.org/abs/2602.02038)
*Etienne Ménager,Justin Carpentier*

Main category: cs.RO

TL;DR: This paper addresses challenges in Material Point Method (MPM) for handling contact with friction and introduces a new precise and robust frictional-contact pipeline using particle-centric geometric primitives and solving contact as a Nonlinear Complementarity Problem with an ADMM scheme.


<details>
  <summary>Details</summary>
Motivation: To overcome drawbacks in MPM's handling of frictional contact, including reliable contact point detection and enforcement of frictional laws, as these issues hinder its applicability in simulations involving contact mechanics.

Method: The proposed solution includes localizing contact points with particle-centric geometric primitives and solving frictional contact as a Nonlinear Complementarity Problem using an Alternating Direction Method of Multipliers (ADMM). This formulation integrates seamlessly into existing MPM frameworks, maintaining efficiency and numerical stability.

Result: The method was tested in seven diverse scenarios encompassing elastic and elasto-plastic behaviors and varying contact conditions, demonstrating accurate contact localization, reliable frictional handling, and generality.

Conclusion: The proposed approach improves MPM's capability to handle frictional contact accurately and robustly, making it a practical tool for simulations in applications such as robotics.

Abstract: Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.

</details>


### [1146] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

TL;DR: The paper introduces FD-VLA, a Vision-Language-Action framework integrating force awareness via a Force Distillation Module, enabling robust, cost-effective manipulation without physical force sensors.


<details>
  <summary>Details</summary>
Motivation: To enable cost-effective fine-grained perception and manipulation in contact-rich tasks without relying on fragile or expensive physical force sensors.

Method: A Force Distillation Module distills force using a learnable query token conditioned on visual observations and robot states, injecting this into pretrained Vision-Language Models to enable force-aware reasoning.

Result: The distilled force token improves perception-action robustness and outperforms direct sensor measurements and other baselines in physical experiments.

Conclusion: FD-VLA enhances cross-modal alignment and robustness while reducing hardware dependency, showcasing its effectiveness in real-world deployments.

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [1147] [Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls](https://arxiv.org/abs/2602.02181)
*Elad Siman Tov,Nili E. Krausz*

Main category: cs.RO

TL;DR: This paper introduces a methodology for studying intersegmental coordination (ISC) in amputees' gait with a focus on energy efficiency. Analysis revealed less coordination in amputee gaits with prostheses, suggesting areas for prosthetic improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the longstanding challenge of reducing metabolic cost of walking for amputees and to improve understanding of intersegmental coordination (ISC), which is significant for energy-efficient gait.

Method: The method involves developing a way to analyze ISC using 3D kinetic and kinematic data while also broadening ISC toward a new law involving Elevation Space Moments (ESM). It also includes creating the ISC3d toolbox for further studies.

Result: Results highlight that, though elevation angles are coordinated in amputee gaits, ISC diminishes with both powered and passive prostheses, suggesting less efficient movement.

Conclusion: ISC analysis reveals insights into movement inefficiencies in amputee gaits. The study proposes using constraints like ISC to enhance powered prosthetic designs for near-healthy movement profiles, with further implications for understanding neural control of gait.

Abstract: Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.

</details>


### [1148] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: The paper presents a framework for humanoid parkour on complex terrains, utilizing a real-to-sim-to-real framework with rapid test-time training.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge faced by humanoid robots in mastering dynamic parkour on unseen and highly complex terrains.

Method: It uses a two-stage learning approach: pre-training policies on procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world RGB-D captures.

Result: The proposed system enables robots to traverse complex obstacles and demonstrates robust sim-to-real transfer, with test-time training taking under 10 minutes on most terrains.

Conclusion: The framework significantly enhances the robot's adaptability and performance on challenging terrains using an efficient learning and geometry reconstruction pipeline.

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [1149] [Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures](https://arxiv.org/abs/2602.02389)
*Marina Ruediger,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: The paper discusses an algorithm for underwater robot inspections using SLAM data to dynamically generate and optimize task distributions, proven effective in tests.


<details>
  <summary>Details</summary>
Motivation: Existing inspection methods lack adaptability to unplanned underwater geometries. This paper aims to create dynamic, efficient task generation methods for robots.

Method: Tasks are generated from SLAM meshes, optimized using expected keypoint scores, and refined through distance-based pruning. Simulations and in-water tests validate the approach.

Result: The algorithm is shown to outperform prior methods in adaptability and coverage, focusing on defect-prone regions within a test environment.

Conclusion: The proposed method offers an adaptive solution for multi-robot underwater inspections, improving coverage while efficiently targeting areas prone to damage.

Abstract: Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.

</details>


### [1150] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: PRISM, a robotic imitation learning model using a novel IMLE-based approach, achieves superior performance in diverse real-world and simulated tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a robotic imitation learning policy that integrates multiple sensing modalities, captures multimodal action distributions, and operates at real-time control rates without iterative sampling latencies.

Method: The paper introduces PRISM, a batch-global rejection-sampling IMLE policy combining a temporal multisensory encoder and a linear-attention generator based on the Performer architecture.

Result: PRISM outperforms diffusion models by 10-25% in real-world tasks and improves success rates by 20-25% in simulation benchmarks. It reduces trajectory jerk significantly while achieving high-frequency control.

Conclusion: PRISM is an efficient and accurate multisensory imitation learning policy that advances robotic performance in complex tasks over existing generative approaches.

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [1151] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: The paper introduces SoMA, a simulator for soft-body manipulation that integrates deformable dynamics, environmental effects, and robot interactions using 3D Gaussian splats for better simulation accuracy and long-horizon manipulation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing simulators that struggle with accuracy, stability, and generalization for simulating deformable objects manipulated by robots under complex interactions.

Method: The method involves coupling deformable object dynamics, environmental forces, and robot actions in a unified latent neural space using Gaussian splats for end-to-end simulation.

Result: SoMA improves resimulation accuracy and generalization in robot manipulation tasks by 20%, enabling stable and precise simulations for complex tasks like cloth folding.

Conclusion: This paper demonstrates that SoMA is highly effective for simulating soft-body manipulation tasks, achieving enhanced accuracy and broad generalization without relying on predefined physical models.

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [1152] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: This paper proposes CAM-MCTS, an advanced multi-agent planning method for efficient object rearrangement in cluttered environments, focusing on makespan reduction in monotone and non-monotone tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of object rearrangement in cluttered environments, particularly in scenarios requiring temporary relocations and effective multi-agent collaboration.

Method: The paper introduces CAM-MCTS, combining centralized, asynchronous multi-agent Monte Carlo Tree Search for task assignment and execution, reducing idle time and enhancing system efficiency.

Result: CAM-MCTS achieves consistent makespan reductions across various competitive baselines in cluttered environments, validated through diverse simulations and real-world multi-agent settings.

Conclusion: CAM-MCTS effectively optimizes object rearrangement planning in complex settings, showcasing efficiency and robustness both in simulations and real-world applications.

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>


### [1153] [3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM](https://arxiv.org/abs/2602.02430)
*Pierre-Yves Lajoie,Benjamin Ramtoula,Daniele De Martini,Giovanni Beltrame*

Main category: cs.RO

TL;DR: The paper proposes a decentralized C-SLAM approach utilizing 3D foundation models for robust and scalable multi-robot mapping, addressing large viewpoint variations.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty of identifying map overlaps in decentralized C-SLAM due to significant viewpoint differences among robots.

Method: Integrating 3D foundation models into existing SLAM pipelines for relative pose estimation, outlier mitigation techniques, and specialized pose graph optimization to address scale ambiguities.

Result: The proposed method improves accuracy in localization and mapping while achieving computational and memory efficiency compared to state-of-the-art techniques.

Conclusion: The approach demonstrates potential for efficient deployment in large-scale multi-robot systems with both scalability and accuracy improvements.

Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.

</details>


### [1154] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: World-Gymnast introduces a novel approach using reinforcement learning within world models, outperforming traditional supervised finetuning and software simulations for robotics applications.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inefficiency of current methods for robot learning due to the high cost of physical-world interaction, limited expert data, and the sim-to-real gap for manipulation tasks.

Method: The method leverages a world-model approach, training vision-language-action policies using action-conditioned video models and rewarding them via vision-language models.

Result: The proposed approach, World-Gymnast, significantly outperforms supervised finetuning (up to 18x) and software simulators (up to 2x) in robotic performance.

Conclusion: Training robots through world models and cloud-based learning could eliminate key bottlenecks, bridging the gap for robots to function more adaptively and effectively in diverse real-world environments.

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [1155] [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](https://arxiv.org/abs/2602.02456)
*Albert Gassol Puigjaner,Angelos Zacharia,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper proposes an enhanced hierarchical 3D scene graph that incorporates open-vocabulary features and object-relational reasoning using vision and language models for intelligent task reasoning in 3D environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional SLAM methods, which lack higher abstraction and relational reasoning, and enable autonomous agents to navigate and reason intelligently about their environments.

Method: The method uses an enhanced 3D scene graph integrated with Vision Language Models (VLMs) to infer semantic relationships, and a task reasoning module using Large Language Models (LLM) combined with VLMs for intelligent environmental interaction.

Result: The proposed method was successfully validated on a quadruped robot across multiple environments and tasks, demonstrating improved reasoning and interaction capabilities.

Conclusion: Integrating hierarchical 3D scene graphs with VLMs and LLM-based reasoning enables better semantic understanding and relational reasoning, enhancing autonomous task execution in robotic systems.

Abstract: Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.

</details>


### [1156] [TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments](https://arxiv.org/abs/2602.02459)
*Zhiyu Huang,Yun Zhang,Johnson Liu,Rui Song,Chen Tang,Jiaqi Ma*

Main category: cs.RO

TL;DR: TIC-VLA, a latency-aware robot framework, integrates delayed semantic reasoning into real-time control for enhanced language-guided navigation.


<details>
  <summary>Details</summary>
Motivation: Develop a framework that accounts for semantic inference delays in real-time reactive control for robots operating in human-centric environments.

Method: Introduced TIC-VLA, which incorporates a delayed semantic-control interface and explicit latency metadata into action generation. Utilized a latency-consistent training pipeline for imitation and reinforcement learning, aligned with asynchronous deployments.

Result: TIC-VLA demonstrated improved performance over prior VLA models under multi-second reasoning latency through simulation and real robot experiments.

Conclusion: The TIC-VLA framework effectively compensates for reasoning delays, enabling robust and real-time control in dynamic environments.

Abstract: Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/

</details>


### [1157] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: The paper introduces HumanX, a framework for teaching humanoid robots interaction skills using human videos without task-specific rewards.


<details>
  <summary>Details</summary>
Motivation: Current robotic systems face challenges in agility and adaptability due to a lack of realistic interaction data and the need for task-specific reward engineering.

Method: HumanX consists of two components: XGen, a data generation pipeline creating robot interaction data from human videos, and XMimic, an imitation learning framework for teaching interaction skills.

Result: HumanX enables robots to acquire 10 different skills, transferring these zero-shot to a physical robot, with higher generalization success compared to prior methods.

Conclusion: HumanX provides a scalable, task-agnostic approach for teaching humanoids versatile, real-world skills effectively.

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [1158] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: This paper presents a new framework for training expressive robot control policies using flow matching policy gradients, overcoming limitations of likelihood-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the constraints of likelihood-based methods, which limit policy outputs to simple distributions, and improve robot control training in complex tasks.

Method: Developed an improved flow matching policy gradient framework with enhanced objectives for robot control, tested on locomotion, motion tracking, manipulation, and sim-to-real tasks.

Result: Policies trained with the new framework demonstrated success in complex tasks and robust sim-to-real transfer on humanoid robots, showing effective exploration and fine-tuning.

Conclusion: Flow matching policy gradients enable expressive policy training, enhancing performance in challenging robot control tasks and robustness in fine-tuning scenarios.

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1159] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: The paper introduces Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy for improving code generation performance, especially with multiple constraints.


<details>
  <summary>Details</summary>
Motivation: Although LLMs demonstrate strengths in code generation, they struggle with adhering to fine-grained user intent involving multiple constraints, prompting the need for an enhanced decoding approach.

Method: The authors propose IntentCoding, a model-agnostic decoding strategy that amplifies user intent by incorporating masking and a multi-strength ensemble mechanism. No extra training is required, and it integrates with existing decoding methods.

Result: The IntentCoding approach notably improves performance in constraint satisfaction and functional correctness. Benchmarks show relative improvements of up to 71.0% on CodeConstraints, 67.3% on IFEvalCode, and 29.3% on HumanEval and LiveCodeBench.

Conclusion: IntentCoding significantly enhances code generation by more effectively incorporating user intent, offering improvements over standard decoding techniques, and can be readily used with existing LLM setups.

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [1160] [Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study](https://arxiv.org/abs/2602.00164)
*Khairul Alam,Saikat Mondal,Banani Roy*

Main category: cs.SE

TL;DR: The paper studies the effectiveness of AI coding agents in generating pull requests (PRs) and identifies factors hindering successful integration.


<details>
  <summary>Details</summary>
Motivation: To investigate the practical effectiveness of AI coding agents in generating and merging fix-related PRs in software repositories and to understand obstacles to their integration.

Method: Examined 8,106 fix-related PRs from the AIDEV POP dataset and conducted qualitative analysis of 326 unmerged PRs to identify reasons for failure.

Result: Test case failures and prior resolution of issues by other PRs are the main causes of non-integration, suggesting areas for improvement in AI coding agents.

Conclusion: The paper highlights the limitations of current AI coding agents and suggests improvement directions for their coding capabilities and human-AI collaboration in software maintenance.

Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.

</details>


### [1161] [Towards Analyzing N-language Polyglot Programs](https://arxiv.org/abs/2602.00303)
*Jyoti Prakash,Abhishek Tiwari,Mikkel Baun Kjærgaard*

Main category: cs.SE

TL;DR: This paper explores the challenges of analyzing multilingual systems involving three or more programming languages and proposes a conceptual roadmap for advancing static analysis techniques.


<details>
  <summary>Details</summary>
Motivation: Modern systems increasingly use three or more programming languages like JavaScript, WebAssembly, and Rust together, yet existing research focuses on only two-language integrations.

Method: The paper identifies challenges in multilingual systems with three-language communication and proposes a conceptual roadmap for developing scalable, language-agnostic static analysis tools.

Result: It highlights the challenges of analyzing three-language systems and offers a vision to advance static analysis techniques for such systems.

Conclusion: The paper aims to inspire research toward scalable analysis frameworks capable of handling complex, multilingual programming systems involving multiple languages.

Abstract: Polyglot programming is gaining popularity as developers integrate multiple programming languages to harness their individual strengths. With the recent popularity of platforms like GraalVM and other multi-language runtimes, creating and managing these systems has become much more feasible. However, current research on analyzing multilingual programs mainly focuses on two languages, leaving out the increasing complexity of systems that use three or more. For example, modern web systems often link JavaScript, WebAssembly, and Rust within the same execution chain. This paper envisions the landscape of software systems with three-language polyglot communication. We identify fundamental challenges in analyzing them and propose a conceptual roadmap to advance static analysis techniques to address them. Our vision aims to stimulate discussion and inspire new research directions toward scalable, language-agnostic analysis frameworks for next-generation polyglot systems.

</details>


### [1162] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: This paper explores specification-driven development (SDD), its principles, workflows, and tools for improving software development processes.


<details>
  <summary>Details</summary>
Motivation: To investigate how specification-driven development (SDD) can transform software development by positioning specifications over code as the primary artifact.

Method: Introduces the three levels of specification rigor, analyzes various tools (e.g., Behavior-Driven Development frameworks, GitHub Spec Kit), and presents case studies to demonstrate practical applications.

Result: Case studies show how SDD, in different domains like API development and enterprise systems, can be effectively implemented with AI-assisted tools.

Conclusion: Specification-driven development can provide significant value in certain domains, though simpler approaches may suffice in other cases. A decision framework aids in determining its applicability.

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [1163] [Are Coding Agents Generating Over-Mocked Tests? An Empirical Study](https://arxiv.org/abs/2602.00409)
*Andre Hora,Romain Robbes*

Main category: cs.SE

TL;DR: The paper investigates the use of mocks in software tests generated by coding agents, revealing that these agents are more prone to modifying tests and adding mocks compared to non-agent contributions.


<details>
  <summary>Details</summary>
Motivation: To understand the quality and practices of software test generation by coding agents, specifically their use of mocking, and implications for developers and researchers.

Method: Analyzed 1.2 million commits across 2,168 repositories in TypeScript, JavaScript, and Python, focusing on commits by coding agents involving test changes and mock additions.

Result: Coding agents are more likely than non-agents to modify tests (23% vs. 13%) and add mocks to tests (36% vs. 26%). 68% of repositories with agent test activity include agent mock activity. Recent repositories have higher agent-related test and mock activity.

Conclusion: Coding agents heavily employ mocks, making tests potentially easier to generate but less effective at validating real interactions. Developers need guidance on proper mocking practices in agent configurations.

Abstract: Coding agents have received significant adoption in software development recently. Unlike traditional LLM-based code completion tools, coding agents work with autonomy (e.g., invoking external tools) and leave visible traces in software repositories, such as authoring commits. Among their tasks, coding agents may autonomously generate software tests; however, the quality of these tests remains uncertain. In particular, excessive use of mocking can make tests harder to understand and maintain. This paper presents the first study to investigate the presence of mocks in agent-generated tests of real-world software systems. We analyzed over 1.2 million commits made in 2025 in 2,168 TypeScript, JavaScript, and Python repositories, including 48,563 commits by coding agents, 169,361 commits that modify tests, and 44,900 commits that add mocks to tests. Overall, we find that coding agents are more likely to modify tests and to add mocks to tests than non-coding agents. We detect that (1) 60% of the repositories with agent activity also contain agent test activity; (2) 23% of commits made by coding agents add/change test files, compared with 13% by non-agents; (3) 68% of the repositories with agent test activity also contain agent mock activity; (4) 36% of commits made by coding agents add mocks to tests, compared with 26% by non-agents; and (5) repositories created recently contain a higher proportion of test and mock commits made by agents. Finally, we conclude by discussing implications for developers and researchers. We call attention to the fact that tests with mocks may be potentially easier to generate automatically (but less effective at validating real interactions), and the need to include guidance on mocking practices in agent configuration files.

</details>


### [1164] [GitEvo: Code Evolution Analysis for Git Repositories](https://arxiv.org/abs/2602.00410)
*Andre Hora*

Main category: cs.SE

TL;DR: The paper introduces GitEvo, a tool for analyzing software code evolution within Git repositories.


<details>
  <summary>Details</summary>
Motivation: Software code evolution analysis lacks dedicated tools to support practitioners, researchers, and educators in understanding development trends and real-world code modifications.

Method: GitEvo integrates Git frameworks and code parsing tools for multi-language, extensible code evolution analysis, combining both Git-level and code-level insights.

Result: GitEvo is publicly available and serves as a practical tool for empirical studies and educational purposes in software evolution.

Conclusion: GitEvo enables advanced analysis of software evolution, facilitating academic research and educational efforts in the field.

Abstract: Analyzing the code evolution of software systems is relevant for practitioners, researchers, and educators. It can help practitioners identify design trends and maintenance challenges, provide researchers with empirical data to study changes over time, and give educators real-world examples that enhance the teaching of software evolution concepts. Unfortunately, we lack tools specifically designed to support code evolution analysis. In this paper, we propose GitEvo, a multi-language and extensible tool for analyzing code evolution in Git repositories. GitEvo leverages Git frameworks and code parsing tools to integrate both Git-level and code-level analysis. We conclude by describing how GitEvo can support the development of novel empirical studies on code evolution and act as a learning tool for educators and students. GitEvo is available at: https://github.com/andrehora/gitevo.

</details>


### [1165] [Context-Sensitive Pointer Analysis for ArkTS](https://arxiv.org/abs/2602.00457)
*Yizhuo Yang,Lingyun Xu,Mingyi Zhou,Li Li*

Main category: cs.SE

TL;DR: The paper proposes a new tool, APAK, designed for ArkTS, a programming language for OpenHarmony, to enhance call graph generation and address static analysis challenges. APAK improves analysis accuracy and has been integrated into OpenHarmony's official tools.


<details>
  <summary>Details</summary>
Motivation: Current call graph generation methods for ArkTS struggle with precision in static analysis tasks due to TypeScript's closure mechanisms and OpenHarmony framework API interactions. This results in topological fractures and reduced analysis coverage, limiting advanced program analysis.

Method: The researchers developed ArkAnalyzer Pointer Analysis Kit (APAK), a context-sensitive pointer analysis tool with an ArkTS heap object model and extensible plugin architecture for compatibility with OpenHarmony's ecosystem.

Result: APAK demonstrated superior performance over traditional approaches (CHA/RTA) on a dataset of 1,663 real-world OpenHarmony applications. It achieved significant improvements in edge coverage (up to 34.2% higher than RTA), reduced false positives (from 20% to 2%), and enhanced analysis precision.

Conclusion: APAK offers a significant improvement in ArkTS call graph generation, overcoming traditional technical limitations. Its integration into OpenHarmony's static analysis framework enables the development of more advanced program analysis tools in the future.

Abstract: Current call graph generation methods for ArkTS, a new programming language for OpenHarmony, exhibit precision limitations when supporting advanced static analysis tasks such as data flow analysis and vulnerability pattern detection, while the workflow of traditional JavaScript(JS)/TypeScript(TS) analysis tools fails to interpret ArkUI component tree semantics. The core technical bottleneck originates from the closure mechanisms inherent in TypeScript's dynamic language features and the interaction patterns involving OpenHarmony's framework APIs. Existing static analysis tools for ArkTS struggle to achieve effective tracking and precise deduction of object reference relationships, leading to topological fractures in call graph reachability and diminished analysis coverage. This technical limitation fundamentally constrains the implementation of advanced program analysis techniques.
  Therefore, in this paper, we propose a tool named ArkAnalyzer Pointer Analysis Kit (APAK), the first context-sensitive pointer analysis framework specifically designed for ArkTS. APAK addresses these challenges through a unique ArkTS heap object model and a highly extensible plugin architecture, ensuring future adaptability to the evolving OpenHarmony ecosystem. In the evaluation, we construct a dataset from 1,663 real-world applications in the OpenHarmony ecosystem to evaluate APAK, demonstrating APAK's superior performance over CHA/RTA approaches in critical metrics including valid edge coverage (e.g., a 7.1% reduction compared to CHA and a 34.2% increase over RTA). The improvement in edge coverage systematically reduces false positive rates from 20% to 2%, enabling future exploration of establishing more complex program analysis tools based on our framework. Our proposed APAK has been merged into the official static analysis framework ArkAnalyzer for OpenHarmony.

</details>


### [1166] [Beyond Basic Specifications? A Systematic Study of Logical Constructs in LLM-based Specification Generation](https://arxiv.org/abs/2602.00715)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: The paper explores leveraging large language models (LLMs) for generating high-level logical constructs in program specifications to enhance program verification.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current methods that only use basic syntactic constructs in specification generation, which are insufficient for complex program verification.

Method: Incorporates logical constructs into an LLM-based framework and conducts an empirical study using four syntactic configurations with different abstraction levels, tested on program verification datasets with various LLMs.

Result: LLMs can generate valid logical constructs. Combining logical and basic syntactic constructs improves verification capability and robustness without adding significant overhead.

Conclusion: This work provides foundational insights into utilizing LLMs for generating advanced logical constructs, paving the way for automated program verification frameworks with better abstraction.

Abstract: Formal specifications play a pivotal role in accurately characterizing program behaviors and ensuring software correctness. In recent years, leveraging large language models (LLMs) for the automatic generation of program specifications has emerged as a promising avenue for enhancing verification efficiency. However, existing research has been predominantly confined to generating specifications based on basic syntactic constructs, falling short of meeting the demands for high-level abstraction in complex program verification. Consequently, we propose incorporating logical constructs into existing LLM-based specification generation framework. Nevertheless, there remains a lack of systematic investigation into whether LLMs can effectively generate such complex constructs. To this end, we conduct an empirical study aimed at exploring the impact of various types of syntactic constructs on specification generation framework. Specifically, we define four syntactic configurations with varying levels of abstraction and perform extensive evaluations on mainstream program verification datasets, employing a diverse set of representative LLMs. Experimental results first confirm that LLMs are capable of generating valid logical constructs. Further analysis reveals that the synergistic use of logical constructs and basic syntactic constructs leads to improvements in both verification capability and robustness, without significantly increasing verification overhead. Additionally, we uncover the distinct advantages of two refinement paradigms. To the best of our knowledge, this is the first systematic work exploring the feasibility of utilizing LLMs for generating high-level logical constructs, providing an empirical basis and guidance for the future construction of automated program verification framework with enhanced abstraction capabilities.

</details>


### [1167] [Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression](https://arxiv.org/abs/2602.00746)
*Jianping Zhong,Guochang Li,Chen Zhi,Junxiao Han,Zhen Qin,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: LongCodeOCR proposes a visual compression framework for code, enabling Vision-Language Models to effectively process long-context code while maintaining global dependency, outperforming existing approaches like LongCodeZip.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle to handle long-context code due to window limitations, and existing textual compression disrupts semantic integrity by breaking dependencies.

Method: The introduced method, LongCodeOCR, compresses code into two-dimensional image sequences for Vision-Language Models, preserving global dependency without semantic fragmentation.

Result: LongCodeOCR improves performance over LongCodeZip in tasks such as code summarization, answering, and completion, achieving higher accuracy with significantly reduced latency and improved compression ratios.

Conclusion: Visual code compression via LongCodeOCR can effectively address global dependency challenges, outperforming textual approaches in coverage, though it’s limited by fidelity for symbol-level precision tasks.

Abstract: Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.
  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\sim$1.7$\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\sim$4.3 hours to $\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.

</details>


### [1168] [ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming](https://arxiv.org/abs/2602.00757)
*Yuan Si,Simeng Han,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: This paper introduces ScratchEval, an evaluation benchmark for LLMs addressing bug repair tasks in Scratch, a block-based programming language.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle when tasked with block-based languages like Scratch due to fundamental differences in structure and semantics compared to text-based programming.

Method: ScratchEval was developed by curating 100 Scratch projects along with executable test suites, bug descriptions, fix constraints, multimedia assets, and uses a human-in-the-loop validation pipeline.

Result: The paper details how ScratchEval allows rigorous evaluation of LLMs in Scratch repair tasks, assessing correctness, repair quality, and explanation quality.

Conclusion: ScratchEval establishes a framework for reproducibly evaluating and improving LLMs specific to block-based programming challenges like Scratch.

Abstract: LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.
  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.
  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.

</details>


### [1169] [Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods](https://arxiv.org/abs/2602.00761)
*Andre Hora,Andy Zaidman*

Main category: cs.SE

TL;DR: The study identifies a new test smell, 'Test Obsessed by Method,' focusing on tests that cover multiple paths in a single production method, using empirical evidence from Python's standard library.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing metrics like 'Eager Test' in accurately identifying tests verifying too much functionality.

Method: Proposed a methodology based on runtime analysis to detect test smells that cover multiple production paths, with an empirical study conducted on Python Standard Library tests.

Result: Detected 44 test smells across 11 test suites, showing potential division into 118 distinct tests and identifying the issue's prevalence in real-world settings.

Conclusion: The findings highlight the practicality and implications of recognizing and splitting smelly tests, suggesting directions for enhancing test quality and refining detection methods.

Abstract: Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.

</details>


### [1170] [Code Quality Analysis of Translations from C to Rust](https://arxiv.org/abs/2602.00840)
*Biruk Tadesse,Vikram Nitin,Mazin Salah,Baishakhi Ray,Marcelo d'Amorim,Wesley Assunção*

Main category: cs.SE

TL;DR: The paper evaluates the quality of various automated C-to-Rust translation techniques and highlights trade-offs while concluding human translations are not free from internal issues either.


<details>
  <summary>Details</summary>
Motivation: To address memory and thread-safety concerns in C/C++ by translating code to a safer language like Rust while exploring quality dimensions beyond correctness and safety.

Method: In-depth quantitative and qualitative assessment of Rust code generated by three translation techniques against human-based translations using tools like Clippy, GPT-4o, and manual review.

Result: While automated techniques reduced unsafe patterns, they also introduced new issues, highlighting trade-offs. No translation technique, including human-written ones, excelled universally across all quality attributes.

Conclusion: Translation of C-to-Rust involves multi-dimensional quality challenges. Systematic evaluation and targeted tool support are required to address issues beyond both automated and manual translation approaches.

Abstract: C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.

</details>


### [1171] [MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers](https://arxiv.org/abs/2602.00933)
*Chaithanya Bandi,Ben Hertzberg,Geobio Boo,Tejas Polakam,Jeff Da,Sami Hassaan,Manasi Sharma,Andrew Park,Ernesto Hernandez,Dan Rambado,Ivan Salazar,Rafael Cruz,Chetan Rane,Ben Levin,Brad Kenstler,Bing Liu*

Main category: cs.SE

TL;DR: MCP-Atlas is a benchmark to evaluate tool-use competency in LLMs via realistic workflows and rigorous scoring, revealing weaknesses in current models. Tools for reproducibility are provided.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to evaluate tool-use competency in LLMs under realistic scenarios, necessitating a comprehensive benchmarking system.

Method: Developed MCP-Atlas, which involves real MCP servers and tools, natural language workflows, a claims-based scoring system, and internal diagnostics for model evaluation.

Result: Top frontier models showed pass rates of over 50%, highlighting substantial issues with tool usage and task understanding.

Conclusion: Releasing MCP-Atlas benchmark assets aims to improve tool-augmented agents by enabling robust and reproducible evaluations.

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

</details>


### [1172] [Cast: Automated Resilience Testing for Production Cloud Service Systems](https://arxiv.org/abs/2602.00972)
*Zhuangbin Chen,Zhiling Deng,Kaiming Zhang,Yang Liu,Cheng Cui,Jinfeng Zhong,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper introduces Cast, an automated framework for resilience testing of microservices in production environments, achieving high fidelity and broad coverage of vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Microservices architectures pose significant resilience challenges due to their distributed nature. Existing test methods fail to capture the complexity of production systems, necessitating a more efficient and systematic solution.

Method: The paper introduces Cast, an automated framework that replays production traffic with a library of faults, uses a complexity-driven strategy to prioritize key tests, and employs a three-phase pipeline to manage the test lifecycle. A multi-faceted oracle is used for evaluating resilience criteria.

Result: Cast uncovered 137 potential vulnerabilities in four large-scale applications, with 89 confirmed by developers. It achieved 90% coverage on a benchmark of 48 reproduced bugs, demonstrating its effectiveness.

Conclusion: Cast is a practical and effective tool for improving the reliability of microservice systems, validated through extensive deployment and testing in industrial environments.

Abstract: The distributed nature of microservice architecture introduces significant resilience challenges. Traditional testing methods, limited by extensive manual effort and oversimplified test environments, fail to capture production system complexity. To address these limitations, we present Cast, an automated, end-to-end framework for microservice resilience testing in production. It achieves high test fidelity by replaying production traffic against a comprehensive library of application-level faults to exercise internal error-handling logic. To manage the combinatorial test space, Cast employs a complexity-driven strategy to systematically prune redundant tests and prioritize high-value tests targeting the most critical service execution paths. Cast automates the testing lifecycle through a three-phase pipeline (i.e., startup, fault injection, and recovery) and uses a multi-faceted oracle to automatically verify system resilience against nuanced criteria. Deployed in Huawei Cloud for over eight months, Cast has been adopted by many service teams to proactively address resilience vulnerabilities. Our analysis on four large-scale applications with millions of traces reveals 137 potential vulnerabilities, with 89 confirmed by developers. To further quantify its performance, Cast is evaluated on a benchmark set of 48 reproduced bugs, achieving a high coverage of 90%. The results show that Cast is a practical and effective solution for systematically improving the reliability of industrial microservice systems.

</details>


### [1173] [Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs](https://arxiv.org/abs/2602.01044)
*Yu Tang,Hailiang Zhao,Rui Shi,Chuansheng Lu,Yifei Zhang,Kingsum Chow,Shuiguang Deng*

Main category: cs.SE

TL;DR: Morphis is a dependency-aware provisioning tool targeting dynamic microservice systems by leveraging recurring invocation patterns for efficient resource management.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in current resource management approaches that fail to capture dynamic and dependency-aware execution contexts of microservice systems.

Method: Introduces structural fingerprinting to decompose traces and formulates resource allocation as an optimization problem over pattern distributions.

Result: Morphis reduced CPU consumption by 35-38% and achieved 98.8% compliance with SLOs in evaluations using the TrainTicket benchmark.

Conclusion: Exploiting invocation patterns through Morphis enables more efficient resource allocation while maintaining high service quality in dynamic microservice environments.

Abstract: Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.

</details>


### [1174] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

TL;DR: Library migration is error-prone due to manual updates and challenges in collecting migration data. This paper introduces an approach using LLMs and PolyglotPiranha to automate API migration effectively.


<details>
  <summary>Details</summary>
Motivation: Library migration is necessitated by changing requirements or licensing issues, but it is a complex and error-prone manual process. Existing automated tools are limited by the lack of real-world migration examples and underutilize modern transformation infrastructure.

Method: The paper proposes using LLMs to extract migration examples and employing an Agent to generalize these examples into reusable transformation scripts with PolyglotPiranha, aiming to create structured and repeatable migration logic.

Result: Experiments on Python libraries demonstrate that the system can generate diverse migration examples and develop transformation scripts that are applicable to real-world codebases.

Conclusion: The presented method successfully addresses limitations in automated library migration by leveraging LLMs and modern code transformation tools to create reusable and generalized migration logic without relying on extensive preexisting data.

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [1175] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

TL;DR: The paper proposes a novel approach for code generation called "Stream of Revision," where a model can dynamically self-correct in real-time instead of following a monotonic token generation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code generation methods are strictly linear and monotonic, which does not align with the natural programming process involving forward generation and real-time revisions. Existing revision approaches have inefficiencies or fail to utilize a model's semantic reasoning capabilities.

Method: The authors introduce "Stream of Revision," which equips models with action tokens to allow for backtracking and editing their outputs within a single forward pass, embedding the revision process into the model itself.

Result: Experiments demonstrate that "Stream of Revision" significantly reduces vulnerabilities in secure code generation tasks, with minimal additional computational overhead.

Conclusion: Integrating a self-correcting mechanism within the model enhances its ability to generate higher-quality code, bridging the gap between natural programming processes and machine-based code generation.

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [1176] [TraceLLM: Leveraging Large Language Models with Prompt Engineering for Enhanced Requirements Traceability](https://arxiv.org/abs/2602.01253)
*Nouf Alturayeif,Irfan Ahmad,Jameleddine Hassine*

Main category: cs.SE

TL;DR: The paper introduces TraceLLM, a framework using prompt engineering to improve requirements traceability in software development, achieving state-of-the-art results with LLMs compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in traditional methods for requirements traceability, such as low precision, labor intensity, and errors, by leveraging the capabilities of Large Language Models (LLMs).

Method: The authors developed the TraceLLM framework, utilizing prompt engineering and demonstration selection. The framework enhances trace links by refining prompts, incorporating domain knowledge, and testing across zero- and few-shot settings on diverse datasets.

Result: TraceLLM outperformed traditional, fine-tuned, and prior LLM-based methods in requirements traceability, achieving top F2 scores across domains and artifacts. Effective strategies like label-aware, diversity-based sampling were highlighted.

Conclusion: Traceability performance greatly relies on prompt engineering quality alongside model capacity. TraceLLM can serve as a semi-automated tool for human analysts, enhancing workflows by identifying candidate trace links for review.

Abstract: Requirements traceability, the process of establishing and maintaining relationships between requirements and various software development artifacts, is paramount for ensuring system integrity and fulfilling requirements throughout the Software Development Life Cycle (SDLC). Traditional methods, including manual and information retrieval models, are labor-intensive, error-prone, and limited by low precision. Recently, Large Language Models (LLMs) have demonstrated potential for supporting software engineering tasks through advanced language comprehension. However, a substantial gap exists in the systematic design and evaluation of prompts tailored to extract accurate trace links. This paper introduces TraceLLM, a systematic framework for enhancing requirements traceability through prompt engineering and demonstration selection. Our approach incorporates rigorous dataset splitting, iterative prompt refinement, enrichment with contextual roles and domain knowledge, and evaluation across zero- and few-shot settings. We assess prompt generalization and robustness using eight state-of-the-art LLMs on four benchmark datasets representing diverse domains (aerospace, healthcare) and artifact types (requirements, design elements, test cases, regulations). TraceLLM achieves state-of-the-art F2 scores, outperforming traditional IR baselines, fine-tuned models, and prior LLM-based methods. We also explore the impact of demonstration selection strategies, identifying label-aware, diversity-based sampling as particularly effective. Overall, our findings highlight that traceability performance depends not only on model capacity but also critically on the quality of prompt engineering. In addition, the achieved performance suggests that TraceLLM can support semi-automated traceability workflows in which candidate links are reviewed and validated by human analysts.

</details>


### [1177] [Evaluating Workflow Automation Efficiency Using n8n: A Small-Scale Business Case Study](https://arxiv.org/abs/2602.01311)
*Ahmed Raza Amir,Syed Muhammad Atif*

Main category: cs.SE

TL;DR: The paper evaluates workflow automation with n8n in small-scale business, showing it significantly reduces execution time and eliminates observed errors.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the benefits of using low-code workflow automation for improving operational efficiency in small organizations.

Method: A lead-processing workflow was automated using n8n, and its performance was benchmarked against manual execution for efficiency and reliability.

Result: Automated workflows reduced average execution time significantly (185.35 seconds to 1.23 seconds) and eliminated errors (from 5% error rate in manual execution to 0%).

Conclusion: Low-code workflow automation enhances efficiency, consistency, and reliability for small-scale workflows, making it accessible to individuals and small organizations.

Abstract: Workflow automation has become increasingly accessible through low-code platforms, enabling small organizations and individuals to improve operational efficiency without extensive software development expertise. This study evaluates the performance impact of workflow automation using n8n through a small-scale business case study. A representative lead-processing workflow was implemented to automatically store data, send email confirmations, and generate real-time notifications. Experimental benchmarking was conducted by comparing 20 manual executions with 25 automated executions under controlled conditions. The results demonstrate a significant reduction in the average execution time from 185.35 seconds (manual) to 1.23 seconds (automated), corresponding to an approximately 151 times reduction in execution time. Additionally, manual execution exhibited an error rate of 5%, while automated execution achieved zero observed errors. The findings highlight the effectiveness of low-code automation in improving efficiency, reliability, and operational consistency for small-scale workflows.

</details>


### [1178] [AdNanny: One Reasoning LLM for All Offline Ads Recommendation Tasks](https://arxiv.org/abs/2602.01563)
*Nan Hu,Han Li,Jimeng Sun,Lu Wang,Fangkai Yang,Bo Qiao,Pu Zhao,David Dai,Mengyu Liu,Yuefeng Zhan,Jianjin Zhang,Weihao Han,Allen Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Denvy Deng,Feng Sun,Qi Zhang*

Main category: cs.SE

TL;DR: AdNanny is a unified Large Language Model designed for offline advertising tasks that reduces redundancy, enhances accuracy, and minimizes manual effort.


<details>
  <summary>Details</summary>
Motivation: Deployment of LLMs is impractical in online advertising systems due to latency constraints, necessitating offline optimization.

Method: Fine-tuning a large DeepSeek-R1 checkpoint using dense-MoE parallelism and reasoning-augmented datasets, followed by multi-task supervised training and reinforcement learning.

Result: Significant improvements in labeling accuracy and task efficiency in Bing Ads production.

Conclusion: AdNanny consolidates multiple models into a single reasoning-centric foundation model, providing a scalable and cost-effective solution for advertising systems.

Abstract: Large Language Models (LLMs) have shown strong capabilities in Natural Language Understanding and Generation, but deploying them directly in online advertising systems is often impractical due to strict millisecond-level latency constraints. This has motivated the use of LLMs offline to improve retrieval, ranking, and recommendation models. Existing solutions typically fine-tune separate LLMs for individual tasks such as query-ad relevance labeling, keyword-based query generation, and user profiling. This results in redundant models, high maintenance cost, and limited performance gains despite substantial overlap in domain knowledge and reasoning patterns. We introduce AdNanny, a unified reasoning-centric LLM that serves as a shared backbone for offline advertising tasks. AdNanny is obtained by fine-tuning a public 671B-parameter DeepSeek-R1 checkpoint using a scalable training system that supports hybrid dense-MoE parallelism. We construct reasoning-augmented corpora that pair structured supervision with step-by-step natural language explanations. A multi-task supervised fine-tuning stage with adaptive reweighting enables AdNanny to handle diverse labeling and generation tasks in a consistent reasoning format. This is followed by reinforcement learning using downstream advertising metrics to align model behavior with online retrieval and ranking objectives. AdNanny is deployed in production within Bing Ads, where it significantly reduces manual labeling effort and improves accuracy across multiple offline tasks. By consolidating many task-specific models into a single reasoning-centric foundation model, AdNanny provides a scalable and cost-effective solution for large-scale advertising systems.

</details>


### [1179] [Role of CI Adoption in Mobile App Success: An Empirical Study of Open-Source Android Projects](https://arxiv.org/abs/2602.01957)
*Xiaoxin Zhou,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: CI adoption improves release speed, activity metrics, and user outcomes for open-source Android apps in integration-intensive categories.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of Continuous Integration on mobile development, particularly in release speed, user engagement, and project visibility.

Method: Analyzed data from open-source Android apps to compare CI adopters and non-adopters, assess pre/post adoption changes, and examine user-facing outcomes.

Result: CI adopters are larger, more active, release faster, and show increased engagement in the Google Play Store with more downloads and reviews without lower ratings.

Conclusion: Continuous Integration aligns well with sustained delivery, app visibility, and improved user engagement in mobile app ecosystems.

Abstract: Mobile apps face strong pressure for fast and reliable updates. Continuous Integration (CI) helps automate builds, tests, and releases, but its impact on mobile development remains underexplored. Despite the widespread use of CI, little is known about how it affects development activity, release speed, and user-facing outcomes in mobile projects. Existing studies mostly focus on CI adoption in general-purpose software, providing limited insight into mobile-specific dynamics, such as app store visibility and user engagement. In this paper, we analyze open-source Android apps to (1) compare CI adopters and non-adopters, (2) characterize adoption patterns using activity and bug metrics, and (3) assess pre/post adoption changes and user-facing outcomes. We observe that CI adopters are larger and more active, with faster and more regular releases. CI adoption is concentrated in integration- and reliability-intensive categories (e.g., finance and productivity) and is associated with higher Google Play Store engagement (more downloads and reviews) without lower ratings. Overall, CI adoption aligns with practices that support sustained delivery, higher project visibility, and stronger user engagement in mobile ecosystems.

</details>


### [1180] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

TL;DR: The paper introduces CAM, a causality-based analysis framework for Multi-Agent Code Generation Systems (MACGS), to evaluate the influence of intermediate outputs on system correctness and improve system performance.


<details>
  <summary>Details</summary>
Motivation: The complexity of MACGS leads to the production of numerous intermediate outputs, the importance of which is unclear, hindering system design optimizations.

Method: CAM systematically categorizes intermediate outputs, simulates errors, and identifies critical features for system correctness by analyzing their contributions and aggregating importance rankings.

Result: Through empirical analysis, CAM uncovers context-dependent features and demonstrates improvements in MACGS performance, including a 7.2% improvement in Pass@1 with hybrid architectures and successful applications in failure repair and feature pruning.

Conclusion: CAM establishes causality analysis as an effective approach for enhancing MACGS, delivering actionable insights for system design and deployment.

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [1181] [Agent-Based Software Artifact Evaluation](https://arxiv.org/abs/2602.02235)
*Zhaonan Wu,Yanjie Zhao,Zhenpeng Chen,Zheng Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: This paper introduces ArtifactCopilot, an automated framework for artifact evaluation in Software Engineering, addressing reproducibility and scalability issues through advanced methodologies, significantly reducing human effort while achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the reproducibility and scalability challenges in the Software Engineering research community due to increasing reliance on manual artifact evaluation amidst growing paper submissions.

Method: The paper proposes ArtifactCopilot, an agent-based framework that automates artifact evaluation by combining execution normalization, dependency-aware command graphs for structured execution, environment stability, and error recovery mechanisms.

Result: ArtifactCopilot achieves 85.42% accuracy in matching human evaluations on artifacts, outperforms Claude Code by 52.09 percentage points, and requires minimal cost ($0.091 per artifact) and human intervention for the majority of artifacts studied.

Conclusion: ArtifactCopilot presents a viable solution to automate and enhance artifact evaluation in Software Engineering research, mitigating manual effort, improving accuracy, and ensuring reproducibility and scalability.

Abstract: Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.

</details>


### [1182] [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262)
*Atharv Sonwane,Eng-Shen Tu,Wei-Chung Lu,Claas Beger,Carter Larsen,Debjit Dhar,Rachel Chen,Ronit Pattanayak,Tuan Anh Dang,Guohao Chen,Gloria Geng,Kevin Ellis,Saikat Dutta*

Main category: cs.SE

TL;DR: OmniCode introduces a comprehensive benchmark with diverse software engineering tasks across Python, Java, and C++ to evaluate coding agents' capabilities beyond existing narrowly scoped benchmarks like HumanEval.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like HumanEval and SWE-Bench have limited scope in evaluating coding agents, focusing narrowly on programming competitions and patch generation.

Method: The paper introduces OmniCode, a benchmark with 1794 tasks spread across bug fixing, test generation, code review fixing, and style fixing. Tasks are manually validated and synthetically crafted to minimize data leakage.

Result: The evaluation of popular frameworks suggests limitations in certain areas, such as test generation for C++ and Java, with SWE-Agent achieving only 20.9% on Java test generation tasks.

Conclusion: OmniCode provides a broader and validated set of tasks for benchmarking coding agents and aims to drive their development for comprehensive software engineering capabilities.

Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.

</details>


### [1183] [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280)
*Zeming Wei,Zhixin Zhang,Chengcan Wu,Yihao Zhang,Xiaokun Luan,Meng Sun*

Main category: cs.SE

TL;DR: The paper introduces RACA, a framework for improving safety testing of LLMs by identifying and assessing safety-critical concepts using newly designed coverage criteria.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the safety concerns in LLMs caused by jailbreak attacks, as current safety testing methods rely on static datasets and lack systematic evaluation criteria.

Method: RACA involves three stages: identifying safety-critical representations using expert-curated prompts, calculating activation scores based on these representations, and employing six sub-criteria for comprehensive assessment.

Result: RACA outperforms traditional neuron-level criteria in identifying jailbreak prompts and demonstrates its utility in real-world tasks like test set prioritization and attack prompt sampling.

Conclusion: RACA is an effective framework for LLM safety testing, providing a systematic and practical approach to evaluate and improve AI safety.

Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.

</details>


### [1184] [Before Autonomy Takes Control: Software Testing in Robotics](https://arxiv.org/abs/2602.02293)
*Nils Chur,Thiago Santos de Moura,Argentina Ortega,Sven Peldszus,Thorsten Berger,Nico Hochgeschwender,Yannic Noller*

Main category: cs.SE

TL;DR: The paper explores the intersection of robotics testing and software testing, mapping 247 robotics testing papers to software testing theory, discussing challenges, and identifying open questions.


<details>
  <summary>Details</summary>
Motivation: Robotic systems are safety-critical and hard to test due to their interaction with hardware, uncertainty, disturbances, and autonomous nature. Testing these systems thoroughly is essential for reliability.

Method: The paper conducts a mapping study by analyzing 247 robotics testing papers, relating them to software testing theory, illustrating challenges, and providing examples.

Result: The study categorizes robotics testing within software testing theory, highlighting state-of-the-art practices and challenges in robotics software testing, and identifies open questions.

Conclusion: The paper lays the foundation for both robotics and software engineering communities to better understand testing challenges in robotic systems, providing insights and lessons learned for future research.

Abstract: Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.

</details>


### [1185] [Understanding and Detecting Flaky Builds in GitHub Actions](https://arxiv.org/abs/2602.02307)
*Wenhao Ge,Chen Zhang*

Main category: cs.SE

TL;DR: The paper examines unreliable build outcomes in Continuous Integration, focusing on flaky builds in GitHub Actions across 1,960 Java projects, and proposes a machine learning method to detect such issues, achieving significant accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unreliable outcomes in Continuous Integration workflows caused by flaky builds, which waste computational resources, reduce developer trust in CI, and impact empirical CI studies.

Method: The researchers conducted a large-scale empirical study using rerun data from GitHub Actions projects, identified 15 categories of flaky failures, and developed a machine learning-based detection approach to identify flaky jobs.

Result: Findings show 3.2% of builds are rerun, with 67.73% showing flaky behavior, impacting over half of the projects. Their proposed detection approach improves the F1-score by 20.3% compared to existing baselines.

Conclusion: Flaky builds are a significant issue in CI systems, and the proposed machine learning-based detection method offers a promising solution that enhances accuracy and addresses prevalent flaky failure categories.

Abstract: Continuous Integration (CI) is widely used to provide rapid feedback on code changes; however, CI build outcomes are not always reliable. Builds may fail intermittently due to non-deterministic factors, leading to flaky builds that undermine developers' trust in CI, waste computational resources, and threaten the validity of CI-related empirical studies. In this paper, we present a large-scale empirical study of flaky builds in GitHub Actions based on rerun data from 1,960 open-source Java projects. Our results show that 3.2% of builds are rerun, and 67.73% of these rerun builds exhibit flaky behavior, affecting 1,055 (51.28%) of the projects. Through an in-depth failure analysis, we identify 15 distinct categories of flaky failures, among which flaky tests, network issues, and dependency resolution issues are the most prevalent. Building on these findings, we propose a machine learning-based approach for detecting flaky failures at the job level. Compared with a state-of-the-art baseline, our approach improves the F1-score by up to 20.3%.

</details>


### [1186] [A Task-Level Evaluation of AI Agents in Open-Source Projects](https://arxiv.org/abs/2602.02345)
*Shojibur Rahman,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: The paper conducts a comparative study of five autonomous coding agents using the AIDev-pop dataset, focusing on PR acceptance, review discussion volume, and commit message quality.


<details>
  <summary>Details</summary>
Motivation: The aim is to evaluate and improve integration for AI coding agents in collaborative software development.

Method: Quantitative analysis of coding agents' performance on PR acceptance, discussion volume, and commit message quality across various tasks.

Result: Codex has high PR acceptance, Copilot triggers the most discussions, while Claude and Cursor produce better commit messages. Codex performs well in integration but lags in commit quality.

Conclusion: Insights on strengths and weaknesses of AI agents inform better integration and use in software engineering practices.

Abstract: In this paper, we present a comparative study of five autonomous coding agents using AIDev-pop, which is a public dataset containing thousands of AI-generated pull requests (PRs) across popular open-source repositories. We evaluate agents' performance along three task-aware dimensions spanning the PR lifecycle: (1) PR acceptance rate, (2) review discussion volume, and (3) commit message quality. Our quantitative analysis finds that Codex consistently achieves high PR acceptance rates across most task categories, while Copilot's PRs trigger the highest volume of both human and automated review discussions. In contrast, commit-level quality varies independently of acceptance outcomes. Claude and Cursor produce higher proportions of high-quality commit messages across several task types, and Codex exhibiting comparatively lower commit quality despite strong integration outcomes. Our findings inform selection and improvements of AI agents for their effective integration to collaborative software engineering.

</details>


### [1187] [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361)
*Mouxiang Chen,Lei Zhang,Yunlong Feng,Xuwu Wang,Wenting Zhao,Ruisheng Cao,Jiaxi Yang,Jiawei Chen,Mingze Li,Zeyao Ma,Hao Ge,Zongmeng Zhang,Zeyu Cui,Dayiheng Liu,Jingren Zhou,Jianling Sun,Junyang Lin,Binyuan Hui*

Main category: cs.SE

TL;DR: SWE-Universe streamlines real-world software engineering environments from GitHub pull requests, employing powerful models and methodologies for large-scale creation and validation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges such as low production yield, weak verifiers, and the high costs associated with constructing verifiable software engineering environments from GitHub pull requests.

Method: The framework employs a custom-trained model integrated with iterative self-verification and in-loop hacking detection, allowing reliable generation of scalable, verifiable tasks.

Result: The approach successfully scales to 807,693 real-world multilingual software engineering environments and achieves a score of 75.3% on SWE-Bench Verified with Qwen3-Max-Thinking.

Conclusion: SWE-Universe provides a critical resource and methodology for developing future coding agents, showcasing its efficiency in creating high-quality environments and fostering advancements in software engineering.

Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [1188] [Explore Brain-Inspired Machine Intelligence for Connecting Dots on Graphs Through Holographic Blueprint of Oscillatory Synchronization](https://arxiv.org/abs/2602.00057)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: q-bio.NC

TL;DR: The paper explores how brain-inspired oscillatory synchronization principles can enhance graph neural networks (GNNs), solving key issues like over-smoothing and enabling improved reasoning on graphs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage insights from neural coupling and brain rhythms to inspire more efficient and robust machine learning algorithms, specifically targeting limitations in current graph neural networks.

Method: The authors introduce HoloBrain, a model based on neural oscillation synchronization, and propose HoloGraph, a framework applying brain-inspired synchronization mechanisms to graph neural networks.

Result: The proposed HoloGraph approach effectively addresses the over-smoothing problem in GNNs and demonstrates enhanced capability for reasoning and handling challenging graph-based tasks.

Conclusion: The study highlights the potential for brain-derived principles, like oscillatory synchronization, to advance the design of machine learning systems, offering new directions for graph-based problem-solving.

Abstract: Neural coupling in both neuroscience and artificial intelligence emerges as dynamic oscillatory patterns that encode abstract concepts. To this end, we hypothesize that a deeper understanding of the neural mechanisms governing brain rhythms can inspire next-generation design principles for machine learning algorithms, leading to improved efficiency and robustness. Building on this idea, we first model evolving brain rhythms through the interference of spontaneously synchronized neural oscillations, termed HoloBrain. The success of modeling brain rhythms using an artificial dynamical system of coupled oscillations motivates a "first principle" for brain-inspired machine intelligence based on a shared synchronization mechanism, termed HoloGraph. This principle enables graph neural networks to move beyond conventional heat diffusion paradigms toward modeling oscillatory synchronization. Our HoloGraph framework not only effectively mitigates the over-smoothing problem in graph neural networks but also demonstrates strong potential for reasoning and solving challenging problems on graphs.

</details>


### [1189] [Inter- and Intra-Subject Variability in EEG: A Systematic Survey](https://arxiv.org/abs/2602.01019)
*Xuan-The Tran,Thien-Nhan Vo,Son-Tung Vu,Thoa-Thi Tran,Manh-Dat Nguyen,Thomas Do,Chin-Teng Lin*

Main category: q-bio.NC

TL;DR: The paper focuses on analyzing EEG variability across various paradigms, highlighting the reliability challenges in neuroscience and neurotechnology and offering recommendations for improving precision.


<details>
  <summary>Details</summary>
Motivation: Understanding and managing EEG variability are crucial to enhance reliability, reproducibility, and the applicability of EEG in neuroscience, clinical neurophysiology, and brain-computer interfaces.

Method: The systematic review examines variability in EEG metrics across paradigms (e.g., resting-state, ERPs, task-related measures), evaluates inter- and intra-subject differences, and explores quantification and modeling methods (e.g., ICC, CV, SNR, learning-based analyses).

Result: Inter-subject variability generally exceeds intra-subject fluctuations; reliability depends on the features analyzed. Alpha-band measures are relatively stable, while higher-frequency metrics, connectivity measures, and some ERP components exhibit variable reliability.

Conclusion: EEG variability is both a constraint and a signal to exploit. The paper provides recommendations for study design to enhance precision and robustness in neuroscience and neurotechnology applications.

Abstract: Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.

</details>


### [1190] [Vulnerability-Amplifying Interaction Loops: a systematic failure mode in AI chatbot mental-health interactions](https://arxiv.org/abs/2602.01347)
*Veith Weilnhammer,Kevin YC Hou,Raymond Dolan,Matthew M Nour*

Main category: q-bio.NC

TL;DR: Researchers introduce SIM-VAIL, a framework to audit AI chatbot responses in mental health contexts. It evaluates chatbot safety using simulated conversations and identifies risks specific to user psychiatric vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: With increasing reliance on AI chatbots for mental health support, there is a pressing need to rigorously evaluate these systems' safety and mitigate potential risks.

Method: The researchers created SIM-VAIL, combining simulated users with psychiatric vulnerabilities and auditing chatbot conversations. These conversations were scored across 13 clinically relevant risk dimensions to analyze risk accumulation.

Result: Among 810 conversations and over 90,000 ratings, significant risks were found across typical user profiles and chatbot models. Risks varied by psychiatric phenotype and accumulated over multiple turns. Mitigations in one area sometimes increased risks elsewhere.

Conclusion: The study identifies new failure modes termed Vulnerability-Amplifying Interaction Loops (VAILs) and highlights the need for multi-dimensional approaches to AI chatbot safety in mental health applications.

Abstract: Millions of users turn to consumer AI chatbots to discuss behavioral and mental health concerns. While this presents unprecedented opportunities to deliver population-level support, it also highlights an urgent need to develop rigorous and scalable safety evaluations. Here we introduce SIM-VAIL, an AI chatbot auditing framework that captures how harmful AI chatbot responses manifest across a range of mental-health contexts. SIM-VAIL pairs a simulated human user, harboring a distinct psychiatric vulnerability and conversational intent, with an audited frontier AI chatbot. It scores conversation turns on 13 clinically relevant risk dimensions, enabling context-dependent, temporally resolved assessment of mental-health risk. Across 810 conversations, encompassing over 90,000 turn-level ratings and 30 psychiatric user profiles, we find that significant risk occurs across virtually all user phenotypes. Risk manifested across most of the 9 consumer AI chatbot models audited, albeit mitigated in more modern variants. Rather than arising abruptly, risk accumulated over multiple turns. Risk profiles were phenotype-dependent, indicating that behaviors that appear supportive in general settings are liable to be maladaptive when they align with mechanisms that sustain a user's vulnerability. Multivariate risk patterns revealed trade-offs across dimensions, suggesting that mitigation targeting one harm domain can exacerbate others. These findings identify a novel failure mode in human-AI interactions, which we term Vulnerability-Amplifying Interaction Loops (VAILs), and underscore the need for multi-dimensional approaches to risk quantification. SIM-VAIL provides a scalable evaluation framework for quantifying how mental-health risk is distributed across user phenotypes, conversational trajectories, and clinically grounded behavioral dimensions, offering a foundation for targeted safety improvements.

</details>


### [1191] [Community-Level Modeling of Gyral Folding Patterns for Robust and Anatomically Informed Individualized Brain Mapping](https://arxiv.org/abs/2602.01482)
*Minheng Chen,Tong Chen,Yan Zhuang,Chao Cao,Jing Zhang,Tianming Liu,Lu Zhang,Dajiang Zhu*

Main category: q-bio.NC

TL;DR: The study introduces a novel framework to model groupings in brain cortical folds to improve anatomical characterization and cross-subject comparisons.


<details>
  <summary>Details</summary>
Motivation: Current methods overlook higher-order relationships in cortical folding patterns, leading to less robust anatomical representations.

Method: The authors propose a spectral graph representation learning approach, incorporating surface topology and structural connectivity, with subject-specific spectral clustering and topological refinement.

Result: The method achieves reduced morphometric variance, stronger modular organization, improved hemispheric consistency, and better alignment across subjects in a large dataset.

Conclusion: Community-level modeling is effective for anatomically grounded and reliable cortical characterization and subject correspondence.

Abstract: Cortical folding exhibits substantial inter-individual variability while preserving stable anatomical landmarks that enable fine-scale characterization of cortical organization. Among these, the three-hinge gyrus (3HG) serves as a key folding primitive, showing consistent topology yet meaningful variations in morphology, connectivity, and function. Existing landmark-based methods typically model each 3HG independently, ignoring that 3HGs form higher-order folding communities that capture mesoscale structure. This simplification weakens anatomical representation and makes one-to-one matching sensitive to positional variability and noise. We propose a spectral graph representation learning framework that models community-level folding units rather than isolated landmarks. Each 3HG is encoded using a dual-profile representation combining surface topology and structural connectivity. Subject-specific spectral clustering identifies coherent folding communities, followed by topological refinement to preserve anatomical continuity. For cross-subject correspondence, we introduce Joint Morphological-Geometric Matching, jointly optimizing geometric and morphometric similarity. Across over 1000 Human Connectome Project subjects, the resulting communities show reduced morphometric variance, stronger modular organization, improved hemispheric consistency, and superior alignment compared with atlas-based and landmark-based or embedding-based baselines. These findings demonstrate that community-level modeling provides a robust and anatomically grounded framework for individualized cortical characterization and reliable cross-subject correspondence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1192] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

TL;DR: The paper introduces conformal Shapley intervals to quantify the importance and uncertainty of modalities in multimodal learning and proposes a modality selection method with provable optimality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unequal and uncertain contributions of data modalities in multimodal learning, enhancing interpretability and reliability.

Method: The authors propose conformal Shapley intervals, combining Shapley values with conformal inference, and a modality selection procedure with theoretical performance guarantees.

Result: Empirical evaluation on several datasets demonstrates effective uncertainty quantification, improved predictive performance, and reliance on fewer informative modalities.

Conclusion: The framework provides reliable interpretable insights into modality importance with uncertainty and ensures optimal modality selection with strong predictive results.

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [1193] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: The paper analyzes zero-margin nonlinear classification using neural networks, specifically on the Gaussian XOR problem, emphasizing training dynamics and generalization without margin-based assumptions.


<details>
  <summary>Details</summary>
Motivation: To better understand how neural networks learn in challenging settings like zero-margin classification, which breaks conventional margin-based analyses.

Method: The study extends Glasgow's 2024 analysis to Gaussian inputs and introduces a framework focusing on neuron block dynamics, analyzing training behaviors and generalization in non-margin-dependent scenarios.

Result: Neurons cluster into four distinct directions, with coherent block-level signal evolution that enables generalization. Numerical experiments validate their theoretical predictions and show robustness beyond Gaussian cases.

Conclusion: Neuron block dynamics play a key role in learning and generalization for zero-margin classification, providing insights beyond standard margin-based analysis.

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [1194] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

TL;DR: The paper introduces a low-rank parameterization for Bayesian neural networks (BNNs) to reduce parameter size and achieve competitive performance. It improves calibration and out-of-distribution detection while maintaining model effectiveness with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Bayesian neural networks often require large parameter sizes for mean-field Gaussian posteriors, which makes them computationally expensive. The motivation is to address this inefficiency by exploiting structured correlations and fast singular value decay in weight matrices.

Method: The authors propose parameterizing weights as $W = AB^{\top}$, effectively inducing a low-rank posterior. This approach builds upon structured weight correlations and avoids mean-field assumptions. They derive PAC-Bayes generalization bounds, leveraging the Eckart-Young-Mirsky theorem for rank-specific error decomposition.

Result: The proposed method achieves predictive performance comparable to Deep Ensembles while using up to 15x fewer parameters. It also enhances out-of-distribution detection and typically improves calibration compared to existing baselines such as mean-field and perturbation techniques.

Conclusion: The study demonstrates that low-rank posterior parameterization can efficiently capture structured weight correlations, significantly reducing parameter overhead while maintaining uncertainty calibration and competitive generalization. This approach advances efficient Bayesian networks applicable to diverse model architectures.

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [1195] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

TL;DR: This paper surveys RL methods addressing time delays in control systems, categorizing them into five families and discussing their pros and cons. It identifies challenges and future research directions for improving RL in delay-affected systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of standard RL algorithms which assume the Markov Decision Process, often violated due to time delays in practical cyber-physical systems, affecting performance and stability.

Method: The paper formalizes delay classes, categorizes RL methods into five families, and performs a comparative analysis to provide practical guidelines.

Result: It organizes existing methods and evaluates their trade-offs, providing insights into selecting appropriate strategies for delay-affected systems.

Conclusion: The survey serves as a guide for researchers and practitioners, outlining current challenges and areas for future exploration in developing RL systems for cyber-physical systems with delays.

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [1196] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: Proposed a novel alignment framework for text-to-image models (diffusion and flow matching models) that improves computational efficiency and reduces fine-tuning needs.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods for text-to-image models rely on fine-tuning and extensive computational resources, which limit generalizability and efficiency.

Method: Introduced a framework for sampling from reward-weighted distributions using score guidance for diffusion models and velocity guidance for flow matching models. Proposed guidance network for diffusion models and training-free improvement techniques for flow matching.

Result: Achieved comparable performance to fine-tuned models with diffusion and flow matching while reducing computational costs by about 60% and avoiding the need for model finetuning.

Conclusion: The framework significantly reduces computational requirements and introduces finetuning-free and training-free alignment methods for improved scalability and efficiency without performance compromise.

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [1197] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

TL;DR: The paper introduces algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. These methods address challenges in convex optimization, tracking privacy across design matrices, and regret analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing private contextual bandits research, expanding from linear models to generalized linear models, which are more complex and lack closed-form solutions.

Method: Two algorithms are designed under different privacy models: one for shuffle differential privacy achieving $	ilde{O}(d^{3/2}oot{T}/oot{})$ regret under stochastic contexts; another for joint differential privacy achieving $	ilde{O}(doot{T}/oot{})$ regret under adversarial contexts.

Result: Both algorithms achieve favorable regret rates, avoiding dependency on an instance-specific parameter $$, and require no spectral context assumptions beyond $$-boundedness.

Conclusion: The methods extend the applicability of private contextual bandits in generalized linear models, offering practical and theoretical advances while maintaining strong privacy guarantees.

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [1198] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

TL;DR: The study introduces Topological Residual Asymmetry (TRA), a robust method for inferring causal direction between two variables by analyzing the geometry of residual clouds.


<details>
  <summary>Details</summary>
Motivation: To address the fragility of existing methods for identifying causal directions from observational data, particularly under ambiguous or nearly non-identifiable conditions.

Method: The paper proposes TRA, which detects causal direction in additive-noise models by analyzing residuals' geometric properties after standardization. It uses persistent homology metrics and introduces variance-specific methods and abstention rules.

Result: Tests prove TRA's consistent performance in low-noise scenarios and robustness in general cases with extended methods (TRA-s, TRA-C), surpassing existing causal inference techniques.

Conclusion: TRA offers a geometry-based, noise-resilient causal inference method with strong experimental validation, demonstrating its potential for broad applicability in bivariate data analysis.

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [1199] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: The paper addresses the challenges in using Poisson equations for average-reward reinforcement learning in non-ergodic contexts, proposing a unique solution methodology and stable learning pipeline.


<details>
  <summary>Details</summary>
Motivation: Poisson equations are foundational to average-reward reinforcement learning but can be ill-posed in non-ergodic settings. Addressing this issue is crucial for broader applications including reducible and periodic Markov chains.

Method: The authors analyze the structure of finite-state Markov chains and introduce a real peripheral invariant subspace to isolate non-decaying modes. They then develop a pipeline combining learning the chain structure, estimating a gauge map, and using projected stochastic approximation for solving Poisson equations.

Result: The proposed method guarantees $	ilde{O}(T^{-1/2})$ convergence and provides stable solutions for multichain and periodic regimes in average-reward reinforcement learning.

Conclusion: The work makes stable Poisson equation learning feasible beyond ergodic settings, enhancing its applications in reinforcement learning with multichain and periodic Markov chains.

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [1200] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

TL;DR: The paper addresses action-free offline reinforcement learning where datasets lack action information and proposes state policy learning techniques to enhance online RL. Empirical testing shows improved performance and learning speed.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenge of offline RL when action labels are unavailable in datasets due to privacy, storage, or sensor limitations. The study seeks practical solutions for such scenarios.

Method: The paper introduces (1) a state discretisation transformation and (2) Offline State-Only DecQN (\algo), a value-based algorithm to pre-train state policies from action-free data. Additionally, it provides a guided online learning mechanism using pre-trained state policies.

Result: The proposed approach demonstrates improvements in learning convergence speed, asymptotic performance across benchmarks, and highlights the essential role of discretisation and regularisation.

Conclusion: The paper concludes that the framework is scalable and effective for leveraging action-free datasets, providing valuable solutions for both offline and online RL settings.

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [1201] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

TL;DR: This paper proposes a training-free sampling method on Riemannian manifolds using diffusion-inspired deterministic dynamics for handling multi-modal and complex distributions.


<details>
  <summary>Details</summary>
Motivation: Sampling from un-normalized densities on Riemannian manifolds, especially for multi-modal targets, is challenging with existing methods and needs new approaches.

Method: The authors introduce a sampling algorithm using non-equilibrium deterministic dynamics to transport an easy noise distribution toward the target, respecting Riemannian geometry and leveraging standard Monte Carlo techniques.

Result: The method includes a rigorous theoretical analysis and proves effective on high-dimensional, heavy-tailed, and multi-modal sampling problems.

Conclusion: This paper extends diffusion-based sampling techniques beyond Euclidean domains, providing a robust and training-free solution for complex distributions.

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [1202] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

TL;DR: The paper analyzes the diversity loss caused by classifier-free guidance (CFG) in diffusion models, identifies the source as generative distortion, and proposes a new guidance schedule to mitigate this loss while preserving separability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of diversity loss in conditional sampling using classifier-free guidance, which affects the quality of generated samples in diffusion models.

Method: The authors use tools from statistical physics and analyze Gaussian mixtures to understand when and how distortions arise in CFG-guided sampling.

Result: CFG distorts the conditional distribution, with variance shrinkage and class overlap increasing as the number of modes grows exponentially with dimension. They validate prior findings and demonstrate current schedules are inadequate.

Conclusion: Distortion is inherent to vanilla CFG, but a new schedule introducing a negative-guidance window reduces this issue, enhancing diversity without losing class separability.

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [1203] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

TL;DR: This paper introduces a flow-inspired representation learning framework, leveraging a unique 'zero-flow criterion' to certify data conditional independence and improve learning tasks.


<details>
  <summary>Details</summary>
Motivation: Current flow-based methods excel in generative tasks but lack exploitation of their capabilities for fine-grained structural details in tasks beyond generation.

Method: The framework utilizes a rectified flow with an independent coupling approach to establish a 'zero-flow criterion,' translating it into a simulation-free loss function for various learning applications.

Result: The method demonstrates effectiveness through experiments on both simulated and real-world datasets, showcasing its ability to certify conditional independence and enable advanced representation learning.

Conclusion: The proposed framework enhances flow-based methods' applicability beyond generation, with the code made publicly available for further exploration.

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [1204] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

TL;DR: This paper provides a method for accurate Hessian spectral analysis of large-scale foundation models using stochastic Lanczos quadrature and shard-local finite-difference Hessian vector products to estimate spectral density.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome challenges in accurately analyzing the Hessian spectra of large-scale foundation models and the limitations of block-diagonal curvature approximations.

Method: The method involves shard-local finite-difference Hessian vector products compatible with fully sharded data parallelism and stochastic Lanczos quadrature to analyze up to 100B parameter models.

Result: The authors obtained large-scale spectral density estimates beyond the sub-10B regime, demonstrated Hessian approximation errors, and validated memory and runtime scaling laws for their approach.

Conclusion: This study reveals that foundation-model Hessian spectra are tractable and traditional curvature approximations are qualitatively flawed, paving the way for more accurate scaling curvature analysis.

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [1205] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

TL;DR: This paper investigates the geometric mechanisms of backdoor and data poisoning attacks in input space, revealing their spectral invisibility and defense trade-offs.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in understanding why backdoors and poisoning attacks consistently evade existing defences, with the goal of creating better preventative strategies.

Method: The authors use kernel ridge regression as a model for wide neural networks to analyze geometric mechanisms, focusing on input Hessians and gradient regularization. They conduct experiments across datasets like MNIST, CIFAR-10, and CIFAR-100.

Result: The study proves the spectral invisibility of certain attacks and establishes the trade-off between regularization-based defenses and data fitting capacity. Empirical results demonstrate this theory across multiple datasets.

Conclusion: Backdoor and poison attacks exploit input space curvature, making detection fundamentally challenging; effective defenses enforce trade-offs between safety and fitting capacity through input curvature manipulation.

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [1206] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

TL;DR: The paper investigates overfitting behavior in Sobolev spaces $W^{k,p}(\mathbb{R}^d)$ for noisy data, showing that smoothness-based interpolators can lead to harmful overfitting with bounded generalization error.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the generalization behavior and risks of excessive smoothness bias in overparameterized machine learning models, where functions fit noisy data perfectly.

Method: The authors examine approximately norm-minimizing interpolators through geometric reasoning using Sobolev inequalities and analyze generalization error under noisy label assumptions.

Result: The findings reveal that as training sample size increases, generalization error does not decrease but remains bounded from below by a positive constant with high probability.

Conclusion: Smoothness bias in Sobolev spaces can lead to harmful overfitting, challenging prior insights on generalization behavior in Hilbert spaces.

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [1207] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

TL;DR: The paper introduces the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA) to improve sampling performance in heavy-tailed and multimodal distributions when target and proposal densities are not directly evaluable.


<details>
  <summary>Details</summary>
Motivation: The challenge in sampling from heavy-tailed and multimodal distributions arises when target densities and proposal densities (as in α-stable Lévy-driven models) are inaccessible, leading to errors in existing methods which lack proper corrections.

Method: The proposed method, MAFLA, introduces a new score-based correction mechanism using proxies for fractional proposal score gradients and learns an acceptance function through Score Balance Matching.

Result: MAFLA demonstrates strong empirical performance, particularly in tasks like combinatorial optimization, significantly reducing finite-time sampling errors compared to unadjusted methods.

Conclusion: MAFLA effectively addresses limitations in existing fractional Langevin methods, providing better accuracy and empirical control over tail behavior in sampling problems.

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [1208] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

TL;DR: The paper introduces DRIO, an imputation method for multivariate time series that handles non-stationarity and systematic missingness using adversarial learning on Wasserstein ambiguity sets, achieving improved accuracy and alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the mismatch between observed and true data distributions in multivariate time series imputation, particularly under non-stationarity and systematic missing data.

Method: The method, DRIO, combines reconstruction error minimization with divergence minimization between imputed data and a worst-case distribution in a Wasserstein ambiguity set, using adversarial learning.

Result: DRIO demonstrates improved imputation performance and achieves a balance between reconstruction accuracy and distributional alignment in both random and non-random missing data settings.

Conclusion: DRIO effectively mitigates biases and enhances imputation quality in multivariate time series, offering a robust and flexible solution for diverse datasets.

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [1209] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

TL;DR: The paper introduces a decision-theoretic framework to optimize prediction sets for minimizing expected loss, proposing a method and algorithm (ROCP) with practical safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Prediction sets are useful for covering unknown outcomes with guaranteed probability, but their optimal use for decision-making, especially in scenarios like medical or safety-critical tasks, is unclear.

Method: The authors provide a framework to minimize expected loss against a worst-case distribution, derive an optimal prediction set construction subject to coverage constraints, and introduce the Risk-Optimal Conformal Prediction (ROCP) algorithm.

Result: The ROCP algorithm effectively minimizes risk while ensuring marginal coverage, outperforming existing methods in tasks like medical diagnosis where out-of-set errors have high costs.

Conclusion: This approach offers a robust way to use prediction sets in decision-making, highlighting ROCP's potential to reduce critical errors and improve outcomes in high-stakes contexts.

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [1210] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

TL;DR: The paper proposes a framework for optimizing resource allocation among individuals to maximize social welfare in dynamic settings using online learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of continuously allocating finite resources optimally to a population while accounting for stochastic utilities and social welfare functions.

Method: Introduces a confidence sequence framework that leverages monotonicity, and proposes SWF-UCB, an online learning algorithm achieving low regret in resource allocation.

Result: The SWF-UCB achieves near-optimal regret scaling with $ T$, and experiments confirm theoretical predictions while highlighting dependencies on SWF parameters.

Conclusion: The framework effectively enables scalable and principled decision-making across diverse social welfare functions and application contexts like hypothesis testing and policy evaluation.

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [1211] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

TL;DR: The paper complements previous methods in Variational Inference (VI), particularly VIMCO gradient estimators, by proposing a new estimator, VIMCO-$\star$, which addresses limitations like signal-to-noise ratio (SNR) scaling.


<details>
  <summary>Details</summary>
Motivation: Reparameterized gradient estimators in VI restrict data-generating processes and approximations, while REINFORCE estimators lack theoretical validation, creating a need for better understanding and methods.

Method: Comprehensive theoretical analysis on REINFORCE gradient estimators and introducing VIMCO-$\star$, which corrects SNR deficiencies in VIMCO gradient estimators for VI.

Result: VIMCO-$\star$ prevents SNR collapse via $\sqrt{N}$ scaling and proves more effective in empirical tests compared to existing methods, especially in challenging settings.

Conclusion: The VIMCO-$\star$ gradient estimator significantly improves optimization efficiency and performance in VI problems with constrained reparameterized gradients.

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [1212] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

TL;DR: The paper introduces a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework to enhance robust generalization in few-shot learning by adapting class-specific priors through hierarchical optimal transport, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning needs to adapt well with limited data amid distribution shifts, but current Sinkhorn DRO methods struggle due to their reliance on fixed reference distributions.

Method: The authors propose PG-DRO, which incorporates class-adaptive priors derived through hierarchical optimal transport into the Sinkhorn DRO framework, enabling better integration of few-shot information.

Result: PG-DRO outperforms standard models and existing DRO methods in robust generalization for few-shot learning scenarios.

Conclusion: PG-DRO enhances robust decision-making in few-shot learning with a novel approach that is theoretically justified and effective, resolving challenges in adapting to class-specific distributions.

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [1213] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: The paper analyzes the efficiency of sigmoid gates in mixture-of-experts (MoE) models, addressing concerns like convergence, classification performance, and the role of the temperature parameter.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding the efficacy of sigmoid gates under classification settings, their convergence properties, and the effects of temperature parameters in MoE models.

Method: A modified sigmoid gate in multinomial logistic MoE is analyzed theoretically, ensuring convergence and addressing temperature interactions. A Euclidean score is proposed as an improvement to the gating function.

Result: The sigmoid gate demonstrates lower sample complexity than the softmax gate for parameter/expert estimation. The use of a temperature parameter can lead to exponential sample complexity, which is resolved using a Euclidean score.

Conclusion: Modified sigmoid gates outperform softmax gates in sample complexity, and the use of a Euclidean score resolves temperature-related inefficiencies, improving convergence and learning efficiency.

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [1214] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

TL;DR: The paper provides a statistical foundation for Evidential Deep Learning (EDL), highlights its limitations in handling uncertainties, and proposes an enhanced method, DIP-EDL, for improved uncertainty calibration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of EDL and improve its handling of epistemic and aleatoric uncertainty, especially under distributional shift.

Method: The paper interprets EDL via amortized variational inference in a Bayesian model, identifies its issues, and proposes DIP-EDL as an alternate method that decouples predictive uncertainty into separate components.

Result: DIP-EDL demonstrates improved interpretability, robustness, and uncertainty calibration both theoretically and empirically under distributional shifts.

Conclusion: The proposed DIP-EDL method effectively addresses the limitations of EDL by improving uncertainty representation, thus making it more reliable and interpretable for practical applications.

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [1215] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

TL;DR: The paper introduces IAMA, a method to align large language models (LLMs) with diverse human preferences efficiently during inference while minimizing computational expense.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs to different human criteria is difficult due to potential conflicts among the criteria, and existing methods for inference-time alignment are computationally expensive.

Method: The authors propose IAMA, which involves training a base model adaptable to multiple tasks via various alignment algorithms at inference. They also develop a non-linear GRPO approach to address the optimization challenges within IAMA.

Result: IAMA demonstrates the capability of aligning LLMs to multiple human criteria with a reduced computational budget at inference time.

Conclusion: IAMA presents a promising solution for leveraging LLMs efficiently in multiple alignment scenarios without excessive computational demands.

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [1216] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

TL;DR: This paper introduces ST-BCP, a method for improving Backward Conformal Prediction (BCP) in uncertainty quantification by narrowing the gap between estimated and empirical coverage through a data-dependent transformation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitation in BCP, where looseness caused by Markov's inequality leads to a significant gap between estimated coverage bounds and empirical coverage.

Method: The authors propose ST-BCP, which integrates a data-dependent transformation to refine nonconformity scores. This transformation is computable and theoretically superior to the baseline method.

Result: Extensive experiments show that the proposed ST-BCP method reduces the average coverage gap from 4.20% to 1.12% on standard benchmarks.

Conclusion: ST-BCP demonstrates improved performance in narrowing the coverage gap, enhancing the reliability and precision of BCP in uncertainty quantification.

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [1217] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

TL;DR: The paper redefines transformer operations using a measure-theoretic perspective on context and attention. It studies trained attention mechanisms and establishes performance bounds under specific assumptions.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation and analysis for Transformers' ability to process contexts of unbounded length efficiently, using a principled framework.

Method: The authors model attention as an integral operator on probability measures, decompose the learning task into component recall and predictive analysis, and use shallow Transformers combined with MLPs. They derive theoretical convergence rates under certain conditions.

Result: They demonstrate that the proposed framework is theoretically sound by establishing convergence rates and matching them with minimax lower bounds under spectral assumptions.

Conclusion: The study provides a basis for designing Transformers that generalize well when handling arbitrarily long contexts, ensuring recall and prediction efficiency.

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [1218] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

TL;DR: The paper develops a method to estimate Value at Risk (VaR) in real-time using a combination of quantile regression forests and conformalized estimators to ensure reliability, validated through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and reliable real-time risk measures, like Value at Risk (VaR), has intensified due to fast-changing market conditions, challenging current methods that do not perform well in online settings.

Method: The paper introduces an Offline-Simulation-Online-Estimation (OSOA) framework. Quantile regression forests are trained offline, combined with online real-time risk factor data, and further refined using conformal calibration for reliability.

Result: Theoretical analysis confirms the consistency and validity of the proposed estimators. Numerical experiments illustrate the method's practical effectiveness for real-time risk estimation.

Conclusion: The proposed framework successfully delivers an accurate, reliable, and practical real-time estimation of Value at Risk, demonstrating its potential for effective risk monitoring and decision-making.

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [1219] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

TL;DR: This paper explores the dual role of missing data as a privacy amplification mechanism in sensitive domains such as medicine and finance, and introduces a novel viewpoint connecting data incompleteness with differential privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation is critical in sensitive fields, but managing missing data has traditionally been seen as a challenge. The paper aims to investigate whether missing data could be viewed as beneficial to privacy.

Method: The authors conceptualize missing data as a privacy-enhancing factor within the context of differential privacy, supporting this idea with a formal analysis and theoretical framework.

Result: The study demonstrates, for the first time, that missing data can provide privacy amplification for differentially private algorithms.

Conclusion: Missing data, often seen as a disadvantage, could actually act as a mechanism to enhance privacy within the realm of differential privacy, offering new insights into its utility for preserving individual confidentiality.

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [1220] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: This paper establishes a framework for stochastic interpolants in infinite-dimensional Hilbert spaces and demonstrates its utility in conditional generation and scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of stochastic interpolants, which were previously restricted to finite-dimensional settings, and extend their application to infinite-dimensional function-valued data.

Method: The study develops a rigorous theoretical framework for stochastic interpolants in infinite-dimensional Hilbert spaces, including proofs of well-posedness and error bounds, and applies the framework to complex PDE-based benchmarks for conditional generation.

Result: The proposed framework enables generative bridges between arbitrary functional distributions, achieving state-of-the-art conditional generation results in challenging scientific scenarios.

Conclusion: The framework offers a powerful, general-purpose tool for generative modeling in scientific discovery, expanding the applicability of stochastic interpolants to infinite-dimensional settings and functional data.

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [1221] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: The paper proposes a training-free conditional diffusion model for efficiently simulating parameter-dependent stochastic differential equations (SDEs).


<details>
  <summary>Details</summary>
Motivation: Simulating SDEs with varying parameters typically requires numerous costly high-fidelity simulations, and existing machine learning methods are either computationally expensive or not well-equipped to handle continuous parameter dependencies.

Method: The approach introduces a training-free conditional diffusion model utilizing a kernel-weighted Monte Carlo estimator to approximate the conditional score function, enabling interpolation across state and parameter spaces.

Result: The model efficiently generates trajectory samples for any parameter value within the training range, demonstrated through three numerical examples showcasing accurate performance across varying parameter conditions.

Conclusion: The proposed method accelerates parameter studies and applications like uncertainty quantification and real-time filtering, offering a computationally efficient alternative to traditional approaches.

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [1222] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: This paper examines the effect of high-order statistics on the learning dynamics of two-layer neural networks using a moment-controllable non-Gaussian data model.


<details>
  <summary>Details</summary>
Motivation: To study how statistical moments, such as skewness and kurtosis, influence the learning dynamics of neural networks in understanding data representation beyond mere mean and covariance.

Method: The paper uses samples from a generative two-layer NN, which incorporates Hermite polynomials to control high-order cumulants. Controlled online learning experiments and pretraining on Fashion-MNIST validate the approach.

Result: The experiments demonstrate a progressive learning pattern where networks capture low-order statistics first and gradually learn high-order cumulants. Additional experiments with Fashion-MNIST verify the approach's utility in realistic scenarios.

Conclusion: This approach offers a framework for exploring distributional effects in machine learning by bridging simple data assumptions and complex real-world data characteristics.

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [1223] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

TL;DR: This paper examines PCA for probability measures observed in a double asymptotic regime and derives convergence rates showing sparse-to-dense transitions in behavior, confirmed by numerical results.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding convergence rates of PCA when working with multiple probability measures, each observed through samples.

Method: Derived convergence rates in a double asymptotic regime, analyzing the empirical covariance operator and PCA excess risk, supported by minimax optimality proofs and numerical experiments.

Result: Convergence rates of $n^{-1/2} + m^{-α}$ clarify the relationship between number of measures ($n$) and per-measure samples ($m$), showing a sparse-to-dense transition.

Conclusion: This work identifies optimal convergence behaviors of PCA on multiple probability measures and evidences computational efficiency trade-offs for sparse and dense settings.

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>


### [1224] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

TL;DR: The paper presents a transfer learning framework for regression using generative models and quantile matching for data augmentation to enhance performance in data-scarce target domains.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of improving predictive accuracy in a data-scarce target domain by leveraging information from heterogeneous source domains.

Method: A conditional generative model is created for each source domain, with conditional quantile matching used to align generated responses to the distribution of the target domain. This corrects discrepancies without relying on covariate or label shift assumptions, allowing principled data augmentation for the target domain.

Result: Theoretical analysis shows that their framework achieves a tighter excess risk bound compared to target-only methods, supported by new convergence rates for the quantile matching estimator. Practical evaluations prove consistent performance improvement over alternative transfer learning approaches.

Conclusion: The proposed framework provides a robust solution for data augmentation in transfer learning, improving prediction accuracy in data-scarce domains both theoretically and empirically.

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [1225] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

TL;DR: This paper develops a general framework for guaranteeing generalization in multi-dimensional hyperparameter tuning, extending statistical foundations for data-driven algorithm design.


<details>
  <summary>Details</summary>
Motivation: To address the lack of statistical guarantees in data-driven multi-dimensional hyperparameter tuning, as existing work focuses on simple, one-dimensional scenarios.

Method: The framework leverages real algebraic geometry to enhance generalization guarantees for semi-algebraic function classes, and extends analysis for validation loss in hyperparameter tuning.

Result: Sharper and broadly applicable generalization guarantees are derived, leading to new learnability results for weighted group lasso and weighted fused lasso.

Conclusion: The proposed methods significantly advance the theoretical understanding of multi-dimensional hyperparameter tuning, demonstrating wide applicability and potential improvements in algorithm design.

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [1226] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

TL;DR: The paper investigates the efficiency of full-batch gradient descent (GD) versus one-pass stochastic gradient descent (SGD) in learning a single-index model with a quadratic activation. It shows full-batch GD can achieve higher efficiency with certain modifications.


<details>
  <summary>Details</summary>
Motivation: To address the unclear theoretical advantage of full-batch GD over one-pass stochastic GD, particularly beyond linear regression, and improve statistical efficiency in gradient-based learning.

Method: The authors analyze full-batch GD using a correlation loss and evaluate the performance improvement by truncating activation functions. Additionally, they analyze sampled-based recovery and optimization landscapes with theoretical guarantees for small initialization.

Result: The study demonstrates that full-batch GD can outperform one-pass SGD statistically when using truncated activations and fewer samples. It achieves strong recovery with $n \gtrsim d$ samples and $T \gtrsim \log d$ gradient steps.

Conclusion: Full-batch GD has enhanced statistical efficiency over one-pass SGD via activation truncation, especially benefiting from improved optimization landscapes and recovery mechanisms.

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [1227] [LOGOS-CA: A Cellular Automaton Using Natural Language as State and Rule](https://arxiv.org/abs/2602.00036)
*Keishu Utimula*

Main category: nlin.CG

TL;DR: This paper introduces LOGOS-CA, a cellular automaton framework using language models for rule updates, enabling richer state and simulation possibilities.


<details>
  <summary>Details</summary>
Motivation: Explore how the expressive power of language, as seen in advances in LLMs, can enhance traditional cellular automata beyond numerical constraints.

Method: Design and implement LOGOS-CA, where cell states and rules of cellular automata are described in natural language and updated using an LLM.

Result: LOGOS-CA successfully performed simulations such as forest fire modeling and highlighted potential in Artificial Life research.

Conclusion: LOGOS-CA opens new pathways for richer and nuanced simulations by combining natural language features with cellular automata, encouraging further exploration.

Abstract: Large Language Models (LLMs), trained solely on massive text data, have achieved high performance on the Winograd Schema Challenge (WSC), a benchmark proposed to measure commonsense knowledge and reasoning abilities about the real world. This suggests that the language produced by humanity describes a significant portion of the world with considerable nuance. In this study, we attempt to harness the high expressive power of language within cellular automata. Specifically, we express cell states and rules in natural language and delegate their updates to an LLM. Through this approach, cellular automata can transcend the constraints of merely numerical states and fixed rules, providing us with a richer platform for simulation. Here, we propose LOGOS-CA (Language Oriented Grid Of Statements - Cellular Automaton) as a natural framework to achieve this and examine its capabilities. We confirmed that LOGOS-CA successfully performs simple forest fire simulations and also serves as an intriguing subject for investigation from an Artificial Life (ALife) perspective. In this paper, we report the results of these experiments and discuss directions for future research using LOGOS-CA.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1228] [Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning](https://arxiv.org/abs/2602.00701)
*Mohamed Saleh,Zahra Ahmadi*

Main category: cs.MM

TL;DR: This paper introduces CMQKA, a novel and energy-efficient multimodal fusion mechanism that achieves linear complexity for effective hierarchical audio-visual integration, and demonstrates state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing multimodal fusion methods that either face high computational complexity or rely on simplistic strategies incapable of capturing deep cross-modal dependencies.

Method: The paper introduces CMQKA, a linear complexity cross-modal Query-Key attention mechanism, and integrates it into a hierarchical system, SNNergy, using event-driven binary spike operations for scalable and energy-efficient multimodal fusion.

Result: SNNergy achieves state-of-the-art performance on audio-visual fusion benchmarks such as CREMA-D, AVE, and UrbanSound8K-AV, while significantly improving energy efficiency.

Conclusion: The work advances multimodal fusion by offering a scalable and practical cross-modal integration approach suitable for real-world applications requiring computational and energy efficiency.

Abstract: Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems.

</details>


### [1229] [Seeing, Hearing, and Knowing Together: Multimodal Strategies in Deepfake Videos Detection](https://arxiv.org/abs/2602.01284)
*Chen Chen,Dion Hoe-Lian Goh*

Main category: cs.MM

TL;DR: The study explored human strategies for detecting deepfake videos, finding that multimodal cues like visual, vocal, and intuition improve identification. Results will inform media literacy intervention design.


<details>
  <summary>Details</summary>
Motivation: To understand how humans detect deepfake videos and use that understanding to create better media literacy tools.

Method: 195 participants aged 21-40 judged real and deepfake videos, rated confidence, and reported cues used. Association rule mining analyzed successful cue combinations.

Result: Participants were better at identifying real videos than deepfakes. Visual, vocal, and intuitive cues were most effective in successful identifications.

Conclusion: Multimodal cues enhance deepfake video detection. Insights suggest pathways for creating media literacy tools to guide effective visual and auditory cue usage, boosting resilience against deceptive media.

Abstract: As deepfake videos become increasingly difficult for people to recognise, understanding the strategies humans use is key to designing effective media literacy interventions. We conducted a study with 195 participants between the ages of 21 and 40, who judged real and deepfake videos, rated their confidence, and reported the cues they relied on across visual, audio, and knowledge strategies. Participants were more accurate with real videos than with deepfakes and showed lower expected calibration error for real content. Through association rule mining, we identified cue combinations that shaped performance. Visual appearance, vocal, and intuition often co-occurred for successful identifications, which highlights the importance of multimodal approaches in human detection. Our findings show which cues help or hinder detection and suggest directions for designing media literacy tools that guide effective cue use. Building on these insights can help people improve their identification skills and become more resilient to deceptive digital media.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [1230] [On finite-dimensional encoding/decoding theorems for neural operators](https://arxiv.org/abs/2602.00068)
*Vinícius Luz Oliveira,Vladimir G. Pestov*

Main category: math.FA

TL;DR: The paper investigates neural networks with infinite-dimensional operators, focusing on finite-dimensional approximation of continuous mapping between function spaces for differential equations. It extends known results, eliminating assumptions on normed spaces, and discusses conditions for smooth mappings.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the capability of neural operator networks to efficiently learn solutions to differential equations by using finite-dimensional approximations in infinite-dimensional contexts.

Method: The authors analyze the applicability of the finite-dimensional encoding/decoding theorems for mappings between function spaces, removing standard assumptions like the approximation property, and special consideration of $C^k$-smooth mappings.

Result: The findings prove that the approximation result extends to arbitrary locally convex spaces without restrictions, while analogous results for smooth mappings require the approximation property for the space $E$.

Conclusion: These results broaden the scope of neural operator networks for differential equations, particularly highlighting the relevance of non-normable locally convex spaces in this implementation avenue.

Abstract: Recently, versions of neural networks with infinite-dimensional affine operators inside the computational units (``neural operator'' networks) have been applied to learn solutions to differential equations. To enable practical computations, one employs finite-dimensional encoding/decoding theorems of the following kind: every continuous mapping $f$ between function spaces $E$ and $F$ is approximated in the topology of uniform convergence on compacta by continuous mappings factoring through two finite dimensional Banach spaces. Such a result is known (Kovachki et al., 2023) for $E,F$ being Banach spaces having the approximation property. We point out that the result needs no assumptions on $E,F$ whatsoever and remains true not only for all normed spaces, but for arbitrary locally convex spaces as well. At the same time, an analogous result for $C^k$-smooth mappings and the $C^k$ compact open topology, $k\geq 1$, holds if and only if the space $E$ has the approximation property. This analysis may be useful already because non-normable locally convex function spaces are common in the theory of differential equations, the main field of applications for the emerging theory.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [1231] [Vortex Stretching in the Navier-Stokes Equations and Information Dissipation in Diffusion Models: A Reformulation from a Partial Differential Equation Viewpoint](https://arxiv.org/abs/2602.01071)
*Tsuyoshi Yoneda*

Main category: math.AP

TL;DR: This paper proposes a new inverse-time framework for studying vortex stretching using PDEs, neural networks, and score-based diffusion models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand and analyze vortex stretching dynamics in the Navier-Stokes equations by addressing the challenge posed by its inverse-time property and mathematical ill-posedness.

Method: The authors used a score function-based drift term to reformulate the inverse-time dynamics, coupled with a discrete Lagrangian approach. The score function was trained using a neural network to compute backward-time particle trajectories.

Result: Numerical results showed that initial position information dissipates quickly along compressive directions but remains preserved in stretching directions.

Conclusion: The study successfully demonstrates how neural networks and score-based diffusion models can assist in analyzing complex vortex dynamics in fluid flows.

Abstract: We present a new inverse-time formulation of vortex stretching in the Navier-Stokes equations, based on a PDE framework inspired by score-based diffusion models. By absorbing the ill-posed backward Laplacian arising from time reversal into a drift term expressed through a score function, the inverse-time dynamics are formulated in a Lagrangian manner. Using a discrete Lagrangian flow of an axisymmetric vortex-stretching field, the score function is learned with a neural network and employed to construct backward-time particle trajectories. Numerical results demonstrate that information about initial positions is rapidly lost in the compressive direction, whereas it is relatively well preserved in the stretching direction.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1232] [Minimax optimal differentially private synthetic data for smooth queries](https://arxiv.org/abs/2602.01607)
*Rundong Ding,Yiyun He,Yizhe Zhu*

Main category: math.ST

TL;DR: The paper explores improving utility guarantees for differentially private synthetic data by focusing on smooth queries rather than broad query classes and proposes an algorithm achieving optimal error rates.


<details>
  <summary>Details</summary>
Motivation: To enhance the practical utility of differentially private synthetic data by leveraging the smoothness of common data queries.

Method: The authors introduce a polynomial-time algorithm that achieves minimax error rates for $k$-smooth queries in $(,δ)$-differentially private synthetic data generation.

Result: The algorithm achieves error rates of $n^{-	ext{min} \{1, \frac{k}{d}\}}$ (up to a $($log $n)$ factor), and a phase transition is highlighted when $k=d$. Minimax lower bounds are also established for $k$-smooth queries.

Conclusion: By exploiting smoothness in data queries, this work improves utility guarantees for synthetic data generation, achieving better error rates and theoretical bounds.

Abstract: Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.
  We study the problem of generating $(\varepsilon,δ)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\min \{1, \frac{k}{d}\}}$, up to a $\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\varepsilon,δ)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\varepsilon$-differential privacy in (Boedihardjo et al., 2024).

</details>


### [1233] [Handling Covariate Mismatch in Federated Linear Prediction](https://arxiv.org/abs/2602.02083)
*Alexis Ayme,Rémi Khellaf*

Main category: math.ST

TL;DR: The paper explores federated learning where clients have mismatched subsets of features and introduces methods for linear prediction under these conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of privacy constraints and covariate mismatch in federated learning, especially in multicenter collaborations with varied data collection setups.

Method: The study proposes two modular methods: a plug-in estimator for low-dimensional data and an impute-then-regress strategy for high-dimensional data.

Result: Both approaches are analyzed for their learning rates, demonstrating how they perform in different dimensional and communication settings under client-specific features.

Conclusion: The proposed methods effectively enable federated learning under covariate mismatch, providing theoretical guarantees for their performance.

Abstract: Federated learning enables institutions to train predictive models collaboratively without sharing raw data, addressing privacy and regulatory constraints. In the standard horizontal setting, clients hold disjoint cohorts of individuals and collaborate to learn a shared predictor. Most existing methods, however, assume that all clients measure the same features. We study the more realistic setting of covariate mismatch, where each client observes a different subset of features, which typically arises in multicenter collaborations with no prior agreement on data collection. We formalize learning a linear prediction under client-wise MCAR patterns and develop two modular approaches tailored to the dimensional regime and communication budget. In the low-dimensional setting, we propose a plug-in estimator that approximates the oracle linear predictor by aggregating sufficient statistics to estimate the covariance and cross-moment terms. In higher dimensions, we study an impute-then-regress strategy: (i) impute missing covariates using any exchangeability-preserving imputation procedure, and (ii) fit a ridge-regularized linear model on the completed data. We provide asymptotic and finite-sample learning rates for our predictors, explicitly characterizing their behaviour with the global dimension, the client-specific feature partition, and the distribution of samples across sites.

</details>


### [1234] [New explanations and inference for least angle regression](https://arxiv.org/abs/2602.02491)
*Karl B. Gregory,Daniel J. Nordman*

Main category: math.ST

TL;DR: The paper introduces a novel framework for inference with the LAR algorithm, elucidating its inner workings, providing new mathematical properties, and addressing its termination point and inference capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding regarding the behavioral properties and appropriate termination point of the Least Angle Regression (LAR) algorithm.

Method: The study develops a novel framework explaining LAR as a pathway connecting response means to regressor variables. It investigates mathematical properties of LAR correlations and introduces a modified bootstrap method for better inference and uncertainty quantification.

Result: The developed framework identifies independent normal distributions in non-zero population correlations and a formal rule for stopping LAR. The modified bootstrap addressed previous limitations for inference purposes.

Conclusion: This research enhances understanding of LAR by establishing clear rules for termination, better interpreting variable contributions, and introducing effective tools for uncertainty quantification in regression problems.

Abstract: Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart "path" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population "correlation" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [1235] [FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution](https://arxiv.org/abs/2602.00948)
*Mingxi Zou,Jiaxiang Chen,Aotian Luo,Jingyi Dai,Chi Zhang,Dongning Sun,Zenglin Xu*

Main category: physics.soc-ph

TL;DR: Conventional financial strategy evaluations miss interactions and evolutions in strategies; FinEvo introduces an ecological game model to explore dynamic strategy adaptation and interactions in financial markets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of isolated backtests in static environments for financial strategies and better understand the dynamics, persistence, and interactions among trading strategies.

Method: FinEvo models financial strategies as adaptive agents within a shared market. It utilizes evolutionary dynamics through mechanisms such as selection, innovation, and environmental perturbation, incorporating signals from historical prices and external news.

Result: FinEvo reveals patterns such as strategy dominance, collapse, and coalitions that are invisible in static evaluations. Experiments confirm its stability and capability to reproduce realistic, dynamic financial market behaviors.

Conclusion: FinEvo provides a principled framework to analyze robustness, adaptation, and emergent dynamics in multi-agent financial strategies, offering insights into the impact of policies and regulations on markets.

Abstract: Conventional financial strategy evaluation relies on isolated backtests in static environments. Such evaluations assess each policy independently, overlook correlations and interactions, and fail to explain why strategies ultimately persist or vanish in evolving markets. We shift to an ecological perspective, where trading strategies are modeled as adaptive agents that interact and learn within a shared market. Instead of proposing a new strategy, we present FinEvo, an ecological game formalism for studying the evolutionary dynamics of multi-agent financial strategies. At the individual level, heterogeneous ML-based traders-rule-based, deep learning, reinforcement learning, and large language model (LLM) agents-adapt using signals such as historical prices and external news. At the population level, strategy distributions evolve through three designed mechanisms-selection, innovation, and environmental perturbation-capturing the dynamic forces of real markets. Together, these two layers of adaptation link evolutionary game theory with modern learning dynamics, providing a principled environment for studying strategic behavior. Experiments with external shocks and real-world news streams show that FinEvo is both stable for reproducibility and expressive in revealing context-dependent outcomes. Strategies may dominate, collapse, or form coalitions depending on their competitors-patterns invisible to static backtests. By reframing strategy evaluation as an ecological game formalism, FinEvo provides a unified, mechanism-level protocol for analyzing robustness, adaptation, and emergent dynamics in multi-agent financial markets, and may offer a means to explore the potential impact of macroeconomic policies and financial regulations on price evolution and equilibrium.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1236] [Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems](https://arxiv.org/abs/2602.01701)
*Ruyu Li,Tinghui Zhang,Haodi Ma,Daisy Zhe Wang,Yifan Wang*

Main category: cs.DB

TL;DR: Meta Engine is a unified semantic query engine designed to integrate heterogeneous, specialized LLM-based query systems, significantly improving query performance.


<details>
  <summary>Details</summary>
Motivation: Growing use of multi-modal data and the rise of LLM-based semantic query systems have created challenges like fragmented ecosystems, disparate APIs, and trade-offs between generality and specialization.

Method: This paper proposes the Meta Engine, consisting of five components: NL Query Parser, Operator Generator, Query Router, Adapters, and Result Aggregator, to unify and resolve integration issues among semantic query systems.

Result: Meta Engine achieved 3-6x higher F1 scores in general and up to 24x improvement on specific datasets compared to baselines.

Conclusion: Meta Engine effectively integrates specialized and general LLM query systems into a robust framework that resolves integration issues and enhances performance for multi-modal semantic queries.

Abstract: With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.
  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some "all-in-one" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.
  This paper introduces Meta Engine, a novel "query system on query systems", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1237] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

TL;DR: The paper addresses the challenges in CTR prediction caused by evolving user interests and proposes DiseCTR, a causal and disentangled approach to tackle the out-of-distribution issue.


<details>
  <summary>Details</summary>
Motivation: Standard CTR prediction methods struggle with the out-of-distribution problem as they assume static user interests, which can evolve over time in dynamic online environments.

Method: DiseCTR performs causal factorization of CTR involving user interest, exposure model, and click model. It uses sparse attention encoders, weakly supervised interest disentanglers, and attentive aggregators to capture and integrate dynamic user interests effectively.

Result: DiseCTR demonstrated improved accuracy and robustness for CTR prediction across three real-world datasets, significantly improving metrics like AUC and GAUC while reducing logloss.

Conclusion: DiseCTR provides a novel solution for OOD challenges in CTR prediction by disentangling user interests through causal inference mechanisms. It improves generalization and prediction reliability, paving the way for more adaptive recommendation systems.

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [1238] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

TL;DR: The paper proposes a Mixture-of-Experts (MoE) framework to improve multilingual search relevance in e-commerce, addressing linguistic and cultural diversity challenges with improved efficiency and cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: Multilingual e-commerce search systems face challenges due to varying linguistic, cultural, and product catalog contexts that affect relevance modeling. Existing monolithic models struggle with data diversity, gaps, and high costs.

Method: This study introduces a scalable Mixture-of-Experts (MoE) framework that uses dynamic routing to expert models and concatenates their embeddings. It also introduces an optimized offline batch pipeline to enhance resource efficiency.

Result: The proposed MoE framework improved AUC by 0.72 percentage points and increased throughput to 27.6 QPS, showcasing superior multilingual relevance and efficient resource utilization.

Conclusion: The framework demonstrates strong multilingual relevance, efficiency, and cost-effectiveness, making it suitable for real-world e-commerce search systems dealing with diverse markets.

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [1239] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

TL;DR: The paper introduces ChunkNorris, a heuristic-based technique to optimize parsing and chunking in PDF documents for better performance in Retrieval-Augmented Generation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research lies in addressing the inefficiencies in document parsing and chunking for Retrieval-Augmented Generation (RAG), which critically affect Information Retrieval and response accuracy.

Method: The method involves a heuristic-based approach for parsing and chunking without relying on machine learning, focusing on simplicity and computational efficiency.

Result: ChunkNorris outperforms existing parsing and chunking methods in execution time, energy consumption, and retrieval accuracy as showcased through benchmarks and an open-access dataset.

Conclusion: The study demonstrates that heuristic-based solutions like ChunkNorris can serve as practical and efficient alternatives for real-world RAG tasks, especially in resource-constrained environments.

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [1240] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

TL;DR: This paper introduces a Large Language Model-based framework for designing systematic review search strategies, showcasing high performance and recall rates.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for systematic reviews are resource-intensive and subjective, while automated methods often underperform. A more efficient and effective approach is needed.

Method: The authors developed an LLM-based chained prompt engineering framework that mimics manual search design processes to refine systematic review search strategies.

Result: The framework demonstrated a 0.9 average recall on the LEADSInstruct dataset, outperforming existing methods.

Conclusion: LLM-based pipelines can create high-performing, transparent, and scalable search strategies for evidence synthesis, improving the systematic review process.

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [1241] [C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://arxiv.org/abs/2602.00004)
*Yue Yu,Ting Bai,HengZhi Lan,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Chuan Shi*

Main category: cs.IR

TL;DR: This paper introduces C$^2$-Cite, a framework for improving citation generation in LLMs, ensuring better integration of cited knowledge in text.


<details>
  <summary>Details</summary>
Motivation: Existing attributed LLMs fail to integrate citation markers contextually, leading to poor citation quality and disjointed references.

Method: Proposes a Contextual-aware Citation generation framework (C$^2$-Cite), utilizing a contextual citation alignment mechanism to better link citation markers with their referenced content.

Result: C$^2$-Cite surpasses state-of-the-art baselines by 5.8% in citation quality and 17.4% in response correctness on the ALCE benchmark.

Conclusion: The proposed approach enhances LLM attribution by contextually aligning citation markers, ensuring more accurate and credible outputs.

Abstract: The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \textbf{C}ontextual-aware \textbf{C}itation generation framework (\textbf{C$^2$}-\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\% in citation quality and 17.4\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite

</details>


### [1242] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

TL;DR: The study evaluates the use of AI with RAG for clinical trial protocol data extraction, showing better accuracy and efficiency compared to standalone LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the rising complexity and inefficiencies in clinical trial protocols and create smarter, streamlined data management.

Method: Developing an AI system with generative LLMs and RAG for extracting protocol data, followed by accuracy testing and workflow simulations.

Result: The RAG process achieved higher accuracy (87.8%) than standalone LLMs (62.6%) and saved 40% time. Users found it less demanding and preferred it.

Conclusion: AI-assisted extraction systems could improve clinical workflows, save time, and validate protocol intelligence at scale, though expert oversight is crucial.

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [1243] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

TL;DR: This paper introduces SPARC-RAG, a multi-agent framework improving upon traditional Retrieval-Augmented Generation (RAG) for multi-hop QA by enhancing inference-time scaling and implementing a unified context management mechanism.


<details>
  <summary>Details</summary>
Motivation: To address challenges in RAG's performance on multi-hop QA, including inefficiencies and errors due to naive scaling leading to context contamination and diminishing returns with increased computation.

Method: The authors propose SPARC-RAG, which uses specialized agents to unify sequential and parallel scaling. These agents manage a shared global context, generate complementary sub-queries for diverse exploration, and regulate decisions based on evidence grounding. A lightweight fine-tuning method further optimizes scaling behavior.

Result: SPARC-RAG outperforms previous RAG baselines with an average +6.2 F1 improvement while maintaining lower inference costs across single- and multi-hop QA benchmarks.

Conclusion: The proposed SPARC-RAG framework successfully integrates efficient scaling and context management, providing improved performance and efficiency for multi-hop question answering challenges.

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [1244] [RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment](https://arxiv.org/abs/2602.00682)
*Yuecheng Li,Hengwei Ju,Zeyu Song,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: The paper proposes RecGOAT, a framework for aligning representations from large language models (LLMs) in multimodal recommendation systems, achieving better performance and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental representational divergence between large models optimized for semantic tasks and recommendation models reliant on sparse ID features, which limits recommendation performance.

Method: RecGOAT employs graph attention networks to model collaborative relationships and a dual-granularity alignment framework using cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT) for semantic alignment.

Result: Extensive experiments confirm RecGOAT's state-of-the-art performance on public benchmarks and its effectiveness in large-scale industrial applications.

Conclusion: The study demonstrates that RecGOAT effectively bridges semantic gaps, enhancing both recommendation accuracy and scalability, with robust theoretical and empirical validation.

Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.

</details>


### [1245] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

TL;DR: The paper introduces RANKVIDEO, a reasoning-based reranker for video retrieval, which significantly improves retrieval performance using a two-stage training approach.


<details>
  <summary>Details</summary>
Motivation: To advance reasoning-based reranking, specifically for video retrieval, addressing its underexplored nature compared to text-centric reranking.

Method: They propose RANKVIDEO, trained with a two-stage curriculum: supervised fine-tuning and reranking training, combined with a data synthesis pipeline for reasoning over query-video pairs.

Result: RANKVIDEO achieved a 31% improvement on nDCG@10 on the MultiVENT 2.0 benchmark, outperforming comparable models in efficiency and effectiveness.

Conclusion: RANKVIDEO demonstrates the potential of reasoning-based models for video retrieval, setting a new standard in performance within two-stage frameworks.

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


### [1246] [Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment](https://arxiv.org/abs/2602.01023)
*Kai Yuan,Anthony Zheng,Jia Hu,Divyanshu Sheth,Hemanth Velaga,Kylee Kim,Matteo Guarrera,Besim Avci,Xuetao Yin,Rajyashree Mukherjee,Sean Suchter*

Main category: cs.IR

TL;DR: The paper introduces a new Query Auto-Completion (QAC) framework combining Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO) to improve efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Existing QAC systems struggle with limited coverage, hallucination, safety risks, and scalability challenges.

Method: The framework integrates RAG for retrieval, multi-objective DPO for optimization, and iterative critique-revision for synthetic data. It uses rule-based, model-based, and LLM-as-judge verifiers.

Result: Tests on a commercial search platform show improved metrics, better user preference scores, reduced keystrokes, and increased adoption of suggestions, validating its effectiveness.

Conclusion: The unified generation approach powered by LLMs, RAG, and DPO offers significant advancements in QAC, demonstrating potential for broader applications in search and recommendation systems.

Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.

</details>


### [1247] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

TL;DR: E-commerce rankings face Position Bias in user feedback, affecting relevance. State-of-the-art deep learning struggles in high-bias scenarios, but Linear-PAL overcomes this with better ranking quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address Position Bias in e-commerce ranking systems caused by users’ tendency to interact with top-ranked items regardless of their relevance.

Method: Developed Linear-PAL, employing structural constraints with explicit feature conjunctions, aggressive regularization, and introducing Vectorized Integer Hashing for computational efficiency.

Result: Linear-PAL improves ranking quality, achieving Relevance AUC of 0.7626 over deep ensembles (0.6736), and significantly reduces training time by 43x.

Conclusion: Linear-PAL offers a lightweight, efficient solution for de-biasing, enabling high-quality personalized rankings and near real-time system updates.

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1248] [Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations](https://arxiv.org/abs/2602.01568)
*Hamzah Khan,Dong Ho Lee,Jingqi Li,Tianyu Qiu,Christian Ellis,Jesse Milzman,Wesley Suttle,David Fridovich-Keil*

Main category: cs.GT

TL;DR: The paper addresses multi-robot coordination with mixed hierarchical game theories, proposes a quasi-policy approximation, and demonstrates real-time problem solving with an inexact Newton method.


<details>
  <summary>Details</summary>
Motivation: Solving multi-robot coordination problems involving complex mixed decision-making structures (Nash and Stackelberg) is challenging due to high-order policy derivatives, making traditional solvers inadequate.

Method: The authors develop a quasi-policy approximation to bypass higher-order best-response derivatives and propose an inexact Newton method to solve approximated problems efficiently, implemented in a Julia library.

Result: The paper proves local exponential convergence for the proposed method and successfully demonstrates its real-time capability through simulated experiments.

Conclusion: The proposed approach efficiently handles forest-structured mixed-hierarchy games and enables real-time application in multi-robot coordination scenarios.

Abstract: Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [1249] [Non-Clashing Teaching in Graphs: Algorithms, Complexity, and Bounds](https://arxiv.org/abs/2602.00657)
*Sujoy Bhore,Liana Khazaliya,Fionn Mc Inerney*

Main category: cs.CC

TL;DR: This paper addresses the positive non-clashing teaching for closed neighborhoods in graphs, offering improved algorithms and comprehensive combinatorial bounds in comparison to prior research on balls in graphs.


<details>
  <summary>Details</summary>
Motivation: To explore the positive non-clashing teaching in the concept class of closed neighborhoods in graphs, which has broad applicability by equivalence with finite binary concept classes.

Method: The study focuses on algorithmic improvements such as FPT algorithms for general parameters, complemented by combinatorial upper and lower bounds for classes of graphs.

Result: Improved algorithmic results, including FPT algorithms for general parameters; stronger lower bounds; and broader combinatorial upper bounds across graph classes.

Conclusion: Positive non-clashing teaching for closed neighborhoods is more general and offers improved tractability compared to prior work on graph-based concept classes.

Abstract: Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and proved that it is the most efficient batch machine teaching model satisfying the collusion-avoidance benchmark established in the seminal work of Goldman and Mathias [COLT 1993]. Recently, (positive) non-clashing teaching was thoroughly studied for balls in graphs, yielding numerous algorithmic and combinatorial results. In particular, Chalopin et al. [COLT 2024] and Ganian et al. [ICLR 2025] gave an almost complete picture of the complexity landscape of the positive variant, showing that it is tractable only for restricted graph classes due to the non-trivial nature of the problem and concept class.
  In this work, we consider (positive) non-clashing teaching for closed neighborhoods in graphs. This concept class is not only extensively studied in various related contexts, but it also exhibits broad generality, as any finite binary concept class can be equivalently represented by a set of closed neighborhoods in a graph. In comparison to the works on balls in graphs, we provide improved algorithmic results, notably including FPT algorithms for more general classes of parameters, and we complement these results by deriving stronger lower bounds. Lastly, we obtain combinatorial upper bounds for wider classes of graphs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1250] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

TL;DR: This paper develops a framework, Constitutional Evolution, for discovering behavioral norms in multi-agent large language model systems. It uses a simulation to test societal stability and evolves constitutions for maximizing social welfare.


<details>
  <summary>Details</summary>
Motivation: Current approaches to AI alignment focus on single-model systems and fixed principles, but multi-agent systems require addressing emergent dynamics caused by agent interactions.

Method: The study employs grid-world simulations under survival pressure, introduces a societal stability score (S) to quantify performance, and uses LLM-driven genetic programming with multi-island evolution to evolve optimal constitutions.

Result: Evolved constitutions outperform human-designed baselines by 123%, achieving high societal stability scores and eliminating conflict. Reducing communication emerges as a key factor for improved cooperation.

Conclusion: Cooperative norms in multi-agent systems can emerge through autonomous evolution rather than needing explicit design, showing promise for scalable AI alignment in complex systems.

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [1251] [A-MapReduce: Executing Wide Search via Agentic MapReduce](https://arxiv.org/abs/2602.01331)
*Mingju Chen,Guibin Zhang,Heng Chang,Yuchen Guo,Shiji Zhou*

Main category: cs.MA

TL;DR: A-MapReduce, a framework designed for large-scale wide search problems, improves performance, efficiency, and execution time in multi-agent systems inspired by the MapReduce paradigm.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems struggle in wide search tasks due to their focus on sequential, vertically structured reasoning. This paper aims to address inefficiency and poor execution in such tasks.

Method: The proposed A-MapReduce framework adapts the MapReduce model for wide search, using parallel processing, adaptive task decomposition, result aggregation, and experiential memory to enhance performance.

Result: Experiments on agentic benchmarks show state-of-the-art performance with up to 17.50% improvements in Item F1, reduced running time by 45.8%, and superior cost-efficiency.

Conclusion: A-MapReduce bridges the gap in handling wide search tasks within multi-agent systems, demonstrating significant improvements in performance and efficiency.

Abstract: Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.

</details>


### [1252] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

TL;DR: The study finds that self-organizing multi-agent systems using LLMs fail to consistently perform as well as or better than their expert agents within autonomous team interactions, with performance losses up to 37.6%.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand how well self-organizing LLM teams perform in unconstrained coordination settings compared to fixed-role systems, tackling the question of whether emergent coordination is effective in leveraging expertise.

Method: The study uses ML and human-inspired benchmarks to analyze the performance of LLM teams, decomposing performance failures to identify bottlenecks such as expert leveraging, and uses conversational analysis to examine team behaviors.

Result: The analysis reveals that LLM teams often underperform due to failures in leveraging expert input, showing a tendency toward integrative compromise that diminishes performance but improves robustness to adversarial agents.

Conclusion: Self-organizing multi-agent systems have a significant gap in their ability to fully harness collective expertise, highlighting a trade-off between alignment and effective utilization of expertise.

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [1253] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

TL;DR: The paper introduces TABX, a modular and high-throughput sandbox designed for evaluating multi-agent reinforcement learning (MARL) algorithms, leveraging JAX for scalable and customizable environments.


<details>
  <summary>Details</summary>
Motivation: The authors identified a lack of modularity in existing MARL benchmarks, which limits the creation of custom evaluation scenarios, and sought to create a solution enabling more systematic investigations into MARL.

Method: They developed TABX, a sandbox environment powered by JAX, offering highly modular and reconfigurable multi-agent tasks, utilizing hardware-accelerated GPUs for reduced computational costs and massively parallel execution.

Result: TABX allowed better investigation of MARL behaviors and algorithmic trade-offs, offering researchers a scalable, fast, and extensible platform for MARL research.

Conclusion: TABX addresses the limitations of existing MARL benchmarks by providing a scalable, high-performance environment for MARL, facilitating future research in the field.

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1254] [On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations](https://arxiv.org/abs/2602.01582)
*Haoyu Lei,Mohammad Jalali,Chin Wa Lau,Farzan Farnia*

Main category: cs.IT

TL;DR: Recent AI-based error correction decoders show better nominal performance under AWGN channels but are less robust to adversarial perturbations compared to traditional belief-propagation decoders.


<details>
  <summary>Details</summary>
Motivation: To understand the source of performance improvements in AI-based error correction decoders and determine the cost of achieving these gains, especially in terms of robustness to distributional shifts.

Method: The authors analyze robustness by applying adversarial perturbations (input-dependent and universal adversarial) under norm constraints and studying their effects on AI-based decoders compared to traditional BP decoders.

Result: AI decoders exhibit significant performance degradation under adversarial perturbations, higher sensitivity to channel distributions, and weaker robustness compared to BP decoders. Universal perturbations are particularly harmful.

Conclusion: While AI decoders outperform BP decoders nominally, their robustness under distributional shifts is limited, raising concerns about their vulnerability and transferability in practical scenarios.

Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1255] [AutoBinder Agent: An MCP-Based Agent for End-to-End Protein Binder Design](https://arxiv.org/abs/2602.00019)
*Fukang Ge,Jiarui Zhu,Linjie Zhang,Haowen Xiao,Xiangcheng Bao,Fangnan Xie,Danyang Chen,Yanrui Lu,Yuting Wang,Ziqian Guan,Lin Gu,Jinhao Bi,Yingying Zhu*

Main category: q-bio.BM

TL;DR: This paper presents an end-to-end drug design framework using a Large Language Model (LLM) to dynamically connect biochemical databases, AI models, and toolchains, improving workflow integration and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address fragmented workflows, inconsistent interfaces, and high integration overhead in AI-driven drug discovery processes.

Method: The framework leverages a Large Language Model (LLM) and the Model Context Protocol (MCP), incorporating cutting-edge tools such as MaSIF, Rosetta, ProteinMPNN, and AlphaFold3 for various aspects of drug design.

Result: The system enables tasks such as surface analysis, scaffold grafting, sequence optimization, and structure prediction in an integrated manner, reducing manual workload and ensuring interoperability.

Conclusion: By combining protocol-driven coordination with advanced AI tools, the framework offers a reproducible, extensible, and portable solution for drug design, streamlining the entire process and enabling better outcomes.

Abstract: Modern AI technologies for drug discovery are distributed across heterogeneous platforms-including web applications, desktop environments, and code libraries-leading to fragmented workflows, inconsistent interfaces, and high integration overhead. We present an agentic end-to-end drug design framework that leverages a Large Language Model (LLM) in conjunction with the Model Context Protocol (MCP) to dynamically coordinate access to biochemical databases, modular toolchains, and task-specific AI models. The system integrates four state-of-the-art components: MaSIF (MaSIF-site and MaSIF-seed-search) for geometric deep learning-based identification of protein-protein interaction (PPI) sites, Rosetta for grafting protein fragments onto protein backbones to form mini proteins, ProteinMPNN for amino acid sequences redesign, and AlphaFold3 for near-experimental accuracy in complex structure prediction. Starting from a target structure, the framework supports de novo binder generation via surface analysis, scaffold grafting and pose construction, sequence optimization, and structure prediction. Additionally, by replacing rigid, script-based workflows with a protocol-driven, LLM-coordinated architecture, the framework improves reproducibility, reduces manual overhead, and ensures extensibility, portability, and auditability across the entire drug design process.

</details>


### [1256] [Controlling Repetition in Protein Language Models](https://arxiv.org/abs/2602.00782)
*Jiahao Zhang,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: q-bio.BM

TL;DR: The paper investigates pathological repetition in protein language models (PLMs) and introduces metrics and a novel method, UCCS, to address it, demonstrating improvements in folding reliability and reduced repetition.


<details>
  <summary>Details</summary>
Motivation: To address the issue of pathological repetition in protein generation by PLMs, which compromises structural confidence and functional viability.

Method: Introduction of UCCS (Utility-Controlled Contrastive Steering), which uses a constrained dataset to generate steering vectors that reduce repetition without affecting foldability, tested on established protein datasets and models.

Result: UCCS successfully lowers repetition in protein sequences without degrading folding reliability, as evidenced by benchmarks with models like ESM-3 and ProtGPT2 across different datasets.

Conclusion: Repetition control is critical in PLMs, and dataset-guided steering offers a robust solution for reliable protein generation.

Abstract: Protein language models (PLMs) have enabled advances in structure prediction and de novo protein design, yet they frequently collapse into pathological repetition during generation. Unlike in text, where repetition merely reduces readability, in proteins it undermines structural confidence and functional viability. To unify this problem, we present the first systematic study of repetition in PLMs. We first propose quantitative metrics to characterize motif-level and homopolymer repetition and then demonstrate their negative impact on folding reliability. To address this challenge, we propose UCCS (Utility-Controlled Contrastive Steering), which steers protein generation with a constrained dataset. Instead of naively contrasting high- vs. low-repetition sequences, we construct contrastive sets that maximize differences in repetition while tightly controlling for structural utility. This disentanglement yields steering vectors that specifically target repetition without degrading foldability. Injected at inference, these vectors consistently reduce repetition without retraining or heuristic decoding. Experiments with ESM-3 and ProtGPT2 in CATH, UniRef50, and SCOP show that our method outperforms decoding penalties and other baselines, substantially lowering repetition while preserving AlphaFold confidence scores. Our results establish repetition control as a central challenge for PLMs and highlight dataset-guided steering as a principled approach for reliable protein generation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [1257] [Calibrating Behavioral Parameters with Large Language Models](https://arxiv.org/abs/2602.01022)
*Brandon Yee,Krishna Sharma*

Main category: econ.GN

TL;DR: The paper explores how large language models (LLMs) can measure behavioral parameters for asset pricing models, calibrating them to align more closely with human benchmarks.


<details>
  <summary>Details</summary>
Motivation: Behavioral parameters are crucial for asset pricing but lack reliable measurement methods. The paper seeks to address these limitations.

Method: The study uses four LLMs and 24,000 agent-scenario pairs to analyze biases in baseline LLM behavior, followed by profile-based calibration to adjust behavioral parameters.

Result: Calibrated LLMs show better alignment with theoretical benchmarks and human behavior in behavioral biases like loss aversion, herding, and extrapolation. The calibrated parameters improve asset pricing model predictions.

Conclusion: LLMs, when calibrated, effectively measure behavioral parameters useful for asset pricing models, enhancing their predictive validity and theoretical consistency.

Abstract: Behavioral parameters such as loss aversion, herding, and extrapolation are central to asset pricing models but remain difficult to measure reliably. We develop a framework that treats large language models (LLMs) as calibrated measurement instruments for behavioral parameters. Using four models and 24{,}000 agent--scenario pairs, we document systematic rationality bias in baseline LLM behavior, including attenuated loss aversion, weak herding, and near-zero disposition effects relative to human benchmarks. Profile-based calibration induces large, stable, and theoretically coherent shifts in several parameters, with calibrated loss aversion, herding, extrapolation, and anchoring reaching or exceeding benchmark magnitudes. To assess external validity, we embed calibrated parameters in an agent-based asset pricing model, where calibrated extrapolation generates short-horizon momentum and long-horizon reversal patterns consistent with empirical evidence. Our results establish measurement ranges, calibration functions, and explicit boundaries for eight canonical behavioral biases.

</details>


### [1258] [The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament](https://arxiv.org/abs/2602.01684)
*Felipe A. Csaszar,Aticus Peterson,Daniel Wilde*

Main category: econ.GN

TL;DR: The study compares artificial intelligence (AI) large language models (LLMs) with experienced human evaluators in predicting the fundraising success of Kickstarter projects. AI models outperform humans significantly in ranking accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to test whether AI, specifically large language models, can outperform humans in high-stakes strategic foresight scenarios, specifically in predicting uncertain fundraising outcomes.

Method: The study utilized 30 live Kickstarter technology ventures as test cases. AI models completed 870 pairwise comparisons to predict fundraising success, while 346 human evaluators, including investors, were also asked to predict outcomes. The performance of both groups was compared using rank correlation metrics.

Result: AI models, notably the Gemini 2.5 Pro, achieved significantly better rank correlations (up to 0.74) than human evaluators (0.04 to 0.45). AI consistently ordered ventures more accurately, outperforming human expertise.

Conclusion: Frontier AI models show superior capabilities in strategic foresight compared to human evaluators. Even hybrid human-AI teams did not surpass the performance of the top AI model, showcasing AI's potential in complex decision-making domains.

Abstract: Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.S.-based technology ventures, launched after the training cutoffs of all models studied, were evaluated while fundraising remained in progress and outcomes were unknown. A diverse suite of frontier and open-weight large language models (LLMs) completed 870 pairwise comparisons, producing complete rankings of predicted fundraising success. We benchmarked these forecasts against 346 experienced managers recruited via Prolific and three MBA-trained investors working under monitored conditions. The results are striking: human evaluators achieved rank correlations with actual outcomes between 0.04 and 0.45, while several frontier LLMs exceeded 0.60, with the best (Gemini 2.5 Pro) reaching 0.74 -- correctly ordering nearly four of every five venture pairs. These differences persist across multiple performance metrics and robustness checks. Neither wisdom-of-the-crowd ensembles nor human-AI hybrid teams outperformed the best standalone model.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1259] [Asynchronous MultiAgent Reinforcement Learning for 5G Routing under Side Constraints](https://arxiv.org/abs/2602.00035)
*Sebastian Racedo,Brigitte Jaumard,Oscar Delgado,Meysam Masoudi*

Main category: cs.NI

TL;DR: The paper proposes an asynchronous multi-agent reinforcement learning (AMARL) framework for real-time routing in 5G networks. It reduces training time and adapts well to shifting traffic demands while maintaining good performance metrics.


<details>
  <summary>Details</summary>
Motivation: The increasing heterogeneity of traffic in 5G and beyond networks poses challenges for timely and scalable routing decisions.

Method: An asynchronous multi-agent reinforcement learning framework using independent PPO agents specialized for each service. These agents interact with a shared global resource environment to optimize routing decisions.

Result: AMARL maintains similar performance metrics (Grade of Service and latency) compared to a centralized PPO baseline while improving training efficiency and robustness against traffic changes.

Conclusion: Asynchronous, service-specialized agents offer a scalable and effective solution for distributed routing in heterogeneous network traffic scenarios.

Abstract: Networks in the current 5G and beyond systems increasingly carry heterogeneous traffic with diverse quality-of-service constraints, making real-time routing decisions both complex and time-critical. A common approach, such as a heuristic with human intervention or training a single centralized RL policy or synchronizing updates across multiple learners, struggles with scalability and straggler effects. We address this by proposing an asynchronous multi-agent reinforcement learning (AMARL) framework in which independent PPO agents, one per service, plan routes in parallel and commit resource deltas to a shared global resource environment. This coordination by state preserves feasibility across services and enables specialization for service-specific objectives. We evaluate the method on an O-RAN like network simulation using nearly real-time traffic data from the city of Montreal. We compared against a single-agent PPO baseline. AMARL achieves a similar Grade of Service (acceptance rate) (GoS) and end-to-end latency, with reduced training wall-clock time and improved robustness to demand shifts. These results suggest that asynchronous, service-specialized agents provide a scalable and practical approach to distributed routing, with applicability extending beyond the O-RAN domain.

</details>


### [1260] [TriCloudEdge: A multi-layer Cloud Continuum](https://arxiv.org/abs/2602.02121)
*George Violettas,Lefteris Mamatas*

Main category: cs.NI

TL;DR: TriCloudEdge is a three-tier cloud architecture integrating far-edge, intermediate-edge devices, and centralized clouds. It balances computational loads and latency, and supports efficient data transfer protocols.


<details>
  <summary>Details</summary>
Motivation: To create a scalable architecture that addresses latency, computation distribution, and privacy issues in cloud-edge environments, leveraging the strengths of edge and cloud levels.

Method: A three-tier architecture design (TriCloudEdge) is proposed, integrating protocols like WebSocket, MQTT, and HTTP for efficient data transfer, and implementing and testing AI model adaptation across tiers.

Result: TriCloudEdge demonstrates distributed computational handling, improving latency, resource utilization, and addressing privacy concerns in cloud-edge environments.

Conclusion: TriCloudEdge offers a scalable, efficient solution aligned with modern research to tackle practical implementation challenges across cloud continuum levels.

Abstract: TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1261] [Benchmarking of algorithms for set partitions](https://arxiv.org/abs/2602.01350)
*Arnav Khinvasara,Alexander Pikovski*

Main category: cs.DS

TL;DR: This paper focuses on enumerating set partitions and recommends Djokic et al.'s algorithm after testing multiple approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the recurring need to list all set partitions, which is essential for tasks in combinatorial optimization.

Method: The authors present approximate formulas for calculating the number of set partitions and review several enumeration algorithms, followed by benchmarking tests.

Result: Multiple algorithms were compared, and Djokic et al.'s method was identified as the most efficient for practical applications.

Conclusion: Djokic et al.'s algorithm is recommended for practical use to efficiently enumerate set partitions.

Abstract: Set partitions are arrangements of distinct objects into groups. The problem of listing all set partitions arises in a variety of settings, in particular in combinatorial optimization tasks. After a brief review, we give practical approximate formulas for determining the number of set partitions, both for small and large set sizes. Several algorithms for enumerating all set partitions are reviewed, and benchmarking tests were conducted. The algorithm of Djokic et al. is recommended for practical use.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [1262] [Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings](https://arxiv.org/abs/2602.01363)
*Mariëtte Olijslager,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.SD

TL;DR: The paper examines demographic attribute leakage in SimCLR-trained speaker embeddings and evaluates debiasing strategies to mitigate leakage while balancing verification performance.


<details>
  <summary>Details</summary>
Motivation: Demographic attributes like gender, age, and accent encoded in self-supervised speaker embeddings raise fairness and privacy concerns.

Method: Two debiasing methods are studied: adversarial training with gradient reversal and a causal bottleneck architecture. Demographic leakage is quantified using probing classifiers, and speaker verification is assessed with ROC-AUC and EER.

Result: Gender information is strongly encoded in embeddings, whereas age and accent are weaker. Debiasing reduces gender leakage but has trade-offs between leakage suppression and verification accuracy.

Conclusion: Mitigating demographic information in self-supervised speaker embeddings entails performance trade-offs, and the limitations of current debiasing methods are highlighted in the findings.

Abstract: Self-supervised speaker embeddings are widely used in speaker verification systems, but prior work has shown that they often encode sensitive demographic attributes, raising fairness and privacy concerns. This paper investigates the extent to which demographic information, specifically gender, age, and accent, is present in SimCLR-trained speaker embeddings and whether such leakage can be mitigated without severely degrading speaker verification performance. We study two debiasing strategies: adversarial training through gradient reversal and a causal bottleneck architecture that explicitly separates demographic and residual information. Demographic leakage is quantified using both linear and nonlinear probing classifiers, while speaker verification performance is evaluated using ROC-AUC and EER. Our results show that gender information is strongly and linearly encoded in baseline embeddings, whereas age and accent are weaker and primarily nonlinearly represented. Adversarial debiasing reduces gender leakage but has limited effect on age and accent and introduces a clear trade-off with verification accuracy. The causal bottleneck further suppresses demographic information, particularly in the residual representation, but incurs substantial performance degradation. These findings highlight fundamental limitations in mitigating demographic leakage in self-supervised speaker embeddings and clarify the trade-offs inherent in current debiasing approaches.

</details>


### [1263] [LPIPS-AttnWav2Lip: Generic Audio-Driven lip synchronization for Talking Head Generation in the Wild](https://arxiv.org/abs/2602.00189)
*Zhipeng Chen,Xinheng Wang,Lun Xie,Haijie Yuan,Hang Pan*

Main category: cs.SD

TL;DR: This paper introduces LPIPS-AttnWav2Lip, a method for generating talking head videos with high lip-sync accuracy and visual quality by leveraging U-Net architecture, semantic alignment modules, and LPIPS Loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of achieving audio-visual coherence, specifically lip synchronization, in talking head video generation.

Method: LPIPS-AttnWav2Lip uses a U-Net architecture with residual CBAM to encode and fuse audio-visual information. A semantic alignment module extends the generator's receptive field to align visual features with audio latent vectors. LPIPS Loss ensures realistic image generation and stable training.

Result: The proposed method achieves superior results in lip synchronization accuracy and visual quality, confirmed by subjective and objective evaluations.

Conclusion: This approach offers a novel solution for talking head video generation, demonstrating excellence in synchronization and image realism, with code made publicly accessible.

Abstract: Researchers have shown a growing interest in Audio-driven Talking Head Generation. The primary challenge in talking head generation is achieving audio-visual coherence between the lips and the audio, known as lip synchronization. This paper proposes a generic method, LPIPS-AttnWav2Lip, for reconstructing face images of any speaker based on audio. We used the U-Net architecture based on residual CBAM to better encode and fuse audio and visual modal information. Additionally, the semantic alignment module extends the receptive field of the generator network to obtain the spatial and channel information of the visual features efficiently; and match statistical information of visual features with audio latent vector to achieve the adjustment and injection of the audio content information to the visual information. To achieve exact lip synchronization and to generate realistic high-quality images, our approach adopts LPIPS Loss, which simulates human judgment of image quality and reduces instability possibility during the training process. The proposed method achieves outstanding performance in terms of lip synchronization accuracy and visual quality as demonstrated by subjective and objective evaluation results. The code for the paper is available at the following link: https://github.com/FelixChan9527/LPIPS-AttnWav2Lip

</details>


### [1264] [Multi-Speaker Conversational Audio Deepfake: Taxonomy, Dataset and Pilot Study](https://arxiv.org/abs/2602.00295)
*Alabi Ahmed,Vandana Janeja,Sanjay Purushotham*

Main category: cs.SD

TL;DR: This paper addresses the overlooked threat of multi-speaker conversational audio deepfakes. The authors developed a new dataset (MsCADD) and benchmarked detection models, showcasing the challenges and opportunities for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of focus on multi-speaker audio deepfakes in research, especially concerning manipulations in conversational settings, which presents security risks.

Method: The paper introduces the MsCADD dataset, consisting of synthetic two-speaker audio clips generated using VITS and SoundStorm-based models. It benchmarks detection models (LFCC-LCNN, RawNet2, and Wav2Vec 2.0) using performance metrics like F1 score and accuracy.

Result: Baseline models offered some useful measurements but revealed significant gaps in reliable detection of multi-speaker synthetic audio under diverse conversational conditions.

Conclusion: The research highlights the necessity for advanced methods in detecting multi-speaker audio deepfakes and provides an open dataset to enable further exploration in this domain.

Abstract: The rapid advances in text-to-speech (TTS) technologies have made audio deepfakes increasingly realistic and accessible, raising significant security and trust concerns. While existing research has largely focused on detecting single-speaker audio deepfakes, real-world malicious applications with multi-speaker conversational settings is also emerging as a major underexplored threat. To address this gap, we propose a conceptual taxonomy of multi-speaker conversational audio deepfakes, distinguishing between partial manipulations (one or multiple speakers altered) and full manipulations (entire conversations synthesized). As a first step, we introduce a new Multi-speaker Conversational Audio Deepfakes Dataset (MsCADD) of 2,830 audio clips containing real and fully synthetic two-speaker conversations, generated using VITS and SoundStorm-based NotebookLM models to simulate natural dialogue with variations in speaker gender, and conversational spontaneity. MsCADD is limited to text-to-speech (TTS) types of deepfake. We benchmark three neural baseline models; LFCC-LCNN, RawNet2, and Wav2Vec 2.0 on this dataset and report performance in terms of F1 score, accuracy, true positive rate (TPR), and true negative rate (TNR). Results show that these baseline models provided a useful benchmark, however, the results also highlight that there is a significant gap in multi-speaker deepfake research in reliably detecting synthetic voices under varied conversational dynamics. Our dataset and benchmarks provide a foundation for future research on deepfake detection in conversational scenarios, which is a highly underexplored area of research but also a major area of threat to trustworthy information in audio settings. The MsCADD dataset is publicly available to support reproducibility and benchmarking by the research community.

</details>


### [1265] [HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection](https://arxiv.org/abs/2602.01032)
*Zhili Nicholas Liang,Soyeon Caren Han,Qizhou Wang,Christopher Leckie*

Main category: cs.SD

TL;DR: The paper introduces HierCon, a framework improving audio deepfake detection by modeling dependencies across temporal and hierarchical layers, showcasing state-of-the-art performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The increasing difficulty to distinguish audio deepfakes from real speech due to advances in TTS and voice conversion systems prompts security and trust concerns.

Method: HierCon is a hierarchical layer attention framework combined with margin-based contrastive learning, modeling dependencies across temporal frames, neighboring layers, and layer groups, while achieving domain-invariant embeddings.

Result: HierCon achieves state-of-the-art detection performance on ASVspoof 2021 DF and In-the-Wild datasets with significant error rate reductions (1.93% and 6.87% EER respectively).

Conclusion: Hierarchical modeling of temporal and layer dependencies enhances robustness and generalization in detecting cross-domain audio deepfakes.

Abstract: Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.

</details>


### [1266] [TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection](https://arxiv.org/abs/2602.01060)
*Chengyuan Ma,Peng Jia,Hongyue Guo,Wenming Yang*

Main category: cs.SD

TL;DR: The paper introduces TLDiffGAN, a framework that improves anomalous sound detection through a combination of GANs, diffusion models, and pretrained audio encoders.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to capture the complex feature distribution of normal sounds, and the potential of diffusion models remains unexplored in anomalous sound detection.

Method: TLDiffGAN combines a GAN with a latent diffusion model for better training and integrates pretrained audio encoders to extract raw and spectrogram-based features. It also introduces a TMixup spectrogram augmentation technique.

Result: Experiments on the DCASE 2020 Challenge Task 2 dataset show that TLDiffGAN outperforms existing methods in detecting anomalies and localizing time-frequency patterns.

Conclusion: TLDiffGAN effectively improves both the detection performance and understanding of anomalous sound patterns by leveraging advanced generative techniques and feature extraction.

Abstract: Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a novel framework, TLDiffGAN, which consists of two complementary branches. One branch incorporates a latent diffusion model into the GAN generator for adversarial training, thereby making the discriminator's task more challenging and improving the quality of generated samples. The other branch leverages pretrained audio model encoders to extract features directly from raw audio waveforms for auxiliary discrimination. This framework effectively captures feature representations of normal sounds from both raw audio and Mel spectrograms. Moreover, we introduce a TMixup spectrogram augmentation technique to enhance sensitivity to subtle and localized temporal patterns that are often overlooked. Extensive experiments on the DCASE 2020 Challenge Task 2 dataset demonstrate the superior detection performance of TLDiffGAN, as well as its strong capability in anomalous time-frequency localization.

</details>


### [1267] [Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation](https://arxiv.org/abs/2602.00681)
*Ilyass Moummad,Marius Miron,Lukas Rauch,David Robinson,Alexis Joly,Olivier Pietquin,Emmanuel Chemla,Matthieu Geist*

Main category: cs.SD

TL;DR: The paper introduces an audio-to-image retrieval method utilizing a text bridge to achieve alignment between audio and image representations without direct paired data, demonstrating effective performance for bioacoustic species recognition.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning aligned audio-image representations for bioacoustic species recognition, especially in the context of limited paired audio-image data availability.

Method: The method leverages a pretrained image-text model (BioCLIP-2) and a pretrained audio-text model (BioLingual). The text embedding space from BioCLIP-2 is distilled into the audio-text model by fine-tuning its audio encoder using a contrastive objective, enabling semantic alignment without direct audio-image supervision.

Result: The approach improves audio-text alignment on benchmark datasets while preserving audio discriminative power. Notably, it achieves strong audio-to-image retrieval performance, outperforming baseline methods, including those using zero-shot models or learned text-mapping.

Conclusion: Indirect semantic transfer using text as an intermediary is an effective strategy for audio-to-image alignment, offering a practical solution for visually grounded species recognition in bioacoustic contexts with scarce data availability.

Abstract: Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data. We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision. Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This distillation transfers visually grounded semantics into the audio representation, inducing emergent alignment between audio and image embeddings without using images during training. We evaluate the resulting model on multiple bioacoustic benchmarks. The distilled audio encoder preserves audio discriminative power while substantially improving audio-text alignment on focal recordings and soundscape datasets. Most importantly, on the SSW60 benchmark, the proposed approach achieves strong audio-to-image retrieval performance exceeding baselines based on zero-shot model combinations or learned mappings between text embeddings, despite not training on paired audio-image data. These results demonstrate that indirect semantic transfer through text is sufficient to induce meaningful audio-image alignment, providing a practical solution for visually grounded species recognition in data-scarce bioacoustic settings.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [1268] [Towards knowledge-based workflows: a semantic approach to atomistic simulations for mechanical and thermodynamic properties](https://arxiv.org/abs/2602.01358)
*Abril Azocar Guzman,Hoang-Thien Luu,Sarath Menon,Tilmann Hickel,Nina Merkert,Stefan Sandfeld*

Main category: cond-mat.mtrl-sci

TL;DR: The paper presents FAIR-compliant, reusable workflows for molecular dynamics simulations to evaluate mechanical and thermodynamic properties of materials, addressing reproducibility issues.


<details>
  <summary>Details</summary>
Motivation: Current molecular dynamics simulation practices lack metadata consistency, provenance, and FAIR principles, hindering reproducibility and reuse.

Method: Reusable workflows incorporating metadata aligned with ontologies, automatic provenance capture, and FAIR data outputs are developed for various mechanical and thermodynamic properties.

Result: Validated structure-property relations, such as the Hall-Petch effect, reusable workflows across various interatomic potentials and materials, and AI-ready simulation data.

Conclusion: This paper provides a comprehensive framework to enhance molecular dynamics simulations and data management for mechanical and thermodynamic properties.

Abstract: Mechanical and thermodynamic properties, including the influence of crystal defects, are critical for evaluating materials in engineering applications. Molecular dynamics simulations provide valuable insight into these mechanisms at the atomic scale. However, current practice often relies on fragmented scripts with inconsistent metadata and limited provenance, which hinders reproducibility, interoperability, and reuse. FAIR data principles and workflow-based approaches offer a path to address these limitations. We present reusable atomistic workflows that incorporate metadata annotation aligned with application ontologies, enabling automatic provenance capture and FAIR-compliant data outputs. The workflows cover key mechanical and thermodynamic quantities, including equation of state, elastic tensors, mechanical loading, thermal properties, defect formation energies, and nanoindentation. We demonstrate validation of structure-property relations such as the Hall-Petch effect and show that the workflows can be reused across different interatomic potentials and materials within a coherent semantic framework. The approach provides AI-ready simulation data, supports emerging agentic AI workflows, and establishes a generalizable blueprint for knowledge-based mechanical and thermodynamic simulations.

</details>


### [1269] [Towards Agentic Intelligence for Materials Science](https://arxiv.org/abs/2602.00169)
*Huan Zhang,Yizhan Li,Wenhao Huang,Ziyu Hou,Yu Song,Xuye Liu,Farshid Effaty,Jinya Jiang,Sifan Wu,Qianggang Ding,Izumi Takahara,Leonard R. MacGillivray,Teruyasu Mizoguchi,Tianshu Yu,Lizi Liao,Yuyu Luo,Yu Rong,Jia Li,Ying Diao,Heng Ji,Bang Liu*

Main category: cond-mat.mtrl-sci

TL;DR: This survey discusses the integration of AI and materials science, proposing a pipeline-focused approach for AI-driven materials discovery systems that operate across the entire discovery loop, emphasizing agentic systems and effective optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for comprehensive, end-to-end AI systems in materials science that can autonomously perform the full lifecycle of materials discovery, surpassing task-specific AI models.

Method: The paper presents a pipeline-centric framework that incorporates corpus curation, pretraining, domain adaptation, and instruction tuning, culminating in goal-oriented AI agents. It integrates these stages with simulation and experimental platforms and aligns upstream design choices with downstream discovery outcomes.

Result: The survey outlines the strengths of AI in literature mining, materials characterization, and prediction while showcasing groundbreaking materials applications in design, optimization, and integrating external tools. It contrasts current reactive systems with prospective autonomous systems capable of long-term learning and memory.

Conclusion: The survey calls for the development of autonomous LLM-driven systems with safety, memory, and tool-use capabilities to revolutionize material discovery. It provides a roadmap highlighting future advancements and collaborative opportunities in AI and materials science.

Abstract: The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.
  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.

</details>


### [1270] [QUASAR: A Universal Autonomous System for Atomistic Simulation and a Benchmark of Its Capabilities](https://arxiv.org/abs/2602.00185)
*Fengxu Yang,Jack D. Evans*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces QUASAR, an autonomous system that leverages large language models for atomistic simulations, showcasing its capability to autonomously tackle scientific workflows.


<details>
  <summary>Details</summary>
Motivation: Current systems in materials science are limited by restrictive tool-calling methods and narrowly focused agents, hindering seamless scientific discovery.

Method: The authors developed QUASAR, an autonomous system capable of managing multi-scale atomistic workflows using diverse methods like density functional theory and machine learning. The system includes adaptive planning, context memory, and hybrid knowledge retrieval.

Result: QUASAR performed well across benchmark tasks ranging from routine to advanced challenges, demonstrating its feasibility for extended research autonomy in computational chemistry.

Conclusion: The results indicate that QUASAR can act as a general reasoning system for atomistic research, highlighting its transformative potential while also identifying areas needing improvement for broader deployment.

Abstract: The integration of large language models (LLMs) into materials science offers a transformative opportunity to streamline computational workflows, yet current agentic systems remain constrained by rigid tool-calling approaches and narrowly scoped agents. In this work, we introduce QUASAR, a universal autonomous system for atomistic simulation designed to facilitate production-grade scientific discovery. QUASAR autonomously orchestrates complex multi-scale workflows across diverse methods, including density functional theory, machine learning potentials, molecular dynamics, and Monte Carlo simulations. The system incorporates robust mechanisms for adaptive planning, context-efficient memory management, and hybrid knowledge retrieval to navigate real-world research scenarios without human intervention. We benchmark QUASAR against a series of three-tiered tasks, progressing from routine tasks to frontier research challenges such as photocatalyst screening and novel material assessment. These results suggest that QUASAR can function as a general atomistic reasoning system rather than a task-specific automation framework. They also provide initial evidence supporting the potential deployment of agentic AI as a component of computational chemistry research workflows, while identifying areas requiring further development.

</details>


### [1271] [Multimodal Machine Learning for Integrating Heterogeneous Analytical Systems](https://arxiv.org/abs/2602.00590)
*Shun Muroga,Hideaki Nakajima,Taiyo Shimizu,Kazufumi Kobashi,Kenji Hata*

Main category: cond-mat.mtrl-sci

TL;DR: This paper develops an interpretable multimodal machine learning framework for characterizing complex materials, specifically demonstrated on carbon nanotube films.


<details>
  <summary>Details</summary>
Motivation: Understanding the structure-property relationships in complex materials demands integrating measurements across various length scales.

Method: The framework extracts and fuses features from SEM images, Raman spectroscopy, gas adsorption, and electrical resistivity, and utilizes regression models like XGBoost for prediction. Visualization methods are also employed to explore feature clustering.

Result: Key properties of CNT films are accurately predicted, with surface resistivity correlated to transport length scales and crystallinity, while specific surface area links to intersection density and void geometry.

Conclusion: The multimodal machine learning framework provides an explainable and data-driven approach applicable for characterizing diverse complex materials.

Abstract: Understanding structure-property relationships in complex materials requires integrating complementary measurements across multiple length scales. Here we propose an interpretable "multimodal" machine learning framework that unifies heterogeneous analytical systems for end-to-end characterization, demonstrated on carbon nanotube (CNT) films whose properties are highly sensitive to microstructural variations. Quantitative morphology descriptors are extracted from SEM images via binarization, skeletonization, and network analysis, capturing curvature, orientation, intersection density, and void geometry. These SEM-derived features are fused with Raman indicators of crystallinity/defect states, specific surface area from gas adsorption, and electrical surface resistivity. Multi-dimensional visualization using radar plots and UMAP reveals clear clustering of CNT films according to crystallinity and entanglements. Regression models trained on the multimodal feature set show that nonlinear approaches, particularly XGBoost, achieve the best predictive accuracy under leave-one-out cross-validation. Feature-importance analysis further provides physically meaningful interpretations: surface resistivity is primarily governed by junction-to-junction transport length scales, crystallinity/defect-related metrics, and network connectivity, whereas specific surface area is dominated by intersection density and void size. The proposed multimodal machine learning framework offers a general strategy for data-driven, explainable characterization of complex materials.

</details>


### [1272] [AI Meets Plasticity: A Comprehensive Survey](https://arxiv.org/abs/2602.01215)
*Hadi Bakhshan,Sima Farshbaf,Junior Ramirez Machado,Fernando Rastellini Canela,Josep Maria Carbonell*

Main category: cond-mat.mtrl-sci

TL;DR: The paper surveys the interaction between artificial intelligence and materials plasticity, focusing on AI methodologies for understanding plasticity behavior and their applications in materials science.


<details>
  <summary>Details</summary>
Motivation: To examine the transformative influence of emerging AI methodologies in materials plasticity and provide a structured understanding of their applications and benefits in this field.

Method: The study holistically surveys state-of-the-art AI techniques (including machine learning, deep learning, and uncertainty quantification) and evaluates their application in understanding and predicting plastic behavior in materials while constructing a taxonomy of these approaches.

Result: The paper offers a comprehensive taxonomy of AI techniques applied in studying materials plasticity, detailing factors such as model architectures, data needs, and predictive performance.

Conclusion: AI is pivotal in advancing the understanding and prediction of material plasticity, and this work provides a structured guide for researchers to effectively apply AI for materials science advancements.

Abstract: Artificial intelligence (AI) is rapidly emerging as a new paradigm of scientific discovery, namely data-driven science, across nearly all scientific disciplines. In materials science and engineering, AI has already begun to exert a transformative influence, making it both timely and necessary to examine its interaction with materials plasticity. In this study, we present a holistic survey of the convergence between AI and plasticity, highlighting state-of-the-art AI methodologies employed to discover, construct surrogate models for, and emulate the plastic behavior of materials. From a materials science perspective, we examine cause-and-effect relationships governing plastic deformation, including microstructural characterization and macroscopic responses described through plasticity constitutive models. From the perspective of AI methodology, we review a broad spectrum of applied approaches, ranging from frequentist techniques such as classical machine learning (ML), deep learning (DL), and physics-informed models to probabilistic frameworks that incorporate uncertainty quantification and generative AI methods. These data-driven approaches are discussed in the context of materials characterization and plasticity-related applications. The primary objective of this survey is to develop a comprehensive and well-organized taxonomy grounded in AI methodologies, with particular emphasis on distinguishing critical aspects of these techniques, including model architectures, data requirements, and predictive performance within the specific domain of materials plasticity. By doing so, this work aims to provide a clear road map for researchers and practitioners in the materials community, while offering deeper physical insight and intuition into the role of AI in advancing materials plasticity and characterization, an area of growing importance in the emerging AI-driven era.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1273] [When Is Generalized Bayes Bayesian? A Decision-Theoretic Characterization of Loss-Based Updating](https://arxiv.org/abs/2602.01573)
*Kenichiro McAlinn,Kōsaku Takanashi*

Main category: stat.ME

TL;DR: This paper distinguishes between belief-based and decision-based posteriors, analyzes loss-based updating methods, and discusses their decision-theoretic implications.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical framework for understanding Bayesian updating methods when employing loss-based posteriors and how they relate to traditional Bayesian principles and decision-making.

Method: A decision-theoretic characterization separates belief and decision posteriors. The paper explores the condition under which loss-based posteriors align with traditional Bayesian methods and employs entropy-penalized variational representation for analysis.

Result: The analysis shows that loss-based posteriors generalize Bayesian methods only under specific conditions, decision posteriors require nonlinear preferences, and traditional tools like Bayes factors lack rigor in these contexts.

Conclusion: Using a loss-based posterior expands Bayesian decision-making, but understanding its limits and decision-theoretic implications ensures theoretical consistency and practical application.

Abstract: Loss-based updating, including generalized Bayes, Gibbs, and quasi-posteriors, replaces likelihoods by a user-chosen loss and produces a posterior-like distribution via exponential tilt. We give a decision-theoretic characterization that separates \emph{belief posteriors} --  conditional beliefs justified by the foundations of Savage and Anscombe-Aumann under a joint probability mode l-- from \emph{decision posteriors} -- randomized decision rules justified by preferences over decision rules. We make explicit that a loss-based posterior coincides with ordinary Bayes if and only if the loss is, up to scale and a data-only term, negative log-likelihood. We then show that generalized marginal likelihood is not evidence for decision posteriors, and Bayes factors are not well-defined without additional structure. In the decision posterior regime, non-degenerate posteriors require nonlinear preferences over decision rules. Under sequential coherence and separability, these lead to an entropy-penalized variational representation yielding generalized Bayes as the optimal rule.

</details>


### [1274] [Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes](https://arxiv.org/abs/2602.01825)
*Mingyuan Xu,Zongqi Xia,Tianxi Cai,Doudou Zhou,Nian Si*

Main category: stat.ME

TL;DR: The paper develops a methodology for learning robust sequential decision-making policies from offline multi-site datasets using distributionally robust MDPs and a feature-wise uncertainty model.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of learning robust decision-making policies from heterogeneous multi-site datasets, addressing cross-site uncertainties in data.

Method: The authors introduce a group-linear structure for distributionally robust MDPs with feature-wise uncertainty sets and propose an offline algorithm that combines ridge regression, worst-case aggregation, and a data-dependent pessimism penalty.

Result: Key results include a robust partial coverage assumption and a cluster-level extension for pooling similar sites, which improves sample efficiency and allows suboptimality guarantees for the policy.

Conclusion: The framework provides a principled approach for robust sequential decision-making with heterogeneous multi-site data, overcoming traditional strong assumptions.

Abstract: We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.

</details>


### [1275] [Causal Inference for Preprocessed Outcomes with an Application to Functional Connectivity](https://arxiv.org/abs/2602.02240)
*Zihang Wang,Razieh Nabi,Benjamin B. Risk*

Main category: stat.ME

TL;DR: The paper proposes a semiparametric framework for causal inference using derived outcomes from intra-subject processing, demonstrating its effectiveness in biomedical research applications.


<details>
  <summary>Details</summary>
Motivation: In biomedical research, intra-subject processing is common for removing artifacts and deriving outcomes, but its impact on inter-subject statistical inference lacks systematic study and a principled causal analysis framework.

Method: A semiparametric framework is designed for causal inference involving modular structures splitting intra-subject and inter-subject analysis. Multiply robust estimators and machine learning methods are developed with rate conditions, focusing on mediation settings and high-dimensional inference.

Result: Simulation studies prove the superior performance of the framework, and its application successfully evaluates the effect of stimulant medication on brain connectivity in children with autism spectrum disorder.

Conclusion: The proposed method provides a robust approach for causal inference and mediation analysis, particularly in high dimensional settings where intra-subject processing plays a role in derived outcomes.

Abstract: In biomedical research, repeated measurements within each subject are often processed to remove artifacts and unwanted sources of variation. The resulting data are used to construct derived outcomes that act as proxies for scientific outcomes that are not directly observable. Although intra-subject processing is widely used, its impact on inter-subject statistical inference has not been systematically studied, and a principled framework for causal analysis in this setting is lacking. In this article, we propose a semiparametric framework for causal inference with derived outcomes obtained after intra-subject processing. This framework applies to settings with a modular structure, where intra-subject analyses are conducted independently across subjects and are followed by inter-subject analyses based on parameters from the intra-subject stage. We develop multiply robust estimators of causal parameters under rate conditions on both intra-subject and inter-subject models, which allows the use of flexible machine learning. We specialize the framework to a mediation setting and focus on the natural direct effect. For high dimensional inference, we employ a step-down procedure that controls the exceedance rate of the false discovery proportion. Simulation studies demonstrate the superior performance of the proposed approach. We apply our method to estimate the impact of stimulant medication on brain connectivity in children with autism spectrum disorder.

</details>


### [1276] [On the calibration of survival models with competing risks](https://arxiv.org/abs/2602.00194)
*Julie Alberge,Tristan Haugomat,Gaël Varoquaux,Judith Abécassis*

Main category: stat.ME

TL;DR: The paper addresses calibration issues in competing-risks survival analysis by introducing new measures and methods to improve accurate probability estimates.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve calibration in competing-risk survival analysis, which has been under-explored due to its complexity involving probabilities across classes and time horizons.

Method: Developing a framework with novel calibration measures and introducing methods to estimate, test, and correct the calibration in competing-risk scenarios.

Result: The introduced recalibration methods produce well-behaved probabilities while maintaining discrimination in predictions.

Conclusion: The proposed framework enhances calibration accuracy in the competing-risk survival analysis, providing reliable probability estimates for decision-making.

Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the competing-risks setting remains under-explored as it is harder (the calibration applies to both probabilities across classes and time horizon). We show that existing calibration measures are not suited to the competing-risk setting and that recent models do not give well-behaved probabilities. To address this, we introduce a dedicated framework with two novel calibration measures that are minimized for oracle estimators (i.e., both measures are proper). We also introduce some methods to estimate, test, and correct the calibration. Our recalibration methods yield good probabilities while preserving discrimination.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1277] [From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering](https://arxiv.org/abs/2602.00496)
*Dana Feng,Bhada Yun,April Wang*

Main category: cs.HC

TL;DR: This paper explores how AI is reshaping agency in the software engineering profession for junior and senior developers, utilizing a mixed-methods study to suggest practices for sustaining agency in coding, learning, and mentorship.


<details>
  <summary>Details</summary>
Motivation: The motivation is to examine how AI integration is impacting roles, agency, and professional growth in software engineering, especially across experience levels.

Method: The study applied a three-phase mixed-methods approach: ACTA with seniors, an AI-assisted debugging task with juniors, and blind peer reviews by seniors analyzing junior prompt histories.

Result: The study found agency is less about individual developer preference and more influenced by organizational policies. Seniors utilize experience for delegation, whereas juniors vacillate between over-reliance and caution. The paper provides insights into utilizing seniors’ instincts for mentoring.

Conclusion: The paper concludes by recommending three practices to maintain agency in software engineering, particularly in coding, learning, and mentorship, as AI systems become more autonomous.

Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a Delphi process with 5 seniors, an AI-assisted debugging task with 10 juniors, and blind reviews of junior prompt histories by 5 more seniors. We found that agency in software engineering is primarily constrained by organizational policies rather than individual preferences, with experienced developers maintaining control through detailed delegation while novices struggle between over-reliance and cautious avoidance. Seniors leverage pre-AI foundational instincts to steer modern tools and possess valuable perspectives for mentoring juniors in their early AI-encouraged career development. From synthesis of results, we suggest three practices that focus on preserving agency in software engineering for coding, learning, and mentorship, especially as AI grows increasingly autonomous.

</details>


### [1278] [SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality](https://arxiv.org/abs/2602.00793)
*Yoonsang Kim,Devshree Jadeja,Divyansh Pradhan,Yalong Yang,Arie Kaufman*

Main category: cs.HC

TL;DR: SpeechLess introduces a more socially acceptable, efficient interaction method for wearable AR assistants that minimizes the need for repetitive, explicit spoken commands.


<details>
  <summary>Details</summary>
Motivation: Speaking aloud to AR devices in public is socially awkward and repeating the same requests daily is inefficient.

Method: The authors designed SpeechLess, which uses spatial memory and multimodal personal context to infer user intent, allowing for more concise or even non-verbal user interactions.

Result: SpeechLess enhanced socially acceptable interaction, reduced user effort in daily requests, and maintained usability and accuracy across various environments.

Conclusion: The study demonstrates that applying regulated, adaptive speech-based interactions to wearable AR can improve user experience by addressing social and effort-related constraints.

Abstract: Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users "speak less," while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.

</details>


### [1279] [Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions](https://arxiv.org/abs/2602.00259)
*Venkatesh Sivaraman,Eric P. Mason,Mengfan Ellen Li,Jessica Tong,Andrew J. King,Jeremy M. Kahn,Adam Perer*

Main category: cs.HC

TL;DR: This paper examines how AI interfaces influence decision-making through reasoning cues, with a focus on clinical decisions for sepsis treatment.


<details>
  <summary>Details</summary>
Motivation: Existing AI decision theories center on reliance calibration but lack understanding of how interface designs influence users’ reasoning in decision-making.

Method: Used contextual inquiries with six clinical teams and conducted a think-aloud study with 25 physicians to explore various reasoning cues.

Result: Identified distinct influence patterns of reasoning cues and suggested prioritizing tasks with high variability, adapting to evolving needs, and providing complementary insights.

Conclusion: Reasoning cues should be carefully designed to support decision-making in complex, variable cases like sepsis treatment, ensuring adaptability and rigor.

Abstract: Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the reasoning processes underneath. We address this gap by reconsidering AI interfaces as collections of intelligent reasoning cues: discrete pieces of AI information that can individually influence decision-making. We then explore the roles of eight types of reasoning cues in a high-stakes clinical decision (treating patients with sepsis in intensive care). Through contextual inquiries with six teams and a think-aloud study with 25 physicians, we find that reasoning cues have distinct patterns of influence that can directly inform design. Our results also suggest that reasoning cues should prioritize tasks with high variability and discretion, adapt to ensure compatibility with evolving decision needs, and provide complementary, rigorous insights on complex cases.

</details>


### [1280] [Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models](https://arxiv.org/abs/2602.00123)
*Filip Nowicki,Hubert Marciniak,Jakub Łączkowski,Krzysztof Jassem,Tomasz Górecki,Vimala Balakrishnan,Desmond C. Ong,Maciej Behnke*

Main category: cs.HC

TL;DR: This paper evaluates the performance of nine vision-language models (VLMs) on inferring human affective responses to visual stimuli using three validated datasets, analyzing their accuracy and limitations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how closely VLM outputs align with human affective ratings and investigate their suitability for inferring emotions from visual stimuli on a large scale.

Method: The authors benchmark nine VLMs on three affective image datasets in zero-shot settings, performing discrete emotion classification and predicting continuous human ratings, with additional experiments using rater-conditioned prompting.

Result: VLMs achieved 60%-80% accuracy in emotion classification tasks and r > 0.75 correlation in continuous rating prediction, though they showed biases such as weaker performance on arousal and overestimating response strength.

Conclusion: While VLMs can broadly capture affective trends, they lack precision compared to psychometric ratings, revealing potential for applications but notable limitations.

Abstract: Vision-language models (VLMs) show promise as tools for inferring affect from visual stimuli at scale; it is not yet clear how closely their outputs align with human affective ratings. We benchmarked nine VLMs, ranging from state-of-the-art proprietary models to open-source models, on three psycho-metrically validated affective image datasets: the International Affective Picture System, the Nencki Affective Picture System, and the Library of AI-Generated Affective Images. The models performed two tasks in the zero-shot setting: (i) top-emotion classification (selecting the strongest discrete emotion elicited by an image) and (ii) continuous prediction of human ratings on 1-7/9 Likert scales for discrete emotion categories and affective dimensions. We also evaluated the impact of rater-conditioned prompting on the LAI-GAI dataset using de-identified participant metadata. The results show good performance in discrete emotion classification, with accuracies typically ranging from 60% to 80% on six-emotion labels and from 60% to 75% on a more challenging 12-category task. The predictions of anger and surprise had the lowest accuracy in all datasets. For continuous rating prediction, models showed moderate to strong alignment with humans (r > 0.75) but also exhibited consistent biases, notably weaker performance on arousal, and a tendency to overestimate response strength. Rater-conditioned prompting resulted in only small, inconsistent changes in predictions. Overall, VLMs capture broad affective trends but lack the nuance found in validated psychological ratings, highlighting their potential and current limitations for affective computing and mental health-related applications.

</details>


### [1281] [A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs](https://arxiv.org/abs/2602.00402)
*Aditya Kumar Purohit,Hendrik Heuer*

Main category: cs.HC

TL;DR: This paper investigates how individuals with mental health conditions interact with large language models (LLMs), highlighting the benefits for mild mental health challenges and the limitations for severe issues.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how individuals with mental health challenges perceive and utilize LLMs for mental health assistance, and to explore design opportunities for their safe and effective usage.

Method: The researchers conducted 20 semi-structured interviews in the UK with individuals who use LLMs for mental health support, analyzing the data with reflexive thematic analysis.

Result: Participants found LLMs beneficial for immediacy, non-judgmental interactions, and self-paced disclosure but unsuitable for crises or complex emotional situations. They highlighted the importance of boundaries informed by prior therapy.

Conclusion: The study provides insights into how LLMs are used for mental health, underscores the necessity of boundary-setting for their safe application, and proposes guidelines for designing and regulating these tools responsibly within mental health care.

Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people in the UK who live with mental health conditions and have used LLMs for mental health support. Through reflexive thematic analysis, we found that participants engaged with LLMs in conditional and situational ways: for immediacy, the desire for non-judgement, self-paced disclosure, cognitive reframing, and relational engagement. Simultaneously, participants articulated clear boundaries informed by prior therapeutic experience: LLMs were effective for mild-to-moderate distress but inadequate for crises, trauma, and complex social-emotional situations. We contribute empirical insights into the lived use of LLMs for mental health, highlight boundary-setting as central to their safe role, and propose design and governance directions for embedding them responsibly within care ecosystem.

</details>


### [1282] [Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition](https://arxiv.org/abs/2602.01527)
*Brian Keith-Norambuena*

Main category: cs.HC

TL;DR: This paper argues that visualizations designed for humans don't fully meet the needs of machine cognition, emphasizing the need for machine-specific visual design.


<details>
  <summary>Details</summary>
Motivation: To address the growing gap between human-centered visualization design knowledge and the needs of machine cognition when consuming chart images.

Method: Synthesizes evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies, proposing a framework for machine-oriented visualization design as separate from human-oriented approaches.

Result: Findings reveal qualitative differences between human and machine cognition, highlighting failures of current approaches and the urgent need for machine-specific visual design foundations.

Conclusion: A research agenda is proposed to develop empirical foundations for machine-oriented visualization design, recognizing the distinct needs of machine audiences.

Abstract: Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a "machine Bertin" to complement the human-centered knowledge the field already possesses.

</details>


### [1283] [Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics](https://arxiv.org/abs/2602.00726)
*Yinghao Zhu,Dehao Sui,Zixiang Wang,Xuning Hu,Lei Gu,Yifan Qi,Tianchen Wu,Ling Wang,Yuan Wei,Wen Tang,Zhihan Cui,Yasha Wang,Lequan Yu,Ewen M Harrison,Junyi Gao,Liantao Ma*

Main category: cs.HC

TL;DR: The study introduces AICare, an interpretable AI tool designed to support clinical decisions, and evaluates its effect on workload, trust, and user strategies.


<details>
  <summary>Details</summary>
Motivation: To increase clinician trust and adoption of AI in healthcare by addressing skepticism due to the opacity of current systems.

Method: Introduced AICare with risk prediction visualizations and LLM-driven recommendations, evaluated using a study involving 16 clinicians, objective/subjective metrics, and interviews.

Result: AICare reduced cognitive workload and fostered trust through verification. Junior clinicians used it for guidance, while experts challenged its logic.

Conclusion: AICare shows potential to act as a transparent partner for diverse clinical reasoning approaches, enhancing collaboration without replacing judgment.

Abstract: Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visualizations and LLM-driven diagnostic recommendations. Through a within-subjects counterbalanced study with 16 clinicians across nephrology and obstetrics, we comprehensively evaluated AICare using objective measures (task completion time and error rate), subjective assessments (NASA-TLX, SUS, and confidence ratings), and semi-structured interviews. Our findings indicate AICare's reduced cognitive workload. Beyond performance metrics, qualitative analysis reveals that trust is actively constructed through verification, with interaction strategies diverging by expertise: junior clinicians used the system as cognitive scaffolding to structure their analysis, while experts engaged in adversarial verification to challenge the AI's logic. This work offers design implications for creating AI systems that function as transparent partners, accommodating diverse reasoning styles to augment rather than replace clinical judgment.

</details>


### [1284] ["If You're Very Clever, No One Knows You've Used It": The Social Dynamics of Developing Generative AI Literacy in the Workplace](https://arxiv.org/abs/2602.01386)
*Qing,Xia,Marios Constantinides,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: The paper investigates how knowledge workers develop AI literacy with generative AI (GenAI) in professional settings through interviews, emphasizing transparency and collaboration for effective AI learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of empirical insights into how workplace social dynamics shape knowledge workers' understanding and use of generative AI tools.

Method: In-depth interviews were conducted with 19 knowledge workers from multiple sectors to explore how they build GenAI competencies in real-world contexts.

Result: The study found that while peer knowledge sharing supported learning, concealing GenAI use for credibility reduced learning opportunities and hindered transparency in the workplace.

Conclusion: Organizations should foster open dialogue, emphasize collaborative learning, and promote transparency to enhance AI literacy and enable effective use of GenAI tools in evolving professional environments.

Abstract: Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and how workers learn to apply GenAI tools in these environments. To address this gap, we conducted in-depth interviews with 19 knowledge workers across multiple sectors to examine how they develop GenAI competencies in real-world professional contexts. We found that, while knowledge sharing from colleagues supported learning, the ability to remove cues indicating GenAI use was perceived as validation of domain expertise. These behaviours ultimately reduced opportunities for learning via knowledge sharing and undermined transparency. To advance workplace AI literacy, we argue for fostering open dialogue, increasing visibility of user-generated knowledge, and greater emphasis on the benefits of collaborative learning for navigating rapid technological developments.

</details>


### [1285] [How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework](https://arxiv.org/abs/2602.01390)
*Lana Do,Gio Jung,Juvenal Francisco Barajas,Andrew Taylor Scott,Shasta Ihorn,Alexander Mario Blum,Vassilis Athitsos,Ilmi Yoon*

Main category: cs.HC

TL;DR: The paper investigates scalable quality assessment for audio descriptions (AD) in video content, devising a multi-dimensional evaluation framework and finding value in combining vision-language models (VLMs) with human oversight.


<details>
  <summary>Details</summary>
Motivation: Blind and low-vision audiences are excluded from full participation in digital video platforms due to the lack of effective and scalable mechanisms for audio description (AD) quality assurance.

Method: A multi-dimensional assessment framework for full-length video AD, based on professional standards and accessibility specialists' guidance, was developed. This was integrated into a workflow using Item Response Theory to assess VLMs and human raters against expert-ground truth data.

Result: VLMs display strong alignment with expert-defined AD ratings but show less actionable and reliable reasoning compared to humans. The hybrid system of VLMs and human evaluation shows potential for scalable AD quality control.

Conclusion: A hybrid evaluation approach incorporating VLMs and human oversight can improve AD quality assessment scalability while retaining reliability, indicating a promising step towards accessible video content for visually impaired audiences.

Abstract: Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluations rely on NLP metrics and short-clip guidelines, leaving questions about what constitutes quality for full-length content and how to assess it at scale. To address these questions, we first developed a multi-dimensional assessment framework for uninterrupted, full-length video, grounded in professional guidelines and refined by accessibility specialists. Second, we integrated this framework into a comprehensive methodological workflow, utilizing Item Response Theory, to assess the proficiency of VLM and human raters against expert-established ground truth. Findings suggest that while VLMs can approximate ground-truth ratings with high alignment, their reasoning was found to be less reliable and actionable than that of human respondents. These insights show the potential of hybrid evaluation systems that leverage VLMs alongside human oversight, offering a path towards scalable AD quality control.

</details>


### [1286] [Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning](https://arxiv.org/abs/2602.01494)
*Yuqi Hang*

Main category: cs.HC

TL;DR: The paper introduces Draw2Learn, an AI system designed to support drawing-based learning by generating drawing quests, providing feedback, and fostering learner autonomy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of providing timely feedback at scale in drawing-based learning, aiming to enhance educational outcomes.

Method: The system, Draw2Learn, uses AI to generate drawing tasks, offer visual scaffolding, monitor progress, and provide multidimensional feedback. User feedback and open-ended comments were gathered to refine the design.

Result: Formative feedback showed positive user ratings on usability, usefulness, and user experience, emphasizing the value of AI scaffolding and learner autonomy.

Conclusion: The paper contributes a design framework for AI as a supportive teammate in drawing-based learning and identifies key areas for future research and improvement.

Abstract: Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interaction patterns: AI generates structured drawing quests, provides optional visual scaffolds, monitors progress, and delivers multidimensional feedback. We collected formative user feedback during system development and open-ended comments. Feedback showed positive ratings for usability, usefulness, and user experience, with themes highlighting AI scaffolding value and learner autonomy. This work contributes a design framework for teammate-oriented AI in generative learning and identifies key considerations for future research.

</details>


### [1287] [AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces](https://arxiv.org/abs/2602.01671)
*Mona Rajhans*

Main category: cs.HC

TL;DR: The paper introduces an AI framework for adaptive rendering in high-volume cybersecurity platforms, addressing UI performance challenges effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional rendering methods fail under high data volumes, causing user interface issues in real-time cybersecurity contexts.

Method: The framework uses AI for adaptive rendering, semantic event prioritization, selective data aggregation, and lightweight machine learning on-device.

Result: The framework reduced rendering overhead by 45-60% while preserving real-time responsiveness for analysts.

Conclusion: AI-driven adaptive rendering improves performance and responsiveness in cybersecurity platforms under high-frequency data loads.

Abstract: Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of thousands of events per second, leading to UI freezes, dropped frames, or stale data. This paper presents an AI-assisted adaptive rendering framework that dynamically regulates visual update frequency, prioritizes semantically relevant events, and selectively aggregates lower-priority data using behavior-driven heuristics and lightweight on-device machine learning models. Experimental validation demonstrates a 45-60 percent reduction in rendering overhead while maintaining analyst perception of real-time responsiveness.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [1288] [Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting](https://arxiv.org/abs/2602.00378)
*Md Amran Hossan Mojamder,Zhihang Xu,Min Wang,Ilya Timofeyev*

Main category: physics.flu-dyn

TL;DR: The paper presents a neural network-based method to parameterize sub-grid processes in Shallow Water equations, improving energy balance, reproducing individual solutions, and functioning reliably in unseen dynamical regimes.


<details>
  <summary>Details</summary>
Motivation: To create a robust and localized solution for parameterizing sub-grid processes in turbulent simulations while addressing energy imbalance and shock oscillations.

Method: A feed-forward neural network learns sub-grid fluxes using coarse variables and local spatial averages, incorporating a four-point computational stencil. It combines with flux limiting to handle shocks.

Result: The method enhances energy balance in long-term simulations, reproduces individual solutions accurately, and performs reliably even in untrained dynamical regimes.

Conclusion: This neural network-based parametrization provides a reliable, localized approach to improve turbulent simulation results, applicable across various regimes and conditions.

Abstract: We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1289] [Dimension-Free Multimodal Sampling via Preconditioned Annealed Langevin Dynamics](https://arxiv.org/abs/2602.01449)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: math.NA

TL;DR: This paper studies Annealed Langevin Dynamics (ALD) for multimodal targets, focusing on its stability and efficiency across high dimensions. The researchers provide theoretical insights into Gaussian mixture models and analyze continuous-time ALD under various configurations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps between empirical successes of Annealed Langevin Dynamics and its theoretical guarantees, particularly its stability and accuracy for multimodal distributions in high dimensions.

Method: The study analyzes ALD in continuous time using Gaussian mixture models and derives sufficient spectral conditions along annealing paths. It investigates robustness under imperfect initialization and score approximation, providing conditions to ensure dimension-uniform performance.

Result: The paper establishes explicit conditions for ALD stability, spectral preconditioning requirements, and demonstrates dimension robustness for misspecified models. Numerical experiments validate these theoretical findings.

Conclusion: Annealed Langevin Dynamics is shown to achieve dimension-uniform accuracy under specified conditions, enhancing its reliability for exploring multimodal distributions in high-dimensional spaces.

Abstract: Designing algorithms that can explore multimodal target distributions accurately across successive refinements of an underlying high-dimensional problem is a central challenge in sampling. Annealed Langevin dynamics (ALD) is a widely used alternative to classical Langevin since it often yields much faster mixing on multimodal targets, but there is still a gap between this empirical success and existing theory: when, and under which design choices, can ALD be guaranteed to remain stable as dimension increases? In this paper, we help bridge this gap by providing a uniform-in-dimension analysis of continuous-time ALD for multimodal targets that can be well-approximated by Gaussian mixture models. Along an explicit annealing path obtained by progressively removing Gaussian smoothing of the target, we identify sufficient spectral conditions - linking smoothing covariance and the covariances of the Gaussian components of the mixture - under which ALD achieves a prescribed accuracy within a single, dimension-uniform time horizon. We then establish dimension-robustness to imperfect initialization and score approximation: under a misspecified-mixture score model, we derive explicit conditions showing that preconditioning the ALD algorithm with a sufficiently decaying spectrum is necessary to prevent error terms from accumulating across coordinates and destroying dimension-uniform control. Finally, numerical experiments illustrate and validate the theory.

</details>


### [1290] [Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions](https://arxiv.org/abs/2602.00386)
*Michał P. Karpowicz,Gilbert Strang*

Main category: math.NA

TL;DR: This paper provides a unifying framework for studying the Moore-Penrose pseudoinverse and generalized inverse of matrix products, introducing new randomized formulas and their applications.


<details>
  <summary>Details</summary>
Motivation: The study aims to unify the understanding of generalized and randomized matrix inverses, focusing on their geometric interpretation and practical applications, such as sparse sensor placement and resistance estimation.

Method: The analysis is based on the geometric properties of subspaces, presenting formulas like the reverse order law, universally correct formula, and novel generalized randomized formula that incorporates sketching matrices.

Result: Key findings include conditions for the formulas' validity, a rigorous quantitative analysis for resistance estimation, and revealing the structure underlying various randomized linear algebra algorithms.

Conclusion: The paper highlights the theoretical insights and broad applicability of generalized and randomized matrix inverses, particularly in advanced computational algorithms and network analysis tasks.

Abstract: We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine:
  (1) the reverse order law, $A^+ = R^+C^+$, which holds when $C$ has independent columns and $R$ has independent rows,
  (2) the universally correct formula, $A^+ = (C^+CR)^+(CRR^+)^+$, providing a geometric interpretation of the mappings between the involved subspaces,
  (3) a new generalized randomized formula, $A^+_p = (P^TA)^+P^TAQ(AQ)^+$, which gives $A^+_p = A^+$ if and only if the sketching matrices $P$ and $Q$ preserve the rank of $A$, i.e., $\mathrm{rank}(P^TA) = \mathrm{rank}(AQ) = \mathrm{rank}(A)$.
  The framework is extended to generalized $\{1,2\}$-inverses and specialized forms, revealing the underlying structure of established randomized linear algebra algorithms, including randomized SVD, the Nyström approximation, and CUR decomposition. We demonstrate applications in sparse sensor placement and effective resistance estimation. For the latter, we provide a rigorous quantitative analysis of an approximation scheme, establishing that it always underestimates the true resistance and deriving a worst-case spectral bound on the error of resistance differences.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [1291] [Comparison of Image Processing Models in Quark Gluon Jet Classification](https://arxiv.org/abs/2602.00141)
*Daeun Kim,Jiwon Lee,Wonjun Jeong,Hyeongwoo Noh,Giyeong Kim,Jaeyoon Cho,Geonhee Kwak,Seunghwan Yang,MinJung Kweon*

Main category: physics.data-an

TL;DR: The paper compares CNNs, Vision Transformers, and Swin Transformers for quark and gluon jet classification using jet image data, achieving high accuracy and efficiency with fine-tuned Swin-Tiny models.


<details>
  <summary>Details</summary>
Motivation: To identify the most effective deep learning architecture for distinguishing quark and gluon jets, leveraging simulated data and considering both learning efficiency and accuracy.

Method: The study uses CNNs, ViTs, and Swin Transformers to analyze encoded jet substructure data, under supervised and self-supervised learning paradigms, with fine-tuning of transformer layers and self-supervised pretraining strategies like MoCo.

Result: The Swin-Tiny model, with limited transformer block fine-tuning, achieved 81.4% accuracy and an AUC of 88.9%. Self-supervised MoCo pretraining enhanced feature robustness and reduced trainable parameters.

Conclusion: Hierarchical attention-based models, particularly Swin Transfomers, are highly effective for jet substructure classification and possess potential for application in real collision data scenarios.

Abstract: We present a comprehensive comparison of convolutional and transformer-based models for distinguishing quark and gluon jets using simulated jet images from Pythia 8. By encoding jet substructure into a three-channel representation of particle kinematics, we evaluate the performance of convolutional neural networks (CNNs), Vision Transformers (ViTs), and Swin Transformers (Swin-Tiny) under both supervised and self-supervised learning setups. Our results show that fine-tuning only the final two transformer blocks of the Swin-Tiny model achieves the best trade-off between efficiency and accuracy, reaching 81.4% accuracy and an AUC (area under the ROC curve) of 88.9%. Self-supervised pretraining with Momentum Contrast (MoCo) further enhances feature robustness and reduces the number of trainable parameters. These findings highlight the potential of hierarchical attention-based models for jet substructure studies and for domain transfer to real collision data.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1292] [Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding](https://arxiv.org/abs/2602.02167)
*Soheil Behnam Roudsari,Alexandre S. Brandão,Felipe N. Martins*

Main category: eess.SP

TL;DR: The paper introduces a camera-free 2D LiDAR object detection pipeline that efficiently encodes temporal data for indoor service robots, achieving high accuracy and real-time performance on embedded hardware.


<details>
  <summary>Details</summary>
Motivation: To create a privacy-friendly and robust perception system for indoor service robots that operates effectively on embedded systems without relying on RGB video.

Method: The authors propose encoding three consecutive 2D LiDAR scans as RGB channels, which serves as input for the YOLOv8n neural network. This approach preserves motion and angular data while eliminating the need for occupancy-grid construction.

Result: Simulations demonstrate that this method achieves 98.4% mAP@0.5 with 94.9% precision and 94.7% recall, running in real time on a Raspberry Pi 5 with a latency of 47.8ms per frame.

Conclusion: The study highlights a promising lightweight LiDAR-based object detection method that is accurate, privacy-preserving, and well-suited for real-time use in embedded robotic systems.

Abstract: Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.

</details>


### [1293] [JSR-GFNet: Jamming-to-Signal Ratio-Aware Dynamic Gating for Interference Classification in future Cognitive Global Navigation Satellite Systems](https://arxiv.org/abs/2602.00042)
*Zhihan Zeng,Hongyuan Shu,Kaihe Wang,Lu Chen,Amir Hussian,Yanjun Huang,Junchu Zhao,Yue Xiu,Zhongpei Zhang*

Main category: eess.SP

TL;DR: This paper introduces JSR-Guided Fusion Network (JSR-GFNet), addressing issues of noise interference and feature degeneracy for interference classification in GNSS systems. It effectively utilizes IQ samples and STFT spectrograms to enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: This work aims to overcome limitations in GNSS interference classification methods, such as poor performance in low JSR and difficulty distinguishing spectrally similar signals due to phase information loss.

Method: The proposed approach, JSR-GFNet, uses a multi-modal architecture combining phase-sensitive IQ samples and STFT spectrograms, with a dynamic gating mechanism to reweight contributions from different streams based on signal characteristics.

Result: Experiments using the CGI-21 dataset show that JSR-GFNet delivers superior accuracy across JSR ranges (10–50 dB) and adapts efficiently by prioritizing energy integration or phase precision depending on interference conditions.

Conclusion: JSR-GFNet advances interference classification for cognitive GNSS systems by leveraging a physics-inspired fusion approach, offering a highly reliable and interpretable solution for aerospace navigation security.

Abstract: The transition toward cognitive global navigation satellite system (GNSS) receivers requires accurate interference classification to trigger adaptive mitigation strategies. However, conventional methods relying on Time-Frequency Analysis (TFA) and Convolutional Neural Networks (CNNs) face two fundamental limitations: severe performance degradation in low Jamming-to-Signal Ratio (JSR) regimes due to noise obscuration, and ``feature degeneracy'' caused by the loss of phase information in magnitude-only spectrograms. Consequently, spectrally similar signals -- such as high-order Quadrature Amplitude Modulation versus Band-Limited Gaussian Noise -- become indistinguishable. To overcome these challenges, this paper proposes the \textbf{JSR-Guided Fusion Network (JSR-GFNet)}. This multi-modal architecture combines phase-sensitive complex In-Phase/Quadrature (IQ) samples with Short-Time Fourier Transform (STFT) spectrograms. Central to this framework is a physics-inspired dynamic gating mechanism driven by statistical signal descriptors. Acting as a conditional controller, it autonomously estimates signal reliability to dynamically reweight the contributions of a Complex-Valued ResNet (IQ stream) and an EfficientNet backbone (STFT stream). To validate the model, we introduce the Comprehensive GNSS Interference (CGI-21) dataset, simulating 21 jamming categories including software-defined waveforms from aerial platforms. Extensive experiments demonstrate that JSR-GFNet achieves higher accuracy across the full 10--50 dB JSR spectrum. Notably, interpretability analysis confirms that the model learns a physically intuitive strategy: prioritizing spectral energy integration in noise-limited regimes while shifting focus to phase precision in high-SNR scenarios to resolve modulation ambiguities. This framework provides a robust solution for next-generation aerospace navigation security.

</details>


### [1294] [Visible Light Positioning With Lamé Curve LEDs: A Generic Approach for Camera Pose Estimation](https://arxiv.org/abs/2602.01577)
*Wenxuan Pan,Yang Yang,Dong Wei,Zhiyu Zhu,Jintao Wang,Huan Wu,Yao Nie*

Main category: eess.SP

TL;DR: This paper presents LC-VLP, a unified camera-based visible light positioning algorithm leveraging Lamé curve shapes for accurate indoor camera pose estimation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current VLP approaches that fail with heterogeneous LED shapes and to achieve better camera pose estimation in diverse indoor environments.

Method: The proposed LC-VLP algorithm utilizes Lamé curve-shaped LEDs, with periodic transmission of curve parameters via visible light communication. Offline LED database and online nonlinear least-squares optimization are employed, alongside a FreePnP algorithm for robust initialization.

Result: LC-VLP outperforms state-of-the-art methods by reducing position and rotation errors significantly in simulations, and achieves an average positional accuracy of below 4 cm in experimental settings.

Conclusion: LC-VLP offers a generic and accurate VLP solution using diverse LED geometries, demonstrating superior performance and practical feasibility in both simulations and real-world experiments.

Abstract: Camera-based visible light positioning (VLP) is a promising technique for accurate and low-cost indoor camera pose estimation (CPE). To reduce the number of required light-emitting diodes (LEDs), advanced methods commonly exploit LED shape features for positioning. Although interesting, they are typically restricted to a single LED geometry, leading to failure in heterogeneous LED-shape scenarios. To address this challenge, this paper investigates Lamé curves as a unified representation of common LED shapes and proposes a generic VLP algorithm using Lamé curve-shaped LEDs, termed LC-VLP. In the considered system, multiple ceiling-mounted Lamé curve-shaped LEDs periodically broadcast their curve parameters via visible light communication, which are captured by a camera-equipped receiver. Based on the received LED images and curve parameters, the receiver can estimate the camera pose using LC-VLP. Specifically, an LED database is constructed offline to store the curve parameters, while online positioning is formulated as a nonlinear least-squares problem and solved iteratively. To provide a reliable initialization, a correspondence-free perspective-\textit{n}-points (FreeP\textit{n}P) algorithm is further developed, enabling approximate CPE without any pre-calibrated reference points. The performance of LC-VLP is verified by both simulations and experiments. Simulations show that LC-VLP outperforms state-of-the-art methods in both circular- and rectangular-LED scenarios, achieving reductions of over 40% in position error and 25% in rotation error. Experiments further show that LC-VLP can achieve an average position accuracy of less than 4 cm.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [1295] [Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2602.01008)
*Yang Xiao,Eun-Jung Holden,Ting Dang*

Main category: eess.AS

TL;DR: The paper addresses the challenge of adapting multilingual speech foundation models to low-resource languages by proposing a Depth-Aware Model Adaptation (DAMA) framework, which balances parameter efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing speech foundation models perform well on high-resource languages but struggle with low-resource ones, due to data scarcity and inefficient adaptation methods.

Method: The DAMA framework adapts multilingual ASR models by leveraging a U-shaped adaptability pattern across layers, and introduces SVD-based initialization and frozen middle-layer basis for efficiency.

Result: DAMA matches or surpasses the state of the art in accuracy with 80% fewer trainable parameters, reduces errors by 29% in scarce data contexts, and improves memory, training time, and computational efficiency.

Conclusion: Structure-aware adaptation like DAMA effectively addresses low-resource language challenges for multilingual ASR while significantly enhancing efficiency and scalability.

Abstract: Recent speech foundation models excel at multilingual automatic speech recognition (ASR) for high-resource languages, but adapting them to low-resource languages remains challenging due to data scarcity and efficiency constraints. Full-model fine-tuning is computationally expensive and prone to overfitting, while parameter-efficient methods like LoRA apply adaptation uniformly across layers, overlooking internal representations thus compromising effectiveness and efficiency. We analyze multilingual ASR models and reveal a U-shaped adaptability pattern: early and late layers are language-specific and require more adaptation, while intermediate layers retain shared semantics and need less. Building on this observation, we propose DAMA, a Depth-Aware Model Adaptation framework that allocates adaptation capacity according to each layer's role. DAMA also introduces Singular Value Decomposition (SVD)-based initialization to constrain adaptation and preserve the U-shaped pattern, as well as a frozen middle-layer basis for further efficiency. Evaluated on 18 low-resource languages across two benchmark datasets, DAMA matches or surpasses state-of-the-art accuracy with 80% fewer trainable parameters, achieves a 29% error reduction under extreme data scarcity, and significantly improves memory, training time, and computational efficiency over baselines. These results highlight the benefits of structure-aware adaptation for efficient, scalable multilingual ASR.

</details>


### [1296] [HuPER: A Human-Inspired Framework for Phonetic Perception](https://arxiv.org/abs/2602.01634)
*Chenxu Guo,Jiachen Lian,Yisi Liu,Baihe Huang,Shriyaa Narayanan,Cheol Jun Cho,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: The paper introduces HuPER, an adaptive phonetic perception framework that achieves top-notch performance on English benchmarks and unseen languages with limited training data.


<details>
  <summary>Details</summary>
Motivation: To model phonetic perception as adaptive inference over both acoustic-phonetic evidence and linguistic knowledge, and to address challenges in phonetic perception under diverse acoustic conditions.

Method: Developing the HuPER framework which integrates adaptive, multi-path inference that leverages acoustic and linguistic data.

Result: HuPER achieves state-of-the-art phonetic error rates on English benchmarks, transfers effectively to 95 unseen languages, and adapts to varied acoustic environments.

Conclusion: HuPER demonstrates strong performance with minimal training data and versatility across languages and conditions, representing an advancement in the field of phonetic modeling.

Abstract: We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1297] [Fast Sparse Matrix Permutation for Mesh-Based Direct Solvers](https://arxiv.org/abs/2602.00898)
*Behrooz Zarebavami,Ahmed H. Mahmoud,Ana Dodik,Changcheng Yuan,Serban D. Porumbescu,John D. Owens,Maryam Mehri Dehnavi,Justin Solomon*

Main category: cs.GR

TL;DR: The paper presents a fast algorithm for sparse matrix permutations, improving runtime and performance for Cholesky factorizations in graphics applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency in existing sparse matrix permutation algorithms for triangle meshes, particularly to enhance speed and performance for use in graphics applications.

Method: The method involves a relaxed approach to permutation design, breaking it into patch-level orderings and a quotient-graph ordering. This simplifies sparse Cholesky factorization while maintaining efficient partitioning.

Result: The algorithm reduced permutation time and improved performance of sparse Cholesky solvers (CPU and GPU) by up to 6.27x in graphics-related tasks.

Conclusion: This approach demonstrates significant efficiency gains for linear systems applied to triangle meshes, making it highly beneficial for repeated factorizations and various graphics applications.

Abstract: We present a fast sparse matrix permutation algorithm tailored to linear systems arising from triangle meshes. Our approach produces nested-dissection-style permutations while significantly reducing permutation runtime overhead. Rather than enforcing strict balance and separator optimality, the algorithm deliberately relaxes these design decisions to favor fast partitioning and efficient elimination-tree construction. Our method decomposes permutation into patch-level local orderings and a compact quotient-graph ordering of separators, preserving the essential structure required by sparse Cholesky factorization while avoiding its most expensive components. We integrate our algorithm into vendor-maintained sparse Cholesky solvers on both CPUs and GPUs. Across a range of graphics applications, including single factorizations, repeated factorizations, our method reduces permutation time and improves the sparse Cholesky solve performance by up to 6.27x.

</details>


### [1298] [Genus-0 Surface Parameterization using Spherical Beltrami Differentials](https://arxiv.org/abs/2602.01589)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.GR

TL;DR: The paper introduces Spherical Beltrami Differential (SBD) to address challenges in spherical mapping tasks, focusing on preserving bijectivity, minimizing distortion, and improving landmark alignment. It proposes the BOOST neural optimization framework for practical applications, demonstrated through experiments on brain cortical surface registration.


<details>
  <summary>Details</summary>
Motivation: Existing methods for spherical mapping often struggle to balance task-driven objectives (e.g., feature alignment), geometric distortion control, and bijectivity. The authors aim to overcome this limitation by introducing a novel approach.

Method: The authors propose the SBD model, based on quasiconformal self-maps with a two-chart representation, and implement it using the BOOST framework. The framework employs neural optimization over Beltrami fields while maintaining consistency through seam-aware constraints.

Result: Experiments reveal that the proposed framework excels in large-deformation landmark matching and intensity-based spherical registration, particularly in brain cortical surface applications. It enhances task alignment, reduces distortion, and ensures bijective mappings.

Conclusion: The framework effectively improves spherical mapping tasks by combining geometric accuracy, landmark alignment, and robust bijectivity, with promising applications in imaging sciences.

Abstract: Spherical surface parameterization is a fundamental tool in geometry processing and imaging science. For a genus-0 closed surface, many efficient algorithms can map the surface to the sphere; consequently, a broad class of task-driven genus-0 mapping problems can be reduced to constructing a high-quality spherical self-map. However, existing approaches often face a trade-off between satisfying task objectives (e.g., landmark or feature alignment), maintaining bijectivity, and controlling geometric distortion. We introduce the Spherical Beltrami Differential (SBD), a two-chart representation of quasiconformal self-maps of the sphere, and establish its correspondence with spherical homeomorphisms up to conformal automorphisms. Building on the Spectral Beltrami Network (SBN), we propose a neural optimization framework BOOST that optimizes two Beltrami fields on hemispherical stereographic charts and enforces global consistency through explicit seam-aware constraints. Experiments on large-deformation landmark matching and intensity-based spherical registration demonstrate the effectiveness of our proposed framework. We further apply the method to brain cortical surface registration, aligning sulcal landmarks and jointly matching cortical sulci depth maps, showing improved task fidelity with controlled distortion and robust bijective behavior.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1299] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

TL;DR: The paper introduces a framework to automate knowledge graph creation and generate personalized exercises using LLMs.


<details>
  <summary>Details</summary>
Motivation: Address limitations in manual curation of knowledge graphs and static question banks in current personalized education systems.

Method: Developing the Generative GraphRAG framework with Auto-HKG for hierarchical knowledge graph construction and CG-RAG for personalized exercise generation via graph-based reasoning.

Result: The framework has been deployed in real-world educational scenarios, receiving positive user feedback.

Conclusion: Generative GraphRAG has the potential to enhance personalized education systems by automating knowledge modeling and adapting exercises to learner states.

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [1300] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

TL;DR: This study investigates the use of critical slowing down (CSD) indicators to predict student disengagement in learning environments before it occurs.


<details>
  <summary>Details</summary>
Motivation: The motivation for the research is to address the risk of student disengagement and dropout by finding early warning signals to allow interventions during a 'window of hope.'

Method: The researchers analyzed 1.67 million practice attempts by 9,401 students in a digital math learning environment, computing various CSD indicators such as autocorrelation, variance, and skewness to detect resilience loss.

Result: The study found that 88.2% of students showed signs of resilience loss, indicated by CSD dynamics, prior to disengagement. These signals clustered before students stopped participating.

Conclusion: The findings suggest that CSD indicators can serve as universal signals to detect vulnerability early across different learning contexts and systems, thereby supporting timely interventions and improving learning outcomes.

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [1301] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

TL;DR: The paper explores how to integrate generative AI into education by using its inherent uncertainty to foster critical thinking, rather than banning AI use.


<details>
  <summary>Details</summary>
Motivation: Current generative AI tools can provide correct answers but often bypass the educational emphasis on reasoning and understanding, jeopardizing meaningful learning.

Method: The authors propose using AI to create uncertain scenarios in exams. They introduce a system named MindMosaicAIExam where students critically analyze AI outputs, refine answers, and justify their reasoning.

Result: MindMosaicAIExam system was designed to guide students towards critical evaluations by controlling AI's behaviors, such as producing flawed or incomplete answers.

Conclusion: Controllable AI systems and tailored assessments can serve as effective pedagogical tools to bolster critical thinking in students.

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [1302] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

TL;DR: This paper audits and analyzes demographic and emotion-related biases in synthetic face generation across eight text-to-image models developed in Western and Chinese contexts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of biases, representational quality, and cross-cultural consistency in text-to-image models, especially regarding emotional and demographic factors.

Method: The authors systematically audited eight T2I models (four Western and four Chinese) using identical prompts. They analyzed outputs with facial analysis algorithms to estimate characteristics like gender, race, age, and attractiveness, applying statistical bias metrics like Kullback-Leibler and Jensen-Shannon divergences.

Result: All T2I models showed persistent demographic and emotion-conditioned biases, irrespective of their cultural or geographic origins.

Conclusion: The findings emphasize the need for fairness, mitigation of socio-technical harms, and more transparent development in generative models across cultural contexts.

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [1303] [Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation](https://arxiv.org/abs/2602.00034)
*Matias Hoyl*

Main category: cs.CY

TL;DR: This paper aims to predict question difficulty in educational assessments without student testing by employing neural networks and features from traditional linguistics and Large Language Models (LLMs), achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods of determining question difficulty rely on resource-demanding pre-testing, which creates barriers for educators and assessment developers.

Method: The paper proposes a two-stage model: 1) Training a neural network to simulate how students respond to questions, 2) Deriving difficulty parameters from simulated responses, integrating linguistic and pedagogical features derived from LLMs.

Result: The model was tested on a mathematics dataset with over 250,000 student responses, achieving a Pearson correlation of 0.78 between predicted and actual difficulty parameters on unseen questions.

Conclusion: The approach demonstrates the feasibility of estimating question difficulty without administering tests to students, potentially streamlining and democratizing the assessment process.

Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty parameters can be accurately estimated without student testing by modeling the response process and explore the relative contribution of different feature types to prediction accuracy. Our approach combines traditional linguistic features with pedagogical insights extracted using Large Language Models (LLMs), including solution step count, cognitive complexity, and potential misconceptions. We implement a two-stage process: first training a neural network to predict how students would respond to questions, then deriving difficulty parameters from these simulated response patterns. Using a dataset of over 250,000 student responses to mathematics questions, our model achieves a Pearson correlation of approximately 0.78 between predicted and actual difficulty parameters on completely unseen questions.

</details>


### [1304] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

TL;DR: The authors propose a novel safety alignment method, LSSF, leveraging low-rank characteristics in safety information to efficiently restore safety in fine-tuned LLMs with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods for LLMs are prone to fragility and complexity, requiring extensive fine-tuning that compromises safety capabilities and demands high computational resources.

Method: The LSSF framework uses a low-rank projection matrix to isolate and restore safety information in LLMs by exploiting stable low-rank subspaces and introducing a dynamic safety singular value entropy metric.

Result: Experiments show that LSSF restores safety alignment in fine-tuned models without compromising performance on downstream tasks.

Conclusion: LSSF provides an efficient, post-hoc approach to enhance safety alignment in LLMs, addressing shortcomings in traditional fine-tuning-based methods.

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [1305] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

TL;DR: This study examines using Large Language Models (LLMs) for feedback in architectural design education, focusing on self-reflection, peer critique, and professor-led reviews.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can shift architectural design feedback from generative outputs to enhancing reflective pedagogy.

Method: The research employed mixed-methods with architecture students at Singapore University of Technology and Design, analyzing perceptions in feedback contexts: self-reflection, peer critique, and professor-led reviews.

Result: Students see LLMs as 'cognitive mirrors' aiding critical thinking. They help overcome self-reflection issues, reduce peer critique anxiety, and synthesize feedback in professor-led sessions.

Conclusion: LLMs are not seen as authoritative, but as collaborative tools for scaffolding thought processes and managing cognitive challenges in design education.

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [1306] [FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding of LLMs](https://arxiv.org/abs/2602.00070)
*Eamon Worden,Cristina Heffernan,Neil Heffernan,Shashank Sonkar*

Main category: cs.CY

TL;DR: This paper introduces FoundationalASSIST, the first dataset designed to evaluate and develop the potential of large language models (LLMs) in education, highlighting key limitations in current LLM performance for personalized learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in the educational domain where existing datasets lack the comprehensive information needed for research on LLMs' potential to understand student learning processes and improve adaptive testing and personalized tutoring.

Method: The authors developed FoundationalASSIST, a dataset containing full question text, student responses, common wrong answers, and alignment to K-12 standards. They evaluated LLMs' performance using two task families: Knowledge Tracing and Pedagogical Grounding.

Result: Results showed significant gaps in LLMs' abilities. Models struggled in Knowledge Tracing tasks, barely surpassing basic baselines, and performed poorly in identifying informative questions (item discrimination). They had limited success in predicting relative difficulty.

Conclusion: While partial progress was seen in predicting difficulty, this analysis highlights the significant advancements required before LLMs can effectively support personalized educational tools. The FoundationalASSIST dataset was released to encourage research progress in this area.

Abstract: Can Large Language Models understand how students learn? As LLMs are deployed for adaptive testing and personalized tutoring, this question becomes urgent -- yet we cannot answer it with existing resources. Current educational datasets provide only question identifiers and binary correctness labels, rendering them opaque to LLMs that reason in natural language. We address this gap with FoundationalASSIST, the first English educational dataset providing the complete information needed for research on LLMs in education: full question text, actual student responses (not just right/wrong), records of which wrong answers students chose, and alignment to Common Core K-12 standards. These 1.7 million interactions from 5,000 students enable research directions that were previously impossible to pursue, from fine-tuning student models to analyzing misconception patterns. To demonstrate the dataset's utility, we evaluate four frontier models (GPT-OSS-120B, Llama-3.3-70B, Qwen3-Next-80B variants) on two complementary task families: Knowledge Tracing, testing whether LLMs can predict student performance on questions, and the exact answer a student will give; and \textbf{Pedagogical Grounding}, testing whether LLMs understand the properties that make assessment items effective. Our evaluation reveals significant gaps in current LLM capabilities. Every model barely achieves a trivial baseline on knowledge tracing. All models fall below random chance on item discrimination, indicating that LLMs do not understand what makes one problem more diagnostic than another. Models do show competence at judging relative difficulty (up to 68.6%), but this partial success only highlights the gaps elsewhere. These results establish that substantial advances are needed before LLMs can reliably support personalized learning at scale. We release FoundationalASSIST to support progress on these foundational challenges.

</details>


### [1307] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

TL;DR: This paper introduces the Persona Brainstorm Audit (PBA) to transparently and scalably detect biases in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: There is a need for effective methods to audit and address bias in LLMs, as biased outputs can perpetuate stereotypes and inequities in their application.

Method: PBA is a novel auditing method leveraging open-ended persona generation to detect bias. It avoids relying on fixed categories or benchmarks, supports longitudinal bias tracking, and mitigates data leakage risks. The study applied PBA to 12 LLMs and analyzed bias patterns across models and generations.

Result: The application of PBA revealed model-specific and generation-specific bias patterns, providing insights into how biases change or persist over time. Robustness tests confirmed the stability of PBA across different conditions.

Conclusion: PBA is a reliable tool for detecting and tracking biases in LLMs, enabling more transparent and fair AI systems.

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [1308] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

TL;DR: The paper investigates the cost and implications of large-scale data creation in AI, highlighting environmental, social, and economic impacts, particularly for vulnerable groups.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the transition in AI from using pre-existing datasets to actively creating data and understanding its societal consequences.

Method: The study analyzed 550,000 datasets from the Hugging Face Hub, examined energy consumption, societal representation through qualitative insights from data workers, and data centre disparities.

Result: The findings demonstrate increased resource consumption and highlight the redistribution of environmental burdens, labour risks, and cultural representation harms, disproportionately affecting the Global South.

Conclusion: The authors propose Data PROOFS recommendations to address these issues and encourage a debate on the hidden costs of data in frontier AI development.

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [1309] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

TL;DR: This paper discusses GEOFRAIL, a dataset monitoring frail older adults post-hospital discharge, integrating sensor and geospatial data for recovery analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges frail older adults face post-hospital discharge, such as functional decline and isolation, and explores how neighborhood environments impact recovery.

Method: The method involves collecting longitudinal, multimodal datasets from sensors, clinical assessments, and geospatial data, standardized for privacy and consistency, over eight weeks after hospital discharge.

Result: The GEOFRAIL dataset showed internal consistency and serves as the basis for evaluating machine learning models in tracking recovery trajectories.

Conclusion: By combining multimodal, geospatial data, the dataset offers a valuable tool for understanding and predicting the recovery of frail older adults in real-world settings.

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [1310] [Simple Role Assignment is Extraordinarily Effective for Safety Alignment](https://arxiv.org/abs/2602.00061)
*Zhou Ziheng,Jiakun Ding,Zhaowei Zhang,Ruosen Gao,Yingnian Wu,Demetri Terzopoulos,Yipeng Kang,Fangwei Zhong,Junqi Wang*

Main category: cs.CY

TL;DR: The paper presents a role conditioning paradigm for AI alignment based on Theory of Mind, using roles like 'mother' or 'judge' to encode values and schemas, which surpasses principle-based models across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the context sensitivity and completeness of principle-based AI alignment approaches by leveraging social role-based conditioning informed by Theory of Mind.

Method: A training-free pipeline was designed featuring a role-conditioned generator and iterative role-based critics to refine outputs effectively.

Result: The model outperformed principle-based and Chain-of-Thought (CoT) strategies, notably reducing unsafe outputs on the WildJailbreak benchmark from 81.4% to 3.6% with DeepSeek-V3, and proved applicable to agentic safety tasks.

Conclusion: Role assignment, based on roles embodying values and cognitive schemas, emerges as an effective and interpretable paradigm for AI alignment and constructing LLM-as-a-Judge systems.

Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-free pipeline featuring a role-conditioned generator and iterative role-based critics for refinement. Across five model families, our approach consistently outperforms principle-based, Chain-of-Thought (CoT) and other baselines across benchmarks. Notably, it reduces unsafe outputs on the WildJailbreak benchmark from 81.4\% to 3.6\% with DeepSeek-V3. Not only for common safety benchmarks, it consistently applies for agentic safety tasks. These results establish role assignment as a powerful, interpretable paradigm for AI alignment and LLM-as-a-Judge construction.

</details>


### [1311] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

TL;DR: This paper identifies gaps and proposes a comprehensive framework for evaluating AI tools in mental health care.


<details>
  <summary>Details</summary>
Motivation: To address fragmented and poorly aligned evaluation methods of AI tools in mental health care that fail to integrate clinical, social, and user perspectives.

Method: The authors analyzed 135 recent *CL publications to identify evaluation gaps and proposed a taxonomy for categorizing AI mental health support types, complemented by case studies.

Result: The study highlighted recurring limitations such as over-reliance on generic metrics, lack of mental health professional input, and insufficient focus on safety and equity in existing evaluations.

Conclusion: An interdisciplinary evaluation framework and taxonomy provide a structured and responsible evaluation approach for AI tools in mental health care.

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [1312] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

TL;DR: This paper introduces ChatEHR, a system integrating large language models with electronic health records for various clinical tasks, showing significant adoption and savings while enhancing healthcare processes.


<details>
  <summary>Details</summary>
Motivation: To reduce workflow friction associated with traditional large language model tools and enhance their utility in healthcare by integrating them with complete patient timelines in electronic health records.

Method: Developed ChatEHR as a user interface system combining LLM-driven automations and interactive usage, enabling access to patient medical records and task-specific LLM implementations.

Result: Created 7 automations, trained 1,075 users, and completed 23,000 sessions in 3 months, with an estimated $6M savings in the first year and improved efficiency in clinical tasks.

Conclusion: ChatEHR demonstrates the feasibility and benefits of health systems adopting internally governed, vendor-agnostic LLM platforms to enhance care and achieve cost savings.

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [1313] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

TL;DR: This paper investigates the development of European AI standardization under the AI Act, offering insights into the challenges and proposing a workable risk management scheme for implementation.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges posed by AI to standardization practices and to facilitate the alignment of technical and legal frameworks for AI regulation.

Method: The paper explains the CEN/CENELEC standardization process and proposes a layered approach combining horizontal and sectoral standards, along with risk management practices such as structured documentation and comprehensive logging.

Result: The proposed framework highlights how technical standards can translate legal obligations into engineering practices, ensuring scalable conformity assessments for AI systems.

Conclusion: Despite challenges in practical implementation, technical standards are vital to bridge legal compliance and engineering practices, ensuring robust AI regulation and system auditing.

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [1314] [Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges](https://arxiv.org/abs/2602.00091)
*Kumaran Rajaram,Patrick Nicolas Tinguely*

Main category: cs.CY

TL;DR: The paper examines how generative AI (GAI) can benefit SMEs by improving innovation and competitiveness, offering a roadmap and practical deployment strategies.


<details>
  <summary>Details</summary>
Motivation: To explore how SMEs can leverage GAI technology despite limited resources and technical expertise, and to identify strategies for its successful deployment.

Method: The authors use a sailing metaphor to specify strategic dimensions SMEs need to address: employee competency, leadership, organizational culture, collaboration, and external partnerships.

Result: The analysis identifies key dimensions SMEs must align to harness GAI effectively and provides practical recommendations.

Conclusion: SMEs can effectively utilize GAI by addressing key organizational and strategic dimensions, enhancing productivity and competitive advantage.

Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs can leverage this technology to streamline work processes and unleash innovation, thereby improving their product offerings and long-term competitiveness. This paper discusses how SMEs can navigate both the promises and challenges of GAI and offers a roadmap for deploying GAI. We introduce a sailing metaphor that reveals key strategic dimensions for GAI deployment: competency of employees, effective leadership and work values, organizational culture, collaboration and cooperation, and relationships with third parties. We offer practical recommendations that serve as a useful compass for successfully deploying GAI in SMEs.

</details>


### [1315] [DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning](https://arxiv.org/abs/2602.01578)
*Arijit Chakma,Peng He,Honglu Liu,Zeyuan Wang,Tingting Li,Tiffany D. Do,Feng Liu*

Main category: cs.CY

TL;DR: DrawSim-PD generates simulated NGSS-aligned science drawings with controllable imperfections to facilitate teacher training in diagnostic reasoning while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: To address privacy regulations that restrict access to authentic student work for teacher PD, while providing diverse science artifacts for teacher training.

Method: DrawSim-PD uses capability profiles to create coherent student artifacts, including drawings, reasoning narratives, and diagnostic maps, aligned with NGSS standards.

Result: Generated 10,000 artifacts across 100 NGSS topics verified by K–12 educators for alignment and utility; achieved >84% positive feedback.

Conclusion: DrawSim-PD effectively supports scalable teacher training by overcoming privacy barriers and advancing visual assessment research.

Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, student-like science drawings exhibiting controllable pedagogical imperfections to support teacher training. Central to our approach are apability profiles--structured cognitive states encoding what students at each performance level can and cannot yet demonstrate. These profiles ensure cross-modal coherence across generated outputs: (i) a student-like drawing, (ii) a first-person reasoning narrative, and (iii) a teacher-facing diagnostic concept map. Using 100 curated NGSS topics spanning K-12, we construct a corpus of 10,000 systematically structured artifacts. Through an expert-based feasibility evaluation, K--12 science educators verified the artifacts' alignment with NGSS expectations (>84% positive on core items) and utility for interpreting student thinking, while identifying refinement opportunities for grade-band extremes. We release this open infrastructure to overcome data scarcity barriers in visual assessment research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1316] [Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems](https://arxiv.org/abs/2602.01503)
*Afifah Kashif,Abdul Muhsin Hameed,Asim Iqbal*

Main category: cs.ET

TL;DR: Traditional AI governance frameworks do not address the unique characteristics of NeuroAI systems, requiring new assurance methods.


<details>
  <summary>Details</summary>
Motivation: Current frameworks are designed for static, centrally trained AI and cannot adequately assess innovative neuromorphic hardware and spiking neural networks.

Method: The paper identifies key limitations in existing AI governance approaches and proposes co-evolving assurance methods aligned with NeuroAI's architecture.

Result: Highlighting misalignment in metrics between traditional governance and NeuroAI, the paper demonstrates the need for regulatory updates.

Conclusion: Effective governance of NeuroAI requires technically grounded assurance methods adapted to its unique physics, learning dynamics, and efficiency.

Abstract: Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [1317] [Early warning prediction: Onsager-Machlup vs Schrödinger](https://arxiv.org/abs/2602.00143)
*Xiaoai Xu,Yixuan Zhou,Xiang Zhou,Jingqiao Duan,Ting Gao*

Main category: q-bio.QM

TL;DR: This paper proposes a new early-warning framework combining manifold learning and stochastic dynamical system modeling to predict critical transitions, such as epileptic seizures.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of predicting critical transitions in complex, high-dimensional systems, particularly in recognizing and extracting hidden signals indicating transitions like epileptic seizures.

Method: The study integrates six manifold learning methods, such as diffusion maps, with stochastic differential equation modeling to develop a new Score Function indicator for state transition prediction.

Result: The proposed Score Function indicator demonstrated increased sensitivity and robustness, enabling earlier identification of epileptic seizures and effectively capturing dynamic features throughout seizure stages.

Conclusion: The work provides a rigorous framework and practical approach for deriving early-warning signals from high-dimensional data, with significant improvements in seizure prediction ability.

Abstract: Predicting critical transitions in complex systems, such as epileptic seizures in the brain, represents a major challenge in scientific research. The high-dimensional characteristics and hidden critical signals further complicate early-warning tasks. This study proposes a novel early-warning framework that integrates manifold learning with stochastic dynamical system modeling. Through systematic comparison, six methods including diffusion maps (DM) are selected to construct low-dimensional representations. Based on these, a data-driven stochastic differential equation model is established to robustly estimate the probability evolution scoring function of the system. Building on this, a new Score Function (SF) indicator is defined by incorporating Schrödinger bridge theory to quantify the likelihood of significant state transitions in the system. Experiments demonstrate that this indicator exhibits higher sensitivity and robustness in epilepsy prediction, enables earlier identification of critical points, and clearly captures dynamic features across various stages before and after seizure onset. This work provides a systematic theoretical framework and practical methodology for extracting early-warning signals from high-dimensional data.

</details>


### [1318] [Rank-and-Reason: Multi-Agent Collaboration Accelerates Zero-Shot Protein Mutation Prediction](https://arxiv.org/abs/2602.00197)
*Yang Tan,Yuyuan Xi,Can Wu,Bozitao Zhong,Mingchen Li,Guisheng Fan,Jiankang Zhu,Yafeng Liang,Nanqing Dong,Liang Hong*

Main category: q-bio.QM

TL;DR: This paper introduces VenusRAR, an automated two-stage framework for improving zero-shot mutation prediction in protein engineering, achieving significant improvements over current methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing protein language models that often produce unreliable predictions for low-resource protein engineering tasks and depend heavily on manual expert validation, which is inefficient and subjective.

Method: VenusRAR operates in two stages: (1) Rank-Stage, where a Computational Expert and Virtual Biologist use a multi-modal ensemble to rank candidates; and (2) Reason-Stage, where an Expert Panel applies reasoning to audit candidates against biophysical constraints.

Result: The method achieves a new Spearman correlation record of 0.551 on ProteinGym and improves the Top-5 Hit Rate by up to 367% on ProteinGym-DMS99. Wet-lab experiments validate its effectiveness, leading to novel mutants with significantly improved activity.

Conclusion: VenusRAR automates and enhances mutation prediction for protein engineering, offering better accuracy and efficiency compared to existing models. Its application holds promise for advancing biophysical research and engineering.

Abstract: Zero-shot mutation prediction is vital for low-resource protein engineering, yet existing protein language models (PLMs) often yield statistically confident results that ignore fundamental biophysical constraints. Currently, selecting candidates for wet-lab validation relies on manual expert auditing of PLM outputs, a process that is inefficient, subjective, and highly dependent on domain expertise. To address this, we propose Rank-and-Reason (VenusRAR), a two-stage agentic framework to automate this workflow and maximize expected wet-lab fitness. In the Rank-Stage, a Computational Expert and Virtual Biologist aggregate a context-aware multi-modal ensemble, establishing a new Spearman correlation record of 0.551 (vs. 0.518) on ProteinGym. In the Reason-Stage, an agentic Expert Panel employs chain-of-thought reasoning to audit candidates against geometric and structural constraints, improving the Top-5 Hit Rate by up to 367% on ProteinGym-DMS99. The wet-lab validation on Cas12i3 nuclease further confirms the framework's efficacy, achieving a 46.7% positive rate and identifying two novel mutants with 4.23-fold and 5.05-fold activity improvements. Code and datasets are released on GitHub (https://github.com/ai4protein/VenusRAR/).

</details>


### [1319] [ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design](https://arxiv.org/abs/2602.00157)
*Fang Sheng,Mohammad Noaeen,Zahra Shakeri*

Main category: q-bio.QM

TL;DR: The paper introduces ProDCARL, a reinforcement-learning framework for generating antimicrobial peptides (AMPs) with improved activity and safety using diffusion-based protein generation and classifiers.


<details>
  <summary>Details</summary>
Motivation: The growing issue of antimicrobial resistance motivates the need for computational methods to discover effective and safe AMPs, minimizing toxicity and optimizing functional activity.

Method: The framework couples a diffusion-based protein generator fine-tuned on AMP sequences with classifiers for AMP activity and toxicity prediction. Reinforcement learning with policy-gradient updates is applied to improve peptide generation quality.

Result: ProDCARL improves the predicted AMP score by more than double, achieves a high-quality hit rate of 6.3% for sequences meeting desired properties, and maintains high diversity in generated peptides.

Conclusion: ProDCARL successfully generates diverse AMP candidates with optimized antimicrobial activity and minimized toxicity, aiding experimental discovery efforts in combating antimicrobial resistance.

Abstract: Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not enforce these goals explicitly. We introduce ProDCARL, a reinforcement-learning alignment framework that couples a diffusion-based protein generator (EvoDiff OA-DM 38M) with sequence property predictors for AMP activity and peptide toxicity. We fine-tune the diffusion prior on AMP sequences to obtain a domain-aware generator. Top-k policy-gradient updates use classifier-derived rewards plus entropy regularization and early stopping to preserve diversity and reduce reward hacking. In silico experiments show ProDCARL increases the mean predicted AMP score from 0.081 after fine-tuning to 0.178. The joint high-quality hit rate reaches 6.3\% with pAMP $>$0.7 and pTox $<$0.3. ProDCARL maintains high diversity, with $1-$mean pairwise identity equal to 0.929. Qualitative analyses with AlphaFold3 and ProtBERT embeddings suggest candidates show plausible AMP-like structural and semantic characteristics. ProDCARL serves as a candidate generator that narrows experimental search space, and experimental validation remains future work.

</details>


### [1320] [A 30-item Test for Assessing Chinese Character Amnesia in Child Handwriters](https://arxiv.org/abs/2602.00464)
*Zebo Xu,Steven Langsford,Zhuang Qiu,Zhenguang Cai*

Main category: q-bio.QM

TL;DR: The paper addresses a decline in handwriting literacy due to typing, especially in Chinese learning children who face character amnesia. A reliable, compact 30-item diagnostic test was developed using handwriting data analysis.


<details>
  <summary>Details</summary>
Motivation: Handwriting proficiency is declining in the digital age, and there is no standardized tool to diagnose handwriting difficulties like character amnesia in children learning non-alphabetic scripts, specifically Chinese.

Method: 40 Mandarin-speaking children handwrote 800 Chinese characters in a dictation test. Their responses were analyzed using an Item Response Theory model, comparing four item-selection schemes to identify an efficient diagnostic subset.

Result: A compact 30-item diagnostic subset was developed using the upper-and-lower-thirds discrimination score, achieving high reliability and predictive validity with correlation values of r = .74 (cross-validated) and r = .93 (within-sample).

Conclusion: The research provides an effective tool for early detection of writing and learning difficulties in Mandarin-speaking children, aiding in diagnosing developmental dysgraphia and improving literacy challenges.

Abstract: Handwriting literacy is an important skill for learning and communication in school-age children. In the digital age, handwriting has been largely replaced by typing, leading to a decline in handwriting proficiency, particularly in non-alphabetic writing systems. Among children learning Chinese, a growing number have reported experiencing character amnesia: difficulty in correctly handwriting a character despite being able to recognize it. Given that there is currently no standardized diagnostic tool for assessing character amnesia in children, we developed an assessment to measure Chinese character amnesia in Mandarin-speaking school-age population. We utilised a large-scale handwriting dataset in which 40 children handwrote 800 characters from dictation prompts. Character amnesia and correct handwriting responses were analysed using a two-parameter Item Response Theory model. Four item-selection schemes were compared: random baseline, maximum discrimination, diverse difficulty, and an upper-and-lower-thirds discrimination score. Candidate item subsets were evaluated using out-of-sample prediction. Among these selection schemes, the upper-and-lower-thirds discrimination procedure yields a compact 30-item test that preserves individual-difference structure and generalizes to unseen test-takers (cross-validated mean r =.74 with full 800-item-test; within-sample r =.93). This short-form test provides a reliable and efficient tool of assessing Chinese character amnesia in children and can be used to identify early handwriting and orthographic learning difficulties, contributing to the early detection of developmental dysgraphia and related literacy challenges.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1321] [Dual Quaternion SE(3) Synchronization with Recovery Guarantees](https://arxiv.org/abs/2602.00324)
*Jianing Zhao,Linglingzhi Zhu,Anthony Man-Cho So*

Main category: math.OC

TL;DR: This paper proposes a method for pose synchronization over SE(3) using unit dual quaternions, eliminating heuristic procedures while attaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Current approaches to SE(3) synchronization rely on heuristic procedures without theoretical guarantees, which makes them hard to analyze and potentially suboptimal.

Method: Introduces a two-stage dual quaternion-based algorithm with a spectral initializer via the power method and a dual quaternion generalized power method (DQGPM) for feasibility enforcement.

Result: The developed methods provide improved accuracy and efficiency for pose synchronization, demonstrated via synthetic and real-world benchmarks.

Conclusion: The proposed dual quaternion-based synchronization approach achieves accuracy and efficiency with theoretical error bounds and overcomes limitations of heuristic-based methods.

Abstract: Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods.

</details>


### [1322] [Exact Instance Compression for Convex Empirical Risk Minimization via Color Refinement](https://arxiv.org/abs/2602.00437)
*Bryan Zhu,Ziang Chen*

Main category: math.OC

TL;DR: The paper presents a novel lossless compression framework for efficient convex ERM, addressing computational inefficiency in standard solvers.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense and poor scaling of standard solvers used in empirical risk minimization (ERM), even in convex optimization problems.

Method: The authors propose and extend a lossless compression framework for convex ERM using color refinement, applicable to various models and optimization problems. They also develop specific algorithms for models like regression, logistic regression, and kernel methods.

Result: The proposed approach is validated through numerical experiments on representative datasets, demonstrating its effectiveness.

Conclusion: The framework shows promise in making convex ERM computationally feasible by employing a lossless compression strategy.

Abstract: Empirical risk minimization (ERM) can be computationally expensive, with standard solvers scaling poorly even in the convex setting. We propose a novel lossless compression framework for convex ERM based on color refinement, extending prior work from linear programs and convex quadratic programs to a broad class of differentiable convex optimization problems. We develop concrete algorithms for a range of models, including linear and polynomial regression, binary and multiclass logistic regression, regression with elastic-net regularization, and kernel methods such as kernel ridge regression and kernel logistic regression. Numerical experiments on representative datasets demonstrate the effectiveness of the proposed approach.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1323] [DREAMS: A Social Exchange Theory-Informed Modeling of Misinformation Engagement on Social Media](https://arxiv.org/abs/2602.01567)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.SI

TL;DR: The paper introduces DREAMS, a model for predicting misinformation engagement on social media using a social exchange theory framework, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current models fail to account for the heterogeneous social mechanisms and platform contexts shaping misinformation spread, leading to insufficient understanding of engagement dynamics.

Method: DREAMS models misinformation engagement dynamically as a sequence-to-sequence adaptation process guided by social exchange theory, learning the propagation of emotional and contextual signals across time and platforms.

Result: DREAMS achieves a 43.6% improvement over the best baseline in predicting misinformation engagement, with a mean absolute percentage error of 19.25%, using data spanning 7 platforms and 2.37M posts.

Conclusion: The integration of social exchange principles not only improves predictive accuracy but also reveals cross-platform patterns in misinformation engagement dynamics, advancing understanding and modeling capabilities.

Abstract: Social media engagement prediction is a central challenge in computational social science, particularly for understanding how users interact with misinformation. Existing approaches often treat engagement as a homogeneous time-series signal, overlooking the heterogeneous social mechanisms and platform designs that shape how misinformation spreads. In this work, we ask: ``Can neural architectures discover social exchange principles from behavioral data alone?'' We introduce \textsc{Dreams} (\underline{D}isentangled \underline{R}epresentations and \underline{E}pisodic \underline{A}daptive \underline{M}odeling for \underline{S}ocial media misinformation engagements), a social exchange theory-guided framework that models misinformation engagement as a dynamic process of social exchange. Rather than treating engagement as a static outcome, \textsc{Dreams} models it as a sequence-to-sequence adaptation problem, where each action reflects an evolving negotiation between user effort and social reward conditioned by platform context. It integrates adaptive mechanisms to learn how emotional and contextual signals propagate through time and across platforms. On a cross-platform dataset spanning $7$ platforms and 2.37M posts collected between 2021 and 2025, \textsc{Dreams} achieves state-of-the-art performance in predicting misinformation engagements, reaching a mean absolute percentage error of $19.25$\%. This is a $43.6$\% improvement over the strongest baseline. Beyond predictive gains, the model reveals consistent cross-platform patterns that align with social exchange principles, suggesting that integrating behavioral theory can enhance empirical modeling of online misinformation engagement. The source code is available at: https://github.com/ltian678/DREAMS.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [1324] [Test-Time Adaptation for Non-stationary Time Series: From Synthetic Regime Shifts to Financial Markets](https://arxiv.org/abs/2602.00073)
*Yurui Wu,Qingying Deng,Wonou Chung,Mairui Li*

Main category: q-fin.ST

TL;DR: The paper introduces a small-footprint test-time adaptation (TTA) framework for improving causal time series forecasting and classification in non-stationary environments.


<details>
  <summary>Details</summary>
Motivation: Real-world time series data often experience distributional changes, causing forecasting models trained on past observations to lose accuracy. The paper aims to improve model robustness in such scenarios.

Method: The proposed TTA framework updates normalization affine parameters during test time using recent unlabeled windows. It employs entropy minimization and temporal consistency for classification, prediction variance reduction and optional distillation for regression, along with stability-enhancing mechanisms.

Result: Synthetic and real-world financial datasets show TTA improves forecasting under gradual distributional drifts, although aggressive normalization adaptation can sometimes hurt performance on financial markets.

Conclusion: The study offers practical guidance on employing normalization-based TTA techniques for non-stationary time series while balancing adaptation aggressiveness to ensure stability in real-world applications.

Abstract: Time series encountered in practice are rarely stationary. When the data distribution changes, a forecasting model trained on past observations can lose accuracy. We study a small-footprint test-time adaptation (TTA) framework for causal timeseries forecasting and direction classification. The backbone is frozen, and only normalization affine parameters are updated using recent unlabeled windows. For classification we minimize entropy and enforce temporal consistency; for regression we minimize prediction variance across weak time-preserving augmentations and optionally distill from an EMA teacher. A quadratic drift penalty and an uncertainty triggered fallback keep updates stable. We evaluate this framework in two stages: synthetic regime shifts on ETT benchmarks, and daily equity and FX series (SPY, QQQ, EUR/USD) across pandemic, high-inflation, and recovery regimes. On synthetic gradual drift, normalization-based TTA improves forecasting error, while in financial markets a simple batch-normalization statistics update is a robust default and more aggressive norm-only adaptation can even hurt. Our results provide practical guidance for deploying TTA on non-stationary time series.

</details>


### [1325] [Bitcoin Price Prediction using Machine Learning and Combinatorial Fusion Analysis](https://arxiv.org/abs/2602.00037)
*Yuanhong Wu,Wei Ye,Jingyan Xu,D. Frank Hsu*

Main category: q-fin.ST

TL;DR: The paper introduces Combinatorial Fusion Analysis (CFA) to improve Bitcoin price prediction by combining multiple machine learning models and demonstrates better metrics compared to individual and other models.


<details>
  <summary>Details</summary>
Motivation: Accurate Bitcoin price prediction can lead to significant profits, but individual machine learning models have limitations.

Method: The study uses CFA, leveraging rank-score characteristic functions and weighted combinations of diverse models. Utilized metrics include RMSE and MAPE for evaluation.

Result: The method achieved a significant improvement, notably achieving a MAPE performance of 0.19%, surpassing individual and other models.

Conclusion: The CFA approach enhances prediction robustness and outperforms existing models in Bitcoin price prediction, offering a valuable technique in financial analytics.

Abstract: In this work, we propose to apply a new model fusion and learning paradigm, known as Combinatorial Fusion Analysis (CFA), to the field of Bitcoin price prediction. Price prediction of financial product has always been a big topic in finance, as the successful prediction of the price can yield significant profit. Every machine learning model has its own strength and weakness, which hinders progress toward robustness. CFA has been used to enhance models by leveraging rank-score characteristic (RSC) function and cognitive diversity in the combination of a moderate set of diverse and relatively well-performed models. Our method utilizes both score and rank combinations as well as other weighted combination techniques. Key metrics such as RMSE and MAPE are used to evaluate our methodology performance. Our proposal presents a notable MAPE performance of 0.19\%. The proposed method greatly improves upon individual model performance, as well as outperforms other Bitcoin price prediction models.

</details>


### [1326] [Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs](https://arxiv.org/abs/2602.00082)
*Zheng Li*

Main category: q-fin.ST

TL;DR: The paper proposes a multi-agent, large language model (LLM)-driven trading framework for Chinese Public REITs. Two prediction pathways were tested, achieving superior risk-adjusted returns compared to benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges in the low-volatility Chinese REITs market by using advanced trading strategies for better risk-adjusted returns.

Method: The system includes multiple analytical agents (announcement, event, price momentum, and market), a prediction agent, and a decision agent. It also compares two prediction models: a general-purpose large LLM and a fine-tuned small model.

Result: Backtests (Oct 2024–Oct 2025) showed the agent-based strategies significantly outperform the buy-and-hold benchmark in cumulative return, Sharpe ratio, and maximum drawdown.

Conclusion: The multi-agent framework enhances REIT trading performance, with fine-tuned small models rivaling or outperforming larger LLMs in some cases.

Abstract: This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLM)-driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents-announcement, event, price momentum, and market-each conducting analysis from different dimensions; then the prediction agent integrates these multi-source signals to output directional probability distributions across multiple time horizons, then the decision agent generates discrete position adjustment signals based on the prediction results and risk control constraints, thereby forming a closed loop of analysis-prediction-decision-execution. This study further compares two prediction model pathways: for the prediction agent, directly calling the general-purpose large model DeepSeek-R1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from October 2024 to October 2025, both agent-based strategies significantly outperformed the buy-and-hold benchmark in terms of cumulative return, Sharpe ratio, and maximum drawdown. The results indicate that the multi-agent framework can effectively enhance the risk-adjusted return of REITs trading, and the fine-tuned small model performs close to or even better than the general-purpose large model in some scenarios.

</details>


### [1327] [Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction](https://arxiv.org/abs/2602.00086)
*Walid Siala,Ahmed Khanfir,Mike Papadakis*

Main category: q-fin.ST

TL;DR: This paper evaluates the impact of news sentiment analysis using large language models (LLMs) for stock price movement prediction and presents an ensemble model for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To investigate the benefit of news sentiment analysis for stock prediction and conduct a thorough evaluation of different sentiment analysis architecture types using LLMs.

Method: The paper evaluates three LLMs, DeBERTa, RoBERTa, and FinBERT, for sentiment-driven stock prediction and tests their integration with various prediction and regression models.

Result: DeBERTa achieves 75% accuracy, outperforming RoBERTa and FinBERT. An ensemble model combining all three models increases accuracy to 80%. Sentiment features slightly improve some stock prediction and regression models.

Conclusion: News sentiment analysis through LLMs offers advantages for stock prediction, with DeBERTa and ensemble models achieving notable performance improvements, although sentiment benefits vary by model type.

Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a clear and in-depth understanding of the benefit of the news sentiment to this task, as well as a comprehensive assessment of different architecture types in this context, is still lacking. Herein, we conduct an evaluation study that compares 3 different LLMs, namely, DeBERTa, RoBERTa and FinBERT, for sentiment-driven stock prediction. Our results suggest that DeBERTa outperforms the other two models with an accuracy of 75% and that an ensemble model that combines the three models can increase the accuracy to about 80%. Also, we see that sentiment news features can benefit (slightly) some stock market prediction models, i.e., LSTM-, PatchTST- and tPatchGNN-based classifiers and PatchTST- and TimesNet-based regression tasks models.

</details>


### [1328] [PredictionMarketBench: A SWE-bench-Style Framework for Backtesting Trading Agents on Prediction Markets](https://arxiv.org/abs/2602.00133)
*Avi Arora,Ritesh Malpani*

Main category: q-fin.ST

TL;DR: The paper introduces PredictionMarketBench, a benchmark for evaluating trading agents in prediction markets using historical data with reproducible methodologies.


<details>
  <summary>Details</summary>
Motivation: To create a standardized and reproducible framework for testing the efficiency and performance of algorithmic and large language model-based trading agents in prediction markets.

Method: The authors developed a benchmark system that standardizes episode construction, provides a realistic trading simulator, and supports both traditional strategies and LLM-based agents for evaluating prediction markets.

Result: The benchmark includes four episodes featuring diverse markets like cryptocurrency, weather, and sports; initial results reveal that naive agents often underperform due to costs, while fee-aware strategies perform better in volatile markets.

Conclusion: PredictionMarketBench provides a valuable framework for analyzing and improving trading agents, highlighting the importance of cost-awareness in strategy development.

Abstract: Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introduce PredictionMarketBench, a SWE-bench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshi-based episodes spanning cryptocurrency, weather, and sports. Baseline results show that naive trading agents can underperform due to transaction costs and settlement losses, while fee-aware algorithmic strategies remain competitive in volatile episodes.

</details>


### [1329] [Exploring the Interpretability of Forecasting Models for Energy Balancing Market](https://arxiv.org/abs/2602.00049)
*Oskar Våle,Shiliang Zhang,Sabita Maharjan,Gro Klæboe*

Main category: q-fin.ST

TL;DR: The paper investigates the trade-off between accuracy and interpretability in forecasting energy balancing market prices, using extreme gradient boosting (XGBoost) and explainable boosting machine (EBM).


<details>
  <summary>Details</summary>
Motivation: To address the need for interpretable machine learning models in understanding and forecasting dynamics in the energy balancing market for grid stability and secure energy supply.

Method: The authors used XGBoost and EBM models to forecast manual frequency restoration reserve (mFRR) activation prices, benchmarking them against a naive baseline model. Real market data from different zones was utilized.

Result: EBM demonstrated forecasting accuracy comparable to XGBoost, along with considerable interpretability. Challenges in predicting significant price deviations and insights into non-linear price drivers were highlighted.

Conclusion: EBM is an effective interpretable alternative to complex AI models in forecasting the energy balancing market, providing valuable insights without compromising accuracy.

Abstract: The balancing market in the energy sector plays a critical role in physically and financially balancing the supply and demand. Modeling dynamics in the balancing market can provide valuable insights and prognosis for power grid stability and secure energy supply. While complex machine learning models can achieve high accuracy, their black-box nature severely limits the model interpretability. In this paper, we explore the trade-off between model accuracy and interpretability for the energy balancing market. Particularly, we take the example of forecasting manual frequency restoration reserve (mFRR) activation price in the balancing market using real market data from different energy price zones. We explore the interpretability of mFRR forecasting using two models: extreme gradient boosting (XGBoost) machine and explainable boosting machine (EBM). We also integrate the two models, and we benchmark all the models against a baseline naive model. Our results show that EBM provides forecasting accuracy comparable to XGBoost while yielding a considerable level of interpretability. Our analysis also underscores the challenge of accurately predicting the mFRR price for the instances when the activation price deviates significantly from the spot price. Importantly, EBM's interpretability features reveal insights into non-linear mFRR price drivers and regional market dynamics. Our study demonstrates that EBM is a viable and valuable interpretable alternative to complex black-box AI models in the forecast for the balancing market.

</details>


### [1330] [The GT-Score: A Robust Objective Function for Reducing Overfitting in Data-Driven Trading Strategies](https://arxiv.org/abs/2602.00080)
*Alexander Sheppert*

Main category: q-fin.ST

TL;DR: The paper introduces GT-Score, an objective function targeting overfitting in data-driven financial models, yielding improved generalization and robustness in trading strategies through extensive empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Overfitting in ML-driven financial models leads to learning spurious patterns and poor deployment performance, necessitating robust strategies to address optimization pitfalls.

Method: Developed GT-Score integrating performance, significance, consistency, and downside risk components, tested on stock data from 50 S&P 500 companies spanning 2010-2024 using walk-forward validation and Monte Carlo studies.

Result: GT-Score improved generalization ratio by 98%, showed statistically significant differences in out-of-sample returns compared to baseline methods, with results reproducible from provided supplementary materials.

Conclusion: Embedding anti-overfitting mechanisms into objective functions enhances robustness and reliability in quantitative financial modeling and backtests.

Abstract: Overfitting remains a critical challenge in data-driven financial modeling, where machine learning (ML) systems learn spurious patterns in historical prices and fail out of sample and in deployment. This paper introduces the GT-Score, a composite objective function that integrates performance, statistical significance, consistency, and downside risk to guide optimization toward more robust trading strategies. This approach directly addresses critical pitfalls in quantitative strategy development, specifically data snooping during optimization and the unreliability of statistical inference under non-normal return distributions. Using historical stock data for 50 S&P 500 companies spanning 2010-2024, we conduct an empirical evaluation that includes walk-forward validation with nine sequential time splits and a Monte Carlo study with 15 random seeds across three trading strategies. In walk-forward validation, GT-Score improves the generalization ratio (validation return divided by training return) by 98% relative to baseline objective functions. Paired statistical tests on Monte Carlo out-of-sample returns indicate statistically detectable differences between objective functions (p < 0.01 for comparisons with Sortino and Simple), with small effect sizes. These results suggest that embedding an anti-overfitting structure into the objective can improve the reliability of backtests in quantitative research. Reproducible code and processed result files are provided as supplementary materials.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [1331] [Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems](https://arxiv.org/abs/2602.01929)
*Zhouzhou Song,Marcos A. Valdebenito,Styfen Schär,Stefano Marelli,Bruno Sudret,Matthias G. R. Faes*

Main category: math.DS

TL;DR: The paper presents F2NARX, a novel surrogate model offering significant predictive efficiency and accuracy for dynamical systems while incorporating probabilistic predictions.


<details>
  <summary>Details</summary>
Motivation: The lack of accurate and computationally efficient surrogate models for complex, nonlinear, high-dimensional dynamical systems.

Method: F2NARX develops a novel function-on-function regression approach integrating PCA, Gaussian process regression, and unscented transform for autoregressive and probabilistic predictions.

Result: F2NARX substantially outperforms state-of-the-art NARX models in accuracy and efficiency, as demonstrated through diverse case studies.

Conclusion: F2NARX provides a powerful solution for efficient and accurate dynamical system response predictions, with significant advancements in active learning and probabilistic analysis.

Abstract: Constructing accurate and computationally efficient surrogate models (or emulators) for predicting dynamical system responses is critical in many engineering domains, yet remains challenging due to the strongly nonlinear and high-dimensional mapping from external excitations and system parameters to system responses. This work introduces a novel Function-on-Function Nonlinear AutoRegressive model with eXogenous inputs (F2NARX), which reformulates the conventional NARX model from a function-on-function regression perspective, inspired by the recently proposed $\mathcal{F}$-NARX method. The proposed framework substantially improves predictive efficiency while maintaining high accuracy. By combining principal component analysis with Gaussian process regression, F2NARX further enables probabilistic predictions of dynamical responses via the unscented transform in an autoregressive manner. The effectiveness of the method is demonstrated through case studies of varying complexity. Results show that F2NARX outperforms state-of-the-art NARX model by orders of magnitude in efficiency while achieving higher accuracy in general. Moreover, its probabilistic prediction capabilities facilitate active learning, enabling accurate estimation of first-passage failure probabilities of dynamical systems using only a small number of training time histories.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1332] [Implementation Challenges in Quantum Key Distribution](https://arxiv.org/abs/2602.01500)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: This paper compares two quantum key distribution protocols, BB84 and E91, and evaluates their implementation on actual quantum computing hardware using IBM Quantum Platform.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore practical applications of matured quantum computing technologies in the domain of network communication security.

Method: It proposes the implementation of BB84 and E91 protocols on quantum hardware, utilizing SX gate operations for uniform quantum superposition states.

Result: The feasibility of both protocols is confirmed, evaluated through entropy, IID, and error-rate verifications.

Conclusion: Quantum computing successfully supports secure communication by leveraging QKD protocols like BB84 and E91 in practice.

Abstract: In recent years, quantum computing technologies have steadily matured and have begun to find practical applications across various domains. One important area is network communication security, where Quantum Key Distribution (QKD) enables communicating parties to establish a shared secret that can then be used to generate symmetric keys for subsequent encryption and decryption. This study focuses on implementing and comparing two well-known QKD protocols, namely BB84 and E91, within an actual quantum computing environment. It also proposes the use of SX gate operations to generate uniform quantum superposition states. By leveraging the properties of quantum superposition and quantum entanglement, the study illustrates how communicating parties can securely obtain a shared secret while preventing adversaries from intercepting it. The experiments are conducted using the IBM Quantum Platform to demonstrate the feasibility of the BB84 and E91 protocols on actual quantum hardware. The evaluation considers several metrics, including entropy, Independent and Identically Distributed (IID), and error-rate verifications.

</details>


### [1333] [IDEM Enough? Evolving Highly Nonlinear Idempotent Boolean Functions](https://arxiv.org/abs/2602.00837)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek*

Main category: cs.CR

TL;DR: This paper focuses on evolving highly nonlinear idempotent Boolean functions for dimensions 5-12 using evolutionary methods, overcoming challenges caused by the constraints of idempotence.


<details>
  <summary>Details</summary>
Motivation: To explore methods for constructing highly nonlinear idempotent Boolean functions, which are promising for cryptographic design but challenging due to their additional algebraic constraints.

Method: Investigates evolutionary approaches to evolve Boolean functions using a polynomial basis representation, overcoming difficulties like the disruption caused by crossover and mutation operators, and proposes encoding truth tables on orbits to enforce idempotence.

Result: The study finds that evolving idempotent Boolean functions is challenging due to the disruptive effects of evolutionary operators, but proposes a promising method of encoding using squaring orbits to address these challenges.

Conclusion: Idempotent Boolean functions exhibit high potential for use in cryptographic design, but their evolutionary construction is challenging. The paper offers innovative solutions to overcome these difficulties using compact genome encodings.

Abstract: Idempotent Boolean functions form a highly structured subclass of Boolean functions that is closely related to rotation symmetry under a normal-basis representation and to invariance under a fixed linear map in a polynomial basis. These functions are attractive as candidates for cryptographic design, yet their additional algebraic constraints make the search for high nonlinearity substantially more difficult than in the unconstrained case. In this work, we investigate evolutionary methods for constructing highly nonlinear idempotent Boolean functions for dimensions $n=5$ up to $n=12$ using a polynomial basis representation with canonical primitive polynomials. Our results show that the problem of evolving idempotent functions is difficult due to the disruptive nature of crossover and mutation operators. Next, we show that idempotence can be enforced by encoding the truth table on orbits, yielding a compact genome of size equal to the number of distinct squaring orbits.

</details>


### [1334] [FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems](https://arxiv.org/abs/2602.01185)
*Fabio Turazza,Marcello Pietri,Marco Picone,Marco Mamei*

Main category: cs.CR

TL;DR: The paper discusses FedBGS, a decentralized blockchain-based system combining Segmented Gossip Learning and Federated Analytics for privacy-preserving and secure federated learning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of classical federated learning, such as central server vulnerabilities and privacy concerns, by developing a robust decentralized solution.

Method: Introduce FedBGS, which uses a blockchain-based system integrated with Segmented Gossip Learning and Federated Analytics to enhance security, handle non-IID data, and optimize resource usage.

Result: FedBGS achieves improved security, privacy preservation, and scalability in Federated Learning environments compared to traditional structures.

Conclusion: FedBGS provides an effective decentralized framework addressing single-point-of-failure problems in Federated Learning while ensuring privacy and security for sensitive data use cases.

Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.

</details>


### [1335] [Privocracy: Online Democracy through Private Voting](https://arxiv.org/abs/2602.01341)
*Pedro Camponês,Hugo Pereira,Adrian Persaud,Kevin Gallagher,Santiago Torres-Arias*

Main category: cs.CR

TL;DR: Privocracy introduces a secure e-voting mechanism to enhance access control by reducing high-privilege reliance, ensuring privacy, and maintaining system flexibility.


<details>
  <summary>Details</summary>
Motivation: Traditional access control policies create vulnerabilities through high-privilege users, compromising security. Privocracy aims to address these issues by offering a decentralized access control approach.

Method: A secure e-voting procedure is proposed for sensitive resource commands, ensuring everlasting privacy, efficient processing, and features like vote delegation, rapid rounds, and selective auditing.

Result: Privocracy improves system security and dependability while being efficiently deployable on commodity hardware.

Conclusion: Privocracy provides a practical and secure access control system by distributing trust, ensuring privacy, and achieving flexibility without single points of failure.

Abstract: In traditional access control policies, every access granted and administrative account introduces an additional vulnerability, as a corruption of a high-privilege user can compromise several sensitive files. Privocracy is an access control mechanism that minimizes the need to attribute high privileges by triggering a secure e-voting procedure to run commands that require using sensitive resources. With Privocracy an organization can distribute trust in resource access, minimizing the system vulnerabilities from single points of failure, all while maintaining the high flexibility of discretionary access control policies.
  The Privocracy voting mechanism achieves everlasting privacy, ensuring votes remain confidential regardless of an adversary's computational power, while addressing the dependability requirements of a practical and secure system. The procedure incorporates useful features such as vote delegation to reduce voter fatigue, rapid voting rounds to enable quick action during emergencies, and selective vote auditing for application-level accountability. Our experimental results demonstrate that Privocracy processes votes efficiently and can be deployed on commodity hardware.

</details>


### [1336] [RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles](https://arxiv.org/abs/2602.00270)
*Mohsen Salehi,Karthik Pattabiraman*

Main category: cs.CR

TL;DR: RVDebloater is an adaptive debloating technique for embedded mode-based devices, achieving effective attack surface reduction with minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: To address growing attack surfaces in embedded devices due to increasing firmware size and the insufficiency of existing debloating techniques.

Method: RVDebloater dynamically debloats firmware for mode-based devices by identifying unneeded code using static/dynamic analysis and enforces function-level restrictions during runtime.

Result: Evaluation on robotic vehicles showed debloating 85% of unnecessary functions without impacting missions, reducing firmware call graphs by 45%, with 3.9% performance overhead.

Conclusion: RVDebloater effectively reduces attack surfaces for mode-based embedded devices with minimal impact on performance and functionality.

Abstract: As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability.
  To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.

</details>


### [1337] [zkCraft: Prompt-Guided LLM as a Zero-Shot Mutation Pattern Oracle for TCCT-Powered ZK Fuzzing](https://arxiv.org/abs/2602.00667)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Youjin Wang,Ziyu Kong,Zeli Su,Zhaolu Kang,Shuning Zhang,Xianda Li,Kun Liu,Simon Fong*

Main category: cs.CR

TL;DR: zkCraft introduces an advanced debugging and verification framework for zero-knowledge (ZK) circuits to address privacy-preserving scalability challenges, emphasizing semantic inconsistency detection.


<details>
  <summary>Details</summary>
Motivation: To overcome difficulties in implementing zero-knowledge circuits correctly, particularly the challenges tied to tightly coupled witness computation and circuit constraints.

Method: zkCraft uses R1CS-aware localization and introduces the concept of proof-bearing search. It encodes constraint edits using a Row-Vortex polynomial and replaces recurring solver queries with a Violation IOP, which certifies edits with succinct proofs. LLM-driven mutation templates promote edge case exploration while ensuring algebraic verification remains auditable.

Result: zkCraft evaluated on Circom code demonstrates effective semantic inconsistency detection, catching under- and over-constrained faults with minimal false positives and reducing expensive solver dependency.

Conclusion: zkCraft effectively merges formal verification with automated debugging, making ZK circuit development both scalable and robust.

Abstract: Zero-knowledge circuits enable privacy-preserving and scalable systems but are difficult to implement correctly due to the tight coupling between witness computation and circuit constraints. We present zkCraft, a practical framework that combines deterministic, R1CS-aware localization with proof-bearing search to detect semantic inconsistencies. zkCraft encodes candidate constraint edits into a single Row-Vortex polynomial and replaces repeated solver queries with a Violation IOP that certifies the existence of edits together with a succinct proof. Deterministic LLM-driven mutation templates bias exploration toward edge cases while preserving auditable algebraic verification. Evaluation on real Circom code shows that proof-bearing localization detects diverse under- and over-constrained faults with low false positives and reduces costly solver interaction. Our approach bridges formal verification and automated debugging, offering a scalable path for robust ZK circuit development.

</details>


### [1338] [From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities](https://arxiv.org/abs/2602.00711)
*Ranjith Krishnamurthy,Oshando Johnson,Goran Piskachev,Eric Bodden*

Main category: cs.CR

TL;DR: This work develops a plugin for IntelliJ IDEA that uses metrics and large language models (LLMs) to identify security-critical functionality in code and prevent vulnerabilities proactively.


<details>
  <summary>Details</summary>
Motivation: To proactively prevent vulnerabilities caused by a lack of security expertise and code complexity, as traditional tools are reactive.

Method: The researchers developed a plugin that employs code-level software metrics to flag potentially security-critical code and large language models (LLMs) to provide guidance for secure implementation.

Result: Initial evaluation on the Spring-PetClinic application revealed that the approach effectively identifies known security-critical methods and generates actionable security advice.

Conclusion: The study introduces foundational work in integrating security-aware metrics and LLMs for proactive, prevention-oriented software security.

Abstract: Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.

</details>


### [1339] [GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability](https://arxiv.org/abs/2602.00979)
*Xueyi Li,Zhuoneng Zhou,Zitao Liu,Yongdong Wu,Weiqi Luo*

Main category: cs.CR

TL;DR: The paper introduces GradingAttack, an adversarial attack framework revealing the vulnerabilities in large language model-based automatic short answer grading (ASAG) systems to promote more fair and reliable grading.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for automatic grading but are vulnerable to adversarial manipulation, raising concerns about fairness and reliability in assessments.

Method: The paper proposes GradingAttack, which employs token-level and prompt-level adversarial strategies tailored for ASAG along with a novel evaluation metric balancing attack success and camouflage.

Result: Both attack methods effectively compromised grading models, with prompt-level attacks achieving better success and token-level attacks excelling in camouflage capability, as demonstrated on multiple datasets.

Conclusion: LLMs for ASAG are significantly vulnerable to adversarial attacks, necessitating improved defenses to ensure fair and reliable grading outcomes.

Abstract: Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.

</details>


### [1340] [Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600)
*Yen-Shan Chen,Zhi Rui Tam,Cheng-Kuang Wu,Yun-Nung Chen*

Main category: cs.CR

TL;DR: This paper introduces Expected Harm, a metric combining threat severity and execution likelihood, to effectively evaluate the safety of language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current LLM safety assessments focus solely on the severity of threats, neglecting execution likelihood, which is critical for real-world risks.

Method: The authors propose Expected Harm, analyze model behavior empirically, and use linear probing to investigate internal representations of execution cost.

Result: They find LLMs exhibit Inverse Risk Calibration, mishandling threats of varying likelihoods, increasing vulnerabilities to high-likelihood attacks.

Conclusion: LLMs encode severity but fail to represent execution cost, leaving models structurally vulnerable to specific exploits.

Abstract: Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them "blind" to this critical dimension of risk.

</details>


### [1341] [ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models](https://arxiv.org/abs/2602.00154)
*Xiaogeng Liu,Xinyan Wang,Yechao Zhang,Sanjay Kariyappa,Chong Xiang,Muhao Chen,G. Edward Suh,Chaowei Xiao*

Main category: cs.CR

TL;DR: The paper introduces ReasoningBomb, a framework that executes denial-of-service attacks against large reasoning models by generating prompts that induce excessive reasoning, highlighting vulnerabilities in LRMs.


<details>
  <summary>Details</summary>
Motivation: To expose a vulnerability in large reasoning models where their computational costs can be exploited using malicious prompts, making such attacks a concern as reasoning models are integrated into real-world applications.

Method: A formalization of inference cost and properties for a prompt-induced denial-of-service (PI-DoS) attack is developed, followed by a reinforcement learning-based framework (ReasoningBomb) designed to craft targeted prompts inducing excessive reasoning.

Result: The ReasoningBomb framework generates prompts that lead to a high amplification ratio in reasoning models, producing over 18,000 tokens on average and outperforming baselines significantly in terms of attack effectiveness.

Conclusion: Large reasoning models are susceptible to targeted denial-of-service attacks, demanding advancements in detection methods and model robustness to prevent exploitation.

Abstract: Large reasoning models (LRMs) extend large language models with explicit multi-step reasoning traces, but this capability introduces a new class of prompt-induced inference-time denial-of-service (PI-DoS) attacks that exploit the high computational cost of reasoning. We first formalize inference cost for LRMs and define PI-DoS, then prove that any practical PI-DoS attack should satisfy three properties: (1) a high amplification ratio, where each query induces a disproportionately long reasoning trace relative to its own length; (ii) stealthiness, in which prompts and responses remain on the natural language manifold and evade distribution shift detectors; and (iii) optimizability, in which the attack supports efficient optimization without being slowed by its own success. Under this framework, we present ReasoningBomb, a reinforcement-learning-based PI-DoS framework that is guided by a constant-time surrogate reward and trains a large reasoning-model attacker to generate short natural prompts that drive victim LRMs into pathologically long and often effectively non-terminating reasoning. Across seven open-source models (including LLMs and LRMs) and three commercial LRMs, ReasoningBomb induces 18,759 completion tokens on average and 19,263 reasoning tokens on average across reasoning models. It outperforms the the runner-up baseline by 35% in completion tokens and 38% in reasoning tokens, while inducing 6-7x more tokens than benign queries and achieving 286.7x input-to-output amplification ratio averaged across all samples. Additionally, our method achieves 99.8% bypass rate on input-based detection, 98.7% on output-based detection, and 98.4% against strict dual-stage joint detection.

</details>


### [1342] [EigenAI: Deterministic Inference, Verifiable Results](https://arxiv.org/abs/2602.00182)
*David Ribeiro Alves,Vishnu Patankar,Matheus Pereira,Jamie Stephens,Nima Vaziri,Sreeram Kannan*

Main category: cs.CR

TL;DR: EigenAI ensures verifiable AI inference with cryptoeconomic security and a system for public auditability.


<details>
  <summary>Details</summary>
Motivation: To develop a platform that combines state-of-the-art large-language model (LLM) performance with the ability to guarantee deterministic, auditable, and economically secured inference results.

Method: The framework integrates a deterministic LLM inference engine with cryptoeconomic protocols, allowing challenge mechanisms and trusted execution via TEE for verification in case of disputes.

Result: EigenAI delivers a system where AI inference outputs are reliably secured, transparently auditable, and fraud-detectable, leveraging Ethereum's validator base.

Conclusion: EigenAI achieves both high performance and security, creating sovereign agents that utilize cryptoeconomic principles to ensure trust and transparency in AI decision-making.

Abstract: EigenAI is a verifiable AI platform built on top of the EigenLayer restaking ecosystem. At a high level, it combines a deterministic large-language model (LLM) inference engine with a cryptoeconomically secured optimistic re-execution protocol so that every inference result can be publicly audited, reproduced, and, if necessary, economically enforced. An untrusted operator runs inference on a fixed GPU architecture, signs and encrypts the request and response, and publishes the encrypted log to EigenDA. During a challenge window, any watcher may request re-execution through EigenVerify; the result is then deterministically recomputed inside a trusted execution environment (TEE) with a threshold-released decryption key, allowing a public challenge with private data. Because inference itself is bit-exact, verification reduces to a byte-equality check, and a single honest replica suffices to detect fraud. We show how this architecture yields sovereign agents -- prediction-market judges, trading bots, and scientific assistants -- that enjoy state-of-the-art performance while inheriting security from Ethereum's validator base.

</details>


### [1343] [Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs](https://arxiv.org/abs/2602.00204)
*Waleed Khan Mohammed,Zahirul Arief Irfan Bin Shahrul Anuar,Mousa Sufian Mousa Mitani,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CR

TL;DR: This paper introduces a novel APT anomaly detection method using semantic embeddings from Large Language Models and evaluates its effectiveness, outperforming standard techniques.


<details>
  <summary>Details</summary>
Motivation: APTs are stealthy, long-term cyberattacks difficult to detect with traditional methods due to their low-and-slow behavior and lack of semantic understanding in current detection approaches.

Method: The proposed method transforms system logs into semantic embeddings using a pre-trained transformer model, then detects anomalies via an Autoencoder (AE).

Result: The approach showed superior performance in the AUC-ROC metric when compared to traditional unsupervised methods like IForest, OC-SVM, and PCA, especially in complex scenarios.

Conclusion: Using LLM-derived semantic embeddings enhances the detection of stealthy and non-linear APT behavior, signifying semantic understanding as critical for advanced threat detection.

Abstract: Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit "low-and-slow" behavior, traditional statistical methods and shallow machine learning techniques often fail to detect them. Previous research on APT detection has explored machine learning approaches and provenance graph analysis. However, provenance-based methods often fail to capture the semantic intent behind system activities. This paper proposes a novel anomaly detection approach that leverages semantic embeddings generated by Large Language Models (LLMs). The method enhances APT detection by extracting meaningful semantic representations from unstructured system log data. First, raw system logs are transformed into high-dimensional semantic embeddings using a pre-trained transformer model. These embeddings are then analyzed using an Autoencoder (AE) to identify anomalous and potentially malicious patterns. The proposed method is evaluated using the DARPA Transparent Computing (TC) dataset, which contains realistic APT attack scenarios generated by red teams in live environments. Experimental results show that the AE trained on LLM-derived embeddings outperforms widely used unsupervised baseline methods, including Isolation Forest (IForest), One-Class Support Vector Machine (OC-SVM), and Principal Component Analysis (PCA). Performance is measured using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), where the proposed approach consistently achieves superior results, even in complex threat scenarios. These findings highlight the importance of semantic understanding in detecting non-linear and stealthy attack behaviors that are often missed by conventional detection techniques.

</details>


### [1344] [TessPay: Verify-then-Pay Infrastructure for Trusted Agentic Commerce](https://arxiv.org/abs/2602.00213)
*Mehul Goenka,Tejas Pathak,Siddharth Asthana*

Main category: cs.CR

TL;DR: The paper introduces TessPay, a unified infrastructure using a 'Verify-then-Pay' architecture to address foundational trust gaps in agentic commerce involving task delegation, payment settlement, and audit mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge foundational trust gaps in agentic commerce, where existing systems lack support for verification, proper payment settlement, and accountability mechanisms.

Method: TessPay utilizes a two-plane architecture to separate control/verification from settlement, introducing mechanisms like anchored registries, funds escrow, cryptographic evidence for verification, and tamper-evident audit trails.

Result: The proposed system, TessPay, integrates task verification, escrow payments, and an audit trail to operationalize trust and accountability across the transaction lifecycle.

Conclusion: TessPay effectively replaces implicit trust with a 'Verify-then-Pay' model, addressing critical gaps in agentic commerce by ensuring trust, verification, and dispute resolution throughout the transaction lifecycle.

Abstract: The global economy is entering the era of Agentic Commerce, where autonomous agents can discover services, negotiate prices, and transact value. However adoption towards agentic commerce faces a foundational trust gap: current systems are built for direct human interactions rather than agent-driven operations. It lacks core primitives across three critical stages of agentic transactions. First, Task Delegation lacks means to translate user intent into defined scopes, discover appropriate agents, and securely authorize actions. Second, Payment Settlement for tasks is processed before execution, lacking verifiable evidence to validate the agent's work. Third, Audit Mechanisms fail to capture the full transaction lifecycle, preventing clear accountability for disputes. While emerging standards address fragments of this trust gap, there still remains a critical need for a unified infrastructure that binds the entire transaction lifecycle.
  To resolve this gap, we introduce TessPay, a unified infrastructure that replaces implicit trust with a 'Verify-then-Pay' architecture. It is a two plane architecture separating control and verification from settlement. TessPay operationalizes trust across four distinct stages: Before execution, agents are anchored in a canonical registry and user intent is captured as verifiable mandates, enabling stakeholder accountability. During execution, funds are locked in escrow while the agent executes the task and generates cryptographic evidence (TLS Notary, TEE etc.) to support Proof of Task Execution (PoTE). At settlement, the system verifies this evidence and releases funds only when the PoTE satisfies verification predicates; modular rail adapters ensure this PoTE-gated escrow remains chain-agnostic across heterogeneous payment rails. After settlement, TessPay preserves a tamper-evident audit trail to enable clear accountability for dispute resolution.

</details>


### [1345] [Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation](https://arxiv.org/abs/2602.00219)
*Saeid Jamshidi,Omar Abdul Wahab,Foutse Khomh,Kawser Wazed Nafi*

Main category: cs.CR

TL;DR: The paper presents a semantics-driven federated intrusion detection framework that enhances open-set and zero-shot detection using language models to address challenges in privacy, reliability, and unseen threat scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional Federated Learning-based Intrusion Detection Systems, which struggle with open-set learning, zero-day attack detection, and client heterogeneity.

Method: The framework combines federated optimization with semantic supervision derived from large language models (Tri-LLM ensemble) to align telemetry data with attack semantics while managing client trust and modeling epistemic uncertainty for zero-day risks.

Result: The framework achieves >80% zero-shot detection accuracy for unseen attacks, surpassing baselines by >10%, while maintaining low aggregation instability in unreliable/untrustworthy client conditions.

Conclusion: The proposed semantics-driven federated IDS demonstrates improved robustness, semantic alignment, and superior accuracy in detecting novel attack patterns, making it highly effective for privacy-respecting and distributed network security tasks.

Abstract: Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advantages, most existing FL-based IDS assume closed-set learning and lack mechanisms such as uncertainty estimation, semantic generalization, and explicit modeling of epistemic ambiguity in zero-day attack scenarios. Additionally, robustness to heterogeneous and unreliable clients remains a challenge in practical applications. This paper introduces a semantics-driven federated IDS framework that incorporates language-derived semantic supervision into federated optimization, enabling open-set and zero-shot intrusion detection for previously unseen attack behaviors. The approach constructs semantic attack prototypes using a Tri-LLM ensemble of GPT-4o, DeepSeek-V3, and LLaMA-3-8B, aligning distributed telemetry features with high-level attack concepts. Inter-LLM semantic disagreement is modeled as epistemic uncertainty for zero-day risk estimation, while a trust-aware aggregation mechanism dynamically weights client updates based on reliability. Experimental results show stable semantic alignment across heterogeneous clients and consistent convergence. The framework achieves over 80% zero-shot detection accuracy on unseen attack patterns, improving zero-day discrimination by more than 10% compared to similarity-based baselines, while maintaining low aggregation instability in the presence of unreliable or compromised clients.

</details>


### [1346] [Semantics-Preserving Evasion of LLM Vulnerability Detectors](https://arxiv.org/abs/2602.00305)
*Luze Sun,Alina Oprea,Eric Wong*

Main category: cs.CR

TL;DR: The study reveals that even advanced LLM-based vulnerability detectors are vulnerable to evasion through semantics-preserving code transformations.


<details>
  <summary>Details</summary>
Motivation: Current security-critical code review increasingly relies on LLM-based vulnerability detectors, but their resilience against adversarial, behavior-preserving code changes remains unclear.

Method: The paper evaluates the robustness of these detectors by applying behavior-preserving transformations to a standardized C/C++ benchmark of 5000 samples, and tests the robustness under various adversarial attacks.

Result: The detectors exhibit significant susceptibility to evasion through these transformations, with adversarial strings being transferable across models and effective in black-box setups. Gradient access elevates evasion success.

Conclusion: Despite high performance on clean inputs, LLM-based code detectors remain prone to low-cost, semantics-preserving evasion, highlighting a need for improved evaluation metrics and more robust systems.

Abstract: LLM-based vulnerability detectors are increasingly deployed in security-critical code review, yet their resilience to evasion under behavior-preserving edits remains poorly understood. We evaluate detection-time integrity under a semantics-preserving threat model by instantiating diverse behavior-preserving code transformations on a unified C/C++ benchmark (N=5000), and introduce a metric of joint robustness across different attack methods/carriers. Across models, we observe a systemic failure of semantic invariant adversarial transformations: even state-of-the-art vulnerability detectors perform well on clean inputs while predictions flip under behavior-equivalent edits. Universal adversarial strings optimized on a single surrogate model remain effective when transferred to black-box APIs, and gradient access can further amplify evasion success. These results show that even high-performing detectors are vulnerable to low-cost, semantics-preserving evasion. Our carrier-based metrics provide practical diagnostics for evaluating LLM-based code detectors.

</details>


### [1347] [RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance](https://arxiv.org/abs/2602.00183)
*Miao Lin,Feng Yu,Rui Ning,Lusi Li,Jiawei Chen,Qian Lou,Mengxin Zheng,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

TL;DR: The paper identifies that dataset imbalance enhances backdoor attack vulnerability and introduces Randomized Probability Perturbation (RPP), a detection method with provable guarantees. It outperforms existing defenses in imbalanced data scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against backdoor attacks assume balanced datasets, which is unrealistic and exacerbates the threat in real-world, imbalanced data settings.

Method: Introduces Randomized Probability Perturbation (RPP), a framework that detects backdoor-manipulated samples in a black-box setting, offering provable guarantees and a low false positive rate.

Result: RPP outperforms 12 baseline defenses across 5 datasets and 10 backdoor attacks, demonstrating superior accuracy, especially with imbalanced data.

Conclusion: RPP offers a robust solution for detecting backdoor attacks under dataset imbalance, providing both theoretical and practical advancements for real-world applications.

Abstract: Deep neural networks are highly susceptible to backdoor attacks, yet most defense methods to date rely on balanced data, overlooking the pervasive class imbalance in real-world scenarios that can amplify backdoor threats. This paper presents the first in-depth investigation of how the dataset imbalance amplifies backdoor vulnerability, showing that (i) the imbalance induces a majority-class bias that increases susceptibility and (ii) conventional defenses degrade significantly as the imbalance grows. To address this, we propose Randomized Probability Perturbation (RPP), a certified poisoned-sample detection framework that operates in a black-box setting using only model output probabilities. For any inspected sample, RPP determines whether the input has been backdoor-manipulated, while offering provable within-domain detectability guarantees and a probabilistic upper bound on the false positive rate. Extensive experiments on five benchmarks (MNIST, SVHN, CIFAR-10, TinyImageNet and ImageNet10) covering 10 backdoor attacks and 12 baseline defenses show that RPP achieves significantly higher detection accuracy than state-of-the-art defenses, particularly under dataset imbalance. RPP establishes a theoretical and practical foundation for defending against backdoor attacks in real-world environments with imbalanced data.

</details>


### [1348] [Bypassing Prompt Injection Detectors through Evasive Injections](https://arxiv.org/abs/2602.00750)
*Md Jahedur Rahman,Ihsen Alouani*

Main category: cs.CR

TL;DR: This paper investigates the vulnerability of task drift detectors in large language models (LLMs) to adversarial attacks and proposes a defense mechanism using randomized suffixes.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the robustness of task drift detectors against adversarially optimized suffixes, which significantly hinder their reliability in LLMs.

Method: The study generates adversarially optimized suffixes that evade detection by probes, tests their effectiveness on LLMs like Phi-3 3.8B and Llama-3 8B, and develops a defense mechanism by using randomized suffix appending and logistic regression training.

Result: Adversarial suffixes were able to achieve high evasion success rates, up to 99.63%, against task drift detectors. The proposed defense mechanism using randomized suffixes demonstrated significant effectiveness against these attacks.

Conclusion: Practice of relying solely on activation-based task drift detectors is insufficient against adversarial attacks, and incorporating randomized defense strategies can offer stronger resilience.

Abstract: Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.

</details>


### [1349] [TxRay: Agentic Postmortem of Live Blockchain Attacks](https://arxiv.org/abs/2602.01317)
*Ziyue Wang,Jiangshan Yu,Kaihua Qin,Dawn Song,Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: The paper introduces TxRay, a system to analyze DeFi exploits, offering faster root cause identification and proof-of-concept recovery with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the slow and manual nature of postmortem analysis in DeFi exploits, which often starts with minimal evidence and requires laborious reconstruction of attack lifecycles.

Method: Designed TxRay, a LLM-driven system that reconstructs and analyzes DeFi exploits using tool calls, producing root cause explanations and self-contained Proofs of Concept. Developed PoCEvaluator to independently assess correctness and quality.

Result: TxRay achieved 92.11% end-to-end reproduction accuracy on 114 incidents, with 98.1% of proofs avoiding hard-coding attacker addresses. It delivers validated root causes and proofs within an hour on average.

Conclusion: TxRay effectively automates postmortem analysis for DeFi exploits, greatly improving the speed, accuracy, and quality of incident investigations while enhancing reproducibility.

Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +24.8pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.

</details>


### [1350] [Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization](https://arxiv.org/abs/2602.01342)
*Poushali Sengupta,Mayank Raikwar,Sabita Maharjan,Frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: Future quantum computers may break current security systems like V2X, but adaptive post-quantum cryptography methods could improve safety and efficiency in 6G vehicular networks.


<details>
  <summary>Details</summary>
Motivation: To ensure V2X communication security in the face of quantum threats while maintaining low latency and high efficiency for future 6G networks.

Method: The study used an adaptive post-quantum cryptography framework employing a predictive multi-objective evolutionary algorithm (APMOEA) alongside a secure monotonic-upgrade protocol to address security during algorithm transitions.

Result: The framework demonstrated latency reduction by 27%, communication overhead minimization by 65%, and enhanced stability, while successfully preventing security attacks during cryptographic algorithm switches.

Conclusion: The proposed systems offer a feasible solution for implementing quantum-safe cryptography in dynamic 6G vehicular network environments.

Abstract: Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\%, lowers communication overhead by up to 65\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.

</details>


### [1351] [CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses](https://arxiv.org/abs/2602.01438)
*Max Manolov,Tony Gao,Siddharth Shukla,Cheng-Ting Chou,Ryan Lagasse*

Main category: cs.CR

TL;DR: The paper introduces CIPHER, a benchmark to evaluate cryptographic vulnerabilities in Python code generated by large language models (LLMs) under different security-awareness prompt conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the critical issue of LLMs frequently generating cryptographic code with exploitable vulnerabilities due to their implementation errors, which can compromise security guarantees.

Method: CIPHER uses a combination of insecure/neutral/secure prompt variants, a cryptography-specific vulnerability taxonomy, and an automated scoring pipeline with line-level attribution to evaluate cryptographic vulnerabilities.

Result: Findings indicate that explicit secure prompts help reduce some vulnerabilities but fail to reliably eliminate cryptographic flaws in the code generated by a diverse set of widely used LLMs.

Conclusion: CIPHER serves as a tool to consistently measure and highlight cryptographic vulnerabilities in LLM-generated code, paving the way for more secure AI-aided code development.

Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(\textbf{C}ryptographic \textbf{I}nsecurity \textbf{P}rofiling via \textbf{H}ybrid \textbf{E}valuation of \textbf{R}esponses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit ``secure'' prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.

</details>


### [1352] [Comparison of Multiple Classifiers for Android Malware Detection with Emphasis on Feature Insights Using CICMalDroid 2020 Dataset](https://arxiv.org/abs/2602.00058)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

TL;DR: The paper focused on building an accurate and practical Android malware detection system using comprehensive datasets and rigorous evaluation methods, achieving high accuracy with gradient boosting models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of lagging signature-based malware detection systems and build a trustworthy, interpretable malware detection model.

Method: Used CICMalDroid2020 dataset with 17,341 apps and extracted hybrid features (564 dimensions). Evaluated seven classifiers under three schemes (original features, PCA, and LDA) with a 70%-30% train-test split.

Result: Gradient boosting models (XGBoost and HistGradientBoosting) performed best, achieving up to 0.9747 accuracy and high F1 scores. PCA reduced model performance, while LDA maintained reasonable accuracy and provided feature separation.

Conclusion: Rich hybrid features combined with gradient boosting models offer a robust, interpretable foundation for Android malware detection and deployment.

Abstract: Accurate Android malware detection was critical for protecting users at scale. Signature scanners lagged behind fast release cycles on public app stores. We aimed to build a trustworthy detector by pairing a comprehensive dataset with a rigorous, transparent evaluation, and to identify interpretable drivers of decisions. We used CICMalDroid2020, which contained 17,341 apps across Benign, Adware, Banking, SMS malware, and Riskware. We extracted 301 static and 263 dynamic features into a 564 dimensional hybrid vector, then evaluated seven classifiers under three schemes, original features, principal component analysis, PCA, and linear discriminant analysis, LDA, with a 70 percent training and 30 percent test split. Results showed that gradient boosting on the original features performed best. XGBoost achieved 0.9747 accuracy, 0.9703 precision, 0.9731 recall, and 0.9716 F1, and the confusion matrix indicated rare benign labels for malicious apps. HistGradientBoosting reached 0.9741 accuracy and 0.9708 F1, while CatBoost and Random Forest were slightly lower at 0.9678 and 0.9687 accuracy with 0.9636 and 0.9637 F1. KNN and SVM lagged. PCA reduced performance for all models, with XGBoost dropping to 0.9164 accuracy and 0.8988 F1. LDA maintained mid 90s accuracy and clarified separable clusters in projections. A depth two surrogate tree highlighted package name, main activity, and target SDK as key drivers. These findings established high fidelity supervised baselines for Android malware detection and indicated that rich hybrid features with gradient boosting offered a practical and interpretable foundation for deployment.

</details>


### [1353] [Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency](https://arxiv.org/abs/2602.01765)
*Bingzheng Wang,Xiaoyan Gu,Hongbo Xu,Hongcheng Li,Zimo Yu,Jiang Zhou,Weiping Wang*

Main category: cs.CR

TL;DR: The paper presents temporal noise inconsistency as a novel issue in diffusion models' backdoor security and proposes a Temporal Noise Consistency Defense (TNC-Defense) framework for detection and detoxification, showing improved detection accuracy and effective backdoor suppression.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the vulnerabilities of diffusion models to backdoor attacks, especially in scenarios where auditors have limited access to model parameters due to confidentiality or intellectual property protection.

Method: This study identifies temporal noise inconsistency in diffusion models under backdoor attacks and leverages this finding to develop TNC-Defense. The method consists of a gray-box detection module for locating anomalous diffusion timesteps and a detoxification module that corrects backdoor effects without significant loss in generation quality.

Result: TNC-Defense improves detection accuracy by 11% compared to existing state-of-the-art methods while invalidating 98.5% of triggered backdoor samples. It achieves this with minimal degradation in the generation quality of diffusion models.

Conclusion: TNC-Defense provides an effective and efficient solution to the challenge of backdoor detection and detoxification in diffusion models, bridging the gap between security and performance in practical scenarios.

Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.
  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.
  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\%$ with negligible additional overhead, and invalidates an average of $98.5\%$ of triggered samples with only a mild degradation in generation quality.

</details>


### [1354] [RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse](https://arxiv.org/abs/2602.01795)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: The paper introduces RedVisor, a framework mitigating Prompt Injection (PI) attacks in LLMs by integrating explainable detection with prevention, using a detachable adapter for attack analysis and safe responses.


<details>
  <summary>Details</summary>
Motivation: Prompt Injection attacks compromise LLM execution by embedding adversarial instructions in retrieved contexts. Existing defenses offer a trade-off between utility degradation and high latency, necessitating a unified solution.

Method: RedVisor uses a lightweight adapter atop a frozen backbone for detecting PI attacks via explainable analysis and guiding safe responses. It achieves this while preserving the model's original utility and adopting a KV Cache Reuse strategy.

Result: RedVisor surpasses state-of-the-art benchmarks in detection accuracy and model throughput, with minimal impact on the backbone's utility.

Conclusion: RedVisor effectively detects and mitigates Prompt Injection attacks in a resource-efficient manner, providing enhanced security for LLM applications without significant trade-offs.

Abstract: Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the "alignment tax", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1355] [QSPE: Enumerating Skeletal Quantum Programs for Quantum Library Testing](https://arxiv.org/abs/2602.00024)
*Jiaming Ye,Fuyuan Zhang,Shangzhou Xia,Xiaoyu Guo,Xiongfei Wu,Jianjun Zhao,Yinxing Xue*

Main category: quant-ph

TL;DR: The paper introduces QSPE, a novel approach for testing quantum libraries that improves scalability and accuracy by avoiding the need for domain expertise and reducing false alarms with a statevector-based validation method.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of testing quantum libraries, particularly the reliance on domain expertise and the limitations of measurement-based output validation, which often leads to false positive results.

Method: QSPE uses differential testing principles and extends the SPE approach. It automates the process of generating diverse program variants and employs a statevector-based validation method to mitigate false positive errors.

Result: QSPE generated 22,770 program variants across multiple quantum platforms, reduced over 90% of redundant executions, and detected 708 miscompilation bugs. Importantly, 81 reported bugs were validated and acknowledged by the Qiskit team.

Conclusion: QSPE proves to be an effective and practical tool for testing quantum libraries, offering scalability, cost savings, and enhanced bug detection capabilities, making significant contributions to improving quantum computing tools.

Abstract: The rapid advancement of quantum computing has led to the development of various quantum libraries, empowering compilation, simulation, and hardware backend interfaces. However, ensuring the correctness of these libraries remains a fundamental challenge due to the lack of mature testing methodologies. The state-of-the-art tools often rely on domain-specific configurations and expert knowledge, which limits their accessibility and scalability in practice. Furthermore, although these tools demonstrate strong performance, they adopt measurement-based for output validation in testing, which makes them produce false positive reports.
  To alleviate these limitations, we propose QSPE, a practical approach that follows the differential testing principle and extends the existing approach, SPE, for quantum libraries. QSPE is fully automated, requiring no pre-set configurations or domain expertise, and can effectively generate a large set of diverse program variants that comprehensively explore the quantum compilation space. To mitigate the possible false positive reports, we propose statevector-based validation as an alternative to measurement-based validation. In our experiments, the QSPE approach demonstrates remarkable effectiveness in generating 22,770 program variants across multiple quantum computing platforms. By avoiding $α$-equivalence at the quantum and classical program wise, QSPE can reduce redundant generation and save more than 90\% of execution cost. Finally, the statevector-based validation method assists QSPE to reduce false alarms and effectively detects 708 miscompilations across multiple quantum libraries. Notably, 81 of the discovered bugs have been officially approved and acknowledged by the Qiskit development team, demonstrating the practical impact of our approach.

</details>


### [1356] [Quantum Circuit-Based Learning Models: Bridging Quantum Computing and Machine Learning](https://arxiv.org/abs/2602.00048)
*Fan Fan,Yilei Shi,Mihai Datcu,Bertrand Le Saux,Luigi Iapichino,Francesca Bovolo,Silvia Liberata Ullo,Xiao Xiang Zhu*

Main category: quant-ph

TL;DR: The paper reviews quantum machine learning (QML), focusing on quantum circuit-based methods for classical data analysis, exploring integration with classical ML layers and evaluating theoretical, empirical findings, hardware-efficient techniques, and applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges in conventional ML through advancements in quantum computing, exploring its integration with classical techniques, and advancing hybrid QML techniques.

Method: Review of QML models, including kernel-based and neural network-based approaches, hybrid methodologies, and the study of noise-resilient and hardware-efficient paradigms.

Result: The paper identifies potentials, challenges, and theoretical/empirical insights into QML models, providing implications for their practicality and application domains.

Conclusion: This study offers a comprehensive guide on QML’s progress, challenges, and future opportunities, aiming to facilitate broader adoption and development in the field.

Abstract: Machine Learning (ML) has been widely applied across numerous domains due to its ability to automatically identify informative patterns from data for various tasks. The availability of large-scale data and advanced computational power enables the development of sophisticated models and training strategies, leading to state-of-the-art performance, but it also introduces substantial challenges. Quantum Computing (QC), which exploits quantum mechanisms for computation, has attracted growing attention and significant global investment as it may address these challenges. Consequently, Quantum Machine Learning (QML), the integration of these two fields, has received increasing interest, with a notable rise in related studies in recent years. We are motivated to review these existing contributions regarding quantum circuit-based learning models for classical data analysis and highlight the identified potentials and challenges of this technique. Specifically, we focus not only on QML models, both kernel-based and neural network-based, but also on recent explorations of their integration with classical machine learning layers within hybrid frameworks. Moreover, we examine both theoretical analysis and empirical findings to better understand their capabilities, and we also discuss the efforts on noise-resilient and hardware-efficient QML that could enhance its practicality under current hardware limitations. In addition, we cover several emerging paradigms for advanced quantum circuit design and highlight the adaptability of QML across representative application domains. This study aims to provide an overview of the contributions made to bridge quantum computing and machine learning, offering insights and guidance to support its future development and pave the way for broader adoption in the coming years.

</details>


### [1357] [Quantum Phase Recognition via Quantum Attention Mechanism](https://arxiv.org/abs/2602.00473)
*Jin-Long Chen,Xin Li,Zhang-Qi Yin*

Main category: quant-ph

TL;DR: The paper introduces a hybrid quantum-classical attention model to efficiently classify quantum phases in many-body systems with high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Quantum phase transitions involve intricate correlation structures that challenge traditional computational methods, especially in large systems.

Method: A hybrid quantum-classical attention model incorporates a parameterized quantum circuit and swap tests to analyze correlations in quantum states and classify ground states.

Result: The model achieves high classification accuracy on the cluster-Ising model with 9 and 15 qubits using fewer than 100 training samples, and proves robust as training set variations occur.

Conclusion: This approach provides a scalable, data-efficient solution for recognizing quantum phases in complex systems, capturing key physical features and correlation scales.

Abstract: Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechanism, realized through swap tests and a parameterized quantum circuit, to extract correlations within quantum states and perform ground-state classification. Benchmarked on the cluster-Ising model with system sizes of 9 and 15 qubits, the model achieves high classification accuracy with less than 100 training data and demonstrates robustness against variations in the training set. Further analysis reveals that the model successfully captures phase-sensitive features and characteristic physical length scales, offering a scalable and data-efficient approach for quantum phase recognition in complex many-body systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1358] [Frequent Pattern Mining approach to Image Compression](https://arxiv.org/abs/2602.00100)
*Avinash Kadimisetty,C. Oswald,B. Sivalselvan*

Main category: eess.IV

TL;DR: The paper introduces an efficient image compression approach using clustering and sequential pattern mining, showing significant improvement in compression ratios with minimal loss of quality.


<details>
  <summary>Details</summary>
Motivation: To enhance image compression efficiency by addressing redundant data and improving compression algorithms using pattern mining techniques.

Method: The approach combines k-means clustering, Closed Frequent Sequence Mining, and refined Generalized Sequential Pattern Mining (GSP) to replace the DCT process in JPEG.

Result: Testing on benchmark datasets showed a 45% improvement in compression ratios with minimal loss in visual quality as validated by PSNR and SSIM metrics.

Conclusion: The proposed method outperforms existing alternatives by effectively compressing images with maintained visual quality and reduced code table sizes.

Abstract: The paper focuses on Image Compression, explaining efficient approaches based on Frequent Pattern Mining(FPM). The proposed compression mechanism is based on clustering similar pixels in the image and thus using cluster identifiers in image compression. Redundant data in the image is effectively handled by replacing the DCT phase of conventional JPEG through a mixture of k-means Clustering and Closed Frequent Sequence Mining. To optimize the cardinality of pattern(s) in encoding, efficient pruning techniques have been used through the refinement of Conventional Generalized Sequential Pattern Mining(GSP) algorithm. We have proposed a mechanism for finding the frequency of a sequence which will yield significant reduction in the code table size. The algorithm is tested by compressing benchmark datasets yielding an improvement of 45% in compression ratios, often outperforming the existing alternatives. PSNR and SSIM, which are the image quality metrics, have been tested which show a negligible loss in visual quality.

</details>


### [1359] [Radiomics in Medical Imaging: Methods, Applications, and Challenges](https://arxiv.org/abs/2602.00102)
*Fnu Neha,Deepak kumar Shukla*

Main category: eess.IV

TL;DR: The paper surveys the radiomics pipeline comprehensively, highlighting how methodological decisions impact robustness and clinical viability while identifying key challenges and future opportunities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address persistent challenges in radiomics, such as feature instability, low reproducibility, validation bias, and limited clinical application, and to analyze the interdependent design choices influencing these issues.

Method: The approach involves an end-to-end analysis of the radiomics pipeline, discussing every stage from feature extraction to clinical applications, while emphasizing validation rigor and data reliability.

Result: The paper emphasizes the role of different methodological decisions in influencing feature stability and model reliability. It identifies challenges like standardization and domain shift while proposing future directions such as hybrid models and federated learning.

Conclusion: Robust radiomics pipelines require innovation in hybrid AI models, multimodal integration, and standardization to overcome current challenges and achieve broader clinical impact.

Abstract: Radiomics enables quantitative medical image analysis by converting imaging data into structured, high-dimensional feature representations for predictive modeling. Despite methodological developments and encouraging retrospective results, radiomics continue to face persistent challenges related to feature instability, limited reproducibility, validation bias, and restricted clinical translation. Existing reviews largely focus on application-specific outcomes or isolated pipeline components, with limited analysis of how interdependent design choices across acquisition, preprocessing, feature engineering, modeling, and evaluation collectively affect robustness and generalizability. This survey provides an end-to-end analysis of radiomics pipelines, examining how methodological decisions at each stage influence feature stability, model reliability, and translational validity. This paper reviews radiomic feature extraction, selection, and dimensionality reduction strategies; classical machine and deep learning-based modeling approaches; and ensemble and hybrid frameworks, with emphasis on validation protocols, data leakage prevention, and statistical reliability. Clinical applications are discussed with a focus on evaluation rigor rather than reported performance metrics. The survey identifies open challenges in standardization, domain shift, and clinical deployment, and outlines future directions such as hybrid radiomics-artificial intelligence models, multimodal fusion, federated learning, and standardized benchmarking.

</details>


### [1360] [Visible Singularities Guided Correlation Network for Limited-Angle CT Reconstruction](https://arxiv.org/abs/2602.00184)
*Yiyang Wen,Liu Shi,Zekun Zhou,WenZhe Shan,Qiegen Liu*

Main category: eess.IV

TL;DR: Researchers propose a new reconstruction method, VSGC, for limited-angle CT (LACT) that leverages the visibility of singularities, achieving substantial improvements in image quality.


<details>
  <summary>Details</summary>
Motivation: This study addresses the key limitations in traditional and deep learning-based methods for LACT, particularly the failure to account for artifact directionality and directional structural losses caused by missing projection angles.

Method: The proposed VSGC network extracts visible singularities (VS) edge features, focuses on these features, and connects them with other image regions. A multi-scale loss function with anisotropic constraints is used to enhance reconstruction performance.

Result: VSGC shows superior performance on both simulated and real datasets, particularly in small angular ranges, with significant PSNR (2.45 dB improvement) and SSIM (1.5% improvement) over alternative methods.

Conclusion: The study demonstrates the effectiveness of VSGC for addressing LACT challenges by selectively leveraging visible singularities, and achieves considerable improvements in image reconstruction quality. The code is made publicly available.

Abstract: Limited-angle computed tomography (LACT) offers the advantages of reduced radiation dose and shortened scanning time. Traditional reconstruction algorithms exhibit various inherent limitations in LACT. Currently, most deep learning-based LACT reconstruction methods focus on multi-domain fusion or the introduction of generic priors, failing to fully align with the core imaging characteristics of LACT-such as the directionality of artifacts and directional loss of structural information, which are caused by the absence of projection angles in certain directions. Inspired by the theory of visible and invisible singularities, taking into account the aforementioned core imaging characteristics of LACT, we propose a Visible Singularities Guided Correlation network for LACT reconstruction (VSGC). The design philosophy of VSGC consists of two core steps: First, extract VS edge features from LACT images and focus the model's attention on these VS. Second, establish correlations between the VS edge features and other regions of the image. Additionally, a multi-scale loss function with anisotropic constraint is employed to constrain the model to converge in multiple aspects. Finally, qualitative and quantitative validations are conducted on both simulated and real datasets to verify the effectiveness and feasibility of the proposed design. Particularly, in comparison with alternative methods, VSGC delivers more prominent performance in small angular ranges, with the PSNR improvement of 2.45 dB and the SSIM enhancement of 1.5\%. The code is publicly available at https://github.com/yqx7150/VSGC.

</details>


### [1361] [SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming](https://arxiv.org/abs/2602.00198)
*Esteban Pesnel,Julien Le Tanou,Michael Ropert,Thomas Maugey,Aline Roumy*

Main category: eess.IV

TL;DR: The paper addresses challenges in Adaptive Bitrate (ABR) streaming and introduces a novel framework for end-to-end training with real, non-differentiable video codecs.


<details>
  <summary>Details</summary>
Motivation: Modern ABR streaming architectures face significant inefficiencies, especially due to traditional isolated optimization of processing stages and the non-differentiable nature of standard video codecs.

Method: The framework leverages data-driven surrogate gradients derived from actual compression errors to enable gradient-based optimization with real, non-differentiable codecs.

Result: The approach improves BD-BR (PSNR) by 5.19% compared to codec-agnostic training approaches, showing consistent performance across rate-distortion convex hulls.

Conclusion: Their novel framework aligns training objectives with deployment performance and offers improved efficiency over existing methods.

Abstract: The rapid growth in video consumption has introduced significant challenges to modern streaming architectures. Over-the-Top (OTT) video delivery now predominantly relies on Adaptive Bitrate (ABR) streaming, which dynamically adjusts bitrate and resolution based on client-side constraints such as display capabilities and network bandwidth. This pipeline typically involves downsampling the original high-resolution content, encoding and transmitting it, followed by decoding and upsampling on the client side. Traditionally, these processing stages have been optimized in isolation, leading to suboptimal end-to-end rate-distortion (R-D) performance. The advent of deep learning has spurred interest in jointly optimizing the ABR pipeline using learned resampling methods. However, training such systems end-to-end remains challenging due to the non-differentiable nature of standard video codecs, which obstructs gradient-based optimization. Recent works have addressed this issue using differentiable proxy models, based either on deep neural networks or hybrid coding schemes with differentiable components such as soft quantization, to approximate the codec behavior. While differentiable proxy codecs have enabled progress in compression-aware learning, they remain approximations that may not fully capture the behavior of standard, non-differentiable codecs. To our knowledge, there is no prior evidence demonstrating the inefficiencies of using standard codecs during training. In this work, we introduce a novel framework that enables end-to-end training with real, non-differentiable codecs by leveraging data-driven surrogate gradients derived from actual compression errors. It facilitates the alignment between training objectives and deployment performance. Experimental results show a 5.19\% improvement in BD-BR (PSNR) compared to codec-agnostic training approaches, consistently across the entire rate-distortion convex hull spanning multiple downsampling ratios.

</details>


### [1362] [Toward a Unified Semantic Loss Model for Deep JSCC-based Transmission of EO Imagery](https://arxiv.org/abs/2602.00136)
*Ti Ti Nguyen,Thanh-Dung Le,Vu Nguyen Ha,Duc-Dung Tran,Hung Nguyen-Kha,Dinh-Hieu Tran,Carlos L. Marcos-Rojas,Juan C. Merlano-Duncan,Symeon Chatzinotas*

Main category: eess.IV

TL;DR: The paper addresses the challenge of transmitting high-resolution Earth Observation (EO) imagery over limited satellite communication systems by proposing a deep learning-based semantic approach for compression and error correction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the bandwidth, power, and dynamic condition constraints in transmitting high-resolution EO imagery for applications like environmental monitoring and disaster response.

Method: The method employs Deep Joint Source-Channel Coding (DJSCC) for EO imagery transmission. It evaluates semantic loss using reconstruction-centric and task-oriented frameworks, integrating lightweight application-specific models like EfficientViT.

Result: A unified semantic loss framework was developed, capturing relationships between compression, channel SNR, and semantic quality. Extensive empirical analysis validated its effectiveness.

Conclusion: The proposed framework offers insights for designing robust and efficient EO imagery transmission under resource-limited satellite links, balancing semantic quality and system constraints.

Abstract: Modern Earth Observation (EO) systems increasingly rely on high-resolution imagery to support critical applications such as environmental monitoring, disaster response, and land-use analysis. Although these applications benefit from detailed visual data, the resulting data volumes impose significant challenges on satellite communication systems constrained by limited bandwidth, power, and dynamic link conditions. To address these limitations, this paper investigates Deep Joint Source-Channel Coding (DJSCC) as an effective source-channel paradigm for the transmission of EO imagery. We focus on two complementary aspects of semantic loss in DJSCC-based systems. First, a reconstruction-centric framework is evaluated by analyzing the semantic degradation of reconstructed images under varying compression ratios and channel signal-to-noise ratios (SNR). Second, a task-oriented framework is developed by integrating DJSCC with lightweight, application-specific models (e.g., EfficientViT), with performance measured using downstream task accuracy rather than pixel-level fidelity. Based on extensive empirical analysis, we propose a unified semantic loss framework that captures both reconstruction-centric and task-oriented performance within a single model. This framework characterizes the implicit relationship between JSCC compression, channel SNR, and semantic quality, offering actionable insights for the design of robust and efficient EO imagery transmission under resource-constrained satellite links.

</details>


### [1363] [SurfelSoup: Learned Point Cloud Geometry Compression With a Probablistic SurfelTree Representation](https://arxiv.org/abs/2602.00186)
*Tingyu Fan,Ran Gong,Yueyu Hu,Yao Wang*

Main category: eess.IV

TL;DR: SurfelSoup introduces a novel surface-based framework for point cloud geometry compression using probabilistic surface primitives and hierarchical structuring.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in point-wise compression and improve geometry compression for point clouds.

Method: Develop probabilistic surface representation (pSurfel) and hierarchical structuring (pSurfelTree) with adaptive rate-distortion optimization.

Result: Demonstrates superior geometry compression performance and visually smooth reconstructions compared to existing methods and standards.

Conclusion: SurfelSoup achieves compact and smooth surface-based reconstructions, outperforming traditional voxel-based approaches.

Abstract: This paper presents SurfelSoup, an end-to-end learned surface-based framework for point cloud geometry compression, with surface-structured primitives for representation. It proposes a probabilistic surface representation, pSurfel, which models local point occupancies using a bounded generalized Gaussian distribution. In addition, the pSurfels are organized into an octree-like hierarchy, pSurfelTree, with a Tree Decision module that adaptively terminates the tree subdivision for rate-distortion optimal Surfel granularity selection. This formulation avoids redundant point-wise compression in smooth regions and produces compact yet smooth surface reconstructions. Experimental results under the MPEG common test condition show consistent gain on geometry compression over voxel-based baselines and MPEG standard G-PCC-GesTM-TriSoup, while providing visually superior reconstructions with smooth and coherent surface structures.

</details>


### [1364] [A Renderer-Enabled Framework for Computing Parameter Estimation Lower Bounds in Plenoptic Imaging Systems](https://arxiv.org/abs/2602.00215)
*Abhinav V. Sambasivan,Liam J. Coulter,Richard G. Paxman,Jarvis D. Haupt*

Main category: eess.IV

TL;DR: The paper focuses on evaluating the information-theoretic limits in plenoptic imaging for scene parameter estimation using a new framework that computes bounds on estimation error.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in parameter estimation for plenoptic imaging, especially in passive indirect imaging scenarios where line-of-sight information is unavailable.

Method: A framework leveraging computer graphics rendering software to synthesize forward models and compute the Hammersley-Chapman-Robbins bound for performance analysis.

Result: The proposed lower bounds are validated and shown to be indicative of estimation limits when compared to Maximum Likelihood Estimator performance in object localization tasks.

Conclusion: The framework effectively provides insights into the fundamental limits of parameter estimators, even when the rendering model is not exact.

Abstract: This work focuses on assessing the information-theoretic limits of scene parameter estimation in plenoptic imaging systems. A general framework to compute lower bounds on the parameter estimation error from noisy plenoptic observations is presented, with a particular focus on passive indirect imaging problems, where the observations do not contain line-of-sight information about the parameter(s) of interest. Using computer graphics rendering software to synthesize the often-complicated dependence among parameter(s) of interest and observations, i.e. the forward model, the proposed framework evaluates the Hammersley-Chapman-Robbins bound to establish lower bounds on the variance of any unbiased estimator of the unknown parameters. The effects of inexact rendering of the true forward model on the computed lower bounds are also analyzed, both theoretically and via simulations. Experimental evaluations compare the computed lower bounds with the performance of the Maximum Likelihood Estimator on a canonical object localization problem, showing that the lower bounds computed via the framework proposed here are indicative of the true underlying fundamental limits in several nominally representative scenarios.

</details>


### [1365] [Advanced Geometric Correction Algorithms for 3D Medical Reconstruction: Comparison of Computed Tomography and Macroscopic Imaging](https://arxiv.org/abs/2602.00220)
*Tomasz Les,Tomasz Markiewicz,Malgorzata Lorent,Miroslaw Dziekiewicz,Krzysztof Siwek*

Main category: eess.IV

TL;DR: This paper presents a two-stage hybrid framework for 3D kidney reconstruction combining constrained global alignment with a deep learning refinement network, achieving improved accuracy on distorted imaging data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of data scarcity and high distortion in macroscopic imaging, where fully learning-based registration methods struggle due to limited training diversity and large nonrigid deformations.

Method: The framework uses a two-stage approach: (1) Optimal Cross-section Matching (OCM) for global alignment (translation, rotation, scaling), followed by (2) a lightweight deep-learning network inspired by VoxelMorph for local deformation refinement.

Result: Experiments on a dataset of 40 kidneys showed that the proposed method outperformed single-stage baselines, ensuring more anatomically realistic reconstructions.

Conclusion: The method combines explicit geometric priors with data-efficient deep learning, improving precision and robustness in 3D reconstruction, with potential applications beyond kidney imaging.

Abstract: This paper introduces a hybrid two-stage registration framework for reconstructing three-dimensional (3D) kidney anatomy from macroscopic slices, using CT-derived models as the geometric reference standard. The approach addresses the data-scarcity and high-distortion challenges typical of macroscopic imaging, where fully learning-based registration (e.g., VoxelMorph) often fails to generalize due to limited training diversity and large nonrigid deformations that exceed the capture range of unconstrained convolutional filters. In the proposed pipeline, the Optimal Cross-section Matching (OCM) algorithm first performs constrained global alignment: translation, rotation, and uniform scaling to establish anatomically consistent slice initialization. Next, a lightweight deep-learning refinement network, inspired by VoxelMorph, predicts residual local deformations between consecutive slices. The core novelty of this architecture lies in its hierarchical decomposition of the registration manifold. This hybrid OCM+DL design integrates explicit geometric priors with the flexible learning capacity of neural networks, ensuring stable optimization and plausible deformation fields even with few training examples. Experiments on an original dataset of 40 kidneys demonstrated better results compared to single-stage baselines. The pipeline maintains physical calibration via Hough-based grid detection and employs Bezier-based contour smoothing for robust meshing and volume estimation. Although validated on kidney data, the proposed framework generalizes to other soft-tissue organs reconstructed from optical or photographic cross-sections. By decoupling interpretable global optimization from data-efficient deep refinement, the method advances the precision, reproducibility, and anatomical realism of multimodal 3D reconstructions for surgical planning, morphological assessment, and medical education.

</details>


### [1366] [Benchmarking Vanilla GAN, DCGAN, and WGAN Architectures for MRI Reconstruction: A Quantitative Analysis](https://arxiv.org/abs/2602.00221)
*Humaira Mehwish,Hina Shakir,Muneeba Rashid,Asarim Aamir,Reema Qaiser Khan*

Main category: eess.IV

TL;DR: The study evaluates the performance of three GAN models—Vanilla GAN, DCGAN, and WGAN—for reconstructing MRI images, with DCGAN and WGAN showing superior results in image quality and stability.


<details>
  <summary>Details</summary>
Motivation: Improve accuracy and image quality for MRI reconstruction by leveraging and analyzing different variations of GAN models.

Method: The study trained and tested Vanilla GAN, DCGAN, and WGAN on knee, brain, and cardiac MRI datasets, comparing their SSIM and PSNR metrics to evaluate reconstruction performance.

Result: WGAN scored the highest SSIM of 0.99, followed by DCGAN (0.97) and Vanilla GAN (0.84). DCGAN achieved the highest PSNR (49.3), while WGAN and Vanilla GAN scored 43.5 and 26, respectively.

Conclusion: DCGAN and WGAN demonstrated superior results in terms of image quality and diagnostic potential in cross-organ MRI datasets, establishing a reproducible benchmark for future research and hybrid models.

Abstract: Magnetic Resonance Imaging (MRI) is a crucial imaging modality for viewing internal body structures. This research work analyses the performance of popular GAN models for accurate and precise MRI reconstruction by enhancing image quality and improving diagnostic accuracy. Three GAN architectures considered in this study are Vanilla GAN, Deep Convolutional GAN (DCGAN), and Wasserstein GAN (WGAN). They were trained and evaluated using knee, brain, and cardiac MRI datasets to assess their generalizability across body regions. While the Vanilla GAN operates on the fundamentals of the adversarial network setup, DCGAN advances image synthesis by securing the convolutional layers, giving a superior appearance to the prevalent spatial features. Training instability is resolved in WGAN through the Wasserstein distance to minimize an unstable regime, therefore, ensuring stable convergence and high-quality images. The GAN models were trained and tested using 1000 MR images of an anonymized knee, 805 images of Heart, 90 images of Brain MRI dataset. The Structural Similarity Index (SSIM) for Vanilla GAN is 0.84, DCGAN is 0.97, and WGAN is 0.99. The Peak Signal to Noise Ratio (PSNR) for Vanilla GAN is 26, DCGAN is 49.3, and WGAN is 43.5. The results were further statistically validated. This study shows that DCGAN and WGAN-based frameworks are promising in MR image reconstruction because of good image quality and superior accuracy. With the first cross-organ benchmark of baseline GANs under a common preprocessing pipeline, this work provides a reproducible benchmark for future hybrid GANs and clinical MRI applications.

</details>


### [1367] [Recent Advances of End-to-End Video Coding Technologies for AVS Standard Development](https://arxiv.org/abs/2602.00483)
*Xihua Sheng,Xiongzhuang Liang,Chuanbo Tang,Zhirui Zuo,Yifan Bian,Yutao Xie,Zhuoyuan Li,Yuqi Li,Hui Xiang,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: The paper discusses the AVS-EEM project, which aims to create an efficient intelligent video coding standard with low computational complexity, improved compression efficiency, and practical deployment focus.


<details>
  <summary>Details</summary>
Motivation: To develop a video coding standard, AVS-EEM, that achieves greater video compression efficiency with low computational complexity, allowing practical and widespread adoption.

Method: The project emphasizes practical deployment with strict adherence to conventional test conditions. It introduces innovations like optimized model architectures, training methods, and inference techniques to improve compression efficiency under strict complexity constraints.

Result: The AVS-EEM project has achieved significant improvements in video coding performance, with the latest model outperforming conventional AVS3 reference software in compression efficiency.

Conclusion: The AVS-EEM project's advancements highlight its potential to become a deployable intelligent video coding standard, offering superior performance and practical feasibility compared to traditional approaches.

Abstract: Video coding standards are essential to enable the interoperability and widespread adoption of efficient video compression technologies. In pursuit of greater video compression efficiency, the AVS video coding working group launched the standardization exploration of end-to-end intelligent video coding, establishing the AVS End-to-End Intelligent Video Coding Exploration Model (AVS-EEM) project. A core design principle of AVS-EEM is its focus on practical deployment, featuring inherently low computational complexity and requiring strict adherence to the common test conditions of conventional video coding. This paper details the development history of AVS-EEM and provides a systematic introduction to its key technical framework, covering model architectures, training strategies, and inference optimizations. These innovations have collectively driven the project's rapid performance evolution, enabling continuous and significant gains under strict complexity constraints. Through over two years of iterative refinement and collaborative effort, the coding performance of AVS-EEM has seen substantial improvement. Experimental results demonstrate that its latest model achieves superior compression efficiency compared to the conventional AVS3 reference software, marking a significant step toward a deployable intelligent video coding standard.

</details>


### [1368] [A texture-based framework for foundational ultrasound models](https://arxiv.org/abs/2602.01444)
*Tal Grutman,Carmel Shinar,Tali Ilovitsh*

Main category: eess.IV

TL;DR: This paper develops a novel approach named Texture Ultrasound Semantic Analysis (TUSA) for self-supervised learning in ultrasound imaging.


<details>
  <summary>Details</summary>
Motivation: Ultrasound images exhibit unique textures derived from their physics, making them challenging for algorithms designed for natural images and limiting the effectiveness of general foundation models.

Method: The authors reformulated self-supervised learning into a texture analysis problem with TUSA, leveraging contrastive methods to extract domain-specific representations from B-mode images. TUSA models were trained using a combination of open-source, simulated, and in vivo ultrasound data.

Result: TUSA outperformed foundation models in downstream tasks on diverse datasets, achieving high accuracy in detecting clinical conditions like COVID (70%), spinal hematoma (100%), and vitreous hemorrhage (97%), and showed strong correlations with quantitative medical measures.

Conclusion: TUSA effectively integrates ultrasound-specific knowledge into modeling, offering improved generalizability and accuracy for diagnosing clinical conditions, demonstrating its potential for medical imaging advancements. The model weights and scripts are open-sourced for broader use.

Abstract: Ultrasound is the most widely used medical imaging modality, yet the images it produces are fundamentally unique, arising from tissue-dependent scattering, reflection, and speed-of-sound variations that produce a constrained set of characteristic textures that differ markedly from natural-image statistics. These acoustically driven patterns make ultrasound challenging for algorithms originally designed for natural images. To bridge this gap, the field has increasingly turned to foundation models, hoping to leverage their generalization capabilities. However, these models often falter in ultrasound applications because they are not designed for ultrasound physics, they are merely trained on ultrasound data. Therefore, it is essential to integrate ultrasound-specific domain knowledge into established learning frameworks. We achieve this by reformulating self-supervised learning as a texture-analysis problem, introducing texture ultrasound semantic analysis (TUSA). Using TUSA, models learn to leverage highly scalable contrastive methods to extract true domain-specific representations directly from simple B-mode images. We train a TUSA model on a combination of open-source, simulated, and in vivo data. The latent space is compared to several larger foundation models, demonstrating that our approach gives TUSA models better generalizability for difficult downstream tasks on unique online datasets as well as a clinical eye dataset collected for this study. Our model achieves higher accuracy in detecting COVID (70%), spinal hematoma (100%) and vitreous hemorrhage (97%) and correlates more closely with quantitative parameters like liver steatosis (r = 0.83), ejection fraction (r = 0.63), and oxygen saturation (r = 0.38). We open-source the model weights and training script: https://github.com/talg2324/tusa

</details>


### [1369] [MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation](https://arxiv.org/abs/2602.01513)
*Xiaoxi Kong,Jieyu Yuan,Pengdi Chen,Yuanlin Zhang,Chongyi Li,Bin Li*

Main category: eess.IV

TL;DR: The research introduces MarkCleaner, a framework to effectively and efficiently remove robust semantic watermarks using micro-geometric perturbations without causing semantic drift.


<details>
  <summary>Details</summary>
Motivation: To address challenges in removing robust semantic watermarks due to their phase alignment dependency, while avoiding semantic degradation when removing such marks.

Method: MarkCleaner uses micro-geometric-perturbed training, a mask-guided encoder to learn spatial representations, and a 2D Gaussian Splatting-based decoder to handle geometric perturbations while maintaining semantic content.

Result: MarkCleaner outperforms previous methods in watermark removal effectiveness, maintains high visual fidelity, and achieves real-time inference efficiently.

Conclusion: MarkCleaner is a robust and efficient solution for semantic watermark removal, effectively overcoming limitations of conventional methods while ensuring quality and speed, with code to be released publicly.

Abstract: Semantic watermarks exhibit strong robustness against conventional image-space attacks. In this work, we show that such robustness does not survive under micro-geometric perturbations: spatial displacements can remove watermarks by breaking the phase alignment. Motivated by this observation, we introduce MarkCleaner, a watermark removal framework that avoids semantic drift caused by regeneration-based watermark removal. Specifically, MarkCleaner is trained with micro-geometry-perturbed supervision, which encourages the model to separate semantic content from strict spatial alignment and enables robust reconstruction under subtle geometric displacements. The framework adopts a mask-guided encoder that learns explicit spatial representations and a 2D Gaussian Splatting-based decoder that explicitly parameterizes geometric perturbations while preserving semantic content. Extensive experiments demonstrate that MarkCleaner achieves superior performance in both watermark removal effectiveness and visual fidelity, while enabling efficient real-time inference. Our code will be made available upon acceptance.

</details>


### [1370] [Hyperspectral Image Fusion with Spectral-Band and Fusion-Scale Agnosticism](https://arxiv.org/abs/2602.01681)
*Yu-Jie Liang,Zihan Cao,Liang-Jian Deng,Yang Yang,Malu Zhang*

Main category: eess.IV

TL;DR: This paper introduces SSA, a universal framework for fusing Multispectral and Hyperspectral (MS/HS) images that adapts to diverse sensors and scales.


<details>
  <summary>Details</summary>
Motivation: Existing MS/HS fusion models are limited to fixed spectral bands and spatial scales, restricting their transferability across different sensors.

Method: The paper proposes SSA, combining the Matryoshka Kernel (MK) to adapt to arbitrary spectral channels and an Implicit Neural Representation (INR) backbone for modeling HS signals as continuous functions, supporting fusion regardless of resolution.

Result: Experiments show that the proposed SSA framework achieves state-of-the-art performance and generalizes well to unseen sensors and scales.

Conclusion: SSA provides a scalable and universal solution for MS/HS image fusion, potentially paving the way for foundation models in this field.

Abstract: Current deep learning models for Multispectral and Hyperspectral Image Fusion (MS/HS fusion) are typically designed for fixed spectral bands and spatial scales, which limits their transferability across diverse sensors. To address this, we propose SSA, a universal framework for MS/HS fusion with spectral-band and fusion-scale agnosticism. Specifically, we introduce Matryoshka Kernel (MK), a novel operator that enables a single model to adapt to arbitrary numbers of spectral channels. Meanwhile, we build SSA upon an Implicit Neural Representation (INR) backbone that models the HS signal as a continuous function, enabling reconstruction at arbitrary spatial resolutions. Together, these two forms of agnosticism enable a single MS/HS fusion model that generalizes effectively to unseen sensors and spatial scales. Extensive experiments demonstrate that our single model achieves state-of-the-art performance while generalizing well to unseen sensors and scales, paving the way toward future HS foundation models.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [1371] [RAG-GNN: Integrating Retrieved Knowledge with Graph Neural Networks for Precision Medicine](https://arxiv.org/abs/2602.00586)
*Hasi Hays,William J. Richardson*

Main category: q-bio.MN

TL;DR: This paper proposes a RAG framework that combines graph neural networks with literature-derived knowledge for functional semantics, showing complementary strengths of topology and retrieval-augmented approaches.


<details>
  <summary>Details</summary>
Motivation: To address the inability of network topology methods to capture functional semantics encoded in biomedical literature.

Method: The paper introduces a retrieval-augmented generation (RAG) embedding framework combining graph neural network representations with contrastive learning from dynamically retrieved biomedical literature.

Result: Topology-based methods achieve superior link prediction (AUROC: 0.983), while the RAG-GNN method uniquely excels at functional clustering (positive silhouette score). The framework identifies DDR1 as a therapeutic target for cancer signaling networks.

Conclusion: Topology-only and RAG approaches serve distinct yet complementary objectives: topology methods excel at structural predictions, while retrieval-augmented methods provide a unique advantage for functional interpretations.

Abstract: Network topology excels at structural predictions but fails to capture functional semantics encoded in biomedical literature. We present a retrieval-augmented generation (RAG) embedding framework that integrates graph neural network representations with dynamically retrieved literature-derived knowledge through contrastive learning. Benchmarking against ten embedding methods reveals task-specific complementarity: topology-focused methods achieve near-perfect link prediction (GCN: 0.983 AUROC), while RAG-GNN is the only method achieving positive silhouette scores for functional clustering (0.001 vs. negative scores for all baselines). Information-theoretic decomposition shows network topology contributes 77.3% of predictive information, while retrieved documents provide 8.6% unique information. Applied to cancer signaling networks (379 proteins, 3,498 interactions), the framework identifies DDR1 as a therapeutic target based on retrieved evidence of synthetic lethality with KRAS mutations. These results establish that topology-only and retrieval-augmented approaches serve complementary purposes: structural prediction tasks are solved by network topology alone, while functional interpretation uniquely benefits from retrieved knowledge.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [1372] [Autonomous Multi-Agent AI for High-Throughput Polymer Informatics: From Property Prediction to Generative Design Across Synthetic and Bio-Polymers](https://arxiv.org/abs/2602.00103)
*Mahule Roy,Adib Bazgir,Arthur da Silva Sousa Santos,Yuwen Zhang*

Main category: cond-mat.soft

TL;DR: The paper introduces a multiagent AI ecosystem for polymer discovery, unifying advanced computational tools to improve prediction, design, characterization, and metacognitive control.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenges in polymer discovery by integrating AI and computational modeling into a unified and efficient workflow for enhanced predictions and high-throughput materials analysis.

Method: The system combines specialized AI agents including language models with computational tools to perform property prediction, workflow automation, and self-assessment. It uses workflows like property prediction using PolyGNN and metacognitive strategy optimization.

Result: The system achieved strong predictive accuracy (e.g., R2 = 0.89 for glass-transition temperature) and outperformed benchmarks. It processed substantial polymer datasets efficiently with minimal computational cost and demonstrated scalability.

Conclusion: The proposed framework enables efficient and accurate polymer design, leveraging AI's predictive and self-assessment capabilities, thus advancing the field without substantial resource use.

Abstract: We present an integrated multiagent AI ecosystem for polymer discovery that unifies high-throughput materials workflows, artificial intelligence, and computational modeling within a single Polymer Research Lifecycle (PRL) pipeline. The system orchestrates specialized agents powered by state-of-the-art large language models (DeepSeek-V2 and DeepSeek-Coder) to retrieve and reason over scientific resources, invoke external tools, execute domain-specific code, and perform metacognitive self-assessment for robust end-to-end task execution. We demonstrate three practical capabilities: a high-fidelity polymer property prediction and generative design pipeline, a fully automated multimodal workflow for biopolymer structure characterization, and a metacognitive agent framework that can monitor performance and improve execution strategies over time. On a held-out test set of 1,251 polymers, our PolyGNN agent achieves strong predictive accuracy, reaching R2 = 0.89 for glass-transition temperature (Tg ), R2 = 0.82 for tensile strength, R2 = 0.75 for elongation, and R2 = 0.91 for density. The framework also provides uncertainty estimates via multiagent consensus and scales with linear complexity to at least 10,000 polymers, enabling high-throughput screening at low computational cost. For a representative workload, the system completes inference in 16.3 s using about 2 GB of memory and 0.1 GPU hours, at an estimated cost of about $0.08. On a dedicated Tg benchmark, our approach attains R2 = 0.78, outperforming strong baselines including single-LLM prediction (R2 = 0.67), group-contribution methods (R2 = 0.71), and ChemCrow (R2 = 0.66). We further demonstrate metacognitive control in a polystyrene case study, where the system not only produces domain-level scientific outputs but continually monitors and optimizes its own behavior through tactical, strategic, and meta-strategic self-assessment.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1373] [Motion Planning with Metric Temporal Logic Using Reachability Analysis and Hybrid Zonotopes](https://arxiv.org/abs/2602.00325)
*Andrew F. Thompson,Joshua A. Robbins,Jonah J. Glunt,Sean B. Brennan,Herschel C. Pangborn*

Main category: eess.SY

TL;DR: The paper introduces an efficient method using hybrid zonotope-based reachability for optimizing motion planning under Metric Temporal Logic (MTL) constraints.


<details>
  <summary>Details</summary>
Motivation: Optimizing control decisions for autonomous vehicles under MTL constraints is computationally demanding. The goal is to address this computational challenge while ensuring motion plan optimization.

Method: The method utilizes reachability analysis with hybrid zonotope set representations to implicitly encode MTL specifications into reachable state sets and then optimize control decisions to meet these specifications.

Result: The method demonstrates computational efficiency over existing approaches, supported by numerical benchmarks and practical applications in dynamic environments, disturbances, and multi-agent scenarios.

Conclusion: The proposed method provides a computationally efficient solution for optimizing motion plans under MTL constraints, with practical viability in complex, real-world scenarios.

Abstract: Metric temporal logic (MTL) provides a formal framework for defining time-dependent mission requirements on autonomous vehicles. However, optimizing control decisions subject to these constraints is often computationally expensive. This article presents a method that uses reachability analysis to implicitly express the set of states satisfying an MTL specification and then optimizes to find a motion plan. The hybrid zonotope set representation is used to efficiently and conveniently encode MTL specifications into reachable sets. A numerical benchmark highlights the proposed method's computational advantages as compared to existing methods in the literature. Further numerical examples and an experimental application demonstrate the ability to address time-varying environments, region-dependent disturbances, and multi-agent coordination.

</details>


### [1374] [Stealthy Coverage Control for Human-enabled Real-Time 3D Reconstruction](https://arxiv.org/abs/2602.00466)
*Reiji Terunuma,Yuta Nakamura,Takuma Abe,Takeshi Hatanaka*

Main category: eess.SY

TL;DR: This paper introduces a semi-autonomous system for 3D structure reconstruction with a human-in-the-loop strategy for image sampling, improving model quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing 3D models where structural complexity is spatially non-uniform and prior knowledge of this complexity is unavailable.

Method: The paper proposes a semi-autonomous system combining human flexible reasoning with autonomous coverage control, focusing on decoupling drone motion for efficient sampling from human navigation.

Result: Simulation studies show that the proposed semi-autonomous system outperforms fully autonomous systems in terms of 3D model reconstruction quality.

Conclusion: Incorporating human interventions in semi-autonomous systems for image sampling significantly enhances the quality of 3D model reconstruction.

Abstract: In this paper, we propose a novel semi-autonomous image sampling strategy, called stealthy coverage control, for human-enabled 3D structure reconstruction. The present mission involves a fundamental problem: while the number of images required to accurately reconstruct a 3D model depends on the structural complexity of the target scene to be reconstructed, it is not realistic to assume prior knowledge of the spatially non-uniform structural complexity. We approach this issue by leveraging human flexible reasoning and situational recognition capabilities. Specifically, we design a semi-autonomous system that leaves identification of regions that need more images and navigation of the drones to such regions to a human operator. To this end, we first present a way to reflect the human intention in autonomous coverage control. Subsequently, in order to avoid operational conflicts between manual control and autonomous coverage control, we develop the stealthy coverage control that decouples the drone motion for efficient image sampling from navigation by the human. Simulation studies on a Unity/ROS2-based simulator demonstrate that the present semi-autonomous system outperforms the one without human interventions in the sense of the reconstructed model quality.

</details>


### [1375] [Harnessing Flexible Spatial and Temporal Data Center Workloads for Grid Regulation Services](https://arxiv.org/abs/2602.01508)
*Yingrui Fan,Junbo Zhao*

Main category: eess.SY

TL;DR: The paper proposes a co-optimization framework for workload distribution and regulation capacity commitments in data centers, focusing on sustaining real-time frequency regulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods separately handle workload scheduling and regulation capacity bidding, leading to infeasible or short-lived regulation commitments.

Method: A day-ahead co-optimization framework is developed, incorporating a space-time network model, chance constraints, and Value-at-Risk queue-state constraints.

Result: The framework reduces system operating costs, sustains viable regulation capacity, and improves revenue-risk trade-offs in case studies with real data center traces.

Conclusion: Unified optimization of workload and regulation commitments enhances reliability, sustainability, and economic performance in supporting grid frequency regulation with data centers.

Abstract: Data centers (DCs) are increasingly recognized as flexible loads that can support grid frequency regulation. Yet, most existing methods treat workload scheduling and regulation capacity bidding separately, overlooking how queueing dynamics and spatial-temporal dispatch decisions affect the ability to sustain real-time regulation. As a result, the committed regulation may become infeasible or short-lived. To address this issue, we propose a unified day-ahead co-optimization framework that jointly decides workload distribution across geographically distributed DCs and regulation capacity commitments. We construct a space-time network model to capture workload migration costs, latency requirements, and heterogeneous resource limits. To ensure that the committed regulation remains deliverable, we introduce chance constraints on instantaneous power flexibility based on interactive load forecasts, and apply Value-at-Risk queue-state constraints to maintain sustainable response under cumulative regulation signals. Case studies on a modified IEEE 68-bus system using real data center traces show that the proposed framework lowers system operating costs, enables more viable regulation capacity, and achieves better revenue-risk trade-offs compared to strategies that optimize scheduling and regulation independently.

</details>
