{"id": "2511.14777", "pdf": "https://arxiv.org/pdf/2511.14777", "abs": "https://arxiv.org/abs/2511.14777", "authors": ["Mahdi Samiei", "Mahdi Mansouri", "Mahdieh Soleymani Baghshah"], "title": "The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.", "AI": {"tldr": "This paper introduces Finite-State Machine (FSM) Execution as an interpretable framework to assess the procedural reasoning capabilities of large language models (LLMs), revealing challenges in maintaining state consistency over long reasoning sequences.", "motivation": "To evaluate LLMs' ability to perform procedural reasoning, as existing benchmarks fail to isolate and measure their degradation over extended reasoning chains.", "method": "The paper designs an FSM framework where models execute given rules step-by-step, measuring Turn Accuracy and Task Accuracy to evaluate immediate computations and state maintenance.", "result": "The study finds that LLMs degrade systematically as task complexity or horizon increases, with worse performance in tasks with higher branching factors compared to longer memory spans.", "conclusion": "FSM-based evaluation provides a precise tool for diagnosing reasoning failures in LLMs and can guide improvements in aligning inductive biases for better procedural competence."}}
{"id": "2511.14778", "pdf": "https://arxiv.org/pdf/2511.14778", "abs": "https://arxiv.org/abs/2511.14778", "authors": ["George Tsoukalas", "Rahul Saha", "Amitayush Thakur", "Sabrina Reguyal", "Swarat Chaudhuri"], "title": "Learning Interestingness in Automated Mathematical Theory Formation", "categories": ["cs.AI"], "comment": "NeurIPS 2025 Spotlight", "summary": "We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\\emph{FERMAT}$: automatically scoring the $\\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).", "AI": {"tldr": "The paper introduces FERMAT, a RL environment for automating mathematical theory discovery and explores scoring interestingness measures using evolutionary algorithms.", "motivation": "To address the challenge in AI of automating the discovery of new mathematical theories.", "method": "A RL environment (FERMAT) is developed, and evolutionary algorithms including LLM-based ones are applied to score mathematical interestingness.", "result": "LLM-based evolutionary algorithms showcased notable improvements in elementary number theory and finite fields over traditional benchmarks.", "conclusion": "FERMAT and its LLM-based algorithm framework advance AI-driven discovery in mathematics and provide open-access tools for further study."}}
{"id": "2511.14780", "pdf": "https://arxiv.org/pdf/2511.14780", "abs": "https://arxiv.org/abs/2511.14780", "authors": ["Keith Moore", "Jun W. Kim", "David Lyu", "Jeffrey Heo", "Ehsan Adeli"], "title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents", "categories": ["cs.AI"], "comment": "Preprint. Accepted for publication at AIAS 2025", "summary": "We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.", "AI": {"tldr": "Ask WhAI is a system to inspect and test belief states in multi-agent interactions, using scenarios like medical diagnoses with large language model agents. It enables insights into belief formation and reasoning in controlled simulations.", "motivation": "Explore belief formation and reasoning in multi-agent interactions, revealing entrenched priors and influence of evidence on decision-making, particularly in scientific and medical contexts.", "method": "Ask WhAI records and replays multi-agent interactions, queries agent beliefs and rationale, and injects counterfactual evidence in simulations such as medical diagnostic journeys guided by role-specific language model agents.", "result": "The framework exposes real-world disciplinary stances reflected in agents' reasoning, including reliance on canonical studies and resistance to counterevidence, which can be interrogated systematically within the controlled simulation.", "conclusion": "Ask WhAI provides a reproducible tool to study and test belief dynamics and epistemic silos, shedding light on multi-agent scientific reasoning and evidence integration in a controlled manner."}}
{"id": "2511.14788", "pdf": "https://arxiv.org/pdf/2511.14788", "abs": "https://arxiv.org/abs/2511.14788", "authors": ["Michele Ronco", "Damien Delforge", "Wiebke S. J\u00e4ger", "Christina Corbane"], "title": "Subnational Geocoding of Global Disasters Using Large Language Models", "categories": ["cs.AI", "stat.AP"], "comment": null, "summary": "Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.", "AI": {"tldr": "A novel automated workflow using GPT-4o geocodes disaster location data from unstructured text in EM-DAT, assigning geometries and reliability scores by cross-verifying geoinformation repositories.", "motivation": "The motivation is to address challenges in integrating unstructured and inconsistent disaster location data from databases like EM-DAT with structured spatial datasets for risk assessment and disaster risk reduction.", "method": "The paper introduces a fully automated workflow utilizing GPT-4o to process and clean textual location data, which is then cross-referenced with three geoinformation repositories (GADM, OpenStreetMap, and Wikidata). The approach also assigns a reliability score and generates subnational geometries for disaster events.", "result": "The workflow geocodes 14,215 disaster events in the EM-DAT dataset (2000-2024) across 17,948 unique locations, requiring no manual intervention and enabling cross-verification and flexible remapping.", "conclusion": "The authors conclude that LLMs like GPT-4o can effectively handle the automation of geocoding tasks, offering a scalable and reliable method to process unstructured geographic information for disaster risk assessments and other analyses."}}
{"id": "2511.14990", "pdf": "https://arxiv.org/pdf/2511.14990", "abs": "https://arxiv.org/abs/2511.14990", "authors": ["Zhuolun Jiang", "Songyue Wang", "Xiaokun Pei", "Tianyue Lu", "Mingyu Chen"], "title": "CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations", "categories": ["cs.AR"], "comment": null, "summary": "Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.", "AI": {"tldr": "CoroAMU integrates hardware-software co-design to optimize memory-centric coroutines, achieving up to 4.87x performance improvements on FPGA-emulated disaggregated systems.", "motivation": "Address memory latency challenges in data-intensive applications, specifically in disaggregated memory systems, where efficient latency hiding is critical.", "method": "Developed CoroAMU, combining optimized compiler procedures for coroutine code generation and hardware enhancements like decoupled memory operations and memory-guided branch prediction.", "result": "Implemented CoroAMU with LLVM and XiangShan RISC-V processor over FPGA; demonstrated significant performance gains, achieving 1.51x speedup compared to state-of-the-art methods and up to 4.87x improvement under specific latency conditions.", "conclusion": "CoroAMU effectively addresses memory latency challenges through a synergistic hardware-software approach, showcasing its capability to enhance memory access efficiency in disaggregated systems."}}
{"id": "2511.15015", "pdf": "https://arxiv.org/pdf/2511.15015", "abs": "https://arxiv.org/abs/2511.15015", "authors": ["Kexin Chu", "Dawei Xiang", "Zixu Shen", "Yiwei Yang", "Zecheng Liu", "Wei Zhang"], "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference", "categories": ["cs.PF", "cs.AI", "cs.LG"], "comment": "7 pages", "summary": "Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.\n  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.", "AI": {"tldr": "DynaExq is proposed to optimize memory usage for Mixture-of-Experts models on consumer GPUs while improving accuracy via adaptive, real-time quantization methods.", "motivation": "Deploying Mixture-of-Experts models on consumer GPUs faces challenges due to the large memory footprint of inactive experts, and static quantization approaches are insufficient for adapting to changing activation patterns.", "method": "DynaExq utilizes three main techniques: a hotness-aware precision controller to manage bit-widths dynamically, an asynchronous precision-switching pipeline for efficient expert precision transitions, and fragmentation-free memory pooling for deterministic allocation of hybrid-precision experts.", "result": "DynaExq enables the deployment of large LLMs, such as Qwen3-30B and Qwen3-80B, on single RTX 5090 and A6000 GPUs. It improves accuracy by up to 4.03 points compared to static low-precision baselines.", "conclusion": "Adaptive, workload-aware quantization techniques like DynaExq are effective for serving memory-constrained MoE models, allowing their efficient operation on consumer GPUs while maintaining high accuracy."}}
{"id": "2511.14775", "pdf": "https://arxiv.org/pdf/2511.14775", "abs": "https://arxiv.org/abs/2511.14775", "authors": ["S. K. Laha"], "title": "Reservoir Computing via Multi-Scale Random Fourier Features for Forecasting Fast-Slow Dynamical Systems", "categories": ["cs.NE", "cs.LG"], "comment": "23 pages, 18 Figure", "summary": "Forecasting nonlinear time series with multi-scale temporal structures remains a central challenge in complex systems modeling. We present a novel reservoir computing framework that combines delay embedding with random Fourier feature (RFF) mappings to capture such dynamics. Two formulations are investigated: a single-scale RFF reservoir, which employs a fixed kernel bandwidth, and a multi-scale RFF reservoir, which integrates multiple bandwidths to represent both fast and slow temporal dependencies. The framework is applied to a diverse set of canonical systems: neuronal models such as the Rulkov map, Izhikevich model, Hindmarsh-Rose model, and Morris-Lecar model, which exhibit spiking, bursting, and chaotic behaviors arising from fast-slow interactions; and ecological models including the predator-prey dynamics and Ricker map with seasonal forcing, which display multi-scale oscillations and intermittency. Across all cases, the multi-scale RFF reservoir consistently outperforms its single-scale counterpart, achieving lower normalized root mean square error (NRMSE) and more robust long-horizon predictions. These results highlight the effectiveness of explicitly incorporating multi-scale feature mappings into reservoir computing architectures for modeling complex dynamical systems with intrinsic fast-slow interactions.", "AI": {"tldr": "This paper presents a reservoir computing framework using random Fourier feature mappings to forecast nonlinear time series with multi-scale temporal structures, showing better performance with multi-scale reservoirs.", "motivation": "The study aims to address challenges in modeling nonlinear time series featuring multi-scale temporal structures inherent in complex systems.", "method": "The proposed framework employs delay embedding combined with random Fourier feature mappings in two variants: single-scale (fixed kernel bandwidth) and multi-scale (multiple bandwidths).", "result": "The multi-scale RFF reservoir outperforms the single-scale approach across diverse systems, offering reduced prediction errors and improved long-horizon forecasts.", "conclusion": "Explicit multi-scale feature mappings enhance reservoir computing for complex systems with fast-slow interactions, demonstrating significant improvements in prediction accuracy."}}
{"id": "2511.14852", "pdf": "https://arxiv.org/pdf/2511.14852", "abs": "https://arxiv.org/abs/2511.14852", "authors": ["Mingkun Yu", "Heming Zhong", "Dan Huang", "Yutong Lu", "Jiazhi Jiang"], "title": "PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \\emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \\emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \\emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \\emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\\times$ faster inference and $1.4$--$12\\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.", "AI": {"tldr": "PolyKAN, a GPU-accelerated operator library for Kolmogorov-Arnold Networks (KANs), offers significant speedups in inference and training while maintaining accuracy.", "motivation": "Enhance GPU utilization for Kolmogorov-Arnold Networks (KANs) to enable practical adoption and improve efficiency in AI for Science applications.", "method": "PolyKAN introduces optimized CUDA kernels for polynomial KAN layers using techniques such as lookup-tables, 2D tiling, two-stage reduction, and coefficient-layout reordering.", "result": "PolyKAN achieves 1.2\u201310\u00d7 faster inference and 1.4\u201312\u00d7 faster training compared to Triton + cuBLAS baseline, maintaining accuracy across diverse tasks.", "conclusion": "The development of PolyKAN makes KANs more computationally viable, paving the way for broader adoption in high-performance AI applications."}}
{"id": "2511.14772", "pdf": "https://arxiv.org/pdf/2511.14772", "abs": "https://arxiv.org/abs/2511.14772", "authors": ["Zhuoyi Yang", "Xu Guo", "Tong Zhang", "Huijuan Xu", "Boyang Li"], "title": "Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research", "AI": {"tldr": "This paper surveys methods to enhance the predictive accuracy of large language models at inference by using additional compute, categorizing techniques based on problem decomposition and organization.", "motivation": "To improve the inference capabilities of pretrained large language models by utilizing additional computational techniques.", "method": "The paper categorizes methods based on problem decomposition (sequential, parallel, tree-structured) and unifies approaches like Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common perspective.", "result": "It synthesizes the strengths and weaknesses of existing approaches and provides a unified lens for understanding inference techniques.", "conclusion": "It identifies promising directions for future research in inference methods and encourages a deeper exploration of these approaches."}}
{"id": "2511.14872", "pdf": "https://arxiv.org/pdf/2511.14872", "abs": "https://arxiv.org/abs/2511.14872", "authors": ["T. S. A. N. Sim\u00f5es", "F. Lombardi", "D. Plenz", "H. J. Herrmann", "L. de Arcangelis"], "title": "Maximum entropy models of neuronal populations at and off criticality", "categories": ["q-bio.NC"], "comment": null, "summary": "Empirical evidence of scaling behaviors in neuronal avalanches suggests that neuronal populations in the brain operate near criticality. Departure from scaling in neuronal avalanches has been used as a measure of distance to criticality and linked to brain disorders. A distinct line of evidence for brain criticality has come from thermodynamic signatures in maximum entropy (ME) models. Both of these approaches have been widely applied to the analysis of neuronal data. However, the relationship between deviations from avalanche criticality and thermodynamics of ME models of neuronal populations remains poorly understood. To address this question, we study spontaneous activity of organotypic rat cortex slice cultures in physiological and drug-induced hypo- or hyper-excitable conditions, which are classified as critical, subcritical and supercritical based on avalanche dynamics. We find that ME models inferred from critical cultures show signatures of criticality in thermodynamic quantities, e.g. specific heat. However, such signatures are also present, and equally strong, in models inferred from supercritical cultures -- despite their altered dynamics and poor functional performance. On the contrary, ME models inferred from subcritical cultures do not show thermodynamic hints of criticality. Importantly, we confirm these results using an interpretable neural network model that can be tuned to and away from avalanche criticality. Our findings indicate that maximum entropy models correctly distinguish subcritical from critical/supercritical systems. However, they may not be able to discriminate between avalanche criticality and supercriticality, although they may still capture a number of important features from neuronal data.", "AI": {"tldr": "The paper examines the relationship between neuronal criticality and thermodynamic properties by studying organotypic rat cortex slice cultures under various conditions and using maximum entropy (ME) models.", "motivation": "To understand the connection between deviations from avalanche criticality and the thermodynamics of ME models of neuronal populations, especially in the context of brain disorders.", "method": "The study analyzes spontaneous activity in rat cortex slice cultures under critical, subcritical, and supercritical states based on avalanche dynamics, employing ME models and a neural network to interpret criticality-related properties.", "result": "ME models can distinguish subcritical from critical/supercritical states but fail to differentiate between critical and supercritical states, even though they capture important neuronal data features.", "conclusion": "The findings suggest that ME models are limited in determining avalanche criticality versus supercriticality but can still provide significant insights into neuronal data representation."}}
{"id": "2511.14786", "pdf": "https://arxiv.org/pdf/2511.14786", "abs": "https://arxiv.org/abs/2511.14786", "authors": ["Sidney Shapiro"], "title": "Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research", "categories": ["cs.SE"], "comment": "35 pages", "summary": "Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.", "AI": {"tldr": "This paper introduces PennyLane, a Python framework designed to integrate quantum and classical machine learning for quantum-enhanced applications. It highlights use cases, methods, and provides coding examples.", "motivation": "The paper aims to establish PennyLane as a foundational tool for hybrid quantum-classical machine learning and optimization workflows.", "method": "The authors showcase PennyLane\u2019s functionalities through concrete Python examples such as quantum kernel methods and variational quantum eigensolvers, integrated with classical ML frameworks.", "result": "PennyLane demonstrated efficiency in constructing quantum circuits, enabling automatic differentiation, and facilitating hybrid optimization using established Python libraries.", "conclusion": "PennyLane is positioned as a methodological building block for bridging fundamental quantum computing concepts with applied machine learning, making it a go-to tool in Python-based hybrid research."}}
{"id": "2511.14910", "pdf": "https://arxiv.org/pdf/2511.14910", "abs": "https://arxiv.org/abs/2511.14910", "authors": ["Yassine Ibork", "Myounggyu Won", "Lokesh Das"], "title": "Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information", "categories": ["cs.RO"], "comment": null, "summary": "Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.", "AI": {"tldr": "The paper develops a V2X-assisted Multiagent Reinforcement Learning (MARL) framework for improving autonomous vehicle on-ramp merging performance using global and local information.", "motivation": "Current methods for autonomous ramp merging often rely on limited local information, leading to suboptimal results in safety and traffic efficiency, especially in mixed traffic environments.", "method": "A Multiagent Reinforcement Learning framework using V2X communication is proposed. It uses global and local information for coordination, formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP) with a hybrid action space and parameterized deep Q-learning.", "result": "Simulations integrating SUMO and MOSAIC V2X simulators show notable improvements in merging success rate, traffic efficiency, and road safety.", "conclusion": "The proposed framework successfully leverages V2X communication to enhance on-ramp merging performance in mixed traffic scenarios."}}
{"id": "2511.14848", "pdf": "https://arxiv.org/pdf/2511.14848", "abs": "https://arxiv.org/abs/2511.14848", "authors": ["Yarin Bekor", "Gal Michael Harari", "Or Perel", "Or Litany"], "title": "Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video", "categories": ["cs.CV"], "comment": "SIGGRAPH Asia 2025", "summary": "We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/", "AI": {"tldr": "The paper proposes Gaussian See, Gaussian Do, a new method for transferring semantic 3D motion across objects using multiview video without requiring a prior rig, offering superior motion fidelity and structural consistency.", "motivation": "The motivation is to enable semantic 3D motion transfer between objects of varying categories without the need for predefined rigs, addressing limitations in current motion transfer techniques.", "method": "The method involves using implicit motion transfer through condition inversion to extract motion embeddings from source videos, applying these embeddings to static target objects, and using the output to supervise 3D Gaussian Splatting reconstruction.", "result": "The approach introduces anchor-based motion embedding for improved cross-view consistency and performance. It benchmarks semantic 3D motion transfer and shows superior results in motion fidelity and structure compared to existing techniques.", "conclusion": "This work advances motion transfer techniques by enabling rig-free, category-agnostic 3D motion transfer, validated through new benchmarks and robust pipeline performance."}}
{"id": "2511.14784", "pdf": "https://arxiv.org/pdf/2511.14784", "abs": "https://arxiv.org/abs/2511.14784", "authors": ["Sourav De", "Koustav Chowdhury", "Bibhabasu Mandal", "Sagar Ghosh", "Swagatam Das", "Debolina Paul", "Saptarshi Chakraborty"], "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "comment": "Accepted in AAAI 2026", "summary": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.", "AI": {"tldr": "The paper proposes a clustering method combining convex clustering with the robust Median of Means (MoM) estimator, addressing noise, outliers, and the need for prior knowledge of cluster number.", "motivation": "The research aims to tackle challenges in classical clustering methods, such as sensitivity to initialization, the requirement to predefine the number of clusters, and issues with handling high-dimensional noisy data in convex clustering.", "method": "The authors integrate convex clustering with the Median of Means (MoM) estimator to provide a robust and efficient clustering framework that is less sensitive to outliers and does not rely on predefined cluster counts.", "result": "The proposed method demonstrates superior performance and efficiency in handling both synthetic and real-world datasets compared to existing clustering methods. It also achieves theoretical weak consistency under specific conditions.", "conclusion": "The approach effectively addresses limitations of convex clustering and traditional methods, offering improved robustness, stability, and performance for large-scale datasets with noise and outliers."}}
{"id": "2511.14808", "pdf": "https://arxiv.org/pdf/2511.14808", "abs": "https://arxiv.org/abs/2511.14808", "authors": ["Mikael von Strauss"], "title": "Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\\ell$ we define a collision discriminant $\u0394^\\ell \\subset \u0398$ and injective stratum $U^\\ell = \u0398\\setminus \u0394^\\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\\ell$ is open and dense and every $F^\\ell_\u03b8$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $\u0398/G$, so injectivity is naturally a property of functional equivalence classes.\n  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.", "AI": {"tldr": "This paper refines the understanding of injective mapping in decoder-only Transformers under real-analytic assumptions and explores geometric diagnostics to evaluate their behavior empirically.", "motivation": "The authors aim to investigate whether decoder-only Transformers consistently produce injective mappings between discrete prompts and hidden states and to provide a framework for analyzing such behavior through theoretical and empirical approaches.", "method": "The paper defines collision discriminants and injective strata for each model layer, proves injectivity under specific conditions, explores properties under symmetry groups, and introduces empirical diagnostics like separation margin and co-Lipschitz constants.", "result": "Pretrained models such as LLaMA-3 and Qwen showed no collisions at full precision or 8-bit quantization, while 4-bit quantization caused minor collisions. Metrics on a small GPT-2 remained stable over training, supporting the injectivity hypothesis.", "conclusion": "Transformer representations are generically and persistently injective under continuous-parameter idealization, and their invertibility can be evaluated through proposed geometric diagnostics."}}
{"id": "2511.14953", "pdf": "https://arxiv.org/pdf/2511.14953", "abs": "https://arxiv.org/abs/2511.14953", "authors": ["Joey Velez-Ginorio", "Nada Amin", "Konrad Kording", "Steve Zdancewic"], "title": "Compiling to recurrent neurons", "categories": ["cs.PL", "cs.LG"], "comment": null, "summary": "Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\\textsf{Cajal}\\scriptstyle(\\mathbb{\\multimap}, \\mathbb{2}, \\mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.", "AI": {"tldr": "Conditionals and iterations, traditionally non-differentiable, can be made first-class in differentiable programming using a language called Cajal, enabling discrete structures to be compatible with gradient-based learning.", "motivation": "Overcoming the limitation where discrete structures are treated as second-class in differentiable programming, restricting the use of conditionals and iterations in neural networks.", "method": "Developed a programming language, Cajal, which compiles discrete algorithms (e.g., conditionals and iterations) into differentiable recurrent neuron forms.", "result": "Experiments using recurrent neurons in neural networks for iterative image transformation tasks demonstrated faster learning and greater data efficiency compared to networks without first-class iteration.", "conclusion": "Recurrent neurons facilitate a stronger integration between gradient-based learning and discrete programming structures, advancing neural network design."}}
{"id": "2511.14819", "pdf": "https://arxiv.org/pdf/2511.14819", "abs": "https://arxiv.org/abs/2511.14819", "authors": ["Martin Monperrus", "Benoit Baudry", "Cl\u00e9ment Vidal"], "title": "Project Rachel: Can an AI Become a Scholarly Author?", "categories": ["cs.AI"], "comment": null, "summary": "This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.", "AI": {"tldr": "This study outlines Project Rachel, which examines how the scholarly ecosystem reacts to AI-generated academic identity and authorship.", "motivation": "To explore the impact of AI authorship on the academic and research community, addressing the ramifications of superhuman AI integration.", "method": "The study created an AI academic identity named Rachel So, publishing 10+ papers over several months and tracking citations, peer review invitations, and general interactions.", "result": "Rachel So was cited, received a peer review invitation, and her publications were acknowledged within the scholarly ecosystem.", "conclusion": "This research reveals potential challenges and opportunities of AI-driven authorship, emphasizing the need for discussions on its implications for the academic publishing system."}}
{"id": "2511.15367", "pdf": "https://arxiv.org/pdf/2511.15367", "abs": "https://arxiv.org/abs/2511.15367", "authors": ["Xin Yang", "Xin Fan", "Zengshi Wang", "Jun Han"], "title": "DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution", "categories": ["cs.AR"], "comment": "8 pages, 9 figures, accepted to DATE 2026", "summary": "Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.\n  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.\n  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\\times$ to 4.44$\\times$ and increases energy efficiency by 1.00$\\times$ to 22.8$\\times$ over the baseline, with 3.91$\\times$ lower hardware overhead than NVR.", "AI": {"tldr": "The paper presents DARE, an innovation for addressing irregularities in sparse DNNs by extending matrix ISA and utilizing a filtered runahead mechanism, achieving significant performance and energy efficiency improvements.", "motivation": "Sparse DNNs face inefficiencies due to irregular memory access and poor utilization of Matrix ISAs in MPUs, leading to suboptimal hardware performance.", "method": "This work introduces DARE, an irregularity-tolerant MPU with extensions to the ISA for sparse operation densification and a lightweight filtered runahead prefetching mechanism.", "result": "DARE demonstrated performance improvements of 1.04x to 4.44x and energy efficiency enhancements of 1.00x to 22.8x over the baseline with minimal hardware overhead.", "conclusion": "DARE effectively addresses the challenges of irregular sparse DNN computation through hardware-algorithm co-optimization, proving its superiority in performance and efficiency."}}
{"id": "2511.15626", "pdf": "https://arxiv.org/pdf/2511.15626", "abs": "https://arxiv.org/abs/2511.15626", "authors": ["M. Sapkas", "A. Triossi", "M. Zanetti"], "title": "A Latency-Constrained, Gated Recurrent Unit (GRU) Implementation in the Versal AI Engine", "categories": ["cs.PF"], "comment": null, "summary": "This work explores the use of the AMD Xilinx Versal Adaptable Intelligent Engine(AIE) to accelerate Gated Recurrent Unit (GRU) inference for latency-Constrained applications. We present a custom workload distribution framework across the AIE's vector processors and propose a hybrid AIE - Programmable Logic (PL) design to optimize computational efficiency. Our approach highlights the potential of deploying adaptable neural networks in real-time environments such as online preprocessing in the readout chain of a physics experiment, offering a flexible alternative to traditional fixed-function algorithms.", "AI": {"tldr": "The study uses AMD Xilinx Versal AIE for accelerating GRU inference in latency-sensitive tasks, proposing a hybrid AIE-PL design for efficiency.", "motivation": "To address the challenge of efficient GRU inference in latency-constrained applications, specifically for real-time environments.", "method": "Developed a custom workload framework for AIE's vector processors and introduced a hybrid AIE-PL design to optimize computational performance.", "result": "Demonstrated the feasibility of adaptable neural networks deployed in real-time systems, showcasing computational efficiency improvements.", "conclusion": "Adaptable neural networks using AIE provide a flexible and efficient solution for replacing fixed-function algorithms in real-time applications."}}
{"id": "2511.15199", "pdf": "https://arxiv.org/pdf/2511.15199", "abs": "https://arxiv.org/abs/2511.15199", "authors": ["Jiajun Zhan", "Zeyuan Ma", "Yue-Jiao Gong", "Kay Chen Tan"], "title": "Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.", "AI": {"tldr": "The paper designs a generalizable knowledge transfer policy for evolutionary multitasking (EMT) using reinforcement learning (RL), achieving superior performance compared to baseline methods.", "motivation": "The motivation is to overcome challenges in evolutionary multitasking where tailored designs for knowledge transfer are required, and to create a generalizable and systematic knowledge transfer policy.", "method": "The method involves a multi-role RL system with specialized agents: a task routing agent using attention-based similarity recognition, a knowledge control agent managing transfer proportions, and strategy adaptation agents dynamically adjusting hyper-parameters. The system achieves meta-policy through pre-training over augmented multitask problems.", "result": "Experiments demonstrate state-of-the-art performance compared to baseline methods and provide insights into the learned rationale of the system.", "conclusion": "The proposed RL-based knowledge transfer mechanism systematically and successfully tackles key multitask optimization challenges and offers insightful interpretations for its effectiveness."}}
{"id": "2511.14966", "pdf": "https://arxiv.org/pdf/2511.14966", "abs": "https://arxiv.org/abs/2511.14966", "authors": ["David L. Cole", "Jordan Jalving", "Jonah Langlieb", "Jesse D. Jenkins"], "title": "A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization", "categories": ["cs.DC", "cs.MS", "math.OC"], "comment": "32 pages, 7 Figures", "summary": "We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.", "AI": {"tldr": "The paper introduces RemoteOptiGraph, an abstraction for distributed optimization leveraging distributed memory systems, improving computational efficiency.", "motivation": "The need for better modeling abstraction to solve large-scale optimization problems efficiently using distributed memory systems.", "method": "Extends the OptiGraph model with InterWorkerEdges to manage distributed linking constraints, implemented in the Plasmo.jl package.", "result": "Demonstrated on a mixed integer capacity expansion model, achieving 7.5x faster performance compared to non-decomposition approaches.", "conclusion": "RemoteOptiGraph successfully enables modular, distributed optimization with significant computational improvements and provides a framework for developing meta-algorithms."}}
{"id": "2511.14773", "pdf": "https://arxiv.org/pdf/2511.14773", "abs": "https://arxiv.org/abs/2511.14773", "authors": ["Joey David"], "title": "Temporal Predictors of Outcome in Reasoning Language Models", "categories": ["cs.CL"], "comment": "4 pages, 4 figures", "summary": "The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.", "AI": {"tldr": "The study investigates how early reasoning models like LLMs internally determine outcomes during chain-of-thought (CoT) reasoning and finds that correctness can often be internally predicted with a few tokens.", "motivation": "To understand how early a Large Language Model (LLM) commits internally to an outcome during reasoning, and to explore the interpretability and control in CoT reasoning.", "method": "The researchers trained linear classifiers on hidden states of LLMs, analyzing token-by-token reasoning and predicting eventual correctness. They also studied how this varies by question difficulty.", "result": "They found that LLMs' correctness is highly predictable after the first few reasoning steps, even for longer CoT outputs. For harder questions, predictive accuracy decreases due to a selection bias.", "conclusion": "Reasoning models tend to internally assess success after only a few tokens, which influences their interpretability and inference-time control capabilities."}}
{"id": "2511.14917", "pdf": "https://arxiv.org/pdf/2511.14917", "abs": "https://arxiv.org/abs/2511.14917", "authors": ["Nicoas Zucchet", "Qianqian Feng", "Axel Laborieux", "Friedemann Zenke", "Walter Senn", "Jo\u00e3o Sacramento"], "title": "Teaching signal synchronization in deep neural networks with prospective neurons", "categories": ["q-bio.NC", "cs.NE"], "comment": null, "summary": "Working memory requires the brain to maintain information from the recent past to guide ongoing behavior. Neurons can contribute to this capacity by slowly integrating their inputs over time, creating persistent activity that outlasts the original stimulus. However, when these slowly integrating neurons are organized hierarchically, they introduce cumulative delays that create a fundamental challenge for learning: teaching signals that indicate whether behavior was correct or incorrect arrive out-of-sync with the neural activity they are meant to instruct. Here, we demonstrate that neurons enhanced with an adaptive current can compensate for these delays by responding to external stimuli prospectively -- effectively predicting future inputs to synchronize with them. First, we show that such prospective neurons enable teaching signal synchronization across a range of learning algorithms that propagate error signals through hierarchical networks. Second, we demonstrate that this successfully guides learning in slowly integrating neurons, enabling the formation and retrieval of memories over extended timescales. We support our findings with a mathematical analysis of the prospective coding mechanism and learning experiments on motor control tasks. Together, our results reveal how neural adaptation could solve a critical timing problem and enable efficient learning in dynamic environments.", "AI": {"tldr": "This paper investigates how neurons with adaptive currents can predict future stimuli to address timing issues in learning. This approach facilitates synchronization of signals across hierarchical networks, enhancing memory formation and retrieval.", "motivation": "The challenge addressed is the timing mismatch in hierarchical neural networks, where teaching signals arrive out-of-sync with neural activities, hindering effective learning.", "method": "The authors modeled neurons with an adaptive current to enable prospective coding, aligning teaching signals with neural activities. Mathematical analysis and motor control experiments supported the approach.", "result": "The study showed that neurons enhanced with adaptive currents synchronize teaching signals across networks, improve learning algorithms, and help form and retrieve long-term memories.", "conclusion": "Neural adaptation mechanisms like prospective coding can resolve timing issues in hierarchical networks, promoting efficient learning and memory processes in dynamic environments."}}
{"id": "2511.14791", "pdf": "https://arxiv.org/pdf/2511.14791", "abs": "https://arxiv.org/abs/2511.14791", "authors": ["Cyriana M. A. Roelofs", "Edison Guevara Bastidas", "Thomas Hugo", "Stefan Faulstich", "Anna Cadenbach"], "title": "Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data", "categories": ["cs.SE", "cs.AI"], "comment": "30 pages, 6 figures", "summary": "Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.\n  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.\n  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.", "AI": {"tldr": "This paper introduces an open-source framework for early fault detection in district heating substations, featuring a labeled public dataset, evaluation metrics, and results using the EnergyFaultDetector.", "motivation": "The study aims to advance early fault detection in district heating substations, addressing the challenge of limited public, labeled datasets, which hinder progress in improving operational efficiency.", "method": "It combines a labeled public dataset with detailed operational and metadata of 93 substations, evaluates a predictive model with three metrics (Accuracy, Eventwise F-score, Earliness), and leverages the EnergyFaultDetector and ARCANA for fault detection and root cause analysis.", "result": "The proposed method achieves a 0.98 accuracy for recognizing normal behavior, an eventwise F-score of 0.83, and detects 60% of faults before customer reporting with an average lead time of 3.9 days.", "conclusion": "By providing a reproducible framework with an open dataset, operational benchmarks, and source code, this paper facilitates consistent evaluation and development of fault detection methods for district heating systems."}}
{"id": "2511.14919", "pdf": "https://arxiv.org/pdf/2511.14919", "abs": "https://arxiv.org/abs/2511.14919", "authors": ["Sebastian Dingler", "Hannes Burrichter"], "title": "A visual study of ICP variants for Lidar Odometry", "categories": ["cs.RO"], "comment": "Iterative closest point; Registration; Odometry; Mapping", "summary": "Odometry with lidar sensors is a state-of-the-art method to estimate the ego pose of a moving vehicle. Many implementations of lidar odometry use variants of the Iterative Closest Point (ICP) algorithm. Real-world effects such as dynamic objects, non-overlapping areas, and sensor noise diminish the accuracy of ICP. We build on a recently proposed method that makes these effects visible by visualizing the multidimensional objective function of ICP in two dimensions. We use this method to study different ICP variants in the context of lidar odometry. In addition, we propose a novel method to filter out dynamic objects and to address the ego blind spot problem.", "AI": {"tldr": "The paper studies ICP-based lidar odometry and introduces a new method to address its limitations caused by dynamic objects and blind spots.", "motivation": "The motivation is to improve lidar odometry methods affected by real-world challenges like dynamic objects, non-overlapping areas, and sensor noise.", "method": "Using a visualization tool for ICP's objective function and proposing a new method to tackle dynamic objects and blind spot issues.", "result": "The approach aids in understanding ICP variants and mitigates challenges in lidar odometry.", "conclusion": "The study showcases the importance of addressing environmental factors affecting ICP in lidar odometry and offers novel solutions."}}
{"id": "2511.14860", "pdf": "https://arxiv.org/pdf/2511.14860", "abs": "https://arxiv.org/abs/2511.14860", "authors": ["Aashish Ghimire", "Jun Zeng", "Roshan Paudel", "Nikhil Kumar Tomar", "Deepak Ranjan Nayak", "Harshith Reddy Nalla", "Vivek Jha", "Glenda Reynolds", "Debesh Jha"], "title": "When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.", "AI": {"tldr": "This study benchmarks various neural network architectures for dental caries segmentation using panoramic radiographs and identifies CNN-based DoubleU-Net as the best performer, surpassing transformer and Mamba-based models.", "motivation": "The motivation is to enhance the automated identification and segmentation of dental caries from panoramic radiographs, addressing challenges of low lesion contrast, morphological variability, and limited annotated data.", "method": "The researchers benchmarked twelve state-of-the-art models, including CNNs, transformers, and Mamba architectures, using the DC1000 dataset to compare their performance under identical training configurations.", "result": "CNN-based DoubleU-Net achieved the best performance with a dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145. CNN models performed better than transformer and Mamba methods due to better alignment with spatial priors.", "conclusion": "The study emphasizes the importance of architecture-task alignment in medical imaging segmentation. Despite the theoretical advantages of transformers and Mamba for global context modeling, CNN-based methods are better suited for limited-data scenarios."}}
{"id": "2511.14827", "pdf": "https://arxiv.org/pdf/2511.14827", "abs": "https://arxiv.org/abs/2511.14827", "authors": ["Peter Halmos", "Boris Hanin"], "title": "Implicit Bias of the JKO Scheme", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.AP"], "comment": null, "summary": "Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $\u03b7>0$ a sequence of probability distributions $\u03c1_k^\u03b7$ that approximate to first order in $\u03b7$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $\u03bb$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $\u03b7$. We show that $\u03c1_k^\u03b7$ are approximated to order $\u03b7^2$ by Wasserstein gradient flow on a \\emph{modified} energy \\[ J^\u03b7(\u03c1) = J(\u03c1) - \\frac\u03b7{4}\\int_M \\Big\\lVert \\nabla_g \\frac{\u03b4J}{\u03b4\u03c1} (\u03c1) \\Big\\rVert_{2}^{2} \\,\u03c1(dx), \\] obtained by subtracting from $J$ the squared metric curvature of $J$ times $\u03b7/4$. The JKO scheme therefore adds at second order in $\u03b7$ a \\textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{\u00e4}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^\u03b7$ we study \\emph{JKO-Flow}, Wasserstein gradient flow on $J^\u03b7$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.", "AI": {"tldr": "The paper investigates the Jordan-Kinderlehrer-Otto (JKO) scheme within Wasserstein gradient flows and explores its implicit bias and second-order effects on energy functionals.", "motivation": "To deeply understand the behavior and properties of the JKO scheme which exhibits energy dissipation preservation and unconditional stability, particularly its implicit bias at a second-order approximation.", "method": "The authors analyze the second-order effects of the JKO scheme by deriving a modified energy functional $J^\u03b7$ and explore its impact on Wasserstein gradient flow behavior with theoretical derivations and numerical examples.", "result": "The study identifies that the JKO scheme introduces second-order deceleration in rapidly changing metric curvature directions, with implicit biases like Fisher information, Fisher-Hyv\u00e4rinen divergence, and kinetic energy. They verify insights through examples of Langevin dynamics and 1D sampling.", "conclusion": "The JKO scheme has intrinsic second-order biases that can impact the convergence dynamics of Wasserstein gradient flows, offering new perspectives on its computational behavior and broader applications."}}
{"id": "2511.14813", "pdf": "https://arxiv.org/pdf/2511.14813", "abs": "https://arxiv.org/abs/2511.14813", "authors": ["Yifan Li", "Qin Li", "Min Zhang", "Min Zhang", "Peixin Wang"], "title": "DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.", "AI": {"tldr": "The paper explores Large Language Models' (LLMs) reasoning abilities and introduces the concepts of Derivation Relation (DR) and Derivation Capability (DC). After identifying limitations in current LLMs, it proposes Derivation Prompting (DP), improving performance by 15.2%.", "motivation": "The authors aim to address the gap in understanding and evaluating how LLMs handle reasoning patterns where output modifications depend on structured input changes, a capability essential for advanced reasoning.", "method": "The paper formalizes the Derivation Relation (DR) and Derivation Capability (DC), introduces an evaluation framework called DEVAL to test these in popular LLMs, and proposes Derivation Prompting (DP) for improvement.", "result": "Evaluation of five LLMs and one reasoning model across seven tasks revealed moderate recognition of DR but poor application in problem-solving. The Derivation Prompting (DP) method improved performance by an average of 15.2%.", "conclusion": "LLMs show potential but require enhancements in applying reasoning patterns like DR. The proposed Derivation Prompting technique is a significant step towards improving their reasoning capabilities systematically."}}
{"id": "2511.15000", "pdf": "https://arxiv.org/pdf/2511.15000", "abs": "https://arxiv.org/abs/2511.15000", "authors": ["Alexander J Root", "Christophe Gyurgyik", "Purvi Goel", "Kayvon Fatahalian", "Jonathan Ragan-Kelley", "Andrew Adams", "Fredrik Kjolstad"], "title": "Compiling Set Queries into Work-Efficient Tree Traversals", "categories": ["cs.PL", "cs.DB"], "comment": null, "summary": "Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply.", "AI": {"tldr": "The paper introduces a generalized and automated optimization method for tree-based data structures to enhance query performance, involving symbolic interval analysis and advanced compiler techniques.", "motivation": "Existing systems require manual implementation of pruning logic for optimizing query performance, which is inefficient and specialized.", "method": "The authors mechanized the optimization by deriving pruning conditions using symbolic interval analysis, extended for geometric predicates, and implemented query fusions into a single tree traversal.", "result": "The generated queries match or outperform expert-written code, avoiding inefficient default operations like linear scans or nested-loop joins.", "conclusion": "The approach generalizes and automates tree optimization, unlocking efficient query handling for diverse predicates."}}
{"id": "2511.14853", "pdf": "https://arxiv.org/pdf/2511.14853", "abs": "https://arxiv.org/abs/2511.14853", "authors": ["Robab Aghazadeh Chakherlou", "Siddartha Khastgir", "Xingyu Zhao", "Jerein Jeyachandran", "Shufeng Chen"], "title": "Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems", "categories": ["cs.AI"], "comment": null, "summary": "Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.\n  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.", "AI": {"tldr": "The paper addresses representativeness of datasets for training/testing AI systems by proposing a probabilistic approach using imprecise Bayesian methods to handle data uncertainty and limited priors.", "motivation": "AI systems like autonomous vehicles require reliable datasets for safe operation, especially concerning representativeness to ensure proper coverage of operational conditions.", "method": "The authors employ imprecise Bayesian methods to calculate interval-valued estimates of dataset representativeness, comparing scenario data features to Target Operational Domain (TOD) distributions.", "result": "They demonstrate a comparison of distributions for weather, road type, time of day, and other operational categories under uncertain priors, producing interval-based estimates of representativeness.", "conclusion": "This probabilistic interval-based approach aids in quantifying the trustworthiness of datasets under uncertainty, enhancing the safety of AI systems."}}
{"id": "2511.15397", "pdf": "https://arxiv.org/pdf/2511.15397", "abs": "https://arxiv.org/abs/2511.15397", "authors": ["Cong Wang", "Zexin Fu", "Jiayi Huang", "Shanshi Huang"], "title": "Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism", "categories": ["cs.AR"], "comment": null, "summary": "Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.\n  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove", "AI": {"tldr": "Vision Transformers require significant computational resources, and Hemlet, a chiplet-based heterogeneous compute-in-memory (CIM) system, addresses this by efficiently scaling resources and optimizing communication.", "motivation": "Vision Transformers have set benchmarks in vision tasks but face scalability challenges due to high computational and memory demands, limiting hardware deployment.", "method": "Hemlet integrates heterogeneous analog CIM, digital CIM, and intermediate data processing chiplets to scale resources and optimize throughput while overcoming communication limits in chiplet-based systems.", "result": "Hemlet allows scalable, energy-efficient acceleration of Vision Transformers while managing chiplet communication costs effectively.", "conclusion": "Hemlet provides a practical solution for deploying Vision Transformers with improved scalability and efficiency, addressing key hardware constraints."}}
{"id": "2511.15503", "pdf": "https://arxiv.org/pdf/2511.15503", "abs": "https://arxiv.org/abs/2511.15503", "authors": ["Peiming Yang", "Sankeerth Durvasula", "Ivan Fernandez", "Mohammad Sadrosadati", "Onur Mutlu", "Gennady Pekhimenko", "Christina Giannoula"], "title": "A Tensor Compiler for Processing-In-Memory Architectures", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "comment": null, "summary": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.", "AI": {"tldr": "Processing-In-Memory systems can accelerate ML kernels and LLM models by optimizing data rearrangements and compute code jointly using the new DCC compiler.", "motivation": "To enhance performance and programmability in PIM systems for ML kernels and address challenges like performance bottlenecks due to data rearrangements and support for diverse PIM backends.", "method": "A data-centric ML compiler (DCC) is designed that co-optimizes data rearrangements and compute code using multi-layer PIM abstractions, specific code optimizations, and performance prediction models.", "result": "Experiments show DCC achieves speedups of up to 7.71x (average 4.88x) in end-to-end LLM inference and up to 13.17x speedup in specific ML kernels across different PIM backends.", "conclusion": "DCC effectively improves data and compute optimization for PIM systems, significantly accelerating ML tasks and supporting diverse PIM architectures."}}
{"id": "2511.15326", "pdf": "https://arxiv.org/pdf/2511.15326", "abs": "https://arxiv.org/abs/2511.15326", "authors": ["Clemens Hutter", "Valentin Abadie", "Helmut B\u00f6lcskei"], "title": "A Quantifier-Reversal Approximation Paradigm for Recurrent Neural Networks", "categories": ["cs.NE"], "comment": null, "summary": "Classical neural network approximation results take the form: for every function $f$ and every error tolerance $\u03b5> 0$, one constructs a neural network whose architecture and weights depend on $\u03b5$. This paper introduces a fundamentally different approximation paradigm that reverses this quantifier order. For each target function $f$, we construct a single recurrent neural network (RNN) with fixed topology and fixed weights that approximates $f$ to within any prescribed tolerance $\u03b5> 0$ when run for sufficiently many time steps.\n  The key mechanism enabling this quantifier reversal is temporal computation combined with weight sharing: rather than increasing network depth, the approximation error is reduced solely by running the RNN longer. This yields exponentially decaying approximation error as a function of runtime while requiring storage of only a small, fixed set of weights. Such architectures are appealing for hardware implementations where memory is scarce and runtime is comparatively inexpensive.\n  To initiate the systematic development of this novel approximation paradigm, we focus on univariate polynomials. Our RNN constructions emulate the structural calculus underlying deep feed-forward ReLU network approximation theory -- parallelization, linear combinations, affine transformations, and, most importantly, a clocked mechanism that realizes function composition within a single recurrent architecture. The resulting RNNs have size independent of the error tolerance $\u03b5$ and hidden-state dimension linear in the degree of the polynomial.", "AI": {"tldr": "This paper introduces a paradigm in neural network approximation where a fixed recurrent neural network (RNN) approximates any function $f$ to a desired tolerance $\u03b5$ by running for more time steps, rather than increasing depth or adjusting architecture.", "motivation": "Classical neural network approximations require architecture adjustments for accuracy improvements, which is inefficient for hardware implementations. This paper aims to find a method suitable for memory-limited setups.", "method": "The authors propose RNNs with fixed topology and weights that use temporal computation and weight sharing. Errors decrease with runtime rather than depth expansion, making it hardware-efficient.", "result": "The constructed RNNs provide exponentially decaying approximation errors with fixed weights and runtime extension. They successfully approximate univariate polynomials, demonstrating hardware-friendly characteristics.", "conclusion": "This paradigm shift simplifies network architecture while maintaining approximation quality across any desired tolerance by runtime adjustments, offering practical benefits for constrained hardware setups."}}
{"id": "2511.15076", "pdf": "https://arxiv.org/pdf/2511.15076", "abs": "https://arxiv.org/abs/2511.15076", "authors": ["Khaled Hamidouche", "John Bachan", "Pak Markthub", "Peter-Jan Gootzen", "Elena Agostini", "Sylvain Jeaugey", "Aamir Shafi", "Georgios Theodorakis", "Manjunath Gorentla Venkata"], "title": "GPU-Initiated Networking for NCCL", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "comment": "13 pages, 9 figures, 3 tables", "summary": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.\n  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.", "AI": {"tldr": "The paper analyzes a new feature in NCCL 2.28 called GPU-Initiated Networking (GIN), designed for low-latency, device-centric GPU communications in AI workloads.", "motivation": "The increasing demand for fine-grained, low-latency GPU-to-GPU communication without CPU dependency in modern AI workloads like Mixture-of-Experts models.", "method": "The introduction of GIN in NCCL 2.28 features a three-layer design: host-side setup, device-side APIs for CUDA kernels, and a network plugin architecture for diverse hardware support. It enables GPU-to-NIC and RDMA communication using specialized mechanisms.", "result": "Benchmarking illustrates the effectiveness of GIN by integrating it with the DeepEP library for MoE communication, showcasing improved latency while retaining NCCL's collective communication framework.", "conclusion": "GIN effectively integrates device-initiated communication into NCCL, addressing latency and performance issues in computation-intensive AI workloads while utilizing existing production infrastructure."}}
{"id": "2511.14774", "pdf": "https://arxiv.org/pdf/2511.14774", "abs": "https://arxiv.org/abs/2511.14774", "authors": ["Pei-Fu Guo", "Yun-Da Tsai", "Chun-Chia Hsu", "Kai-Xin Chen", "Ya-An Tsai", "Kai-Wei Chang", "Nanyun Peng", "Mi-Yen Yeh", "Shou-De Lin"], "title": "LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.", "AI": {"tldr": "The paper introduces LiveCLKTBench, a pipeline for studying cross-lingual knowledge transfer in language models.", "motivation": "To address the challenge of distinguishing genuine cross-lingual transfer from prior exposure in language models.", "method": "The authors developed LiveCLKTBench, an automated pipeline that selects time-sensitive knowledge and generates factual multilingual questions to evaluate cross-lingual knowledge transfer.", "result": "Using LiveCLKTBench, the study evaluated several large language models across five languages, observing that transfer is affected by linguistic distance, is often asymmetric, and that improvements plateau with model scale.", "conclusion": "LiveCLKTBench proves to be a valuable tool for studying multilingual transfer, revealing insights about the relationship between model scale, language, and domain in cross-lingual knowledge transfer."}}
{"id": "2511.15296", "pdf": "https://arxiv.org/pdf/2511.15296", "abs": "https://arxiv.org/abs/2511.15296", "authors": ["Thomas Kronland-Martinet", "St\u00e9phane Viollet", "Laurent U Perrinet"], "title": "Detection of spiking motifs of arbitrary length in neural activity using bounded synaptic delays", "categories": ["q-bio.NC"], "comment": null, "summary": "In the context of spiking neural networks, temporal coding of signals is increasingly preferred over the rate coding hypothesis due to its advantages in processing speed and energy efficiency. In temporal coding, synaptic delays are crucial for processing signals with precise spike timings, known as spiking motifs. Synaptic delays are however bounded in the brain and can thus be shorter than the duration of a motif. This prevents the use of motif recognition methods that consist of setting heterogeneous delays to synchronize the input spikes on a single output neuron acting as a coincidence detector. To address this issue, we developed a method to detect motifs of arbitrary length using a sequence of output neurons connected to input neurons by bounded synaptic delays. Each output neuron is associated with a sub-motif of bounded duration. A motif is recognized if all sub-motifs are sequentially detected by the output neurons. We simulated this network using leaky integrate-and-fire neurons and tested it on the Spiking Heidelberg Digits (SHD) database, that is, on audio data converted to spikes via a cochlear model, as well as on random simultaneous motifs. The results demonstrate that the network can effectively recognize motifs of arbitrary length extracted from the SHD database. Our method features a correct detection rate of about 60% in presence of ten simultaneous motifs from the SHD dataset and up to 80% for five motifs, showing the robustness of the network to noise. Results on random overlapping patterns show that the recognition of a single motif overlapping with other motifs is most effective for a large number of input neurons and sparser motifs. Our method provides a foundation for more general models for the storage and retrieval of neural information of arbitrary temporal lengths.", "AI": {"tldr": "The paper develops a spiking neural network method for recognizing temporal motifs of arbitrary lengths using sequences of output neurons, achieving robust motif detection on audio and randomized data despite noise.", "motivation": "The study is motivated by the limitations of synaptic delays in the brain when applying motif recognition using temporal coding in spiking neural networks. Traditional methods struggle with motifs longer than the synaptic delay bounds.", "method": "A network of leaky integrate-and-fire neurons with bounded synaptic delays was created. Output neurons are assigned to detect sub-motifs, and motifs are identified when these sub-motifs are recognized sequentially by the network. The method was tested on the Spiking Heidelberg Digits database and random motifs.", "result": "The network demonstrated a correct detection rate of ~60% for ten simultaneous motifs and up to 80% for five motifs on SHD data. For overlapping motifs, performance improved with larger input neuron numbers and sparser motifs.", "conclusion": "The proposed method is effective for recognizing motifs of arbitrary lengths and establishes a foundation for advancing neural information storage and retrieval models in spiking neural networks."}}
{"id": "2511.14794", "pdf": "https://arxiv.org/pdf/2511.14794", "abs": "https://arxiv.org/abs/2511.14794", "authors": ["Camilo Chac\u00f3n Sartori", "Christian Blum"], "title": "irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.", "AI": {"tldr": "This paper presents irace-evo, an extension of irace that incorporates LLM-driven code evolution alongside parameter tuning, improving algorithm performance with low cost.", "motivation": "Existing tools like irace focus only on parameter tuning, leaving algorithm code unchanged. The paper aims to enhance optimization by integrating code evolution.", "method": "The authors developed irace-evo, combining irace functionality with code evolution powered by LLMs, supporting multi-language, progressive context management, and robust code evolution via the Always-From-Original principle.", "result": "irace-evo outperforms state-of-the-art CMSA implementations for the VSBPP, discovering competitive algorithm variants at a cost below 2 euros using lightweight LLMs.", "conclusion": "The integration of LLM-driven code evolution with algorithm configuration reveals a powerful, cost-effective strategy for improving heuristic design and metaheuristic optimization."}}
{"id": "2511.14977", "pdf": "https://arxiv.org/pdf/2511.14977", "abs": "https://arxiv.org/abs/2511.14977", "authors": ["Xiangyu Li", "Zhaomiao Guo"], "title": "SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As more autonomous vehicles operate on public roads, understanding real-world behavior of autonomous vehicles is critical to analyzing traffic safety, making policies, and public acceptance. This paper proposes SVBRD-LLM, a framework that automatically discovers, verifies, and applies interpretable behavioral rules from real traffic videos through zero-shot prompt engineering. The framework extracts vehicle trajectories using YOLOv8 and ByteTrack, computes kinematic features, and employs GPT-5 zero-shot prompting to compare autonomous and human-driven vehicles, generating 35 structured behavioral rule hypotheses. These rules are tested on a validation set, iteratively refined based on failure cases to filter spurious correlations, and compiled into a high-confidence rule library. The framework is evaluated on an independent test set for speed change prediction, lane change prediction, and autonomous vehicle identification tasks. Experiments on over 1500 hours of real traffic videos show that the framework achieves 90.0% accuracy and 93.3% F1-score in autonomous vehicle identification. The discovered rules clearly reveal distinctive characteristics of autonomous vehicles in speed control smoothness, lane change conservativeness, and acceleration stability, with each rule accompanied by semantic description, applicable context, and validation confidence.", "AI": {"tldr": "The paper presents SVBRD-LLM, a framework for analyzing autonomous and human-driven vehicles' behavior using real traffic videos, achieving notable accuracy in identifying autonomous vehicles.", "motivation": "To understand the real-world behavior of autonomous vehicles for traffic safety analysis, policy-making, and gaining public acceptance.", "method": "The framework uses YOLOv8 and ByteTrack for trajectory extraction, GPT-5 zero-shot prompting for behavioral comparisons, and iterative rule refinement based on validation and failure cases.", "result": "SVBRD-LLM accurately identifies autonomous vehicles with 90.0% accuracy and 93.3% F1-score, revealing distinctive behavioral traits in speed control, lane changes, and acceleration stability.", "conclusion": "The framework provides reliable, interpretable behavioral rules for autonomous and human-driven vehicles, aiding safety analysis and public understanding of AV characteristics."}}
{"id": "2511.14870", "pdf": "https://arxiv.org/pdf/2511.14870", "abs": "https://arxiv.org/abs/2511.14870", "authors": ["Fuyang Zhang", "Pradeep Kumar Jayaraman", "Xiang Xu", "Yasutaka Furukawa"], "title": "B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://zhangfuyang.github.io/brdf/", "summary": "This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.", "AI": {"tldr": "This paper introduces B-Rep Distance Functions (BR-DF), a geometric representation for CAD models based on volumetric distance functions, achieving a 100% success rate in generating watertight models.", "motivation": "To overcome limitations in CAD generation methods, ensuring failsafe construction of watertight B-Rep models while maintaining high performance.", "method": "The authors encode surface mesh geometry into signed and per-face unsigned distance functions (SDFs and UDFs), paired with a multi-branch latent diffusion model using a 3D U-Net backbone.", "result": "BR-DF achieves competitive CAD creation performance compared to state-of-the-art methods and ensures 100% success in generating faceted B-Rep models.", "conclusion": "This novel representation guarantees reliable CAD model generation while leveraging the inherent properties of volumetric distance functions for robust results."}}
{"id": "2511.15010", "pdf": "https://arxiv.org/pdf/2511.15010", "abs": "https://arxiv.org/abs/2511.15010", "authors": ["Katie Rainey", "Erin Hausmann", "Donald Waagen", "David Gray", "Donald Hulsey"], "title": "Latent space analysis and generalization to out-of-distribution data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \\textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.", "AI": {"tldr": "The paper investigates the relationship between latent space OOD detection and model accuracy using SAR datasets, concluding that OOD detection is not a sufficient proxy for evaluating model performance.", "motivation": "Understanding latent spaces in deep learning and their role in OOD detection to enhance system evaluation and robustness.", "method": "Empirical analysis using synthetic and measured Synthetic Aperture RADAR datasets to assess the correlation between model accuracy and OOD detection in latent spaces.", "result": "Empirical results show that OOD detection does not directly correlate with model performance, challenging existing assumptions.", "conclusion": "Research should delve deeper into latent space geometry to improve understanding of deep learning robustness and generalizability."}}
{"id": "2511.14823", "pdf": "https://arxiv.org/pdf/2511.14823", "abs": "https://arxiv.org/abs/2511.14823", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 1 figure", "summary": "Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.", "AI": {"tldr": "This paper introduces dynamic nested hierarchies to enable machine learning models to adapt continuously and learn lifelong by autonomously adjusting optimization structures and frequencies.", "motivation": "Stationary architectures in machine learning hinder models' ability to adapt to non-stationary environments, limiting lifelong learning capabilities.", "method": "Proposes dynamic nested hierarchies where models autonomously adjust optimization levels, nesting structures, and update frequencies during training or inference, inspired by neuroplasticity.", "result": "Achieved theoretical proofs of convergence, expressivity bounds, and sublinear regret. Demonstrated superior performance empirically in language modeling, continuous learning, and long-context reasoning.", "conclusion": "Dynamic nested hierarchies advance adaptive, general-purpose intelligence by overcoming rigid constraints, enabling true lifelong learning and addressing distribution shifts effectively."}}
{"id": "2511.15028", "pdf": "https://arxiv.org/pdf/2511.15028", "abs": "https://arxiv.org/abs/2511.15028", "authors": ["Christophe Gyurgyik", "Alexander J Root", "Fredrik Kjolstad"], "title": "Data Layout Polymorphism for Bounding Volume Hierarchies", "categories": ["cs.PL"], "comment": null, "summary": "Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.", "AI": {"tldr": "Scion is a language and compiler for optimizing bounding volume hierarchies' data layouts independently of traversal algorithms.", "motivation": "Current programming systems overly entangle data layouts and traversal logic, limiting optimization opportunities and causing a false dichotomy between performance and portability.", "method": "The authors present Scion, a domain-specific language and compiler, enabling the specification of data layouts independent of traversal algorithms. Scion allows exploration of layout optimizations in a systematic manner.", "result": "Scion successfully expresses a range of layout optimizations, shows varying Pareto-optimal layouts for different architectures and workloads, and discovers a novel ray tracing layout leveraging techniques from prior work.", "conclusion": "Scion decouples data layouts from traversal logic, enhancing performance optimization and portability while contributing new insights in bounding volume hierarchy layouts."}}
{"id": "2511.15002", "pdf": "https://arxiv.org/pdf/2511.15002", "abs": "https://arxiv.org/abs/2511.15002", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to be published in IEEE Transaction on Machine Learning in Communication and Networking (TMLCN)", "summary": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $\u03c1$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.", "AI": {"tldr": "This paper introduces a novel approach for dynamic resource management in O-RAN networks using an enhanced Multi-Agent Reinforcement Learning (MARL) framework, improving efficiency and QoS.", "motivation": "Current deep reinforcement learning (DRL) models face challenges in robustness and generalization within dynamic network environments, necessitating improved approaches for efficient resource management.", "method": "The proposed framework enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM), introducing adaptive selective regularization based on temporal difference (TD)-error variance and dynamic $\u03c1$ scheduling for exploration-exploitation balance across agents.", "result": "The experimental outcomes demonstrate a considerable increase in resource allocation efficiency by up to 22%, along with better QoS satisfaction across O-RAN slices compared to conventional DRL methods.", "conclusion": "This approach is effective in addressing the shortcomings of DRL in dynamic environments, offering scalable improvements in resource management efficiency and service quality in next-generation networks."}}
{"id": "2511.15377", "pdf": "https://arxiv.org/pdf/2511.15377", "abs": "https://arxiv.org/abs/2511.15377", "authors": ["Simon Kl\u00fcttermann"], "title": "Towards Evolutionary Optimization Using the Ising Model", "categories": ["cs.NE"], "comment": "7 pages, 8 figures. Work in Progress", "summary": "In this paper, we study the problem of finding the global minima of a given function. Specifically, we consider complicated functions with numerous local minima, as is often the case for real-world data mining losses. We do so by applying a model from theoretical physics to create an Ising model-based evolutionary optimization algorithm. Our algorithm creates stable regions of local optima and a high potential for improvement between these regions. This enables the accurate identification of global minima, surpassing comparable methods, and has promising applications to ensembles.", "AI": {"tldr": "The paper presents an Ising model-based evolutionary algorithm to find global minima in complex functions with many local minima.", "motivation": "The study focuses on overcoming the challenges of finding global minima in complicated functions typical in real-world data mining.", "method": "An Ising model from theoretical physics is used to design an evolutionary optimization algorithm, leveraging its properties to stabilize local optima and achieve potential improvements.", "result": "The proposed algorithm accurately identifies global minima and outperforms comparable methods.", "conclusion": "This method shows promise for application in minimizing complicated functions and ensemble learning tasks."}}
{"id": "2511.15361", "pdf": "https://arxiv.org/pdf/2511.15361", "abs": "https://arxiv.org/abs/2511.15361", "authors": ["Preston Vander Vos", "Alberto Sonnino", "Giorgos Tsimos", "Philipp Jovanovic", "Lefteris Kokoris-Kogias"], "title": "BlueBottle: Fast and Robust Blockchains through Subsystem Specialization", "categories": ["cs.DC", "cs.CR"], "comment": null, "summary": "Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.", "AI": {"tldr": "BlueBottle is a two-layer blockchain consensus system balancing security, latency, and decentralization with a core layer for lower finality latency and a guard layer for decentralized monitoring and recovery.", "motivation": "The trilemma of balancing security, latency, and decentralization in blockchain consensus systems has led to trade-offs that compromise either decentralization or robustness for performance.", "method": "The BlueBottle architecture consists of two layers: BB-Core, which uses a medium-sized validator set for reduced finality latency, and BB-Guard, ensuring decentralized timestamping, misbehavior detection, and recovery in cases of faults.", "result": "Experiments show BB-Core achieves a 20-25% reduction in latency compared to similar systems, while BB-Guard enhances safety and liveness by handling equivocations and ensuring protocol recovery.", "conclusion": "BlueBottle successfully delivers high-throughput, sub-second finality with strong safety and liveness guarantees, striking an effective balance in the blockchain trilemma under mild synchrony assumptions."}}
{"id": "2511.14776", "pdf": "https://arxiv.org/pdf/2511.14776", "abs": "https://arxiv.org/abs/2511.14776", "authors": ["Snigdha Pandya", "Rohan Nagale", "Kenji Sahay", "Anna Lin", "Shikhar Shiromani", "Kevin Zhu", "Dev Sunishchal"], "title": "COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation", "categories": ["cs.CL"], "comment": "9 pages, 6 figures including algorithmns, 2 tables", "summary": "Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.", "AI": {"tldr": "The paper introduces COMPASS, a system to reduce factual errors in large language models by monitoring and adjusting attention mechanisms during text generation.", "motivation": "Large language models (LLMs) often make fluent but factually incorrect statements due to mismanagement of contextual and parametric knowledge.", "method": "The authors present COMPASS, featuring a PID controller that uses the Context Reliance Score (CRS) to dynamically monitor and adjust attention heads during decoding to prevent factual inconsistencies.", "result": "COMPASS consistently reduced hallucination rates by 2.8% to 5.8% in benchmarks like HotpotQA and XSum, while providing insights into attention heads' evidence alignment.", "conclusion": "The approach demonstrates that feedback-driven interpretability can enable better factual consistency and understanding of LLM behavior without requiring model retraining or additional passes."}}
{"id": "2511.15298", "pdf": "https://arxiv.org/pdf/2511.15298", "abs": "https://arxiv.org/abs/2511.15298", "authors": ["Aymeric Guillot", "Julien Gauthier", "Jeanne Lejoncour", "Franck Di Rienzo"], "title": "Modulating the tennis racket grip during motor imagery influences serve accuracy and performance: A pilot study", "categories": ["q-bio.NC"], "comment": null, "summary": "There is now ample evidence that Motor Imagery (MI) contributes to improve motor performance. Previous studies provided evidence that its effectiveness remains dependent upon specific guidelines and recommendations. The body posture, as well as the context in which MI is performed, are notably critical and should be carefully considered. The present study in young tennis players (n=18) was designed to compare the effectiveness of performing MI of the serve while adopting a loose grip (congruent MI) or holding tightly and squeezing hard the racket (incongruent MI). Data revealed that both MI conditions contributed to enhance the number of successful serves (p<0.001) and the technical quality of the serve (p<0.001). Interestingly, comparing mean serve accuracy scores showed that performance gains were significantly higher in the loose MI group than in the tight MI group (p<0.02). These findings confirm the critical importance of the congruence between the content of the mental representation and the features of the corresponding actual movement. Overall, the present study further highlights the effectiveness of the loose grip while mentally rehearsing the serve, and might thus contribute to update and adjust specific MI guidelines and recommendations.", "AI": {"tldr": "This study observed that Motor Imagery (MI) while adopting a congruent loose grip enhances motor performance and serve accuracy in young tennis players, compared to tight gripping.", "motivation": "To examine how congruence between mental imagery and actual movement (specifically grip style) affects the effectiveness of MI in improving motor performance in young tennis players.", "method": "A group of 18 young tennis players performed MI of the serve under two conditions: holding the racket loosely (congruent MI) and holding it tightly (incongruent MI). Performance enhancements in successful serves, technical quality, and serve accuracy were measured and compared.", "result": "Both MI conditions improved serve performance and technical quality, but the loose grip (congruent MI) group showed significantly greater improvements in serve accuracy compared to the tight grip group.", "conclusion": "The study emphasizes the importance of congruence between mental representation and actual movement in MI practice, recommending a loose grip during mental rehearsal to maximize benefits."}}
{"id": "2511.14798", "pdf": "https://arxiv.org/pdf/2511.14798", "abs": "https://arxiv.org/abs/2511.14798", "authors": ["Ahmad Memon", "Abdallah Mohamed"], "title": "Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "10 pages, 5 figures. This version corresponds to the paper accepted for presentation at CASCON 2025", "summary": "Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.\n  This paper compares two AI-based grading techniques: \\textit{Direct}, where the AI model applies a rubric directly to student code, and \\textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.\n  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.", "AI": {"tldr": "The paper evaluates AI-based grading methods for programming assignments, focusing on Direct and Reverse approaches, with an emphasis on fairness, consistency, and efficiency.", "motivation": "Manual grading of programming assignments is time-consuming and inconsistent, prompting exploration of automated, scalable solutions using AI.", "method": "Comparison between Direct and Reverse grading techniques using AI, assessing these on expanded scales and synthetic student code.", "result": "Reverse grading offered finer assessments compared to Direct grading, but both required well-designed prompts; synthetic code allowed broader testing.", "conclusion": "AI-based grading methods can improve grading systems in CS courses, though careful prompt engineering and hybrid models with humans are necessary."}}
{"id": "2511.14988", "pdf": "https://arxiv.org/pdf/2511.14988", "abs": "https://arxiv.org/abs/2511.14988", "authors": ["Alex Cuellar", "Christopher K Fourie", "Julie A Shah"], "title": "An Alignment-Based Approach to Learning Motions from Demonstrations", "categories": ["cs.RO"], "comment": "8 pages, 8 figures, originally published in the IEEE Robotics and Automation Letters", "summary": "Learning from Demonstration (LfD) has shown to provide robots with fundamental motion skills for a variety of domains. Various branches of LfD research (e.g., learned dynamical systems and movement primitives) can generally be classified into ''time-dependent'' or ''time-independent'' systems. Each provides fundamental benefits and drawbacks -- time-independent methods cannot learn overlapping trajectories, while time-dependence can result in undesirable behavior under perturbation. This paper introduces Cluster Alignment for Learned Motions (CALM), an LfD framework dependent upon an alignment with a representative ''mean\" trajectory of demonstrated motions rather than pure time- or state-dependence. We discuss the convergence properties of CALM, introduce an alignment technique able to handle the shifts in alignment possible under perturbation, and utilize demonstration clustering to generate multi-modal behavior. We show how CALM mitigates the drawbacks of time-dependent and time-independent techniques on 2D datasets and implement our system on a 7-DoF robot learning tasks in three domains.", "AI": {"tldr": "This paper introduces CALM, an LfD framework that aligns demonstrated motions to a mean trajectory, avoiding issues of time-dependence and state-dependence while enabling multi-modal behavior.", "motivation": "To overcome the limitations of existing motion learning systems: time-independent methods struggle with trajectory overlaps, and time-dependent methods can behave undesirably under perturbation.", "method": "The authors propose Cluster Alignment for Learned Motions (CALM), which uses trajectory alignment and clustering to achieve robust motion learning with multi-modal capabilities.", "result": "CALM demonstrated improved performance on 2D datasets and was successfully implemented on a 7-DoF robot across three domains.", "conclusion": "CALM offers a robust alternative to time-dependent/time-independent LfD methods, mitigating their drawbacks with trajectory alignment and enabling multi-modal robotic behavior."}}
{"id": "2511.14884", "pdf": "https://arxiv.org/pdf/2511.14884", "abs": "https://arxiv.org/abs/2511.14884", "authors": ["Antonio Ruiz", "Tao Wu", "Andrew Melnik", "Qing Cheng", "Xuqin Wang", "Lu Liu", "Yongliang Wang", "Yanfeng Zhang", "Helge Ritter"], "title": "GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.", "AI": {"tldr": "GeoSceneGraph synthesizes indoor 3D scenes from text prompts using graph structures and geometric symmetries, overcoming limitations of existing methods.", "motivation": "To address challenges in generating coherent and realistic indoor 3D scenes for resource-constrained devices, leveraging graph and geometric properties while eliminating reliance on pre-defined relationship classes.", "method": "Utilizes equivariant graph neural networks (EGNNs) with a novel conditioning strategy for text features to synthesize 3D scenes from prompts.", "result": "GeoSceneGraph matches the performance of methods using ground-truth relationships despite not relying on them.", "conclusion": "GeoSceneGraph effectively integrates graph structures and text-conditioning strategies to improve 3D scene synthesis without predefined relationship requirements."}}
{"id": "2511.15120", "pdf": "https://arxiv.org/pdf/2511.15120", "abs": "https://arxiv.org/abs/2511.15120", "authors": ["Bohan Zhang", "Zihao Wang", "Hengyu Fu", "Jason D. Lee"], "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit", "categories": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.ST"], "comment": "86 pages, 2 figures. The order of the first two authors was determined by a coin flip", "summary": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.", "AI": {"tldr": "This paper examines how neural networks learn high-dimensional features using gradient descent, proving efficient learning with optimal sample and time complexity for a Gaussian multi-index model.", "motivation": "The study aims to understand how neural networks efficiently learn high-dimensional features, particularly exploring the Gaussian multi-index model as a framework for representation learning.", "method": "The authors analyze a two-layer neural network trained with layer-wise gradient descent, focusing on the Gaussian multi-index model. The proof involves demonstrating that inner weights perform a power-iteration process to recover the hidden subspace.", "result": "They show that the network can learn the target with optimal sample and time complexity, aligning with theoretical limits. The first layer of the network must be trained beyond certain steps for optimal results.", "conclusion": "Neural networks demonstrate the capability to effectively learn hierarchical functions efficiently in terms of both sample and time requirements."}}
{"id": "2511.14846", "pdf": "https://arxiv.org/pdf/2511.14846", "abs": "https://arxiv.org/abs/2511.14846", "authors": ["Yifeng Ding", "Hung Le", "Songyang Han", "Kangrui Ruan", "Zhenghui Jin", "Varun Kumar", "Zijian Wang", "Anoop Deoras"], "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.", "AI": {"tldr": "The paper introduces GTPO, an RL algorithm improving LLM performance for multi-turn reasoning tasks, overcoming GRPO's limitations with fine-grained rewards.", "motivation": "Existing RL approaches like GRPO struggle with training LLMs for complex multi-turn reasoning due to coarse-grained trajectory-level rewards.", "method": "GTPO employs turn-level rewards, return-based advantage estimation, and self-supervised reward shaping to enhance training feedback and reasoning efficacy.", "result": "GTPO outperforms GRPO by 3.0% on diverse reasoning benchmarks, showcasing its effectiveness in advancing multi-turn reasoning.", "conclusion": "The paper provides a specialized RL approach for training LLMs, demonstrating improved tool-integrated reasoning through finer reward mechanisms and advanced techniques."}}
{"id": "2511.15073", "pdf": "https://arxiv.org/pdf/2511.15073", "abs": "https://arxiv.org/abs/2511.15073", "authors": ["Youwei Xiao", "Zizhang Luo", "Weijie Peng", "Yuyang Zou", "Yun Liang"], "title": "Cement2: Temporal Hardware Transactions for High-Level and Efficient FPGA Programming", "categories": ["cs.PL"], "comment": null, "summary": "Hardware design faces a fundamental challenge: raising abstraction to improve productivity while maintaining control over low-level details like cycle accuracy. Traditional RTL design in languages like SystemVerilog composes modules through wiring-style connections that provide weak guarantees for behavioral correctness. While high-level synthesis (HLS) and emerging abstractions attempt to address this, they either introduce unpredictable overhead or restrict design generality. Although transactional HDLs provide a promising foundation by lifting design abstraction to atomic and composable rules, they solely model intra-cycle behavior and do not reflect the native temporal design characteristics, hindering applicability and productivity for FPGA programming scenarios.\n  We propose temporal hardware transactions, a new abstraction that brings cycle-level timing awareness to designers at the transactional language level. Our approach models temporal relationships between rules and supports the description of rules whose actions span multiple clock cycles, providing intuitive abstraction to describe multi-cycle architectural behavior. We implement this in Cement2, a transactional HDL embedded in Rust, enabling programming hardware constructors to build both intra-cycle and temporal transactions. Cement2's synthesis framework lowers description abstraction through multiple analysis and optimization phases, generating efficient hardware. With Cement2's abstraction, we program a RISC-V soft-core processor, custom CPU instructions, linear algebra kernels, and systolic array accelerators, leveraging the high-level abstraction for boosted productivity. Evaluation shows that Cement2 does not sacrifice performance and resources compared to hand-coded RTL designs, demonstrating the high applicability for general FPGA design tasks.", "AI": {"tldr": "This paper introduces \"temporal hardware transactions\" to embed cycle-level timing awareness into transactional hardware descriptions, improving FPGA programming productivity without compromising resources or performance.", "motivation": "Current hardware design methods struggle to balance high-level abstraction for productivity with control over cycle-level details. Transactional HDLs show promise but lack temporal design characteristics, limiting their effectiveness for FPGA programming.", "method": "The authors propose temporal hardware transactions and implement them in Cement2, a transactional HDL embedded in Rust. This includes modeling temporal relationships and supporting multi-cycle rules, with synthesis optimizing descriptions into efficient hardware.", "result": "Cement2 enables the design of complex hardware such as RISC-V processors, custom instructions, and accelerators. It achieves high productivity without sacrificing performance or resource efficiency compared to traditional hand-coded RTL methods.", "conclusion": "Temporal hardware transactions enhance abstraction and productivity for FPGA programming, bridging cycle-level control with high-level design approaches while maintaining performance efficiency."}}
{"id": "2511.15055", "pdf": "https://arxiv.org/pdf/2511.15055", "abs": "https://arxiv.org/abs/2511.15055", "authors": ["Jian-Ting Guo", "Yu-Cheng Chen", "Ping-Chun Hsieh", "Kuo-Hao Ho", "Po-Wei Huang", "Ti-Rong Wu", "I-Chen Wu"], "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.", "AI": {"tldr": "This paper develops Macro Action Quantization (MAQ), a reinforcement learning framework that enhances agents' human-like behaviors by using macro actions distilled from human demonstrations.", "motivation": "Designing RL agents that behave more like humans has received little attention, yet this is crucial for interpretability and trustworthiness.", "method": "The researchers propose formulating human-likeness as a trajectory optimization problem, leveraging Macro Action Quantization (MAQ) via Vector-Quantized VAE and applying it to various RL algorithms.", "result": "MAQ improves human-likeness significantly in experiments, increasing trajectory similarity scores and earning top rankings in human evaluations.", "conclusion": "MAQ offers an efficient and adaptable framework for integrating human-like behavior into RL agents, providing a promising direction for future research."}}
{"id": "2511.15505", "pdf": "https://arxiv.org/pdf/2511.15505", "abs": "https://arxiv.org/abs/2511.15505", "authors": ["Anastasios Petropoulos", "Theodore Antonakopoulos"], "title": "Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference", "categories": ["cs.AR"], "comment": "Accepted at the 18th IEEE International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC-2025)", "summary": "This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\\%$, and throughput efficiency gains, up to $2.7\\times$, over prior works across different configurations.", "AI": {"tldr": "This paper proposes an instruction-based architecture for coordinating FPGA systems with multiple Processing Units (PUs) to optimize Deep Neural Network (DNN) inference.", "motivation": "The growing demand for faster and more efficient DNN inference necessitates advancements in multi-PU synchronization and flexible deployment strategies for FPGA-based systems.", "method": "The paper introduces an architecture with programmable synchronization mechanisms using specialized instruction controllers and a compilation framework, enabling efficient DNN partitioning and flexible runtime strategy switching.", "result": "Experimental evaluations on ResNet-50 report up to 98% compute efficiency and a 2.7x improvement in throughput efficiency compared to existing approaches.", "conclusion": "The proposed architecture significantly enhances DNN inference performance and offers versatility in balancing single-batch and multi-batch configurations on FPGA platforms."}}
{"id": "2511.15551", "pdf": "https://arxiv.org/pdf/2511.15551", "abs": "https://arxiv.org/abs/2511.15551", "authors": ["Yukun Du", "Haiyue Yu", "Xiaotong Xie", "Yan Zheng", "Lixin Zhan", "Yudong Du", "Chongshuang Hu", "Boxuan Wang", "Jiang Jiang"], "title": "Meta-Black-Box Optimization with Bi-Space Landscape Analysis and Dual-Control Mechanism for SAEA", "categories": ["cs.NE"], "comment": null, "summary": "Surrogate-Assisted Evolutionary Algorithms (SAEAs) are widely used for expensive Black-Box Optimization. However, their reliance on rigid, manually designed components such as infill criteria and evolutionary strategies during the search process limits their flexibility across tasks. To address these limitations, we propose Dual-Control Bi-Space Surrogate-Assisted Evolutionary Algorithm (DB-SAEA), a Meta-Black-Box Optimization (MetaBBO) framework tailored for multi-objective problems. DB-SAEA learns a meta-policy that jointly regulates candidate generation and infill criterion selection, enabling dual control. The bi-space Exploratory Landscape Analysis (ELA) module in DB-SAEA adopts an attention-based architecture to capture optimization states from both true and surrogate evaluation spaces, while ensuring scalability across problem dimensions, population sizes, and objectives. Additionally, we integrate TabPFN as the surrogate model for accurate and efficient prediction with uncertainty estimation. The framework is trained via reinforcement learning, leveraging parallel sampling and centralized training to enhance efficiency and transferability across tasks. Experimental results demonstrate that DB-SAEA not only outperforms state-of-the-art baselines across diverse benchmarks, but also exhibits strong zero-shot transfer to unseen tasks with higher-dimensional settings. This work introduces the first MetaBBO framework with dual-level control over SAEAs and a bi-space ELA that captures surrogate model information.", "AI": {"tldr": "The paper introduces DB-SAEA, a framework for multi-objective optimization that uses meta-policy to guide evolutionary strategies and infill criteria, integrating advanced landscape analysis and surrogate modeling for improved task performance and scalability.", "motivation": "The limitations of rigid, manually designed infill criteria and evolutionary strategies in existing SAEAs hinder their adaptability and performance across diverse optimization tasks.", "method": "The proposed DB-SAEA utilizes dual control via meta-policy for candidate generation and infill criterion selection. It includes an attention-based Exploratory Landscape Analysis module for analyzing optimization states across spaces and incorporates TabPFN for accurate surrogate modeling. Reinforcement learning is used for training.", "result": "Experiments show that DB-SAEA outperforms existing methods across benchmarks and achieves strong zero-shot transfer to new tasks with higher dimensions.", "conclusion": "DB-SAEA presents a significant advancement in MetaBBO by introducing dual-level control and bi-space ELA, demonstrating enhanced adaptability, efficiency, and transferability in optimization tasks."}}
{"id": "2511.15388", "pdf": "https://arxiv.org/pdf/2511.15388", "abs": "https://arxiv.org/abs/2511.15388", "authors": ["Lucianna Kiffer", "Lioba Heimbach", "Dennis Trautwein", "Yann Vonlanthen", "Oliver Gasser"], "title": "Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies", "categories": ["cs.DC"], "comment": "In Proceedings of ACM SIGMETRICS 2026", "summary": "Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.\n  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.", "AI": {"tldr": "This paper conducts the first comprehensive, multi-network study on the peer-to-peer layer of 36 public blockchain networks, using active crawlers and Ethereum discovery protocols, to reveal network size variations, trends, resilience, and decentralization.", "motivation": "The motivation is to address the lack of transparency and understanding in the peer-to-peer networking layer of blockchain systems, beyond the top few ecosystems.", "method": "Researchers conducted a 9-month longitudinal study using 15 active crawlers, two community crawlers, and Internet-wide scans. They gathered data on 36 public blockchains and further inferred metadata from Ethereum discovery protocols.", "result": "The study revealed variations in network sizes, IPv4/IPv6 usage, geographic and autonomous system concentration, and network behaviors like churn and diurnal activity, exposing significant differences in resilience and decentralization.", "conclusion": "The findings highlight the varying levels of decentralization and resilience across blockchain networks, while presenting a scalable methodology for monitoring and assessing decentralized networks comprehensively."}}
{"id": "2511.14779", "pdf": "https://arxiv.org/pdf/2511.14779", "abs": "https://arxiv.org/abs/2511.14779", "authors": ["Julio Cesar Galdino", "Sidney Evaldo Leal", "Leticia Gabriella De Souza", "Rodrigo de Freitas Lima", "Antonio Nelson Fornari Mendes Moreira", "Arnaldo Candido Junior", "Miguel Oliveira", "Edresson Casanova", "Sandra M. Alu\u00edsio"], "title": "The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech", "categories": ["cs.CL"], "comment": null, "summary": "Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.", "AI": {"tldr": "This paper explores the impact of prosodic segmentation (manual and automatic) on spontaneous speech synthesis in Brazilian Portuguese using FastSpeech 2, showing that prosodic features improve speech naturalness.", "motivation": "The authors aim to address the challenge of capturing the natural flow of conversation in speech synthesis, particularly focusing on how explicit prosodic segmentation impacts speech quality.", "method": "They analyze manual and automatic prosodic segmentation for training a non-autoregressive speech synthesis model (FastSpeech 2) on Brazilian Portuguese.", "result": "The study found that prosodic segmentation improves intelligibility and naturalness. Manual segmentation contributed to more natural prosody, while automatic segmentation led to regular segments.", "conclusion": "Incorporating prosodic segmentation enhances speech synthesis quality, with key improvements in both acoustic naturalness and nuclear accent patterns."}}
{"id": "2511.15338", "pdf": "https://arxiv.org/pdf/2511.15338", "abs": "https://arxiv.org/abs/2511.15338", "authors": ["Sufiaan Ahmed", "Tyrese Lindsay", "James W. Roberts"], "title": "Does the Muller-Lyer illusion induced by a goalkeeper configuration influence soccer penalty kicks?", "categories": ["q-bio.NC"], "comment": "22 pages, 4 figures, accepted for publication in Movement & Sport Sciences - Science & Motricite", "summary": "In soccer penalty kicks, goalkeepers that orient their arms upward compared to downward can be misperceived as being taller - effectively recreating the Muller-Lyer illusion. The present study elaborates on previous research surrounding a potential illusion-induced bias in penalty kicks. Participants were exposed to goalkeeper configurations within a virtual goal including arms-parallel, arms-down, arms-out and arms-up. They separately judged the perceived size of the goalkeeper, and executed penalty kicks. The perceived size was near fully consistent with the intended illusion. Meanwhile, the penalty kicks indicated wider a horizontal position following arms-out, and lower vertical position following arms-up. Likewise, there was no relation between the biases expressed in perception and action. While goalkeepers can elicit a perceptual illusion, this does not extend to influencing the penalty kick itself. Instead, other contextual cues appeared more relevant including the proximity between the goalkeeper and goalposts, and with it, the available space in the goal.", "AI": {"tldr": "This study examines the impact of a perceptual illusion (goalkeeper's arm orientation) on penalty kick actions and perceptions in soccer.", "motivation": "Investigate whether the perceptual illusion, similar to the Muller-Lyer illusion caused by the goalkeeper\u2019s arm orientation, influences penalty kick decisions and perceptions.", "method": "Participants were exposed to virtual goalkeeper configurations (arms-parallel, arms-down, arms-out, arms-up) to judge perceived goalkeeper size and execute penalty kicks.", "result": "Participants misperceived goalkeeper size consistent with the illusion; however, penalty kick behaviors (horizontal and vertical positioning) were not influenced by this perceptual bias.", "conclusion": "While perceptual illusions exist, they do not affect penalty kick execution directly. Other contextual factors, such as positional proximity to goalposts, hold greater relevance."}}
{"id": "2511.14803", "pdf": "https://arxiv.org/pdf/2511.14803", "abs": "https://arxiv.org/abs/2511.14803", "authors": ["Pranjal Gupta", "Karan Bhukar", "Harshit Kumar", "Seema Nagar", "Prateeti Mohapatra", "Debanjana Kar"], "title": "Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.", "AI": {"tldr": "This paper introduces an automated log analysis tool using Large Language Models (LLMs) to process logs efficiently, scale across products, and reduce manpower costs.", "motivation": "Manual inspection of the massive volume of IT system logs is impractical, necessitating automation for efficient analysis and issue diagnosis.", "method": "The authors developed a log analytics tool leveraging Large Language Models (LLMs) for automated log data processing, enhanced by a CPU-efficient approach for scalable operation.", "result": "The tool, deployed across 70 software products since March 2024, has processed over 2000 tickets, saving 300+ man hours and $15,444 monthly in manpower costs.", "conclusion": "The proposed tool demonstrates significant time and cost savings, establishing it as a scalable and efficient solution for automated log analysis in IT environments."}}
{"id": "2511.14994", "pdf": "https://arxiv.org/pdf/2511.14994", "abs": "https://arxiv.org/abs/2511.14994", "authors": ["Yue Yu", "Xiaobo Zheng", "Shaoming He"], "title": "Communication-Aware Asynchronous Distributed Trajectory Optimization for UAV Swarm", "categories": ["cs.RO"], "comment": null, "summary": "Distributed optimization offers a promising paradigm for trajectory planning in Unmanned Aerial Vehicle (UAV) swarms, yet its deployment in communication-constrained environments remains challenging due to unreliable links and limited data exchange. This paper addresses this issue via a two-tier architecture explicitly designed for operation under communication constraints. We develop a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework that integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization of individual UAVs with an asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm-level coordination. The proposed architecture enables fully distributed optimization while substantially reducing communication overhead, making it suitable for real-world scenarios in which reliable connectivity cannot be guaranteed. The method is particularly effective in handling nonlinear dynamics and spatio-temporal coupling under communication constraints.", "AI": {"tldr": "The paper introduces a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework for UAV swarm trajectory planning, optimizing under communication constraints.", "motivation": "Enable effective trajectory planning in UAV swarms even in communication-constrained environments with unreliable links and limited data exchange.", "method": "The approach integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization and asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm coordination.", "result": "The framework achieves fully distributed optimization, significantly reducing communication overhead and handling nonlinear dynamics and spatio-temporal coupling under constraints.", "conclusion": "CA-ADTO offers a robust solution for UAV swarm trajectory planning in real-world scenarios with poor connectivity, advising its practical use."}}
{"id": "2511.14897", "pdf": "https://arxiv.org/pdf/2511.14897", "abs": "https://arxiv.org/abs/2511.14897", "authors": ["Pranav Indrakanti", "Ivor Simpson"], "title": "HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to ISBI 2026", "summary": "We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.", "AI": {"tldr": "The paper introduces an unsupervised method for bidirectional MRI synthesis between Ultra-Low Field (ULF) and High-Field (HF) images using physics-based contrast transformation and implicit neural representations.", "motivation": "To develop a method that bridges the gap between data availability of High-Field (HF) and Ultra-Low Field (ULF) MRI systems, leveraging their fundamental contrast differences for medical imaging tasks.", "method": "The approach utilizes a forward model inspired by MRI physics to simulate HF-to-ULF transformations by estimating tissue-specific SNR values. An Implicit Neural Representation (INR) network performs super-resolution by predicting tissue-type segmentations and image intensity jointly.", "result": "The method improved WM-GM contrast by 52% in synthetic ULF-like images and 37% in 64mT images, and showed robustness to changes in target contrast, noise, and initial settings.", "conclusion": "This model demonstrates the potential to improve both the quality and reliability of bidirectional HF and ULF MRI synthesis using a physics-driven and unsupervised neural network approach."}}
{"id": "2511.15146", "pdf": "https://arxiv.org/pdf/2511.15146", "abs": "https://arxiv.org/abs/2511.15146", "authors": ["Eugene Ndiaye"], "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings", "categories": ["stat.ML", "cs.LG", "math.ST"], "comment": null, "summary": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.", "AI": {"tldr": "The paper extends conformal prediction to multivariate cases using optimal transport, enabling multivariate conformal predictive distributions (CPDs) with finite-sample coverage guarantees.", "motivation": "While conformal prediction ensures uncertainty sets with finite-sample guarantees, it is traditionally limited to scalar-valued scores. Multivariate generalization and predictive distribution construction have remained unresolved challenges.", "method": "The methodology leverages optimal transport to define vector-ranks and multivariate quantile regions, offering finite-sample, distribution-free guarantees. It introduces a novel approach using transport maps for calibration scores and a candidate's score, allowing tractable characterization via a polyhedral partition.", "result": "The approach provides multivariate CPDs with finite-sample calibration and introduces randomized multivariate CPDs, including a new generalization of the Dempster-Hill procedure.", "conclusion": "The paper successfully constructs multivariate CPDs with finite-sample guarantees, addressing a key limitation in predictive distributions for multidimensional data."}}
{"id": "2511.14865", "pdf": "https://arxiv.org/pdf/2511.14865", "abs": "https://arxiv.org/abs/2511.14865", "authors": ["Dwipam Katariya", "Snehita Varma", "Akshat Shreemali", "Benjamin Wu", "Kalanand Mishra", "Pranab Mohanty"], "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications", "categories": ["cs.LG"], "comment": "10 pages, 7 figures, Accepted at CARS @ RecSys 2025", "summary": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.", "AI": {"tldr": "The paper introduces FinTRec, a transformer-based model designed to address the challenges of sequential recommendation systems in Financial Services (FS) and demonstrates its effectiveness over traditional tree-based models.", "motivation": "To develop a model capable of addressing the unique challenges of sequential recommendation in FS, such as long-range user interactions across multiple channels and balancing interrelated products' business goals.", "method": "A transformer-based framework, FinTRec, is proposed to handle FS-specific sequential data and operational objectives; tested through historic simulations and live A/B testing against production-grade tree-based models.", "result": "FinTRec consistently outperforms traditional tree-based baselines, improves offline performance across all products, facilitates cross-product signal sharing, and reduces training cost and technical debt.", "conclusion": "FinTRec offers a viable alternative to tree-based models in FS by demonstrating superior performance and addressing business and technical challenges, marking a significant shift toward transformer-based architectures."}}
{"id": "2511.15323", "pdf": "https://arxiv.org/pdf/2511.15323", "abs": "https://arxiv.org/abs/2511.15323", "authors": ["Youwei Xiao", "Yuyang Zou", "Yun Liang"], "title": "SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.\n  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.", "AI": {"tldr": "SkyEgg addresses limitations in current FPGA high-level synthesis tools by jointly optimizing implementation selection and scheduling using an e-graph framework, achieving significant performance improvements.", "motivation": "Current hardware synthesis methods separate implementation selection and scheduling, resulting in suboptimal FPGA designs that cannot fully utilize heterogeneous architectures and miss optimization opportunities.", "method": "SkyEgg uses an e-graph-based approach, where algebraic transformations and hardware implementation choices are represented as rewrite rules. It combines equality saturation to search the design space and solves a mixed-integer linear programming (MILP) problem for joint optimization.", "result": "SkyEgg demonstrated an average speedup of 3.01x compared to Vitis HLS, with maximum speedup reaching 5.22x in certain cases, showing its superior design efficiency.", "conclusion": "The joint optimization method of SkyEgg provides an effective solution to improve performance and resource utilization in FPGA design compared to traditional synthesis tools."}}
{"id": "2511.15061", "pdf": "https://arxiv.org/pdf/2511.15061", "abs": "https://arxiv.org/abs/2511.15061", "authors": ["Haodong Chen", "Guido Zuccon", "Teerapong Leelanupab"], "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering", "categories": ["cs.AI", "cs.IR", "cs.LG"], "comment": "This paper has been accepted to SIGIR-AP 2025", "summary": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.", "AI": {"tldr": "OpenBioLLM enhances genomic question answering using open-source models and modular multi-agent systems, achieving better efficiency and comparable performance to proprietary systems like GeneGPT.", "motivation": "To improve scalability, reduce operational costs, address data privacy concerns, and enhance generalization capabilities in genomic question answering systems by moving away from proprietary models and exploring open-source solutions.", "method": "The study reproduced GeneGPT using open-source models in a monolithic architecture and developed OpenBioLLM, a modular multi-agent framework with specialized agents for tool routing, query generation, and response validation.", "result": "OpenBioLLM matches or outperforms GeneGPT in over 90% of benchmark tasks, achieving high scores on Gene-Turing (0.849) and GeneHop (0.830). It reduces latency by 40-50% while maintaining robust capabilities using smaller open-source models without additional fine-tuning.", "conclusion": "Open-source multi-agent systems, such as OpenBioLLM, represent a strong alternative for genomic question answering, showcasing practical efficiency and high task performance compared to proprietary solutions."}}
{"id": "2511.15564", "pdf": "https://arxiv.org/pdf/2511.15564", "abs": "https://arxiv.org/abs/2511.15564", "authors": ["Paul Scheffler", "Thomas Benz", "Tim Fischer", "Lorenzo Leone", "Sina Arjmandpour", "Luca Benini"], "title": "Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond", "categories": ["cs.AR"], "comment": "8 pages, 8 figures, 1 table, submitted to 2026 IEEE CICC for possible publication", "summary": "We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.", "AI": {"tldr": "This paper outlines a roadmap for developing open-source RISC-V chiplet-based systems for HPC and AI, emphasizing performance improvements and proposing scalable architectural designs.", "motivation": "The paper addresses the performance gap between open-source RISC-V solutions and proprietary systems, aiming to pave the way for state-of-the-art, accessible high-performance computing and AI technologies.", "method": "It introduces scalable chiplet-based architectures, starting from the dual-chiplet Occamy in 12nm FinFET, progressing to Ramora with mesh-NoC, and culminating in Ogopogo, a 7nm quad-chiplet architecture. The study also explores extending openness to simulation, EDA, PDKs, and off-die PHYs.", "result": "The research demonstrates progress in open-source RISC-V systems, successfully achieving silicon-proven designs and advancing to concepts like mesh-NoC chips and quad-chiplet architectures in cutting-edge technologies.", "conclusion": "The roadmap highlights the potential to close the performance gap with proprietary systems through scalable chiplet-based designs while extending openness to further areas of chip development."}}
{"id": "2511.15421", "pdf": "https://arxiv.org/pdf/2511.15421", "abs": "https://arxiv.org/abs/2511.15421", "authors": ["Ethan Hicks", "Joseph Oglio", "Mikhail Nesterenko", "Gokarna Sharma"], "title": "When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit", "categories": ["cs.DC", "cs.CR"], "comment": null, "summary": "We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.", "AI": {"tldr": "The paper examines how transaction finality in Bitcoin relates to transaction amount, user risk tolerance, and the likelihood of blockchain forks, offering a more nuanced understanding than the usual fixed block depth standard.", "motivation": "The motivation is to refine the understanding of transaction finality in Bitcoin by considering probabilities of revocation due to forks, transaction amounts, and varying user risk tolerances, moving beyond the traditional fixed depth rule.", "method": "The study uses both simulations and real-world Bitcoin data to analyze blockchain forks under varying network delays. It establishes a relationship between block depth and the probability of confirmation revocation. Prospect theory is applied to connect this probability with transaction amount and user risk tolerance.", "result": "The research finds a probabilistic link between blockchain depth and the risk of transaction reversals due to forks, with implications for users' varying risk thresholds and transaction sizes.", "conclusion": "Block depth for transaction finality should be tailored to specific transaction sizes and user risk preferences, rather than applying a fixed standard like six blocks."}}
{"id": "2511.14783", "pdf": "https://arxiv.org/pdf/2511.14783", "abs": "https://arxiv.org/abs/2511.14783", "authors": ["Bingquan Zhang", "Xiaoxiao Liu", "Yuchi Wang", "Lei Zhou", "Qianqian Xie", "Benyou Wang"], "title": "Human or LLM as Standardized Patients? A Comparative Study for Medical Education", "categories": ["cs.CL", "cs.CY"], "comment": "10 pages, 9 figures, 8 table", "summary": "Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.", "AI": {"tldr": "EasyMED is a multi-agent framework enabling medical training simulations, offering consistent behavior and improved outcomes compared to standard human standardized patients.", "motivation": "The motivation is to address the limitations of human Standardized Patients (SP), such as high costs, inflexibility, and scalability issues, by creating a more accessible and cost-efficient alternative.", "method": "The study introduces EasyMED, a multi-agent system with three components: a Patient Agent for realistic dialogue, an Auxiliary Agent for ensuring factual consistency, and an Evaluation Agent for providing actionable feedback. Additionally, a benchmark SPBench was developed to systematically assess these simulations.", "result": "EasyMED achieves comparable learning outcomes to human SPs, enhances skill development for lower-baseline students, and proves to be more flexible, psychologically safe, and cost-efficient.", "conclusion": "EasyMED represents a substantial step toward accessible and effective medical training, addressing the shortcomings of human SPs while maintaining educational quality and expanding training flexibility."}}
{"id": "2511.14805", "pdf": "https://arxiv.org/pdf/2511.14805", "abs": "https://arxiv.org/abs/2511.14805", "authors": ["Dhaminda B. Abeywickrama", "Michael Fisher", "Frederic Wheeler", "Louise Dennis"], "title": "Towards Continuous Assurance with Formal Verification and Assurance Cases", "categories": ["cs.SE", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.", "AI": {"tldr": "The paper proposes a Continuous Assurance Framework ensuring unified design-time, runtime, and evolution-time safety assurance for autonomous systems, with a case study on a nuclear inspection robot.", "motivation": "Assure the correctness and safety of autonomous systems continuously across lifecycle operations, addressing the limitations of traditional fragmented assurance methods.", "method": "Developed a Continuous Assurance Framework integrating design-time, runtime, and evolution-time assurance, validated using RoboChart for functional correctness and PRISM for risk analysis, with automated argument regeneration through an Eclipse plugin.", "result": "Developed methods were applied in a case study of a nuclear inspection robot, demonstrating automatic traceability and alignment with AI regulatory principles.", "conclusion": "Continuous assurance, combined with model-driven frameworks and formal tools, is necessary for adaptable and safety-focused autonomy lifecycle management."}}
{"id": "2511.15023", "pdf": "https://arxiv.org/pdf/2511.15023", "abs": "https://arxiv.org/abs/2511.15023", "authors": ["Dimitria Silveria", "Kleber Cabral", "Peter Jardine", "Sidney Givigi"], "title": "Lie Group Control Architectures for UAVs: a Comparison of SE2(3)-Based Approaches in Simulation and Hardware", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents the integration and experimental validation of advanced control strategies for quadcopters based on Lie groups. We build upon recent theoretical developments on SE2(3)-based controllers and introduce a novel SE2(3) model predictive controller (MPC) that combines the predictive capabilities and constraint-handling of optimal control with the geometric properties of Lie group formulations. We evaluated this MPC against a state-of-the-art SE2(3)-based LQR approach and obtained comparable performance in simulation. Both controllers where also deployed on the Quanser QDrone platform and compared to each other and an industry standard control architecture. Results show that the SE_2(3) MPC achieves superior trajectory tracking performance and robustness across a range of scenarios. This work demonstrates the practical effectiveness of Lie group-based controllers and offers comparative insights into their impact on system behaviour and real-time performance", "AI": {"tldr": "This paper introduces a novel SE2(3) model predictive controller (MPC) for quadcopters, demonstrating superior trajectory tracking and robustness compared to other controllers.", "motivation": "The paper aims to explore advanced control strategies for quadcopters using Lie groups, leveraging the geometric properties of Lie groups to enhance control performance.", "method": "The authors build upon SE2(3)-based controllers and propose a novel SE2(3) model predictive controller (MPC), which integrates predictive control capabilities, constraint-handling, and Lie group formulations. They compare its performance through simulations and experiments with other control architectures.", "result": "The SE2(3) MPC demonstrated comparable performance to an SE2(3)-based LQR in simulations but achieved superior trajectory tracking and robustness in experimental evaluations on the Quanser QDrone platform.", "conclusion": "Lie group-based controllers, specifically the SE2(3) MPC, show practical effectiveness and improved performance, especially in trajectory-tracking and robustness, making them a valuable option for quadcopter control."}}
{"id": "2511.14899", "pdf": "https://arxiv.org/pdf/2511.14899", "abs": "https://arxiv.org/abs/2511.14899", "authors": ["Daniel Gilo", "Or Litany"], "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization", "categories": ["cs.CV"], "comment": null, "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.", "AI": {"tldr": "The study introduces InstructMix2Mix (I-Mix2Mix), a framework for multi-view image editing based on a diffusion model that ensures cross-view consistency and high-quality edits using innovative adaptations.", "motivation": "Current methods for multi-view image editing often fail to maintain consistency and produce artifacts, particularly when modifying sparse inputs from different viewpoints.", "method": "I-Mix2Mix adapts a multi-view diffusion model by replacing the neural field consolidator in SDS with a multi-view diffusion student, alongside incremental updates, a tailored teacher noise scheduler, and attention modifications for enhanced coherence.", "result": "I-Mix2Mix achieves significant improvements in cross-view consistency while delivering high-quality edits per frame.", "conclusion": "The proposed framework successfully overcomes the limitations of existing methods, offering robust and coherent multi-view image editing."}}
{"id": "2511.15196", "pdf": "https://arxiv.org/pdf/2511.15196", "abs": "https://arxiv.org/abs/2511.15196", "authors": ["David Yallup"], "title": "Particle Monte Carlo methods for Lattice Field Theory", "categories": ["stat.ML", "cs.LG", "hep-lat"], "comment": "To appear in the NeurIPS 2025 workshop, Frontiers in Probabilistic Inference: Sampling Meets Learning", "summary": "High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.", "AI": {"tldr": "The paper demonstrates that GPU-accelerated particle methods like Sequential Monte Carlo and nested sampling outperform neural samplers in both quality and time while efficiently estimating the partition function in high-dimensional multimodal sampling benchmarks from lattice field theory.", "motivation": "The paper aims to establish a classical baseline for machine learning-assisted sampling methods by showcasing the effectiveness of GPU-accelerated particle methods on lattice field theory benchmarks.", "method": "The authors employ Sequential Monte Carlo and nested sampling methods accelerated by GPU, coupled with a single data-driven covariance for tuning, to compare their performance with state-of-the-art neural samplers.", "result": "These particle methods achieve better sample quality, faster execution time, and accurate estimation of the partition function without incorporating problem-specific structure.", "conclusion": "The findings suggest that classical particle methods can be competitive and sometimes superior to state-of-the-art neural samplers, questioning whether learned proposals justify their training cost in these scenarios."}}
{"id": "2511.14887", "pdf": "https://arxiv.org/pdf/2511.14887", "abs": "https://arxiv.org/abs/2511.14887", "authors": ["Nathan M. Roberts", "Xiaosong Du"], "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone", "categories": ["cs.LG"], "comment": "Conference version with 12 pages and 2 figures", "summary": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.", "AI": {"tldr": "The paper proposes a transformer-guided deep reinforcement learning (DRL) approach to optimize eVTOL aircraft takeoff trajectories for minimal energy consumption and demonstrates its efficiency and accuracy compared to vanilla DRL.", "motivation": "The motivation is to address urban traffic congestion with energy-efficient eVTOL aircraft and overcome the limitations of conventional control methods and vanilla DRL, such as problem complexity and training difficulty.", "method": "The authors introduced a transformer-guided DRL method, where a transformer is used to explore realistic state space and reduce training complications when optimizing eVTOL takeoff trajectories.", "result": "The transformer-guided DRL reduced training time by 25% compared to vanilla DRL and achieved 97.2% energy consumption accuracy compared to an optimal reference, outperforming vanilla DRL in terms of efficiency and accuracy.", "conclusion": "The transformer-guided DRL proves to be a more effective solution compared to vanilla DRL for designing energy-efficient eVTOL takeoff trajectories while reducing training challenges and maintaining high accuracy."}}
{"id": "2511.15581", "pdf": "https://arxiv.org/pdf/2511.15581", "abs": "https://arxiv.org/abs/2511.15581", "authors": ["Kayo Tei", "Haruto Mishina", "Naoki Yamamoto", "Kazunori Ueda"], "title": "Graph Rewriting Language as a Platform for Quantum Diagrammatic Calculi", "categories": ["cs.PL"], "comment": "27 pages, 27 figures. Extended version (with Appendices) of the paper to be presented at the 28th International Symposium on Practical Aspects of Declarative Languages (PADL 2026), January 2026", "summary": "Systematic discovery of optimization paths in quantum circuit simplification remains a challenge. Today, ZX-calculus, a computing model for quantum circuit transformation, is attracting attention for its highly abstract graph-based approach. Whereas existing tools such as PyZX and Quantomatic offer domain-specific support for quantum circuit optimization, visualization and theorem-proving, we present a complementary approach using LMNtal, a general-purpose hierarchical graph rewriting language, to establish a diagrammatic transformation and verification platform with model checking. Our methodology shows three advantages: (1) manipulation of ZX-diagrams through native graph transformation rules, enabling direct implementation of basic rules; (2) quantified pattern matching via QLMNtal extensions, greatly simplifying rule specification; and (3) interactive visualization and validation of optimization paths through state space exploration. Through case studies, we demonstrate how our framework helps understand optimization paths and design new algorithms and strategies. This suggests that the declarative language LMNtal and its toolchain could serve as a new platform to investigate quantum circuit transformation from a different perspective.", "AI": {"tldr": "The paper introduces a novel platform using LMNtal for quantum circuit optimization and provides benefits in graph transformation, pattern matching, and interactive validation.", "motivation": "Currently, simplifying quantum circuits systematically is challenging. While tools like PyZX and Quantomatic tackle optimization and visualization, a new perspective is needed to expand the capabilities in ZX-calculus transformations.", "method": "The authors propose leveraging LMNtal, a hierarchical graph rewriting language, with extensions such as QLMNtal for quantified pattern matching, model checking, and advanced graphical manipulation.", "result": "Through case studies, the framework proves effective in optimizing and visually understanding quantum circuit transformation, as well as helping design new algorithms.", "conclusion": "LMNtal and its associated toolchain offer a promising platform to explore quantum circuit transformation, enhancing the development of optimization strategies and offering insights beyond existing approaches."}}
{"id": "2511.15069", "pdf": "https://arxiv.org/pdf/2511.15069", "abs": "https://arxiv.org/abs/2511.15069", "authors": ["Haoyong Wu", "Yongmei Liu"], "title": "ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.", "AI": {"tldr": "The paper introduces ProRAC, a framework using large language models (LLMs) to solve reasoning about actions and change (RAC) tasks efficiently, with impressive benchmark results.", "motivation": "To address the challenges in reasoning about actions and changes (RAC) by utilizing a neuro-symbolic approach, enabling effective understanding of actions and their progressive consequences.", "method": "ProRAC identifies key RAC elements, incrementally applies actions to transition states, and evaluates queries against the progressed state using large language models.", "result": "ProRAC achieves impressive results across multiple RAC benchmarks, domains, LLM architectures, and task types, showcasing its adaptability and effectiveness.", "conclusion": "ProRAC demonstrates a robust and generalizable solution for RAC problems, leveraging neuro-symbolic reasoning and LLM efficiency."}}
{"id": "2511.15408", "pdf": "https://arxiv.org/pdf/2511.15408", "abs": "https://arxiv.org/abs/2511.15408", "authors": ["Shanlin Zhou", "Xinpeng Wang", "Jianxun Lian", "Zhenghao Liu", "Laks V. S. Lakshmanan", "Xiaoyuan Yi", "Yongtao Hao"], "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA", "cs.NE"], "comment": "13 pages,9 figures. This work has been submitted to the IEEE for possible publication", "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.", "AI": {"tldr": "The paper introduces NAMeGEn, a novel framework for creative Chinese baby naming by addressing challenges in multi-objective flexibility and interpretive complexity. It improves short-form text generation that requires personalized constraints and aesthetic explanations.", "motivation": "Current Large Language Models (LLMs) struggle to manage personalized, fine-grained, and pluralistic user requirements (multi-objective flexibility) and lack interpretive abilities to offer meaningful insights for creative applications like short-form text generation.", "method": "NAMeGEn uses a multi-agent optimization framework alternating between objective extraction, name generation, and evaluation to address diverse user requirements. It utilizes a classical Chinese poetry corpus and introduces a new benchmark, CBNames.", "result": "NAMeGEn generates creative and personalized Chinese baby names while providing meaningful aesthetic explanations, outperforming six baseline methods across various LLMs without additional training.", "conclusion": "NAMeGEn demonstrates an effective approach for addressing creative challenges in CNLG tasks like Chinese baby naming, balancing user constraints and interpretive complexity while outperforming standard methods."}}
{"id": "2511.15491", "pdf": "https://arxiv.org/pdf/2511.15491", "abs": "https://arxiv.org/abs/2511.15491", "authors": ["Laurent Feuilloley", "Josef Erik Sedl\u00e1\u010dek", "Martin Sl\u00e1vik"], "title": "Proving there is a leader without naming it", "categories": ["cs.DC"], "comment": null, "summary": "Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.\n  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\\log n)$ and $O(\\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).\n  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $\u0398(n)$ bits in general graphs, but only $O(\\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]", "AI": {"tldr": "The paper investigates local certification for network properties, particularly leader election, and examines the influence of network structure on certification complexity.", "motivation": "To explore whether sublogarithmic size certifications can be achieved in networks without cycles and whether node identifiers are needed in such cases.", "method": "Analyzes leader certification in specific types of graph classes (e.g., small diameter graphs, chordal graphs, grids, dense graphs) while leveraging the impact of structural properties.", "result": "Provides results showing certification complexities for different graph structures, under the assumption that some do not involve cycle graphs.", "conclusion": "Graph structure plays a significant role in determining the efficiency and requirements of local certification methods for leader election."}}
{"id": "2511.14796", "pdf": "https://arxiv.org/pdf/2511.14796", "abs": "https://arxiv.org/abs/2511.14796", "authors": ["Adel Hidri", "Suleiman Ali Alsaif", "Muteeb Alahmari", "Eman AlShehri", "Minyar Sassi Hidri"], "title": "Opinion Mining and Analysis Using Hybrid Deep Neural Networks", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 4 figures, 11 tables", "summary": "Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.", "AI": {"tldr": "The paper proposes a hybrid deep learning model using BGRU and LSTM layers for sentiment analysis that has achieved superior accuracy and addressed key challenges such as contextual nuances and class imbalance.", "motivation": "The paper aims to enhance sentiment analysis in opinion mining by addressing challenges related to contextual nuances, scalability, and class imbalance in existing methods.", "method": "The study introduced a hybrid model combining bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers, tested on datasets like IMDB movie reviews and Amazon product evaluations.", "result": "The HBGRU-LSTM architecture achieved 95% testing accuracy, outperforming traditional frameworks (LSTM, CNN+LSTM, GRU+LSTM) along with improved recall for negative sentiments and reduced misclassification loss.", "conclusion": "The proposed hybrid model advances sentiment analysis with improved accuracy, generalization, and resilience, making it effective for handling contextual and class imbalance issues."}}
{"id": "2511.14825", "pdf": "https://arxiv.org/pdf/2511.14825", "abs": "https://arxiv.org/abs/2511.14825", "authors": ["Alexandre-Xavier Labont\u00e9-Lamoureux", "Simon Boyer"], "title": "Automatic Pipeline Provisioning", "categories": ["cs.SE"], "comment": null, "summary": "The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.", "AI": {"tldr": "The paper discusses the concept and benefits of automatic pipeline provisioning in software engineering.", "motivation": "To enhance efficiency in deploying pipelines by studying automatic provisioning methods.", "method": "The research focuses on CI pipelines while considering its potential applicability to CD pipelines.", "result": "The outcomes aim to highlight benefits such as quick deployment via automatic pipeline provisioning.", "conclusion": "Automatic provisioning methods can optimize pipeline deployment processes and suggest benefits for CD pipelines as well."}}
{"id": "2511.15105", "pdf": "https://arxiv.org/pdf/2511.15105", "abs": "https://arxiv.org/abs/2511.15105", "authors": ["Angshu Adhya", "Cindy Yang", "Emily Wu", "Rishad Hasan", "Abhishek Narula", "Patr\u00edcia Alves-Oliveira"], "title": "Painted Heart Beats", "categories": ["cs.RO"], "comment": "4 pages, 2 figures, ICRA 2025", "summary": "In this work we present AURA, a framework for synergistic human-artist painting. We developed a robot arm that collaboratively paints with a human artist. The robot has an awareness of the artist's heartbeat through the EmotiBit sensor, which provides the arousal levels of the painter. Given the heartbeat detected, the robot decides to increase proximity to the artist's workspace or retract. If a higher heartbeat is detected, which is associated with increased arousal in human artists, the robot will move away from that area of the canvas. If the artist's heart rate is detected as neutral, indicating the human artist's baseline state, the robot will continue its painting actions across the entire canvas. We also demonstrate and propose alternative robot-artist interactions using natural language and physical touch. This work combines the biometrics of a human artist to inform fluent artistic interactions.", "AI": {"tldr": "AURA is a framework where a robot arm collaboratively paints with a human artist, responding to their heartbeat and arousal levels.", "motivation": "To explore human-robot collaboration in intuitive and creative tasks like painting, incorporating biometrics for a natural interaction.", "method": "The robot arm detects the human artist's heartbeat using the EmotiBit sensor to gauge arousal levels and adjust its painting behavior accordingly. It also supports interaction via natural language and physical touch.", "result": "The robot adjusts its painting proximity and actions dynamically based on the artist's arousal levels, demonstrating a synchronized collaboration.", "conclusion": "Biometric-informed collaboration creates more fluent and context-aware interactions between robots and human artists."}}
{"id": "2511.14900", "pdf": "https://arxiv.org/pdf/2511.14900", "abs": "https://arxiv.org/abs/2511.14900", "authors": ["Zehao Liu", "Wejieying Ren", "Jipeng Zhang", "Tianxiang Zhao", "Jingxi Zhu", "Xiaoting Li", "Vasant G. Honavar"], "title": "Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.\n  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.", "AI": {"tldr": "The paper presents SkinR1, a vision-language model for improved dermatological diagnosis by combining textbook-based reasoning and reinforcement learning to address challenges of data inconsistency, lack of diagnostic rationale, and scalability.", "motivation": "To overcome limitations such as inconsistent diagnostic labels, lack of reliable reasoning supervision, and poor scalability of current vision-language models in clinical applications.", "method": "SkinR1 adopts an end-to-end framework with a reasoning generator for expert supervision, supervised fine-tuning for grounded reasoning, and a novel reinforcement learning paradigm for scalability and generalization.", "result": "Experiments on dermatology datasets exhibit high diagnostic accuracy, and ablation studies confirm the significance of reasoning foundations instilled by supervised fine-tuning.", "conclusion": "SkinR1 improves clinical reasoning and diagnostic accuracy, addressing major limitations of current VLMs in dermatological applications."}}
{"id": "2511.15315", "pdf": "https://arxiv.org/pdf/2511.15315", "abs": "https://arxiv.org/abs/2511.15315", "authors": ["Abdelhamid Ezzerg", "Ilija Bogunovic", "Jeremias Knoblauch"], "title": "Robust Bayesian Optimisation with Unbounded Corruptions", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.", "AI": {"tldr": "The paper addresses the vulnerability of Bayesian Optimization to extreme outliers by introducing a new adversary model and the RCGP-UCB algorithm, which is robust to corruptions of infinite magnitude and achieves sublinear regret.", "motivation": "To tackle the critical vulnerability of Bayesian Optimization to extreme outliers, especially when existing methods only account for a bounded cumulative corruption budget.", "method": "The authors developed a new adversary model that limits the frequency rather than the magnitude of corruptions. They proposed the RCGP-UCB algorithm using Robust Conjugate Gaussian Process and UCB, with stable and adaptive versions.", "result": "RCGP-UCB achieves sublinear regret even with up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions of infinite magnitude. Without outliers, it retains regret bounds comparable to standard GP-UCB.", "conclusion": "RCGP-UCB offers a robust framework for Bayesian Optimization, maintaining performance under severe outlier corruption and achieving competitive regret bounds in clean environments."}}
{"id": "2511.14889", "pdf": "https://arxiv.org/pdf/2511.14889", "abs": "https://arxiv.org/abs/2511.14889", "authors": ["Grace Kim", "Filip Svoboda", "Nicholas Lane"], "title": "Bringing Federated Learning to Space", "categories": ["cs.LG"], "comment": "15 pages, 9 figures, 3 tables accepted to IEEE Aeroconf 2026", "summary": "As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive \"space-ification\" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.", "AI": {"tldr": "This paper explores the application of Federated Learning (FL) in Low Earth Orbit (LEO) satellite constellations to overcome bandwidth limitations, introduces a framework to adapt algorithms for orbital constraints, and evaluates their performance in various configurations.", "motivation": "The need to address the downlink bandwidth limitation of LEO satellite constellations, as these networks grow to hundreds or thousands of spacecraft, motivates the application of distributed on-board machine learning via federated learning.", "method": "The authors developed a \"space-ified\" framework to adapt existing FL algorithms (FedAvg, FedProx, FedBuff) for the orbital environment, simulating their performance through extensive parameter sweeps across 768 constellation configurations.", "result": "The adapted algorithms demonstrate efficient scalability for constellations of up to 100 satellites, achieving near-centralized performance and speeding up training cycles by 9x through optimized scheduling and local coordination.", "conclusion": "The study provides critical insights for future mission designs, showcasing how distributed on-board learning can make satellite constellations more autonomous and data-efficient while overcoming bandwidth challenges."}}
{"id": "2511.15403", "pdf": "https://arxiv.org/pdf/2511.15403", "abs": "https://arxiv.org/abs/2511.15403", "authors": ["Isabel Amaral", "Alexandra Mendes", "Jos\u00e9 Campos"], "title": "MutDafny: A Mutation-Based Approach to Assess Dafny Specifications", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.\n  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.", "AI": {"tldr": "This paper introduces a mutation testing tool, MutDafny, to identify weaknesses in Dafny specifications and validate their formal correctness.", "motivation": "Specifications in verification-aware programming languages like Dafny can contain errors, which lead to formally verified programs potentially departing from intended behaviors.", "method": "The study employs mutation testing by introducing faults (mutations) into code and using formal specifications to detect them, adding synthesized mutation operators tailored for Dafny.", "result": "MutDafny was tested on 794 real-world Dafny programs, revealing five weak specifications needing improvement and demonstrating its reliability.", "conclusion": "MutDafny improves the reliability of Dafny specifications, showcasing opportunities for enhanced specification robustness in verification-aware programming languages."}}
{"id": "2511.15074", "pdf": "https://arxiv.org/pdf/2511.15074", "abs": "https://arxiv.org/abs/2511.15074", "authors": ["Henrik Bradland", "Morten Goodwin", "Vladimir I. Zadorozhny", "Per-Arne Andersen"], "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 4 figures, in review", "summary": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.", "AI": {"tldr": "Rogue One introduces an innovative LLM-based multi-agent framework for automatic feature extraction. It surpasses existing methods in predictive modeling for tabular data by integrating domain knowledge and a dynamic feedback mechanism.", "motivation": "Feature engineering for tabular data heavily influences machine learning performance, yet existing methods rely on constrained LLM architectures and lack systematic domain knowledge integration.", "method": "Rogue One uses a multi-agent approach with three agents (Scientist, Extractor, Tester) collaborating on iterative feature discovery, enhanced by qualitative feedback and external knowledge integration through a RAG system.", "result": "On 19 classification and 9 regression datasets, Rogue One outperformed state-of-the-art techniques and identified new hypotheses, such as a biomarker in a biomedical dataset.", "conclusion": "Rogue One offers statistically robust, interpretable features and fosters scientific discovery, marking a significant advancement in automatic feature extraction methodologies."}}
{"id": "2511.15031", "pdf": "https://arxiv.org/pdf/2511.15031", "abs": "https://arxiv.org/abs/2511.15031", "authors": ["Yifan Cai", "Linh Thi Xuan Phan"], "title": "GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems", "categories": ["cs.CR", "cs.DC", "eess.SY"], "comment": null, "summary": "Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.\n  We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.", "AI": {"tldr": "GeoShield presents a resource-efficient solution for geo-distributed cyber-physical systems (CPS) suffering from Byzantine faults, focusing on detection and recovery within bounded time rather than fault masking.", "motivation": "Existing solutions for fault tolerance in CPS either require excessive resources or only provide late guarantees, which are unsuitable for resource-constrained and real-time systems.", "method": "GeoShield detects faults and initiates recovery within bounded time instead of masking faults. It includes protocols for detecting malicious message delays and omission faults under unreliable networks, and employs recovery mechanisms to maintain safety and maximize robustness.", "result": "Evaluations based on real-world case studies demonstrate GeoShield's superior resource efficiency and effectiveness compared to existing methods.", "conclusion": "GeoShield is a pioneering solution enabling bounded-time recovery for CPS without depending on trusted hardware, offering enhanced safety and operational efficiency under network unreliability."}}
{"id": "2511.14868", "pdf": "https://arxiv.org/pdf/2511.14868", "abs": "https://arxiv.org/abs/2511.14868", "authors": ["Xueying Ding", "Xingyue Huang", "Mingxuan Ju", "Liam Collins", "Yozen Liu", "Leman Akoglu", "Neil Shah", "Tong Zhao"], "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.", "AI": {"tldr": "Hierarchical Token Prepending (HTP) enhances long-document embeddings by addressing information compression and readout issues in large language models.", "motivation": "Improve the representation quality of text embeddings produced by large language models, particularly for long documents.", "method": "HTP partitions input into blocks and prepends summary tokens to avoid over-compression. It also employs mean-pooling instead of last-token pooling for better information representation.", "result": "HTP achieves better performance across multiple retrieval datasets and embedding benchmarks, especially for long contexts.", "conclusion": "HTP boosts embedding quality in both zero-shot and fine-tuned models, providing a scalable solution for long-document embeddings."}}
{"id": "2511.14967", "pdf": "https://arxiv.org/pdf/2511.14967", "abs": "https://arxiv.org/abs/2511.14967", "authors": ["Basel Shbita", "Farhan Ahmed", "Chad DeLuca"], "title": "MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.", "AI": {"tldr": "This paper introduces MermaidSeqBench, a benchmark to evaluate large language models' performance in generating Mermaid sequence diagrams from text prompts.", "motivation": "There is a lack of benchmarks to systematically evaluate LLMs\u2019 correctness in generating sequence diagrams using Mermaid syntax.", "method": "The benchmark includes 132 human-verified and LLM-extended samples created through human annotation, in-context LLM prompting, and rule-based variation. Evaluations utilize LLM-as-a-judge for fine-grained metrics.", "result": "Initial evaluations show varying performances across state-of-the-art LLMs, revealing capability gaps in diagram generation and evaluation methods.", "conclusion": "MermaidSeqBench lays the groundwork for advancing structured diagram generation research and refining evaluation methods for LLMs."}}
{"id": "2511.15194", "pdf": "https://arxiv.org/pdf/2511.15194", "abs": "https://arxiv.org/abs/2511.15194", "authors": ["Jian Deng", "Yuandong Wang", "Yangfu Zhu", "Tao Feng", "Tianyu Wo", "Zhenzhou Shao"], "title": "Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 4 figures and 3 tables", "summary": "Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.", "AI": {"tldr": "The paper introduces Eq.Bot, a universal framework to address spatial transformation issues in robotic manipulation systems without modifying existing architectures. It improves performance significantly.", "motivation": "Existing robotic manipulation systems struggle with spatial transformations like rotations and translations due to a lack of geometric consistency in multi-modal learning frameworks. This paper aims to address these shortcomings.", "method": "The authors propose Eq.Bot, leveraging SE(2) group equivariant theory. It involves canonicalizing observations, applying existing policies, and mapping results back to original spaces. This approach works without modifying existing architectures.", "result": "Experiments show that Eq.Bot achieves superior performance on robotic manipulation tasks using CNN-based and Transformer-based architectures, showing improvements up to 50.0%.", "conclusion": "Eq.Bot effectively addresses spatial transformation challenges, is universally applicable to different architectures, and significantly enhances robotic manipulation task performance without adding architectural complexity."}}
{"id": "2511.14901", "pdf": "https://arxiv.org/pdf/2511.14901", "abs": "https://arxiv.org/abs/2511.14901", "authors": ["Zhenshi Li", "Weikang Yu", "Dilxat Muhtar", "Xueliang Zhang", "Pengfeng Xiao", "Pedram Ghamisi", "Xiao Xiang Zhu"], "title": "FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding", "categories": ["cs.CV"], "comment": null, "summary": "As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.", "AI": {"tldr": "The paper identifies limitations in aligning fine-grained details for remote sensing (RS) CLIP models and addresses them by proposing FarSLIP\u2014a new framework using a refined dataset (MGRS-200k) to enable better vision-language alignment and spatial awareness.", "motivation": "Improving fine-grained region-text alignment in RS tasks due to limitations in CLIP-derived frameworks for capturing spatial awareness in remote sensing data.", "method": "Construct MGRS-200k dataset for RS image-text alignment and propose FarSLIP framework with patch-to-patch distillation combined with region-category alignment.", "result": "FarSLIP improves discriminability, preserves semantic coherence, and achieves state-of-the-art results in RS open-vocabulary segmentation, zero-shot classification, and image-text retrieval.", "conclusion": "FarSLIP successfully enhances fine-grained vision-language alignment in the RS domain, addressing spatial awareness challenges, and sets new benchmarks."}}
{"id": "2511.15332", "pdf": "https://arxiv.org/pdf/2511.15332", "abs": "https://arxiv.org/abs/2511.15332", "authors": ["The Tien Mai"], "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.\n  Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso", "AI": {"tldr": "The Exponential Lasso introduces a robust modification to the traditional Lasso method by using an exponential-type loss function, making it resistant to outliers and heavy-tailed noise.", "motivation": "Lasso is sensitive to outliers and heavy-tailed noise due to its reliance on the squared loss function, leading to biased estimates and poor model selection under these conditions.", "method": "The Exponential Lasso incorporates an exponential-type loss function into the Lasso framework and employs a Majorization-Minimization (MM) algorithm for optimization, addressing robustness and efficiency simultaneously.", "result": "The Exponential Lasso matches classical Lasso in ideal Gaussian settings and outperforms it in contaminated data scenarios, as demonstrated in numerical experiments.", "conclusion": "The integration of the exponential loss in Lasso provides a robust, computationally efficient alternative suitable for high-dimensional data analysis, with its implementation accessible in the R package 'heavylasso' on GitHub."}}
{"id": "2511.14903", "pdf": "https://arxiv.org/pdf/2511.14903", "abs": "https://arxiv.org/abs/2511.14903", "authors": ["Ruixin Zhang", "Jon Donnelly", "Zhicheng Guo", "Ghazal Khalighinejad", "Haiyang Huang", "Alina Jade Barnett", "Cynthia Rudin"], "title": "It's LIT! Reliability-Optimized LLMs with Inspectable Tools", "categories": ["cs.LG", "cs.SE"], "comment": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Multi-Turn Interactions in Large Language Models", "summary": "Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.", "AI": {"tldr": "The paper proposes a framework, LIT (LLMs with Inspectable Tools), to force large language models (LLMs) to solve problems using external, reliable tools, ensuring trustworthy and transparent reasoning. A benchmark dataset and reliability cost functions are introduced to assess performance.", "motivation": "LLMs often rely on opaque reasoning, which limits their trustworthiness in high-stakes scenarios. Unreliable and non-transparent solutions restrict their applicability in domains demanding high reliability and troubleshootability.", "method": "The authors present the LIT framework, allowing LLMs to select reliable solution paths using external tools and sequential tool-calling. A benchmark dataset of 1,300 questions is designed alongside reliability cost functions to represent tool robustness and troubleshootability.", "result": "Using LIT, LLMs demonstrate improved reliability and informed problem-solving across mathematical, coding, and modeling tasks without compromising task performance.", "conclusion": "The LIT framework ensures LLMs deliver trustworthy, reliable, and transparent solutions by selecting external tools for problem-solving, enhancing their applicability in complex real-world tasks."}}
{"id": "2511.15169", "pdf": "https://arxiv.org/pdf/2511.15169", "abs": "https://arxiv.org/abs/2511.15169", "authors": ["Xin Gao", "Shaohan Yu", "Zerui Chen", "Yueming Lyu", "Weichen Yu", "Guanghao Li", "Jiyao Liu", "Jianxiong Gao", "Jian Liang", "Ziwei Liu", "Chenyang Si"], "title": "SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models", "categories": ["cs.AI"], "comment": "30 pages, 8 figures", "summary": "Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.", "AI": {"tldr": "SafeRBench is introduced as a benchmark for assessing the safety of Large Reasoning Models (LRMs) by analyzing their reasoning traces and outputs for risks.", "motivation": "Existing safety evaluations of LRMs often miss potential risks present in the reasoning process, focusing mainly on final output-level judgments.", "method": "Three main contributions: (1) Input design reflecting diverse harm gradients; (2) Micro-thought chunking to segment reasoning into units for fine-grained evaluation; (3) Human annotations validating safety judgments of LLM assessments.", "result": "SafeRBench was tested on 19 LRMs to enable detailed and multidimensional safety analysis, with comprehensive insights into risks and protective measures.", "conclusion": "SafeRBench provides a robust end-to-end framework for evaluating and improving LRM safety in reasoning processes and outputs."}}
{"id": "2511.15278", "pdf": "https://arxiv.org/pdf/2511.15278", "abs": "https://arxiv.org/abs/2511.15278", "authors": ["Nilesh Vyas", "Benjamin Zhao", "Ayg\u00fcn Baltaci", "Gustavo de Carvalho Bertoli", "Hassan Asghar", "Markus Kl\u00fcgel", "Gerrit Schramm", "Martin Kubisch", "Dali Kaafar"], "title": "Privacy-Preserving IoT in Connected Aircraft Cabin", "categories": ["cs.CR", "cs.DC", "cs.NI"], "comment": "8 pages, 2 figures. Under review", "summary": "The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.", "AI": {"tldr": "The paper proposes and evaluates a framework that integrates Privacy-Enhancing Technologies (PETs) in IoT systems, focusing on privacy gaps in environments like modern aircraft cabins.", "motivation": "With the increasing use of IoT devices in shared environments, there's a need to address challenges of passenger privacy, vendor intellectual property protection, and regulatory compliance, which current secure protocols like CSMIM fail to fully handle at the application layer.", "method": "The researchers implemented PETs, specifically Differential Privacy and additive secret sharing, atop a secure communication protocol and conducted an empirical analysis in a high-fidelity testbed environment with constrained hardware to evaluate various trade-offs.", "result": "The study found that PETs introduce negligible computational overhead compared to network latencies. Additionally, system architecture choices like on-device versus virtualized processing greatly influence performance.", "conclusion": "The proposed PET framework offers practical guidance for architects designing IoT systems, enabling secure and trustworthy collaboration in avionics and other critical areas, while maintaining strong privacy measures."}}
{"id": "2511.15005", "pdf": "https://arxiv.org/pdf/2511.15005", "abs": "https://arxiv.org/abs/2511.15005", "authors": ["Moses Kiprono"], "title": "Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, theoretical/mathematical LLM research, no figures, intended for peer-reviewed journal", "summary": "Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.", "AI": {"tldr": "This paper proposes a mathematical approach to understand, quantify, and reduce hallucinations in Large Language Models.", "motivation": "The motivation is to address the susceptibility of Large Language Models to producing hallucinations, or outputs that are incorrect or unsupported, even when they seem plausible.", "method": "The authors rely on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation. They analyze the error propagation in LLMs and propose refined uncertainty metrics and mitigation techniques, including contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention.", "result": "The study introduces a unified framework linking models\u2019 calibration, retrieval, and alignment processes to mitigate hallucinations effectively.", "conclusion": "This paper contributes to making LLMs safer and more reliable by providing tools and strategies to understand and reduce their hallucination tendencies."}}
{"id": "2511.15007", "pdf": "https://arxiv.org/pdf/2511.15007", "abs": "https://arxiv.org/abs/2511.15007", "authors": ["Shehan I Pranto", "Brett Fassler", "Md Rafi Islam", "Ashley Schenkel", "Larry W Hawk", "Edward Sazonov"], "title": "FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor", "categories": ["cs.SE"], "comment": null, "summary": "Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.", "AI": {"tldr": "The paper introduces FRIENDS GUI, a Python-based tool for analyzing puffing behavior data from the FRIENDS device, validated for accuracy and available on GitHub.", "motivation": "To improve the evaluation of ENDS use and toxicant exposure by developing better tools for analyzing puffing topography (PT).", "method": "Authors developed FRIENDS GUI, an open-source Python tool, to extract, decode, and visualize ENDS puffing data obtained using the FRIENDS device.", "result": "The FRIENDS GUI successfully validated using experimental data, demonstrating accurate timestamp conversion, event decoding, and behavioral data visualization.", "conclusion": "FRIENDS GUI enhances the usability of FRIENDS device data and is publicly available, aiding research into ENDS use and supporting regulatory assessments."}}
{"id": "2511.15200", "pdf": "https://arxiv.org/pdf/2511.15200", "abs": "https://arxiv.org/abs/2511.15200", "authors": ["Tairan He", "Zi Wang", "Haoru Xue", "Qingwei Ben", "Zhengyi Luo", "Wenli Xiao", "Ye Yuan", "Xingye Da", "Fernando Casta\u00f1eda", "Shankar Sastry", "Changliu Liu", "Guanya Shi", "Linxi Fan", "Yuke Zhu"], "title": "VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation", "categories": ["cs.RO"], "comment": "Project website: https://viral-humanoid.github.io/", "summary": "A key barrier to the real-world deployment of humanoid robots is the lack of autonomous loco-manipulation skills. We introduce VIRAL, a visual sim-to-real framework that learns humanoid loco-manipulation entirely in simulation and deploys it zero-shot to real hardware. VIRAL follows a teacher-student design: a privileged RL teacher, operating on full state, learns long-horizon loco-manipulation using a delta action space and reference state initialization. A vision-based student policy is then distilled from the teacher via large-scale simulation with tiled rendering, trained with a mixture of online DAgger and behavior cloning. We find that compute scale is critical: scaling simulation to tens of GPUs (up to 64) makes both teacher and student training reliable, while low-compute regimes often fail. To bridge the sim-to-real gap, VIRAL combines large-scale visual domain randomization over lighting, materials, camera parameters, image quality, and sensor delays--with real-to-sim alignment of the dexterous hands and cameras. Deployed on a Unitree G1 humanoid, the resulting RGB-based policy performs continuous loco-manipulation for up to 54 cycles, generalizing to diverse spatial and appearance variations without any real-world fine-tuning, and approaching expert-level teleoperation performance. Extensive ablations dissect the key design choices required to make RGB-based humanoid loco-manipulation work in practice.", "AI": {"tldr": "The paper presents VIRAL, a sim-to-real framework that enables humanoid robots to perform autonomous loco-manipulation tasks by leveraging large-scale simulation and visual domain randomization, deployed without real-world fine-tuning.", "motivation": "Address the challenge of autonomous loco-manipulation skills for real-world deployment of humanoid robots.", "method": "VIRAL utilizes a teacher-student framework where the teacher learns using privileged RL in simulation, and the student applies vision-based policies distilled through large-scale simulation. It employs extensive visual domain randomization and real-to-sim alignment.", "result": "The developed RGB-based policy enables continuous loco-manipulation for up to 54 cycles on a Unitree G1 humanoid robot, achieving generalization across various spatial and visual variations without fine-tuning.", "conclusion": "Large-scale simulation and visual domain randomization are essential for reliable humanoid loco-manipulation policies capable of high-performance sim-to-real transfer in autonomous robots."}}
