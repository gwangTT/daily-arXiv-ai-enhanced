<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.SE](#cs.SE) [Total: 11]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.CR](#cs.CR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: The paper critiques the linear model of AI progress, introduces 'familiar' versus 'strange' intelligence concepts, and advocates a nonlinear model of general intelligence, highlighting implications for testing AI capabilities.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the linear model of AI progress and better analyze AI systems' performance variability.

Method: The authors expand Susan Schneider's critique, introduce concepts of 'familiar' and 'strange' intelligence, and develop a nonlinear model of intelligence that assesses AI's varied capacities across multiple environments and goal types.

Result: The nonlinear model highlights that AI systems can display superhuman performance in some domains while failing in tasks requiring seemingly basic competence, emphasizing the non-linear nature of AI abilities.

Conclusion: AI intelligence defies simple linear metrics; adversarial testing must account for these nonlinear patterns when evaluating capabilities, as task-specific performance cannot fully represent general intelligence.

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [2] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead, a structure-aware, multi-turn reasoning agent, is introduced to enhance long-document question answering by leveraging document hierarchies and sequential discourse structure.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing agentic search frameworks in handling long documents, which often disregard hierarchical organization and discourse structure.

Method: DeepRead utilizes an LLM-based OCR model to convert PDFs into structured Markdown to preserve sections and paragraph boundaries. It further indexes content with metadata and equips the model with two tools: Retrieve for finding relevant paragraphs and ReadSection for sequential reading.

Result: DeepRead outperforms traditional agentic search frameworks in document question answering tasks and demonstrates human-like reasoning and reading behavior.

Conclusion: Incorporating document-native structures into retrieval and reading improves long-document question answering and highlights the need for structure-aware approaches in similar tasks.

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [3] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: Existing studies on Large Language Model inference energy consumption overlook non-linear dependencies, revealing efficiency 'Sweet Spots' for sequence lengths. Study proposes analytical model verified with tests achieving a low mean error of 1.79%, recommending strategies to optimize energy use.


<details>
  <summary>Details</summary>
Motivation: Understanding and optimizing energy consumption in LLM inference due to its importance in modern AI applications.

Method: Developed an analytical model based on Transformer computational and memory-access complexities, tested over diverse LLMs on NVIDIA H100 GPUs with varied sequence lengths to evaluate energy efficiency.

Result: Achieved mean absolute percentage error of 1.79% in predicting energy efficiency patterns, uncovering specific sequence length 'Sweet Spots' to minimize energy usage.

Conclusion: Energy usage can be significantly optimized by aligning sequence lengths to identified efficiency zones, aiding strategic input truncation, summarization, and adaptive generation approaches.

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [4] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: This paper introduces the Minimal Information Neuro-Symbolic Tree (MINT) framework for AI agents to elicit optimal human inputs in object-driven planning under incomplete information scenarios, improving rewards and success rates in joint human-AI planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of knowledge gaps in human-AI joint planning, which hinder optimal interactions and planning outcomes in open-world tasks.

Method: MINT uses symbolic reasoning and neural planning policy to understand uncertainties due to knowledge gaps. It incorporates self-play and LLMs to refine elicitation strategies and generates optimal queries for humans.

Result: MINT-based planning achieves near-expert performance with limited questions per task, showing improved rewards and success in benchmarks with increasing realism.

Conclusion: MINT enables AI agents to achieve effective planning by optimally interacting with humans to bridge knowledge gaps, confirming its success in realistic scenarios.

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [5] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: The study evaluates Large Language Models (LLMs) in advanced graph theory tasks, finding strong performance on solved problems but limited effectiveness on open, unsolved ones.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs in supporting rigorous mathematical thinking and their integration into computer science education, particularly graph theory.

Method: The authors tested LLMs on two graph theory problems (one solved, one unsolved) via an eight-stage evaluation protocol involving interpretation, exploration, proof construction, and validation.

Result: The LLM excelled in solving the known problem by generating valid proofs but struggled with the open problem, showing coherent strategies yet no advancement towards solutions.

Conclusion: LLMs are effective for exploring established concepts in math but not for novel insights or deep reasoning; students should use them for learning but verify results independently.

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [6] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: The paper emphasizes the necessity of uncertainty quantification (UQ) frameworks tailored for interactive large language model (LLM) agents and proposes a novel approach to address this.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the gap in UQ research, which currently focuses on single-turn question-answering. There's a need to shift focus to more realistic interactive LLM agent settings for greater applicability and safety in complex tasks.

Method: The authors propose a new perspective of UQ as a conditional uncertainty reduction process, explicitly accounting for the interactivity of actions in an agent's trajectory.

Result: The paper outlines a conceptual framework for designing UQ in LLM agent settings, providing a foundational structure for practical application and further research.

Conclusion: Emphasizing the importance of this new UQ approach for future LLM advancements, the study also highlights open problems and the potential practical benefits in domain-specific applications.

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [7] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: The paper presents a reinforcement learning-based framework for active debris removal missions using small satellites, optimizing collision avoidance, fuel efficiency, and mission planning.


<details>
  <summary>Details</summary>
Motivation: To address the increasing danger of orbital debris and the challenges of multi-debris removal with minimal risk of in-orbit collisions.

Method: The authors use a reinforcement learning framework, employing masked Proximal Policy Optimization (PPO) algorithms to enable adaptive and fuel-efficient collision avoidance and optimized debris rendezvous.

Result: The RL framework demonstrated reduced collision risk and improved efficiency compared to traditional heuristic methods, verified through simulations with the Iridium 33 dataset.

Conclusion: The study offers a scalable RL solution for planning complex ADR missions and highlights its broader applicability to other multi-target rendezvous scenarios in autonomous space missions.

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [8] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: This paper evaluates the clinical validity of VERA-MH, an automated AI safety benchmark for assessing the safety of AI in handling mental health issues, particularly suicide risk detection.


<details>
  <summary>Details</summary>
Motivation: To address the pressing need for safe and effective AI tools in mental health, particularly for suicide detection and response, given the increasing use of AI chatbots for psychological support.

Method: The study simulated large-scale conversations between AI chatbots and LLM-based user-agents. Licensed mental health clinicians and an LLM-based judge evaluated these conversations using a rubric to assess safety, alignment, and realism.

Result: Clinicians showed strong consistency in safety ratings, and the LLM judge aligned well with clinical consensus. Clinicians also perceived user-agents as realistic, validating VERA-MH as a reliable safety evaluation tool.

Conclusion: VERA-MH is a valid and reliable automated benchmark for assessing the safety of AI in mental health settings. Future studies will explore its broader applicability and robustness.

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [9] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: The paper evaluates three planning approaches for Active Debris Removal missions, focusing on balancing efficiency, adaptability, and feasibility within constraints.


<details>
  <summary>Details</summary>
Motivation: Improving planning methods for Active Debris Removal missions is critical to meet efficiency, flexibility, and constraint requirements, particularly to manage the increasing challenge of orbital debris.

Method: The study compares three planners: a nominal Masked PPO, a domain-randomized Masked PPO for robustness, and Monte Carlo Tree Search, tested in a realistic orbital simulation with variable conditions.

Result: Nominal PPO performs best under training conditions but struggles with changes, domain-randomized PPO enhances adaptability at a slight performance cost, and MCTS adapts well but at a high computational cost.

Conclusion: There is a trade-off between speed and adaptability; combining diverse training with online planning shows promise for robustly managing future ADR missions.

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [10] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: This paper introduces GAMMS, a lightweight and extensible simulation framework for multi-agent systems that emphasizes scalability, integration, ease of use, visualization, and realistic applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a scalable and accessible simulation tool for multi-agent coordination and intelligent systems, addressing limitations of existing high-fidelity simulators.

Method: The authors developed GAMMS, leveraging graph-based environments to enable efficient simulations. It integrates external tools, supports various policy types, and includes visualization capabilities.

Result: GAMMS enables fast and efficient simulation of complex domains, lowers barriers for researchers, and supports diverse agents and methodologies, including reinforcement learning and adversarial models.

Conclusion: GAMMS provides a practical, open-source framework to advance experimentation and innovation in multi-agent systems and other related fields.

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [11] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: The paper proposes a structured framework to evaluate the reasoning quality and stability of large language models (LLMs) in merchant risk assessment tasks and reveals biases in their evaluation processes.


<details>
  <summary>Details</summary>
Motivation: The reliability and bias of LLMs used as evaluators in high-stakes payment-risk settings need deeper investigation to ensure their outputs align with human judgment and improve operational fairness.

Method: The framework combines a five-criterion rubric, multi-evaluator Monte-Carlo scoring, and a consensus-deviation metric to assess rationale quality, evaluator stability, and bias. Multiple LLMs are evaluated under attributed and anonymized conditions, supplemented by human expert evaluation and real-world payment data validation.

Result: Findings show heterogeneity in LLM bias, with anonymization reducing bias by 25.8%. GPT-5.1 and Claude 4.5 Sonnet negatively self-evaluate, aligning more with human judgment, while other models show positive bias. Real-world data confirms statistically significant alignment of models with quality metrics.

Conclusion: The framework offers a standard methodology for evaluating LLM judges in payment-risk workflows, identifying biases, and emphasizing the need for bias-aware protocols in operational financial systems.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: The paper introduces RISC-V extensions (Zicfiss and Zicfilp) for enhancing Control-Flow Integrity (CFI), including hardware units that protect against control-flow hijacking attacks.


<details>
  <summary>Details</summary>
Motivation: Develop hardware-backed security mechanisms to address control-flow hijacking vulnerabilities in programs by leveraging RISC-V architecture.

Method: Designed, integrated, and evaluated two independent hardware units for forward-edge and backward-edge control in a CVA6 RISC-V core using shadow stack and landing pad techniques.

Result: The design shows 1.0% area overhead and up to 15.6% performance overhead when tested on a 22 nm FDX technology and automotive benchmarks.

Conclusion: The proposed extensions effectively enhance control-flow security with manageable overhead, shared as open-source for wider usage.

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [13] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: This study introduces COFFEE, a carbon modeling framework for hafnium-zirconium-oxide ferroelectric non-volatile memory (FeFET eNVMs), evaluating their environmental impact over their lifecycle in comparison to traditional memory technologies.


<details>
  <summary>Details</summary>
Motivation: Information and communication technologies increasingly contribute to global environmental impacts. There is a need to quantify the lifecycle footprint of emerging energy-efficient technologies, like non-volatile memories, to ensure sustainable computing.

Method: The study uses real semiconductor fabrication data to model embodied carbon and architecture-level tools to analyze operational carbon and performance across memory capacities. It compares HZO-based FeFETs with traditional memory solutions like CMOS and SRAM.

Result: HZO-based FeFETs demonstrate a reduction in embodied carbon per MB capacity compared to SRAM, while slightly increasing embodied carbon per unit area compared to CMOS. For edge ML accelerators, the use of FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

Conclusion: FeFET-based eNVMs represent a promising solution for energy-efficient and sustainable computing, significantly lowering the environmental footprint of hardware systems over their lifecycle.

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [14] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: The paper introduces a flexible FP8 DCIM accelerator with innovative solutions to support FP8's variable precision, achieving high efficiency and flexibility in Transformer computations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in existing DCIM architectures that struggle with variable FP8 precision due to unified alignment strategies and fixed-precision MAC units.

Method: The authors propose three innovations: (1) Dynamic Shift-Aware Bitwidth Prediction (DSBP) for adaptive precision adjustment; (2) FIFO-based Input Alignment Unit (FIAU) to simplify operations; (3) Precision-Scalable INT MAC Array for flexible weight precision.

Result: The design implemented in 28nm CMOS achieved 20.4 TFLOPS/W for fixed E5M7 and showed 2.8x higher efficiency than previous works. The DSBP outperformed fixed bitwidth mode in efficiency (at the same accuracy) on datasets like BoolQ and Winogrande.

Conclusion: The proposed accelerator supports all FP8 formats while enabling efficient and accurate Transformer inference and training. It offers significant improvements in efficiency with a configurable balance between accuracy and performance.

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: The study introduces a framework analyzing human navigation in semantic spaces using trajectories in embedding models to understand and quantify semantic dynamics for applications like clinical and linguistic analysis.


<details>
  <summary>Details</summary>
Motivation: To understand how humans retrieve and navigate meaning within semantic spaces by modeling this process as structured trajectories in embedding models, aiming to bridge cognitive modeling with computational tools.

Method: The authors use transformer-based text embedding models to create participant-specific semantic trajectories and analyze them using geometric and dynamic metrics like distance, entropy, velocity, and acceleration across four datasets involving different tasks and languages.

Result: The framework effectively distinguishes between clinical groups and concept types, with cumulative embeddings showing better results for longer trajectories and similar outcomes across different embedding models.

Conclusion: Semantic navigation can be modeled as structured trajectories through embedding spaces, providing a computational approach for studying semantic dynamics with implications in clinical, cross-linguistic, and artificial cognition research.

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [16] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: The paper introduces BioACE, an automated framework for evaluating biomedical answers and their supporting citations against facts, considering completeness, correctness, precision, and recall.


<details>
  <summary>Details</summary>
Motivation: With the growing usage of large language models generating biomedical answers, there is an urgent need for methods to verify answer quality and consistency with reliable references due to the intricate medical terminology.

Method: BioACE framework leverages aspects like completeness, precision, correctness, and recall of answers, and incorporates techniques like natural language inference and pre-trained LLMs for citation quality evaluation.

Result: Experiments show the framework correlates well with human evaluations, and BioACE identifies the best approaches for biomedical answers and citation evaluations.

Conclusion: BioACE proves to be effective and provides tools for evaluating biomedical answers/citations, aiming for reliable assessment with automated methods.

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [17] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

TL;DR: CoWork-X framework addresses real-time coordination and long-term adaptation in language-conditioned agents, achieving improved performance and reduced latency.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle with real-time cooperative tasks due to either latency issues or inefficient post-episode enhancements.

Method: CoWork-X integrates a Skill-Agent that utilizes an HTN-based skill retrieval and a Co-Optimizer for post-episode skill modifications.

Result: Experiments in Overcooked-AI-like benchmarks indicate cumulative performance improvements with reduced latency and token usage.

Conclusion: The framework effectively balances rapid task execution and collaboration quality while adhering to strict computational budgets.

Abstract: Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [18] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: Multilingual language models (LMs) often under-perform compared to monolingual counterparts in tasks requiring precise semantics, such as lexical disambiguation.


<details>
  <summary>Details</summary>
Motivation: To understand why multilingual language models show reduced performance compared to monolingual models, focusing on constrained capacity issues.

Method: The study uses datasets of human relatedness judgments for ambiguous English and Spanish words to compare performance of monolingual and multilingual LMs and assesses constraints like embedding isotropy, attentional focus, and vocabulary segmentation.

Result: Multilingual LMs consistently underperform monolingual LMs and exhibit reduced embedding isotropy, attentional limitations, and issues with vocabulary segmentation.

Conclusion: Multilingual language models are hindered by multiple capacity constraints which contribute to their lower performance in lexical disambiguation tasks.

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [19] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: The paper introduces Locas, a parametric memory mechanism for transformers, designed for efficient continual learning with minimal additional parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of test-time training and continual learning by introducing a parametric memory system that integrates and offloads model parameters efficiently, minimizing catastrophic forgetting.

Method: Proposed Locas, a Locally-Supported parametric memory, with two variants: one using a two-layer MLP design and another utilizing a GLU-FFN structure for efficient continual learning. The initialization reuses model parameters, activations, and gradients for fast convergence and better generalization.

Result: Validated Locas on tasks like PG-19 language modeling and LoCoMo dialogue QA. Locas-GLU incorporated parametric memory with as little as 0.02% additional parameters, showing minimized catastrophic forgetting while maintaining performance.

Conclusion: Locas enables models to integrate and remember past context effectively while preserving prior knowledge, making it a promising tool for continual learning challenges.

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [20] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: This paper addresses data scarcity for generative AI by proposing a method called Data Kernel Perspective Space (DKPS) to assess synthetic data quality produced by LLMs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of limited labeled training data, which hinders the development of effective language technology models.

Method: The authors propose DKPS, a mathematical framework providing statistical guarantees for assessing synthetic data quality generated by transformer models.

Result: The paper demonstrates DKPS's derivation, its ability to yield performance guarantees, and its application in analyzing downstream tasks like neural machine translation and Contrastive Preference Optimization.

Conclusion: DKPS can provide insights into the synthetic data performance, aiding LLM engineers with reliable mathematical tools, though limitations and future research directions are highlighted.

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [21] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: This paper introduces a multimodal method combining text and audio for implicit discourse relation classification across English, French, and Spanish, focusing on cross-lingual and low-resource language scenarios.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenge of implicit discourse relation classification by leveraging cues that are often distributed across modalities and languages, which are not fully captured by text alone.

Method: The paper proposes a multimodal approach using Qwen2-Audio to jointly model textual and acoustic data, constructing a multilingual dataset for English, French, and Spanish.

Result: Text-based models perform better than audio-based ones alone, but integrating text and audio boosts accuracy, and cross-lingual transfer benefits classification in low-resource languages.

Conclusion: Combining modalities and enabling cross-lingual transfer improves implicit discourse relation classification, demonstrating the advantage of multimodal approaches particularly for low-resource scenarios.

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [22] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: The paper introduces GreekMMLU, a benchmark dataset designed specifically for evaluating large language models (LLMs) in Greek, using native-sourced questions that avoid translation artifacts.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of reliable evaluation benchmarks for Greek LLMs, as existing datasets are predominantly translations from English, which fail to capture Greek-specific linguistic and cultural features.

Method: The researchers developed GreekMMLU, a dataset with 21,805 native-sourced multiple-choice questions across 45 topics, organized by a new subject taxonomy, with difficulty levels ranging from primary education to professional exams. They made most of the dataset public while reserving a subset for a private leaderboard.

Result: Evaluations of over 80 LLMs show notable gaps in performance between advanced proprietary models and open-weight models, as well as between Greek-adapted and general multilingual models. The paper also identifies factors like model scale, adaptation, and prompts that influence Greek LLM performance.

Conclusion: GreekMMLU enables more reliable and culturally relevant evaluation for Greek LLMs, and the findings highlight the need for improved model specialization and adaptation to Greek-specific linguistic characteristics.

Abstract: Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [23] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: The paper assesses the risks posed by malicious language models in multi-model collaborations and proposes mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To address the growing safety risks from malicious LMs in decentralized collaborative systems.

Method: The authors classify malicious LMs into four categories, test them in four types of collaborative systems across ten datasets, and propose external supervisors for mitigation.

Result: Malicious models significantly deteriorate reasoning and safety domain performance, with mitigation strategies recovering 95.31% system performance.

Conclusion: While mitigation strategies help significantly, achieving full resilience against malicious models in collaborative systems remains a challenge.

Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [24] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: The paper proposes a method to distill collaborative behaviors of multiple language models into a single model to reduce computational costs and improve performance.


<details>
  <summary>Details</summary>
Motivation: Collaborative systems of multiple LMs can enhance performance, but they are computationally expensive. A more efficient method to harness collaborative advantages is needed.

Method: The authors introduce a distillation process to train a single model on the outputs of a collaborative system. Additionally, they propose a single-multi evolution loop where LMs iteratively distill collaboration outputs and evolve collectively.

Result: Experiments show an average improvement of 8% in model performance and a 14.9% increase in collaboration benefits after evolution. The method also outperforms existing evolutionary AI methods.

Conclusion: This approach effectively distills collaboration benefits into single models, reduces computational costs, and provides a mechanism for iterative self-improvement through collective evolution.

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR is a privacy protection framework for facial images that separates identity features and visual appearances to generate anonymous adversarial faces, ensuring both privacy and recoverability.


<details>
  <summary>Details</summary>
Motivation: With increasing usage of facial recognition in sensitive digital services, there is a critical need to balance effective privacy protection with identity authentication, especially in scenarios involving image storage and transmission.

Method: The authors developed SIDeR, which decomposes facial images into distinct identity and appearance components. Utilizing a diffusion model, it creates anonymous adversarial faces while maintaining machine-level identity consistency. It also ensures image restoration with a password.

Result: SIDeR achieved a 99% attack success rate in black-box scenarios and improved restoration quality by 41.28% compared to prior approaches on datasets like CelebA-HQ and FFHQ.

Conclusion: SIDeR is an effective tool for facial privacy protection, providing both anonymity and accurate restoration capabilities, marking a significant advancement over existing methods.

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [26] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack introduces a novel loss function to improve multi-object tracking (MOT) performance, integrating detection, identity preservation, and motion consistency into a unified training framework.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based MOT methods often require redesigning tracking architectures, creating a need for a universal, efficient, and adaptable solution to streamline training objectives.

Method: The paper proposes UniTrack, a graph-theoretic loss function for unified differentiable learning, enhancing MOT systems by directly optimizing detection accuracy, identity preservation, and spatiotemporal consistency without architectural changes.

Result: UniTrack consistently improved MOT across benchmarks and models, with reductions in identity switches up to 53%, IDF1 improvements of 12%, and a 9.7% MOTA gain for the GTR model on SportsMOT.

Conclusion: UniTrack proves effective in improving overall tracking performance while being universally compatible with established MOT architectures, providing significant advancements in identity preservation and tracking consistency.

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [27] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: This paper addresses visual-action misalignment in Vision-Language-Action (VLA) models and proposes a training approach to improve visual conditioning, enhancing robot task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to mitigate the issue of vision-action misalignment in VLA models, which results in unreliable action predictions and limits their effectiveness in robotic manipulation tasks.

Method: The method involves a two-stage training framework: preference optimization on a track-following task to strengthen visual-action alignment, followed by latent-space distillation to transfer this improvement to instruction-following tasks.

Result: The proposed approach improves visual conditioning and task performance for both discrete and continuous OpenVLA models without requiring changes in architecture or additional data collection.

Conclusion: The framework successfully enhances visual dependence and task execution in VLA models, highlighting that stronger visual-action alignment leads to better reliability and performance across robotic tasks.

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [28] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: The paper examines strategies for accurate food portion estimation from images, addressing health monitoring challenges.


<details>
  <summary>Details</summary>
Motivation: To improve dietary assessment and health monitoring by enhancing food portion estimation from images, aiding in the prevention of chronic diseases and obesity.

Method: The paper reviews methods like depth maps, multi-view inputs, template matching, and deep learning-based approaches for 3D portion estimation from 2D images.

Result: Various techniques improve the estimation accuracy of food portions using image-based methods and auxiliary inputs.

Conclusion: Effective estimation of food portions from images supports dietary assessment and chronic disease prevention strategies.

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [29] [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](https://arxiv.org/abs/2602.05737)
*Luca Ciampi,Ludovico Iannello,Fabrizio Tonelli,Gabriele Lagani,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.CV

TL;DR: The paper introduces a biological reservoir computing (BRC) system using in vitro cortical neurons for static visual pattern recognition tasks within a computer vision framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the application of biological neural networks as computational models to bridge biological principles and machine learning, addressing challenges in neuromorphic computing and biologically inspired AI.

Method: The BRC system uses living cortical neuron networks interfaced with an HD-MEA for dual stimulation and readout. Neural responses are processed through a linear readout layer for classification tasks ranging from simple points to intricate handwritten digit recognition.

Result: Despite biological variability, the living neural circuits yield high-dimensional representations enabling accurate classification across tasks, including MNIST digit recognition.

Conclusion: In vitro cortical networks are effective reservoirs for visual pattern recognition, supporting neuro-inspired computing designs and integrating living neural substrates into computational frameworks.

Abstract: In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

</details>


### [30] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: The paper introduces a method to identify key visual concepts in large multimodal models to assess their behavior in medical image classification tasks. It highlights performance disparities across demographic subgroups and validates key visual features.


<details>
  <summary>Details</summary>
Motivation: To ensure machine learning model reliability in critical fields like healthcare by uncovering their shortcomings in specific tasks, especially medical imaging.

Method: The authors proposed Visual Concept Ranking (VCR), a method that identifies important visual features for model auditing, and applies it to probe LMMs' performance, supplemented by manual validation.

Result: VCR revealed performance gaps in LMMs' medical image classification between demographic groups and provided hypotheses about visual feature dependencies.

Conclusion: The study underscores the importance of understanding model behavior in medical contexts and offers VCR as a practical tool for improving auditing of multimodal models.

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [31] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: This paper introduces CLEAR-HPV, a framework for interpretable attention-based MIL models, facilitating concept discovery and better understanding of HPV-related histopathology.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the interpretability of attention-based MIL models for analyzing HPV status in head, neck, and cervical cancers by enabling morphologic concept discovery without additional labels.

Method: The method involves restructuring the MIL latent space with CLEAR-HPV using attention to discover morphologic concepts, generate spatial maps, and summarize slides with concise concept-fraction vectors.

Result: CLEAR-HPV discovers key morphologic concepts, creates spatial concept maps, and reduces high-dimensional features to 10 interpretable concepts while preserving predictive accuracy.

Conclusion: CLEAR-HPV enhances interpretability of attention-based MIL models, generalizing well across datasets, and providing a compact and efficient representation for HPV-related histopathology.

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [32] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: The study introduces ARGaze, an autoregressive model for online egocentric gaze estimation, using temporal continuity in gaze to predict visual attention effectively.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of predicting where an individual in a first-person recording is looking, without explicit head or eye data, for applications in augmented reality and assistive tools.

Method: The authors propose ARGaze, a transformer decoder model that uses a Gaze Context Window of recent gaze estimates and current visual features to predict gaze sequentially.

Result: ARGaze demonstrates state-of-the-art performance on various egocentric benchmarks with causality-driven and resource-efficient online inference.

Conclusion: The study validates the importance of autoregressive modeling with bounded gaze history for achieving robust and accurate egocentric gaze estimation.

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [33] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: The paper investigates the limitations of both sensor-based and vision-based hand tracking for sensing gloves. It proposes AirGlove, a method that enhances hand pose tracking on gloved hands by generalizing glove representations with limited data.


<details>
  <summary>Details</summary>
Motivation: To address the performance limitations of hand tracking models on sensing gloves due to the appearance differences between bare hands and gloves.

Method: Propose a novel approach, AirGlove, that enhances vision-based hand tracking through generalization of learned glove representations to adapt to new glove designs with limited additional data.

Result: Experiments demonstrate that AirGlove achieves significant improvements in hand pose tracking accuracy across various sensing gloves and outperforms existing schemes.

Conclusion: AirGlove offers a promising solution for effective hand pose tracking on sensing gloves and contributes significantly to teleoperation and robotic policy learning applications.

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [34] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: The paper introduces SHaSaM, a novel method to mitigate bias in deep neural networks using submodular hard-sample mining and fairness-focused representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing models perpetuate social and demographic biases due to data imbalance and sensitive attributes, leading to unfair outcomes.

Method: SHaSaM is a two-stage approach: SHaSaM-MINE selects hard positives and negatives via a submodular subset strategy, and SHaSaM-LEARN utilizes combinatorial loss functions to separate target classes while minimizing sensitive attribute influence.

Result: SHaSaM improves fairness by up to 2.7 points and accuracy by 3.5%, achieving state-of-the-art outcomes on CelebA and UTKFace datasets.

Conclusion: The proposed method enhances fairness and performance in deep neural networks, addressing bias effectively.

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: The paper presents an HPC solution for scaling agent-based cellular models in tumor simulations, achieving significant speed and memory improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in scaling cellular models for real-scale tumor simulations, critical for digital twin models of diseases.

Method: Introduction and evaluation of a scalable Biological Finite Volume Method (BioFVM) library using HPC for molecular diffusion modeling.

Result: Achieved nearly 200x speedup and up to 36% reduced memory usage compared to existing solutions.

Conclusion: The proposed HPC approach advances the computational capacity for biological modeling, enabling efficient real-scale applications.

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [36] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: The paper discusses the findings of the Workflows Community Summit 2025, including identified challenges and proposed solutions to advance scientific workflows.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and address challenges in the adoption and sustainability of scientific workflows, enhancing reproducibility and efficiency in research.

Method: Experts at the summit analyzed barriers and proposed actionable strategies spanning technology, policy, and community dimensions.

Result: The summit identified challenges like lack of recognition for developers, insufficient standardization, and gaps in funding and collaboration. It recommended shifting focus to scientific impact and formalizing benchmarks and patterns.

Conclusion: Advancing scientific workflows requires improved metrics, community engagement, formal standards, dedicated roles, and enhanced training to foster reproducibility and innovation.

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [37] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: The paper introduces ORACL, a framework using large language models to improve autoscaling in microservice architectures, enhancing performance without extensive retraining.


<details>
  <summary>Details</summary>
Motivation: Current autoscaling methods for microservices rely on either complex models requiring significant retraining or fragile rules that fail to generalize across deployments.

Method: ORACL leverages large language models for few-shot reasoning, translating runtime telemetry into semantic descriptions and generating resource allocation recommendations through chain-of-thought reasoning.

Result: ORACL demonstrates 15% improvement in root-cause identification accuracy, 24x faster training, and 6% better quality of service across represented microservice workloads.

Conclusion: Large language models can be effective universal few-shot autoscaling mechanisms, providing better scalability and diagnostics in evolving microservice systems.

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [38] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus introduces a distributed consensus protocol integrating CFT with BFT for enhanced TEE security.


<details>
  <summary>Details</summary>
Motivation: The paper responds to vulnerabilities in hardware TEEs that threaten the guarantees of distributed ledgers.

Method: Proteus embeds a BFT protocol within a CFT protocol, leveraging a refined alignment of structures without extra messaging.

Result: Proteus delivers performance comparable to TEE-enabled consensus protocols while ensuring integrity against TEE compromises.

Conclusion: Proteus offers improved resiliency and a cautious trust approach for distributed systems using TEEs.

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [39] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: The paper examines the Dolev-Reischuk lower bound for Byzantine Agreement protocols and establishes that the quadratic communication complexity arises from dissemination rather than reaching decision univalency.


<details>
  <summary>Details</summary>
Motivation: To understand the origins of the quadratic message complexity in Byzantine Agreement protocols and investigate whether reaching univalency inherently requires such costs.

Method: The authors introduce a relaxed version called $ε$-BA, where a fraction of correct processors can output incorrectly. They demonstrate that $ε$-BA can achieve deterministic solutions with communication complexity $O(n \log n)$. Additionally, they propose the Extractable BA for authenticated scenarios, achieving $O(f \log f)$ complexity.

Result: The study finds that reaching univalency does not require quadratic communication. The quadratic cost is attributed to the dissemination process in Byzantine Agreement.

Conclusion: The Dolev-Reischuk quadratic cost is not due to reaching univalency but instead from disseminating the agreed outcome. Techniques like $ε$-BA and Extractable BA optimize communication complexity.

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [40] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze optimizes pipeline parallelism by smartly freezing parameters to minimize training time while maintaining model accuracy, achieving up to 40% faster training throughput.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in pipeline parallelism caused by pipeline bubbles and unnecessary accuracy degradation from over-freezing parameters.

Method: Proposes a method that transforms pipeline schedules into directed acyclic graphs and solves a linear program to determine optimal freeze ratios under accuracy constraints.

Result: TimelyFreeze improves training throughput by up to 40% on LLaMA-8B without degrading accuracy.

Conclusion: TimelyFreeze effectively accelerates large-scale model training while maintaining convergence and generalizes well to different pipeline-parallel configurations.

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [41] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: A novel problem called LOCATION-AWARE DISPERSION is introduced, extending the DISPERSION problem by adding location awareness with colored nodes and robots.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the DISPERSION problem in distributed robotics to incorporate location awareness, where robots not only disperse to distinct nodes on a graph but occupy nodes of matching colors.

Method: Deterministic algorithms are developed with guaranteed bounds on time and memory requirements, addressing the location-awareness challenge. Impossibility results and a lower bound for deterministic algorithms are also provided.

Result: The algorithms demonstrate the feasibility of solving LOCATION-AWARE DISPERSION on anonymous networks, while challenges related to efficiency compared to DISPERSION are highlighted.

Conclusion: The study establishes LOCATION-AWARE DISPERSION as algorithmically feasible and presents a range of results, emphasizing the increased complexity introduced by location awareness.

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: The paper proposes using denoising diffusion probabilistic models (DDPMs) for neuroimaging normative modeling, improving over traditional univariate methods by capturing multivariate dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing neuroimaging pipelines often analyze imaging-derived phenotypes (IDPs) individually, which fails to capture multivariate correlations that could reveal coordinated patterns.

Method: The study introduces DDPMs using two denoising backbones: (i) an MLP with FiLM and (ii) the SAINT tabular transformer, with an evaluation covering calibration, distributional fidelity, and multivariate analyses in synthetic datasets and UK Biobank phenotypes.

Result: Diffusion models achieve comparable results to traditional methods for low-dimensional data and, using the transformer backbone, outperform alternative approaches for high-dimensional data, ensuring better multivariate dependency modeling and calibration.

Conclusion: Diffusion-based normative modeling is a viable method for generating multivariate deviation profiles, offering precision and scalability in neuroimaging applications.

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [43] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: This paper introduces 'Causal Analyst,' a framework leveraging causal analysis to study jailbreaks in large language models (LLMs) and improve their reliability.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and reliability of LLMs, it is essential to unveil the mechanisms behind jailbreak prompts. Current research lacks sufficient understanding of causal relationships between prompt features and jailbreak occurrences.

Method: The paper proposes using a dataset of 35k jailbreak attempts paired with interpretable prompt features. The approach integrates LLM-based prompt encoding and GNN-based causal graph learning to identify direct causal drivers of jailbreak occurrences. Specific prompt features and their causal relationships are analyzed.

Result: Key causative prompt features, like 'Positive Character' and 'Number of Task Steps,' are identified as direct drivers of jailbreak responses. Practical applications include a Jailbreaking Enhancer and a Guardrail Advisor, both of which leverage these causal relationships to improve LLM security and reliability.

Conclusion: Analyzing jailbreak features through causal relationships is a robust and interpretable method that enhances the safety and security of LLMs, presenting promising applications in attack mitigation and prevention.

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [44] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: The paper introduces Momentum Attention, a symplectic augmentation for Transformers incorporating kinematic dynamics to overcome existing depth constraints and improve induction performance, validated by extensive experiments.


<details>
  <summary>Details</summary>
Motivation: To enhance the mechanistic understanding and computational efficiency of Transformers by embedding physical principles like conservation laws and kinematics, enabling breakthroughs in single-layer induction capabilities.

Method: The authors extend the existing Transformer computational graph with Momentum Attention, incorporating symplectic dynamics through the kinematic difference operator. They identify a Symplectic-Filter Duality and formalize an Orthogonality Theorem to analyze signal segregation.

Result: The Momentum model achieves superior performance on induction-heavy tasks, reducing depth requirements, and validates theoretical claims through over 5,100 controlled experiments, showing comparable results to larger baseline models within ~2.9% validation loss.

Conclusion: This work establishes a novel connection between Generative AI, Hamiltonian Physics, and Signal Processing, offering a new framework that effectively enhances Transformer architectures and provides deeper mechanistic insights.

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [45] [A logical re-conception of neural networks: Hamiltonian bitwise part-whole architecture](https://arxiv.org/abs/2602.04911)
*E Bowen,R Granger,A Rodriguez*

Main category: cs.LG

TL;DR: The paper introduces a new system for representing relations as graphs with low-cost arithmetic, blending symbolic computation with standard neural network examples.


<details>
  <summary>Details</summary>
Motivation: To create a system that directly represents relations like part-whole intrinsically within its architecture, addressing limitations of traditional neural networks in logical and hierarchical relational processing.

Method: The system encodes data as graphs and uses a novel graph-Hamiltonian operator to calculate relational constraints. It operates with low-precision arithmetic and identifies logical structures, enabling hierarchical and relational representations.

Result: The architecture can process standard neural network examples and produce symbolic representations, with characteristics useful for semantic representation and logical reasoning.

Conclusion: The system is a promising unconventional approach blending symbolic and statistical methods, with the potential for further enhancements to expand its capabilities in semantic and logical computations.

Abstract: We introduce a simple initial working system in which relations (such as part-whole) are directly represented via an architecture with operating and learning rules fundamentally distinct from standard artificial neural network methods. Arbitrary data are straightforwardly encoded as graphs whose edges correspond to codes from a small fixed primitive set of elemental pairwise relations, such that simple relational encoding is not an add-on, but occurs intrinsically within the most basic components of the system. A novel graph-Hamiltonian operator calculates energies among these encodings, with ground states denoting simultaneous satisfaction of all relation constraints among graph vertices. The method solely uses radically low-precision arithmetic; computational cost is correspondingly low, and scales linearly with the number of edges in the data. The resulting unconventional architecture can process standard ANN examples, but also produces representations that exhibit characteristics of symbolic computation. Specifically, the method identifies simple logical relational structures in these data (part-of; next-to), building hierarchical representations that enable abductive inferential steps generating relational position-based encodings, rather than solely statistical representations. Notably, an equivalent set of ANN operations are derived, identifying a special case of embedded vector encodings that may constitute a useful approach to current work in higher-level semantic representation. The very simple current state of the implemented system invites additional tools and improvements.

</details>


### [46] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: Feature steering in large language models (LLMs) can control behaviors but significantly reduces performance quality in accuracy and coherence, compared to prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Feature steering has shown potential for controlling LLMs by directly altering internal representations, but its real-world application and trade-offs with quality are unclear and underexplored.

Method: The paper evaluates Goodfire's Auto Steer feature steering against prompt engineering using 14 queries around behavior control on 171 MMLU questions, tested on Llama-8B and Llama-70B. Metrics include accuracy, coherence, and behavioral control.

Result: Auto Steer successfully modifies behavioral control but at the cost of significant degradation in task performance. For instance, accuracy drops by over 20% and coherence substantially decreases compared to prompt engineering methods.

Conclusion: Although feature steering can effectively adjust behaviors, it is not suitable for practical use given its significant trade-offs in task performance. Mechanistic control methods should carefully assess behavior-performance trade-offs before real-world deployment.

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [47] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER addresses robustness issues in multimodal fusion by using dual-stage compression for noise and energy-based reconstruction for missing modalities, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Multimodal fusion often fails due to noisy inputs and missing modalities, and solutions are needed to ensure reliable predictions.

Method: Dual-stage compression (wavelet/DCT for noise removal and bottleneck tokens for integration) and energy-based representation recovery via gradient descent.

Result: Achieved state-of-the-art results on CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets. Showed robustness under varying modality conditions.

Conclusion: DCER proves to effectively handle both noise and missing modalities, demonstrating reliability and robustness in multimodal fusion systems.

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [48] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: The paper introduces LISA, a method for adapting Laplacian-based time-series models using observed prefixes, achieving better performance in forecasting and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To improve inference-time adaptation for time-series models, especially under scenarios with changing dynamics.

Method: LISA integrates delay-coordinate embeddings, Laplacian spectral learning for state representation, and uses latent-space residual adapters like Gaussian-process regression and attention-like Markov operators for prediction.

Result: LISA outperforms traditional baselines, particularly in forecasting and dynamically changing settings.

Conclusion: The proposed method successfully links in-context adaptation with nonparametric spectral methods and proves effective for time-series model adaptation.

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [49] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: This paper introduces an integrative causal discovery framework incorporating partial physical knowledge into a stochastic differential equation model for dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To improve identifiability, stability, and robustness in causal discovery for dynamical systems by leveraging physical knowledge as an inductive bias.

Method: The system evolution is modeled with a stochastic differential equation (SDE) combining known ODE dynamics as drift terms and unknown causal couplings as diffusion terms. A scalable MLE algorithm is developed for parameter estimation using a sparsity-inducing approach.

Result: The proposed method demonstrates improved causal graph recovery and stability compared to data-driven baselines in experiments involving dynamical systems with varied causal structures.

Conclusion: Integrating physical knowledge into causal discovery enhances the performance in terms of graph recovery and produces estimates consistent with underlying physical dynamics.

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [50] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: The paper introduces Temporal Pair Consistency (TPC), a principle to reduce variance in continuous-time generative models without altering model architecture or probability paths.


<details>
  <summary>Details</summary>
Motivation: Current objectives in continuous-time generative models like diffusion models have high variance and inefficiency due to treating timesteps independently. Previous methods require additional penalties or modifications to address this.

Method: The authors propose TPC, which couples velocity predictions at paired timesteps within the same probability path, effectively reducing variance without changing the architecture, probability path, or solver.

Result: TPC improves sample quality and efficiency, achieving better FID scores on CIFAR-10 and ImageNet while being computationally efficient. It also integrates well with modern generative pipelines.

Conclusion: TPC is an effective, lightweight technique for reducing gradient variance in generative models, improving performance without major changes in architecture or processing.

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [51] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: The paper introduces Geometric Anchor Preference Optimization (GAPO), a method to dynamically align large language models by replacing static references with adaptive, geometry-aware anchors, improving robustness and performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations in methods like Direct Preference Optimization, which face issues with distributional mismatch due to static references and suffer from noisy supervision or unconstrained reward drift.

Method: Propose GAPO, which leverages a dynamic, geometry-aware anchor representing adversarial local perturbations. It includes an adaptive reweighting mechanism based on sensitivity and introduces the Anchor Gap for robust optimization of preferences.

Result: GAPO achieves better robustness in noisy environments and either matches or improves performance on large language model alignment and reasoning tasks.

Conclusion: GAPO provides an effective alternative to fixed reference methods, ensuring dynamic alignment through geometry-aware adjustments and adaptive optimization principles, offering improved robustness and stability.

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [52] [Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention](https://arxiv.org/abs/2602.05466)
*Iván Olarte Rodríguez,Gokhan Serhat,Mariusz Bujny,Fabian Duddeck,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: The paper addresses optimization of laminated composite structures, emphasizing the importance of incorporating domain knowledge and comparing sequential and concurrent optimization approaches.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in analyzing how problem formulation and domain knowledge influence outcomes in black-box optimization for engineering design problems.

Method: The study compares two strategies for optimizing cantilever beam designs: a context-agnostic concurrent approach and a sequential approach leveraging domain knowledge.

Result: The sequential strategy produces superior, interpretable solutions, while the context-agnostic concurrent approach results in suboptimal or non-physical designs.

Conclusion: Domain knowledge is crucial for effective black-box optimization, and benchmarks should reward context-aware strategies in engineering design.

Abstract: Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.

</details>


### [53] [Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2602.05675)
*Xuepeng Ren,Maocai Wang,Guangming Dai,Zimin Liang,Qianrong Liu,Shengxiang Yang,Miqing Li*

Main category: cs.NE

TL;DR: This paper introduces Variable Stepsize Randomized Local Search (VS-RLS), a method to improve performance on multi-objective combinatorial optimization problems (MOCOPs) by dynamically adjusting stepsize.


<details>
  <summary>Details</summary>
Motivation: Despite major research focus on continuous domains in evolutionary multi-objective optimization, MOCOPs received limited attention, with current MOEAs often underperforming on these problems compared to simple randomized local search.

Method: The paper presents VS-RLS, a local search technique that dynamically adjusts stepsize to gradually transition from exploratory to focused searches, avoiding traps in local optima.

Result: Extensive evaluations demonstrate that VS-RLS outperforms existing local search methods and MOEAs on a variety of MOCOPs.

Conclusion: VS-RLS provides an effective and generalizable approach for tackling MOCOPs by overcoming limitations of fixed-neighborhood methods.

Abstract: Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.

</details>


### [54] [DARWIN: Dynamic Agentically Rewriting Self-Improving Network](https://arxiv.org/abs/2602.05848)
*Henry Jiang*

Main category: cs.NE

TL;DR: DARWIN is an evolutionary GPT model that uses a genetic-algorithm-like approach to optimize its performance by iterative model updates and selection.


<details>
  <summary>Details</summary>
Motivation: To explore the possibility of leveraging genetic algorithms and human-in-the-loop (HITL) approaches to improve GPT model performance and create a scalable foundation for evolutionary training.

Method: DARWIN uses multiple GPT agents trained independently with unique training codes. Iteratively, the GPTs modify each other’s training codes to enhance performance, benchmark improvements using a genetic algorithm, and use persistent memory files and HITL interfaces for additional interventions.

Result: DARWIN showed a 1.26% improvement in model FLOPS utilization and a 2.07% improvement in perplexity over 5 training iterations compared to baseline.

Conclusion: DARWIN demonstrates potential for scaling evolutionary GPT training and could lay the groundwork for further advances in model optimization techniques.

Abstract: DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [55] [Wasure: A Modular Toolkit for Comprehensive WebAssembly Benchmarking](https://arxiv.org/abs/2602.05488)
*Riccardo Carissimi,Ben L. Titzer*

Main category: cs.PF

TL;DR: WebAssembly's performance evaluation varies across many parameters. The paper introduces Wasure, a toolkit for streamlined benchmarking and analysis of WebAssembly engines.


<details>
  <summary>Details</summary>
Motivation: Evaluating WebAssembly performance involves complex factors, including hardware variations and runtime configurations. The goal is to provide a systematic tool to aid researchers and developers in benchmarking and comparison.

Method: Development of Wasure, a modular and extensible toolkit for executing and comparing WebAssembly benchmarks. Conducted dynamic analysis using associated benchmark suites.

Result: The study highlighted substantial disparities in code coverage, control flow, and execution, demonstrating the importance of diverse benchmarking strategies.

Conclusion: Wasure facilitates systematic, transparent evaluations, aiding researchers and developers to better analyze WebAssembly runtimes across varied platforms and scenarios.

Abstract: WebAssembly (Wasm) has become a key compilation target for portable and efficient execution across diverse platforms. Benchmarking its performance, however, is a multi-dimensional challenge: it depends not only on the choice of runtime engines, but also on hardware architectures, application domains, source languages, benchmark suites, and runtime configurations. This paper introduces Wasure, a modular and extensible command-line toolkit that simplifies the execution and comparison of WebAssembly benchmarks. To complement performance evaluation, we also conducted a dynamic analysis of the benchmark suites included with Wasure. Our analysis reveals substantial differences in code coverage, control flow, and execution patterns, emphasizing the need for benchmark diversity. Wasure aims to support researchers and developers in conducting more systematic, transparent, and insightful evaluations of WebAssembly engines.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [56] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec is a framework using LLMs to convert natural-language programming rules into formal specifications, improving reliability and quality.


<details>
  <summary>Details</summary>
Motivation: To enhance software correctness, security, and reliability by formalizing natural-language API rules, which are hard to manually specify.

Method: Doc2Spec employs a multi-agent framework using LLMs to induce specification grammar and generates formal specifications, leveraging grammar constraints.

Result: Doc2Spec surpasses baseline models without grammar induction and performs comparably to manually crafted grammar techniques across multiple benchmarks.

Conclusion: Automated grammar induction effectively enhances the process of formalizing natural-language programming rules, showcasing its potential for widespread application.

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [57] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: This paper explores the asynchronous extension to algebraic computational effects, introduces a core calculus, and studies its normalisation properties using structured proofs and Agda formalisation.


<details>
  <summary>Details</summary>
Motivation: To extend the conventional synchronous handling of algebraic computational effects with asynchronous features allowing better modeling of real-world processes like pre-emptive multi-threading and cancellable remote function calls.

Method: The authors developed a core calculus, analyzed its normalisation properties, and employed compositional proofs based on $$-lifting as well as formalised all results in Agda.

Result: The paper shows that the calculus without general recursion is strongly normalising. Additionally, the sequential fragment of the calculus remains strongly normalising when controlled interrupt-driven recursion is reintroduced.

Conclusion: The study proves the strong normalisation of the proposed asynchronous calculus while addressing its practical applicability and provides rigorous formalisation of its results.

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [58] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: The paper develops an algebraic theory to model dynamic threads using algebraic effects, focusing on primitives 'fork' and 'wait' to create and manage threads. It introduces a complete equational axiomatization for both closed and open expressions and extends the framework to a concurrent programming language.


<details>
  <summary>Details</summary>
Motivation: To provide a formal algebraic foundation for modeling dynamic threads and concurrent programming by capturing their key operations ('fork' and 'wait'), aligned with established concurrency models.

Method: The authors use parameterized algebraic theories to define operations of 'fork' and 'wait' paired with an axiomatic system. They provide operational and denotational semantics, validated for soundness, adequacy, and full abstraction using labelled-poset observations.

Result: The paper achieves complete equational axiomatization for dynamic threads, proving model completeness and syntactic completeness. It successfully extends the algebraic framework to a simple concurrent programming language with validated semantics.

Conclusion: The proposed algebraic framework and analysis effectively model dynamic threads and concurrency, showing strong correspondence with established semantics models and advancing the study of algebraic effects for programming languages.

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [59] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: The study explores how people react to a coffee robot's errors in a public field deployment and finds that social responses are varied and rich, but noisy.


<details>
  <summary>Details</summary>
Motivation: To understand how humans socially respond to robot errors in real-world settings, especially in group and repeated interactions, with non-social robots.

Method: Conducted a public field deployment involving 49 participants interacting with a coffee robot to observe social signals in response to errors.

Result: Participants showed diverse, rich, but noisy social signals during interactions, particularly in group settings.

Conclusion: Social signals can provide valuable insights into human-robot interaction in real-world environments, though challenges like noise exist.

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [60] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: This paper introduces a neuro-graphics model enabling robots to estimate and interact with unseen objects through zero-shot scene reconstruction and grasping, outperforming current methods in model-free pose estimation and avoiding reliance on extensive datasets.


<details>
  <summary>Details</summary>
Motivation: Robots need efficient methods to operate in novel environments and interact with unseen objects without large amounts of training data or test-time sampling.

Method: The authors developed a differentiable neuro-graphics model combining neural foundation models with physics-based differentiable rendering to reconstruct and estimate scene parameters from single RGBD images and bounding boxes.

Result: The model outperformed existing algorithms on few-shot pose estimation benchmarks and successfully executed zero-shot grasping tasks, validating its effectiveness in scene reconstruction.

Conclusion: This approach enables data-efficient, interpretable, and generalizable robot autonomy, advancing capabilities in unexplored environments without dependence on extensive data or sampling.

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [61] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: This paper proposes a neuro-symbolic feature representation and SFOL reward function to enhance Reinforcement Learning for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based Deep Reinforcement Learning approaches lack high-level scene context and rely on rigid reward functions.

Method: The paper develops a neuro-symbolic feature representation and a Soft First-Order Logic (SFOL) reward function using segmentation maps and reasoning to align decisions with human values.

Result: Experiments in CARLA simulation demonstrate improved policy robustness and safety performance under varying traffic densities and occlusions compared to baselines.

Conclusion: Integrating holistic representations and soft reasoning improves context-aware, value-aligned decision-making in autonomous driving.

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [62] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: This paper presents a novel approach to solving inverse kinematics (IK) problems by integrating analytic solutions into optimization methods, improving success rates in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of combining analytic and optimization methods for inverse kinematics problems while managing nonlinear relationships and additional constraints like collision avoidance.

Method: The authors propose a new optimization IK formulation that incorporates an analytic IK solution as a change of variables, simplifying the optimization process.

Result: Experimental comparisons show that the proposed formulation achieves higher success rates across various complex IK problems, including collision avoidance and humanoid stability, using three popular optimization solvers.

Conclusion: The integration of analytic IK solutions into optimization methods significantly enhances performance and reliability, making it a valuable improvement for solving difficult IK problems.

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [63] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: This paper introduces the PLATO Hand, a novel robotic hand design with advanced fingertip features for diverse manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robotic hand designs with precise manipulation and interaction across various object geometries.

Method: Developing a hybrid fingertip with rigid fingernail and compliant pulp, alongside a strain-energy-based bending-indentation model for design optimization.

Result: The PLATO Hand showcases improved pinching stability, better force observability, and successful execution of complex tasks like singulating paper and peeling oranges.

Conclusion: Integrating structured contact geometry with a force-motion transparent mechanism leads to advancements in precise robotic manipulation.

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [64] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: The paper develops a method to plan efficient paths for environmental monitoring robots that guarantee estimation quality using Gaussian-process posterior variance.


<details>
  <summary>Details</summary>
Motivation: Environmental monitoring robots often face challenges in balancing measurement accuracy with distance and energy constraints. Classical survey techniques oversample, while informative path planning methods lack guarantees on reconstruction quality.

Method: The proposed approach has three stages: learning a GP model, creating binary coverage maps to reduce uncertainty below a threshold, and planning a near-shortest route that satisfies the global uncertainty constraint. It uses nonstationary kernels for heterogeneous data and addresses environments with obstacles.

Result: Tests on topographic data showed improved efficiency in terms of fewer sensing locations and shorter travel distances compared to baselines. Field experiments confirmed the practicality of the approach for autonomous mapping vehicles.

Conclusion: The new approach successfully combines efficiency and estimation uncertainty guarantees, offering improvements in environmental monitoring tasks while being feasible for real-world applications.

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [65] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: This paper introduces MobileManiBench, a large-scale simulated benchmark to evaluate vision-language-action (VLA) models for mobile robotic manipulation with enhanced realism and diversity.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation models typically rely on large teleoperation datasets focused on static tabletop scenes, limiting their applicability in dynamic mobile environments.

Method: The authors developed MobileManiBench using NVIDIA Isaac Sim, where reinforcement learning produces diverse manipulation trajectories annotated with language instructions, multi-view visual data, and robot/object states.

Result: MobileManiBench includes 630 objects, 20 categories, 5 skills, across 100 realistic scenes that result in 300K trajectories to test VLA models for mobile manipulation.

Conclusion: MobileManiBench enables controlled evaluation and accelerates research for data-efficient, generalized robot policy development, addressing gaps in current benchmarks.

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [66] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: The paper proposes a minimal-sensing approach using an IMU, pressure sensor, and two sonars to allow underwater autonomous robots navigate flooded pipes.


<details>
  <summary>Details</summary>
Motivation: Underwater pipeline inspection is difficult due to challenging environments like turbidity, confined spaces, and limited localization cues.

Method: A minimal-sensing approach using range estimation from sonar intensity data, a geometric model, and adaptive PD control achieves centering and traversal of submerged pipes.

Result: Experiments demonstrated stable centering and successful traversal in a submerged pipe with challenging conditions and structural deformations.

Conclusion: Lightweight, computationally efficient sensing architectures can enable reliable in-pipe navigation for underwater autonomous robots and improve inspection practicality.

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [67] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: This paper introduces AIDE, a framework for robots to interactively identify task-relevant objects under ambiguous instructions by combining exploration and vision-language reasoning. It outperforms existing methods in task success and execution accuracy.


<details>
  <summary>Details</summary>
Motivation: Current vision-language methods struggle to enable robots to act under ambiguous human instructions due to inefficient reasoning and lack of interaction with the environment.

Method: The proposed AIDE framework includes a dual-stream approach: Multi-Stage Inference (MSI) for decision-making and Accelerated Decision-Making (ADM) for execution, allowing zero-shot affordance analysis and instruction interpretation.

Result: AIDE achieved over 80% task planning success and 95% execution accuracy in both simulations and real-world environments, surpassing existing methods.

Conclusion: AIDE demonstrates improved performance in handling ambiguous instructions and open-world scenarios, highlighting its potential in robotic implementation.

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: The paper introduces VeruSyn, a data synthesis pipeline for creating verified programs in Rust, enabling fine-tuned large language models (LLMs) capable of generating formal correctness proofs alongside code.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for code generation, but their ability to generate formally verified, correct code remains limited due to higher reasoning demands and insufficient training data.

Method: The authors propose VeruSyn, which uses self-synthesis, tutorial-based synthesis, and agent trajectory synthesis to generate a large dataset of Verus-verified Rust programs, including formal specifications and proofs.

Result: VeruSyn synthesizes 6.9 million verified Rust programs and fine-tunes a Qwen2.5-Coder-32B-Instruct model. This model offers a better cost-proof tradeoff and outperforms state-of-the-art commercial and research models.

Conclusion: The paper demonstrates VeruSyn's capability to advance code-proof generation by creating a large verified program dataset and improving LLM performance for formal verification tasks.

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [69] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: SLO-as-code is extended to create 'Emergence-as-Code' (EmaC), enabling computable and governable journey reliability in microservices.


<details>
  <summary>Details</summary>
Motivation: Address limitations where microservice-based user journeys have emergent reliability properties, leading to inefficiencies like over-provisioning or missing user expectations.

Method: Proposes Emergence-as-Code (EmaC), a declarative spec that integrates journey intent with microservice telemetry, operational artifacts, and runtime inference to model and monitor reliability.

Result: The system generates journey models, control-plane artifacts, and uses a Git workflow to review artifacts for better journey governance.

Conclusion: EmaC bridges the gap between per-service SLOs and overall journey reliability by making it computable and governable in a systematic way.

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [70] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: The paper introduces Activation Steering Adapter (ASA), a training-free mechanism for efficient domain adaptation in LLM agents.


<details>
  <summary>Details</summary>
Motivation: Address the high costs and brittleness of current adaptation methods for changing toolsets, APIs, and protocols in large language models.

Method: ASA uses inference-time activation signals and an ultra-light router for domain adaptation without additional training.

Result: ASA demonstrates comparable adaptation to LoRA with lower overhead and strong cross-model transferability.

Conclusion: ASA offers practical, scalable adaptation suitable for multi-domain ecosystems with dynamic changes in interfaces.

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [71] [Large Language Models in Software Documentation and Modeling: A Literature Review and Findings](https://arxiv.org/abs/2602.04938)
*Lukas Radosky,Ivan Polasek*

Main category: cs.SE

TL;DR: The paper reviews the use of generative AI, particularly large language models, for solving software engineering tasks related to documentation and modeling.


<details>
  <summary>Details</summary>
Motivation: Large language models excel in understanding natural and structured languages, making them valuable for software engineering tasks, particularly documentation and modeling.

Method: A literature review was conducted by analyzing articles from four major venues to explore tasks, prompt techniques, metrics, human-based evaluation approaches, and datasets used in relation to large language models.

Result: The study categorized tasks solved by large language models, offered insights into prompting methods, evaluation metrics, human-based assessments, and key datasets used in software engineering contexts.

Conclusion: Generative AI, especially large language models, holds great potential for enhancing software documentation and modeling tasks, as shown in the reviewed literature.

Abstract: Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.

</details>


### [72] [Applying a Requirements-Focused Agile Management Approach for Machine Learning-Enabled Systems](https://arxiv.org/abs/2602.05042)
*Lucas Romao,Luiz Xavier,Júlia Condé Araújo,Marina Condé Araújo,Ariane Rodrigues,Marcos Kalinowski*

Main category: cs.SE

TL;DR: RefineML method combines ML-focused refinement and agile practices to address challenges in developing ML-enabled systems, showing high usability and improving project governance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in Requirements Engineering and agile management for ML-enabled systems, including data dependence and experimentation.

Method: The authors applied RefineML, integrated with specifications and agile practices tailored for ML systems, and evaluated it using questionnaires and interviews.

Result: RefineML was positively received, showing high usability, improving communication, early feasibility assessments, and dual-track governance benefits.

Conclusion: RefineML improves the integration and refinement process of ML systems but faces challenges in translating ML concerns to agile requirements and estimating effort.

Abstract: Machine Learning (ML)-enabled systems challenge traditional Requirements Engineering (RE) and agile management due to data dependence, experimentation, and uncertain model behavior. Existing RE and agile practices remain poorly integrated and insufficiently tailored to these characteristics. This paper reports on the practical experience of applying RefineML, a requirements-focused approach for the continuous and agile refinement of ML-enabled systems, which integrates ML-tailored specification and agile management approaches with best practices derived from a systematic mapping study. The application context concerns an industry-academia collaboration project between PUC-Rio and EXA, a Brazilian cybersecurity company. For evaluation purposes, we applied questionnaires assessing RefineML's suitability and overall acceptance and semi-structured interviews. We applied thematic analysis to the collected qualitative data. Regarding suitability and acceptance, the results of the questionnaires indicated high perceived usefulness and intention to use. Based on the interviews, stakeholders perceived RefineML as improving communication and facilitating early feasibility assessments, as well as enabling dual-track governance of ML and software work, allowing continuous refinement of the model while evolving the overall software project. However, some limitations remain, particularly related to difficulties in operationalizing ML concerns into agile requirements and in estimating ML effort.

</details>


### [73] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: The paper proposes a quality model specifically for machine learning (ML) components to address gaps in requirements elicitation, negotiation, and testing, validated through a survey and integrated into an open-source tool.


<details>
  <summary>Details</summary>
Motivation: ML prototypes often fail to transition to production due to insufficient and narrow testing focused only on model properties, ignoring broader system-derived requirements like robustness and resource consumption.

Method: The authors developed a quality model for ML components, validated its efficacy through a survey with stakeholders, and integrated the model into an open-source tool to ensure practical applicability.

Result: Survey participants agreed on the model's relevance and value, and its integration into an open-source tool shows practical utility in ML component testing and evaluation.

Conclusion: The quality model effectively bridges the gap between ML component requirements and system constraints, facilitating better model testing and enhancing production readiness.

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [74] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: The paper introduces TestMigrationsInPy, a dataset of real-world migrations from unittest to pytest, aiding research on Python test frameworks.


<details>
  <summary>Details</summary>
Motivation: Testing frameworks unittest and pytest are popular in Python, but migrating between them is time-consuming; automated solutions are needed.

Method: Proposes a publicly available dataset, TestMigrationsInPy, capturing 923 unittest-to-pytest migrations with detailed migration types.

Result: The dataset provides insights into real-world migrations and enables verification of future migration solutions in Python testing frameworks.

Conclusion: TestMigrationsInPy facilitates research and innovation on Python framework migration and supports gradual unittest-to-pytest adoption.

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [75] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: The paper studies how frequently exceptional behaviors are tested in real-world Python systems, analyzing both exceptions that reach and do not reach the tests.


<details>
  <summary>Details</summary>
Motivation: To understand how well exceptional behaviors are tested in real-world software, and to identify opportunities for improving test effectiveness and tool support.

Method: The authors conducted an empirical study using instrumented test suites to monitor runtime exceptions in 25 Python systems, analyzing their occurrences, frequencies, and impacts.

Result: 21.4% of executed methods raise exceptions at runtime. Among methods raising exceptions, 1 in 10 calls exercise exceptional behavior on median, with 20% raising exceptions more frequently.

Conclusion: Exception-raising behaviors are not always rare or abnormal. The findings call for new tools and techniques to handle exceptional behaviors better and identify expensive try/except blocks.

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [76] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: This paper highlights how AI integration in driving systems requires re-assessing safety in Quality Management (QM) components despite their traditional exclusion from safety analyses.


<details>
  <summary>Details</summary>
Motivation: AI integration in driving automation systems introduces risks in components classified under QM, challenging established perceptions that QM components are not safety-relevant.

Method: The paper employs case studies to analyze the safety risks posed by AI-driven perception systems deployed in driving automation technologies.

Result: The findings reveal that AI-driven QM components can cause hazardous behavior, demonstrating their impact on violating safety standards.

Conclusion: To ensure safety in AI-integrated systems, existing frameworks must expand to incorporate rigorous safety analysis for all component classifications, especially AI-related QM components.

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [77] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: The paper introduces Entropy-Guided Stepwise Scaling (EGSS), a Test-Time Scaling framework designed to address computational overhead in software engineering tasks, improving both performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance practical adoption of TTS methods in complex software tasks by addressing inefficiencies caused by large ensembles and lack of optimal solution selection.

Method: The method involves entropy-guided adaptive search and robust test-suite augmentation to balance efficiency and effectiveness in the TTS framework.

Result: EGSS improves model performance by 5-10% in all tested scenarios, sets a new state-of-the-art with GLM-4.6, and reduces inference-time token usage by over 28%.

Conclusion: EGSS offers a significant improvement in accuracy and computational efficiency over existing TTS methods, making it a valuable tool for software engineering tasks.

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [78] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru provides an automated method using large language models to validate patches through runtime assertions, offering better bug detection and precision compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Validating patches is challenging due to incomplete regression tests and informal language descriptions of their intention.

Method: PatchGuru uses LLMs to extract developer intent, then synthesizes runtime assertions (patch oracles) to validate patches iteratively, compare pre- and post-patch behaviors, and generate bug reports.

Result: PatchGuru achieves a precision of 0.62, detects 24 bugs (17 more than Testora), including 12 previously unknown bugs, with an average cost of 8.9 minutes and USD 0.07 per pull request.

Conclusion: PatchGuru improves patch validation by complementing code review and regression testing, providing executable documentation and reduced cost for bug detection in open-source projects.

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [79] [Finite-Particle Rates for Regularized Stein Variational Gradient Descent](https://arxiv.org/abs/2602.05172)
*Ye He,Krishnakumar Balasubramanian,Sayan Banerjee,Promit Ghosal*

Main category: stat.ML

TL;DR: The paper analyzes the regularized Stein variational gradient descent (R-SVGD) algorithm, providing finite-particle convergence guarantees and outlining tuning rules for various parameters.


<details>
  <summary>Details</summary>
Motivation: To address the constant-order bias in the original Stein variational gradient descent (SVGD) and derive better convergence guarantees in terms of Fisher information and Wasserstein distance.

Method: The method introduces a resolvent-type preconditioner to the kernelized Wasserstein gradient for R-SVGD. It derives non-asymptotic bounds for the interacting particle system, analyzing continuous- and discrete-time dynamics, and provides parameter tuning guidelines.

Result: Explicit non-asymptotic bounds for time-averaged empirical measures are established, showing convergence in Fisher information and Wasserstein distances for a wide class of kernels. The analysis quantifies trade-offs between Wasserstein gradient flow approximation and finite-particle error.

Conclusion: The paper offers a mathematical framework for analyzing R-SVGD with explicit guarantees, making it more robust with principled parameter tuning. This ensures improved approximation and control over estimation errors.

Abstract: We derive finite-particle rates for the regularized Stein variational gradient descent (R-SVGD) algorithm introduced by He et al. (2024) that corrects the constant-order bias of the SVGD by applying a resolvent-type preconditioner to the kernelized Wasserstein gradient. For the resulting interacting $N$-particle system, we establish explicit non-asymptotic bounds for time-averaged (annealed) empirical measures, illustrating convergence in the \emph{true} (non-kernelized) Fisher information and, under a $\mathrm{W}_1\mathrm{I}$ condition on the target, corresponding $\mathrm{W}_1$ convergence for a large class of smooth kernels. Our analysis covers both continuous- and discrete-time dynamics and yields principled tuning rules for the regularization parameter, step size, and averaging horizon that quantify the trade-off between approximating the Wasserstein gradient flow and controlling finite-particle estimation error.

</details>


### [80] [Total Variation Rates for Riemannian Flow Matching](https://arxiv.org/abs/2602.05174)
*Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma*

Main category: stat.ML

TL;DR: Riemannian flow matching (RFM) adapts flow-based generative modeling for manifold-supported data by developing nonasymptotic Total Variation (TV) convergence analysis, separating numerical discretization and learning errors.


<details>
  <summary>Details</summary>
Motivation: To handle generative modeling for data residing on manifolds and ensure effective convergence analysis under RFM framework.

Method: Proposes a Riemannian flow matching (RFM) method with a time-dependent tangent vector field and develops TV convergence guarantees using differential inequalities and control techniques for manifold data.

Result: Derives explicit convergence bounds for TV error on compact and Hadamard manifolds, demonstrating concrete iteration complexities for specific manifolds like hyperspheres and SPD(n).

Conclusion: The paper establishes a well-defined convergence analysis for RFM in manifold settings, separating discretization and learning errors explicitly, which contributes to scalable generative modeling in non-Euclidean spaces.

Abstract: Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ (with an additional higher-order $\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\varepsilon$ is the target accuracy. Instantiations yield \emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.

</details>


### [81] [Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions](https://arxiv.org/abs/2602.05227)
*Elias Hess-Childs,Dejan Slepčev,Lantian Xu*

Main category: stat.ML

TL;DR: The paper introduces high-dimensional gradient flows for the KL divergence using Radon-based geometries, enabling efficient particle-based algorithms with linear cost scaling.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing gradient flows for the Kullback–Leibler (KL) divergence, particularly focusing on improving their efficiency and practicality in high-dimensional spaces.

Method: The authors propose new geometries—Radon-Wasserstein geometry and Regularized Radon-Wasserstein geometry—using the Radon transform, which simplifies gradient-flow velocities to depend only on one-dimensional projections. This approach enables the creation of efficient particle-based algorithms with linear computational cost per step.

Result: The paper introduces algorithms that leverage Fast Fourier Transform for efficient computation. Numerical experiments demonstrate the algorithm's performance in convergence behavior and quantization. Theoretical proofs ensure the well-posedness and long-term convergence of the flows.

Conclusion: The newly developed gradient flows provide an efficient framework for evolving distributions in high-dimensional spaces, with demonstrated accuracy, low computational cost, and theoretical guarantees.

Abstract: Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.

</details>


### [82] [Logarithmic-time Schedules for Scaling Language Models with Momentum](https://arxiv.org/abs/2602.05298)
*Damien Ferbach,Courtney Paquette,Gauthier Gidel,Katie Everett,Elliot Paquette*

Main category: stat.ML

TL;DR: The paper discusses introducing dynamic hyperparameter schedules (β₁, β₂, λ) for AdamW optimizer based on the power-law structure of language data, resulting in improved large-scale language model performance.


<details>
  <summary>Details</summary>
Motivation: In large-scale language model training, fixed hyperparameters in AdamW may limit optimization effectiveness. The paper explores the potential of dynamically varying these hyperparameters to harness the structure of language data.

Method: The study introduces logarithmic-time scheduling and damping mechanisms to dynamically adjust hyperparameters. It develops ADANA, an optimized variation of AdamW, employing these schedules for stability and enhanced performance.

Result: ADANA exhibited up to 40% compute efficiency relative to AdamW when tuned correctly. The gains improved with scaling model size and were also applicable to other optimizers. Logarithmic-time weight-decay showed significant individual benefits.

Conclusion: Dynamic hyperparameter scheduling enhances optimizer efficiency, stability, and robustness, enabling scalability and broader applicability in language model training.

Abstract: In practice, the hyperparameters $(β_1, β_2)$ and weight-decay $λ$ in AdamW are typically kept at fixed values. Is there any reason to do otherwise? We show that for large-scale language model training, the answer is yes: by exploiting the power-law structure of language data, one can design time-varying schedules for $(β_1, β_2, λ)$ that deliver substantial performance gains.
  We study logarithmic-time scheduling, in which the optimizer's gradient memory horizon grows with training time. Although naive variants of this are unstable, we show that suitable damping mechanisms restore stability while preserving the benefits of longer memory. Based on this, we present ADANA, an AdamW-like optimizer that couples log-time schedules with explicit damping to balance stability and performance. We empirically evaluate ADANA across transformer scalings (45M to 2.6B parameters), comparing against AdamW, Muon, and AdEMAMix.
  When properly tuned, ADANA achieves up to 40% compute efficiency relative to a tuned AdamW, with gains that persist--and even improve--as model scale increases. We further show that similar benefits arise when applying logarithmic-time scheduling to AdEMAMix, and that logarithmic-time weight-decay alone can yield significant improvements. Finally, we present variants of ADANA that mitigate potential failure modes and improve robustness.

</details>


### [83] [Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach](https://arxiv.org/abs/2602.05340)
*Beichen Wan,Mo Liu,Paul Grigas,Zuo-Jun Max Shen*

Main category: stat.ML

TL;DR: The paper tackles the inefficiency of traditional sequential experimental design in the predict-then-optimize framework by proposing a computationally efficient directional-based metric to optimize design choices based on decision loss instead of prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional sequential experimental designs focus on improving prediction accuracy but fail in scenarios where the ultimate goal is decision-making via optimization. This creates inefficiencies in real-world applications with decision-based objectives.

Method: The authors introduce a new directional-based metric for predictive uncertainty that aligns with decision loss and is computationally feasible by not requiring optimization oracle solutions.

Result: The proposed design criterion achieves faster stopping times and better efficiency compared to traditional designs under various distributions. These findings are supported by experiments on an LLM job allocation problem.

Conclusion: The study provides a sequential design method that better integrates prediction and optimization goals, leading to improved practical outcomes in predict-then-optimize applications.

Abstract: We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.

</details>


### [84] [Variance Reduction Based Experience Replay for Policy Optimization](https://arxiv.org/abs/2602.05379)
*Hua Zheng,Wei Xie,M. Ben Feng,Keilung Choy*

Main category: stat.ML

TL;DR: This paper introduces Variance Reduction Experience Replay (VRER), a method for selectively reusing past data in reinforcement learning to improve sample efficiency and reduce gradient variance, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods inefficiently reuse past experiences without considering their varying importance, which hampers policy optimization in complex stochastic environments.

Method: The authors propose VRER, an algorithm-agnostic experience replay mechanism that prioritizes informative samples to reduce variance in policy gradient estimation. They also introduce the Policy Gradient with VRER (PG-VRER) algorithm.

Result: VRER demonstrates empirical success in accelerating policy optimization and surpassing state-of-the-art policy optimization algorithms in performance.

Conclusion: Selective reuse of past experiences via VRER addresses bias-variance trade-offs effectively, leading to sample-efficient reinforcement learning with stronger theoretical guarantees.

Abstract: Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.

</details>


### [85] [Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers](https://arxiv.org/abs/2602.05395)
*Jingkai Huang,Will Ma,Zhengyuan Zhou*

Main category: stat.ML

TL;DR: The paper introduces an optimized strategy for selecting consistent responses from large language models (LLMs) using Bayesian priors, saving on computational cost without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving the accuracy of LLMs in math and reasoning tasks efficiently has become essential due to high computational costs.

Method: It leverages Bayesian prior information to stop sampling once sufficient consistency is reached. An efficient 'L-aggregated' policy tracks the L-1 most frequent answers, with theoretical proof that L=3 suffices for optimality.

Result: The approach reduces LLM inference costs by up to 50% while maintaining similar accuracy in identifying consistent answers.

Conclusion: The methodology enhances efficiency in selecting consistent LLM answers, mixing theoretical rigor with practical performance gains in computational cost savings.

Abstract: A simple strategy for improving LLM accuracy, especially in math and reasoning problems, is to sample multiple responses and submit the answer most consistently reached. In this paper we leverage Bayesian prior information to save on sampling costs, stopping once sufficient consistency is reached. Although the exact posterior is computationally intractable, we further introduce an efficient "L-aggregated" stopping policy that tracks only the L-1 most frequent answer counts. Theoretically, we prove that L=3 is all you need: this coarse approximation is sufficient to achieve asymptotic optimality, and strictly dominates prior-free baselines, while having a fast posterior computation. Empirically, this identifies the most consistent (i.e., mode) LLM answer using fewer samples, and can achieve similar answer accuracy while cutting the number of LLM calls (i.e., saving on LLM inference costs) by up to 50%.

</details>


### [86] [Fast Rates for Nonstationary Weighted Risk Minimization](https://arxiv.org/abs/2602.05742)
*Tobias Brock,Thomas Nagler*

Main category: stat.ML

TL;DR: The paper studies weighted empirical risk minimization under distribution drift and provides a thorough analysis of out-of-sample prediction errors, offering oracle inequalities and illustrating sharpness through various models.


<details>
  <summary>Details</summary>
Motivation: To address the prediction challenges under distribution drift and analyze how weighted empirical risk minimization performs in such nonstationary scenarios.

Method: The authors decompose excess risk into learning and distribution drift error terms, derive oracle inequalities under mixing conditions, and analyze their framework under various model setups.

Result: They demonstrate uniform learning bounds across weight classes, effective sample size considerations, error decomposition, and achieve minimax-optimal rates in unweighted, stationary cases for linear models, basis approximations, and neural networks.

Conclusion: The proposed analysis framework provides detailed insights into learning bounds and excess risk under nonstationarity, validating the robustness and relevance of weighted empirical risk minimization.

Abstract: Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.

</details>


### [87] [Optimal scaling laws in learning hierarchical multi-index models](https://arxiv.org/abs/2602.05846)
*Leonardo Defilippis,Florent Krzakala,Bruno Loureiro,Antoine Maillard*

Main category: stat.ML

TL;DR: This paper analyzes scaling laws in two-layer neural networks for hierarchical targets in a representation-limited regime, providing exact scaling laws, phase transition patterns, and optimal estimation methods.


<details>
  <summary>Details</summary>
Motivation: To understand how two-layer neural networks learn hierarchical targets in a representation-limited regime and explain observed scaling behaviors and phenomena in such networks.

Method: The authors derive information-theoretic scaling laws to analyze subspace recovery and prediction error, examine phase transitions, and use a spectral estimator as a model for gradient descent.

Result: Exact scaling laws and a cascade of phase transitions are uncovered; the spectral estimator is shown to achieve optimal rates, and these findings explain scaling behaviors and plateau phenomena.

Conclusion: The work provides mathematical insights into scaling laws, phase transitions, and statistical efficiency in learning hierarchical targets within shallow networks.

Abstract: In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [88] [System-Level Isolation for Mixed-Criticality RISC-V SoCs: A "World" Reality Check](https://arxiv.org/abs/2602.05002)
*Luis Cunha,Jose Martins,Manuel Rodriguez,Tiago Gomes,Sandro Pinto,Uwe Moslehner,Kai Dieffenbach,Glenn Farrall,Kajetan Nuernberger,Thomas Roecker*

Main category: cs.CR

TL;DR: The paper analyzes RISC-V hardware isolation primitives for heterogeneous SoCs in mixed-criticality environments, proposing modified designs to improve real-time performance and scalability with other benefits like area reduction.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the core safety and security challenges posed by system-level isolation in mixed-criticality environments using RISC-V for domains like IoT and automotive, where stringent SWaP-C constraints impact SoC designs.

Method: The study evaluates RISC-V isolation primitives (e.g., RISC-V Worlds, IOPMP, SmMTT) through a comparative analysis. It involves implementing and modifying these primitives (e.g., creating a new version of RISC-V World checker) and testing their trade-offs in security, performance, power, and area.

Result: The modified World-based checker demonstrates lower worst-case delay and more predictable scalability compared to alternatives. Additionally, proposed changes save up to 5% of SoC area while maintaining or improving security and performance.

Conclusion: The findings can directly influence RISC-V specification improvements and guide the development of future RISC-V SoCs to better support mixed-criticality, real-time systems under SWaP-C constraints. The open-source release of artifacts aims to accelerate adoption and innovation.

Abstract: As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [89] [Quantum Sequential Circuits](https://arxiv.org/abs/2602.05166)
*D. -S. Wang*

Main category: quant-ph

TL;DR: This paper introduces quantum sequential circuits (QSCs), which utilize quantum transistors, providing a new architecture for quantum computing with memory and sequencing capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to establish a new quantum computing framework that integrates core principles of memory and temporal sequencing, addressing the limitations of the current qubit-based architectures.

Method: The authors propose a new model based on quantum transistors and symmetry-protected topological junctions, encoding quantum gates as Choi states and enabling activated feedback loops via bulk measurements.

Result: QSCs were developed as a universal computational model that integrates memory and sequencing naturally, complementing existing combinational circuits and advancing the path toward a quantum von Neumann architecture.

Conclusion: The work highlights the feasibility and significance of hybrid and modular designs for scalable quantum processors and indicates a step forward in achieving high-performance integrated quantum systems.

Abstract: This work introduces and characterizes quantum sequential circuits (QSCs) as a hardware-oriented paradigm for quantum computing, built upon a novel foundational element termed the quantum transistor. Unlike conventional qubit-based architectures, QSCs employ symmetry-protected topological junctions where quantum gates are encoded as Choi states via channel-state duality and activated through bulk measurements, utilizing ebits to realize the functional analog of feedback loops in classical sequential circuits. This framework establishes a universal model for quantum computation that inherently incorporates memory and temporal sequencing, complementing existing combinational quantum circuit model. Our work advances the conceptual bridge towards a quantum von Neumann architecture, underscoring the potential of hybrid and modular design principles for the development of large-scale, integrated quantum information processors.

</details>


### [90] [Distributed Quantum Error Mitigation: Global and Local ZNE encodings](https://arxiv.org/abs/2602.04981)
*Maria Gragera Garces*

Main category: quant-ph

TL;DR: This paper explores the effectiveness of Zero Noise Extrapolation (ZNE) in distributed quantum computing environments, finding that global ZNE scales better and reduces errors significantly across multiple QPUs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of errors in distributed quantum computing due to communication-induced noise, which limits practical implementation.

Method: The study compares two ZNE optimization approaches: Global (applied before circuit partitioning) and Local (applied independently to sub-circuits), evaluates their performance across varying QPUs, and introduces noisy teleportation-based communication.

Result: Global ZNE demonstrates error reduction up to 48% across six QPUs and counterintuitive findings such as improved effectiveness with an increased number of QPUs.

Conclusion: Fundamental trade-offs in distributed quantum error mitigation are revealed, suggesting new areas of exploration regarding circuit design, partitioning strategies, and network-induced noise behavior.

Abstract: Errors are the primary bottleneck preventing practical quantum computing. This challenge is exacerbated in the distributed quantum computing regime, where quantum networks introduce additional communication-induced noise. While error mitigation techniques such as Zero Noise Extrapolation (ZNE) have proven effective for standalone quantum processors, their behavior in distributed architectures is not yet well understood. We investigate ZNE in this setting by comparing Global optimization (ZNE is applied prior to circuit partitioning), against Local optimization (ZNE is applied independently to each sub-circuit). Partitioning is performed on a monolithic circuit, which is then transformed into a distributed implementation by inserting noisy teleportation-based communication primitives between sub-circuits. We evaluate both approaches across varying numbers of quantum processing units (QPUs) and under heterogeneous local and network noise conditions. Our results demonstrate that Global ZNE exhibits superior scalability, achieving error reductions of up to $48\%$ across six QPUs. Moreover, we observe counterintuitive noise behavior, where increasing the number of QPUs improves mitigation effectiveness despite higher communication overhead. These findings highlight fundamental trade-offs in distributed quantum error mitigation and raise new questions regarding the interplay between circuit structure, partitioning strategies, and network noise.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [91] [From Sequential to Parallel: Reformulating Dynamic Programming as GPU Kernels for Large-Scale Stochastic Combinatorial Optimization](https://arxiv.org/abs/2602.05179)
*Jingyi Zhao,Linxin Yang,Haohua Zhang,Tian Ding*

Main category: math.OC

TL;DR: The paper presents a GPU-based framework to efficiently solve full-fidelity integer second-stage stochastic programming models at scale, overcoming the bottleneck of computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the bottleneck in stochastic programming (SP) caused by computationally expensive exact second-stage problems for every scenario, especially with NP-hard combinatorial structures.

Method: The authors propose a GPU-based framework with hardware-aware, scenario-batched kernels that exploit parallelism across scenarios, dynamic-programming layers, and actions, enabling efficient Bellman updates.

Result: Their framework achieves up to four-five orders of magnitude speedup, scales nearly linearly in the number of scenarios, and improves decision quality by enabling the evaluation of larger scenario sets and better first-stage candidates.

Conclusion: Full-fidelity integer second-stage models are now tractable at unprecedented scales, marking a practical path to large-scale and realistic stochastic discrete optimization.

Abstract: A major bottleneck in scenario-based Sample Average Approximation (SAA) for stochastic programming (SP) is the cost of solving an exact second-stage problem for every scenario, especially when each scenario contains an NP-hard combinatorial structure. This has led much of the SP literature to restrict the second stage to linear or simplified models. We develop a GPU-based framework that makes full-fidelity integer second-stage models tractable at scale. The key innovation is a set of hardware-aware, scenario-batched GPU kernels that expose parallelism across scenarios, dynamic-programming (DP) layers, and route or action options, enabling Bellman updates to be executed in a single pass over more than 1,000,000 realizations. We evaluate the approach in two representative SP settings: a vectorized split operator for stochastic vehicle routing and a DP for inventory reinsertion. Implementation scales nearly linearly in the number of scenarios and achieves a one-two to four-five orders of magnitude speedup, allowing far larger scenario sets and reliably stronger first-stage decisions. The computational leverage directly improves decision quality: much larger scenario sets and many more first-stage candidates can be evaluated within fixed time budgets, consistently yielding stronger SAA solutions. Our results show that full-fidelity integer second-stage models are tractable at scales previously considered impossible, providing a practical path to large-scale, realistic stochastic discrete optimization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [92] [Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization](https://arxiv.org/abs/2602.04900)
*Sai Sindhur Malleni,Raúl Sevilla,Aleksei Vasilevskii,José Castillo Lema,André Bauer*

Main category: cs.ET

TL;DR: The paper demonstrates how Kubernetes-native projects can improve the scalability and resource efficiency of Generative AI (GenAI) workflows, showcasing performance improvements through multi-stage use cases.


<details>
  <summary>Details</summary>
Motivation: Generative AI workloads, particularly inference tasks, are becoming increasingly dominant but face challenges in scalability and resource utilization, requiring tailored solutions in the Kubernetes ecosystem.

Method: The paper showcases two scenarios: batch inference using Kueue and DAS for speech recognition, and discrete online inference with llm-d and GAIE for summarization, leveraging Kubernetes-native projects.

Result: Key performance improvements in GenAI workloads include Kueue reducing total makespan by 15%, DAS shortening mean job completion time by 36%, and GAIE improving Time to First Token by 82%.

Conclusion: Emerging Kubernetes-native projects (Kueue, DAS, and GAIE) demonstrate impressive potential in optimizing and scaling Generative AI workflows, reinforcing Kubernetes as a robust framework for such tasks.

Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\%.

</details>
