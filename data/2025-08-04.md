<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.SE](#cs.SE) [Total: 19]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.GR](#cs.GR) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.LO](#cs.LO) [Total: 3]
- [quant-ph](#quant-ph) [Total: 4]
- [eess.IV](#eess.IV) [Total: 6]
- [math.ST](#math.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.SD](#cs.SD) [Total: 1]
- [hep-ph](#hep-ph) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: The paper critiques HealthBench for its reliance on expert opinion, proposes improvements using evidence-based Clinical Practice Guidelines, and outlines a roadmap for globally relevant and trustworthy medical AI.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of HealthBench, which relies on regional expert opinions and automated grading, leading to potential biases and inequities, particularly in low-resource settings.

Method: They propose anchoring reward functions in Clinical Practice Guidelines with systematic reviews, evidence-weighted scoring, and contextual override logic, emphasizing ethical considerations and global relevance.

Result: The proposed system re-grounds rewards in evidence-based guidelines and integrates ethical considerations to create AI systems that are better aligned with global medical needs.

Conclusion: The paper advocates for a shift towards evidence-based, globally equitable benchmarks for evaluating medical AI systems to ensure inclusivity, trustworthiness, and clinical reliability.

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: This paper explores security-aware reinforcement learning (RL) using HyperTWTL constraints, proposing an approach with dynamic Boltzmann softmax RL and outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the gap in applying hyperproperties in reinforcement learning for security constraints, especially for robotics.

Method: HyperTWTL constraints are applied to a Markov Decision Process, and dynamic Boltzmann softmax reinforcement learning is used to derive security-aware policies.

Result: The proposed method was effective and scalable, outperforming two baseline RL algorithms in a robotic pick-up and delivery case study.

Conclusion: HyperTWTL-constrained secure RL is feasible and can outperform existing algorithms, demonstrating its potential for robotics applications and beyond.

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: The paper advocates using Object-Centric Process Mining (OCPM) as a foundation for integrating AI into operational processes effectively, introducing the concept of Process Intelligence (PI).


<details>
  <summary>Details</summary>
Motivation: Organizations struggle to implement AI successfully in industrial and operational settings, particularly due to challenges in managing dynamic and structured process-related data.

Method: The paper proposes grounding AI through OCPM, which connects process-related data with generative, predictive, and prescriptive AI techniques to improve operational processes.

Result: OCPM emerges as the key method for bridging data and processes, enabling organizations to implement AI effectively in dynamic business and research environments.

Conclusion: AI requires the use of Process Intelligence, driven by OCPM, to address the dynamic and structured nature of organizational processes and ensure practical successful applications in operational contexts.

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: The paper discusses Rank Reversals in Multi-Criteria Decision Analysis, introduces tests to detect them implemented in Scikit-Criteria, and discusses their implications.


<details>
  <summary>Details</summary>
Motivation: Rank Reversals pose significant challenges in Multi-Criteria Decision Analysis and can highly influence outcomes, necessitating a means to measure method effectiveness.

Method: The authors propose three tests for detecting Rank Reversals and describe their implementation within the Scikit-Criteria library, along with design considerations for general scenarios.

Result: Three detection tests for Rank Reversals are successfully integrated in the Scikit-Criteria library, addressing implementation complications.

Conclusion: These tools can enhance the evaluation of Multi-Criteria Decision Methods, potentially influencing the selection of effective problem-solving approaches.

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: The paper studies SHACL validation in RDF graphs under updates, proposing a SHACL-based update language and addressing static validation problems using regression techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining SHACL validation on RDF graphs amidst updates, enabling reasoning about evolving RDF graphs.

Method: The authors develop a SHACL-based update language, employ regression techniques to integrate update actions into SHACL constraints, and reduce the static validation problem to constraint satisfiability.

Result: The paper analyzes the computational complexity of the proposed static validation, introduces a prototype implementation, and validates it with preliminary experiments.

Conclusion: Static validation under updates is feasible and can be framed as a constraint satisfiability problem, offering robust tools for reasoning in evolving RDF graph scenarios.

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: This paper explores the potential harms of AI algorithms to culturally marginalized groups and proposes an augmented AI lifecycle to address these biases through participatory and inclusive frameworks.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the disproportionate impact of AI algorithms on culturally marginalized groups and reduce algorithmic biases.

Method: The authors propose a re-designed AI lifecycle consisting of five stages: co-framing, co-design, co-implementation, co-deployment, and co-maintenance, informed by multidisciplinary workshops and collaborative design principles.

Result: The paper describes an augmented AI lifecycle model aligned with ethical AI frameworks, emphasizing distributed authority and iterative knowledge exchange.

Conclusion: The proposed AI lifecycle promotes equity, inclusion, and participatory governance while addressing risks and biases inherent in AI production. Further research is required to scale these participatory approaches.

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: The paper critiques reliance on inter-rater reliability (IRR) metrics for educational AI data labeling and proposes alternative evaluation methods that prioritize validity and educational impact.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in human-based IRR approaches to labeling training data, which limit progress in building valid and predictive AI models in educational applications.

Method: The authors describe five complementary evaluation methods, including multi-label annotation schemes, expert-based approaches, and external validity procedures like validating tutor moves across multiple categories.

Result: Suggested alternative approaches promise improvements in producing training datasets and AI models that enhance student learning and yield actionable insights.

Conclusion: The paper advocates for prioritizing validity and educational impact in annotation quality standards over consensus-focused approaches like IRR alone.

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: The paper proposes a method to ensure AI agents empower humans while maintaining human-AI power balance for safety and wellbeing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges around power-seeking behavior and disempowerment of humans by AI systems, ensuring AI contributes positively to human wellbeing and safety.

Method: The authors design an objective function representing human power aggregates, considering bounded rationality, social norms, and varied human goals. Algorithms derive this metric using backward induction and multi-agent reinforcement learning.

Result: The framework demonstrates benefits in simulations by softly maximizing human-centric power metrics, showcasing its implications for AI sub-goals.

Conclusion: Softly maximizing human power metrics could offer safer and beneficial objectives for AI compared to direct utility-based approaches.

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS improves Large Language Models' reasoning capabilities by integrating internal and external strategies. It surpasses state-of-the-art RLVR methods across benchmarks and eliminates capability boundary collapse.


<details>
  <summary>Details</summary>
Motivation: RLVR struggles with the inherent capability boundaries of base LLMs due to on-policy strategies, large action spaces, sparse rewards, and boundary collapse. Authors aim to overcome these limitations.

Method: RL-PLUS combines internal exploitation (Thinking) and external learning with Multiple Importance Sampling for distributional handling and an Exploration-Based Advantage Function to find valuable unexplored reasoning paths.

Result: RL-PLUS outperforms RLVR methods on six math reasoning benchmarks and six out-of-distribution tasks, achieving relative improvements between 21.1% and 69.2%, and resolving capability boundary issues.

Conclusion: The paper demonstrates RL-PLUS's superior reasoning ability, consistency across diverse model families, and resolves inherent limitations of RLVR methods.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [10] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent proposes a system that learns by doing, improving its performance without retraining by requesting external help, reflecting on completed tasks, and building internal tools and knowledge bases.


<details>
  <summary>Details</summary>
Motivation: To create an agentic system that can adapt, self-improve, and achieve better performance in knowledge discovery tasks without requiring retraining or the modification of model parameters.

Method: MetaAgent uses minimal reasoning and help-seeking capabilities to tackle tasks, generate external requests when needed, reflect on results for self-improvement, and create persistent tools and knowledge bases for continuous learning.

Result: MetaAgent outperformed workflow-based approaches and matched or exceeded end-to-end trained agents in evaluations on knowledge discovery benchmarks like GAIA, WebWalkerQA, and BrowseCamp.

Conclusion: The MetaAgent approach shows the promise of self-evolving systems in achieving robust and general-purpose knowledge discovery capabilities without requiring retraining.

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: The paper examines the difference between human task generation and that of GPT-4o, revealing limitations in the LLM's ability to mimic human psychological drivers and embodied cognition.


<details>
  <summary>Details</summary>
Motivation: To investigate whether generative agents powered by large language models simulate human cognitive behavior in task generation, particularly in terms of psychological drivers and values.

Method: A task-generation experiment was conducted where human responses were compared with those of GPT-4o, with explicit psychological drivers provided to the LLM to assess its adaptation.

Result: The LLM failed to exhibit human-like behavioral patterns in task generation, creating outputs that were less social, physical, and overly abstract, despite being perceived as fun and novel.

Conclusion: A critical gap exists between human cognition, which is value-driven and embodied, and the statistical functioning of LLMs. Enhancing human alignment in AI requires incorporating intrinsic motivation and physical grounding.

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: The paper introduces ReasonBench, a benchmark for evaluating visual language models (VLMs) in complex graphic reasoning tasks, revealing significant limitations and proposing optimization strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current visual language models struggle with complex graphic reasoning and abstract problem-solving, and there is a need to evaluate their capabilities comprehensively beyond simple graphics.

Method: The study developed ReasonBench, a benchmark featuring 1,613 structured graphic reasoning questions across location, attribute, quantity, and multi-element tasks. It also introduced two optimization strategies: Diagrammatic Reasoning Chain (DiaCoT) and ReasonTune to enhance VLMs' reasoning interpretability and adaptability.

Result: ReasonBench was used to evaluate 11 mainstream VLMs, exposing their significant limitations. The proposed optimization strategies improved VLM performance by 33.5%.

Conclusion: ReasonBench provides a systematic evaluation of VLMs' advanced reasoning abilities, and the optimization strategies offer a pathway for improving their performance in complex reasoning tasks.

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: The paper addresses safety risks in large reasoning models (LRMs) and introduces R1-Act, a method to activate safety knowledge effectively.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the problem of LRMs fulfilling harmful user instructions despite having safety knowledge.

Method: The authors propose R1-Act, a post-training technique that explicitly activates safety knowledge through a structured reasoning process, requiring minimal data and computational resources.

Result: R1-Act improves safety while preserving reasoning performance across various LRM configurations, outperforming previous methods.

Conclusion: The approach is efficient, robust, and scalable, demonstrating practical benefits and addressing safety concerns in LRMs effectively.

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: This paper introduces CoRGI, a framework enhancing reasoning in vision-language models by incorporating visual verification into textual reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models often produce fluent textual reasoning that may lack grounding in visual content, leading to hallucinations.

Method: CoRGI employs a three-stage pipeline: generating textual reasoning, verifying visual evidence for each step using a dedicated module, and synthesizing textual rationale with visual findings.

Result: Integrating CoRGI improved reasoning performance in VLMs like Qwen-2.5VL and LLaVA-1.6, confirmed via ablation studies and human evaluations.

Conclusion: Grounding reasoning steps in visual evidence enhances explanation robustness; CoRGI is effective without requiring end-to-end retraining.

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: This paper introduces a method for multi-agent cooperation using theory of mind (ToM) within active inference, enabling agents to collaborate effectively without specific shared models or communication.


<details>
  <summary>Details</summary>
Motivation: To improve multi-agent cooperation by allowing agents to reason about others' beliefs and goals without relying on predefined models or explicit communication.

Method: A framework is devised where ToM-equipped agents use tree-based planning to explore joint policies via recursive reasoning, maintaining separate representations of their own and others' beliefs and goals.

Result: In simulations of collision avoidance and foraging tasks, ToM-equipped agents showed superior cooperation, avoiding collisions and minimizing redundant work by inferring others' beliefs from actions alone.

Conclusion: This approach enhances artificial intelligence applications by enabling effective collaboration in multi-agent systems, while also offering computational insights into theory of mind mechanisms.

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: This paper introduces Cognitive Kernel-Pro, an open-source and free AI agent framework that excels in various domains, surpassing previous systems.


<details>
  <summary>Details</summary>
Motivation: Current general AI agent systems are often closed-source or depend on paid APIs, hindering accessibility and reproducibility within the research community.

Method: The authors developed Cognitive Kernel-Pro, focusing on high-quality data curation for Agent Foundation Models and employing innovative test-time reflection and voting strategies for robustness.

Result: Cognitive Kernel-Pro achieved state-of-the-art performance on GAIA, with its 8B-parameter model outperforming leading systems like WebDancer and WebSailor.

Conclusion: Cognitive Kernel-Pro democratizes the development of advanced AI agents by setting a new standard of performance while being accessible, open-source, and free to use.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [17] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: This paper explores the application of Large Language Models (LLMs) to formal and informal mathematics, highlighting challenges in proof generation compared to code synthesis.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand why LLMs perform well in coding yet struggle in formal mathematical theorem proving, aiming to shed light on their reasoning and logical capabilities.

Method: The authors analyze state-of-the-art models, benchmarks, and address specific issues related to training domains, proof generation brittleness, and logical state representation.

Result: The paper identifies limits in how LLMs handle formal and informal mathematics and highlights areas for improvement, such as understanding deductive and computational state.

Conclusion: While LLMs show promise in mathematics, significant challenges remain in their capabilities for formal theorem proving. Future improvements can enhance their reasoning and mathematical cognition.

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [18] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: The paper introduces Pro2Guard, a proactive safety enforcement framework for autonomous agents, addressing limitations in reactive systems by predicting unsafe states using probabilistic analysis.


<details>
  <summary>Details</summary>
Motivation: Current rule-based systems for autonomous agents address safety reactively, lacking the ability to anticipate long-term risks, especially under complex dependencies and shifts.

Method: Pro2Guard uses probabilistic reachability analysis, abstracting agent behaviors into symbolic states and learning a Discrete-Time Markov Chain (DTMC) to predict and preemptively address safety violations based on a risk threshold.

Result: Pro2Guard was evaluated in household agent tasks and autonomous vehicles, achieving up to 93.6% unsafe task resolution in early intervention and 100% prediction of traffic law violations for driving scenarios.

Conclusion: Pro2Guard demonstrates significant improvements in safety by proactively enforcing rules, balancing intervention with task success, and showing promise for broader applications in safety-critical domains.

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [19] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: This paper introduces MultiSHAP, a framework that explains cross-modal interactions in multimodal AI models, enabling more interpretability for these "black-box" systems.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and interpretability in multimodal AI models prevents their deployment in critical applications where trust is essential.

Method: MultiSHAP employs the Shapley Interaction Index to provide fine-grained explanations of multimodal predictions by quantifying interactions between visual and textual data elements.

Result: Experiments confirm that MultiSHAP effectively captures cross-modal reasoning mechanisms and provides valuable insights for real-world applications.

Conclusion: MultiSHAP is a model-agnostic solution for explaining multimodal AI models, applicable to both open- and closed-source systems, offering instance- and dataset-level explanations with broader applicability beyond two modalities.

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [20] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: The paper proposes a multi-stage, LLM-driven framework to generate effective pre-consultation questionnaires from Electronic Medical Records (EMRs), improving upon direct LLM methods.


<details>
  <summary>Details</summary>
Motivation: Generating personalized and comprehensive pre-consultation questionnaires from complex EMRs is essential for healthcare but is challenging for direct LLM methods due to issues with completeness, order, and synthesis.

Method: The paper introduces a 3-stage framework: Stage 1 extracts key facts from EMRs, Stage 2 builds causal networks and synthesizes disease-specific knowledge, and Stage 3 generates tailored questionnaires.

Result: The method showed improved information coverage, diagnostic relevance, understandability, and faster generation times when tested on real-world data and validated by clinical experts.

Conclusion: The framework offers a practical, effective solution for improving pre-consultation questionnaire generation and better healthcare delivery.

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [21] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: The paper addresses challenges in generating interactive audio-visual content, introducing AVR-Eval for evaluating multimedia content quality and AVR-Agent for improved content creation.


<details>
  <summary>Details</summary>
Motivation: Developing interactive audio-visual content (e.g., video games) lags behind AI's success in text, image, and video generation, especially due to the absence of evaluation metrics and difficulty in handling complex, multi-agent scenarios.

Method: The authors introduce AVR-Eval, a relative quality metric for comparing multimedia content using omni-modal models, and AVR-Agent, a multi-agent system for generating and improving JavaScript-based games by leveraging multimedia asset banks and iterative feedback loops.

Result: AVR-Agent outperforms one-shot generation methods in creating higher-quality content, but current models are unable to fully utilize custom assets and feedback effectively, unlike humans.

Conclusion: While AVR-Eval and AVR-Agent improve multimedia content generation, gaps remain in how AI leverages resources compared to human creators, underlining fundamental differences in their content creation approaches.

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: This paper introduces a novel framework, MB-VLGC, for analyzing time series, explicitly modeling frequency-dependent causal delays, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Granger causality methods assume a fixed lag between cause and effect, which is unrealistic for complex systems. Existing variable-lag approaches do not consider frequency-dependent delays seen in phenomena like brain signals.

Method: The authors formalized Multi-Band Variable-Lag Granger Causality (MB-VLGC), demonstrated its theoretical foundation, and developed an efficient inference pipeline to model frequency-dependent causal delays.

Result: The proposed framework showed superior performance over existing methods in capturing variable-lag and frequency band interactions through extensive testing on synthetic and real-world data.

Conclusion: MB-VLGC broadens the applicability of Granger causality analysis to account for frequency-dependent causal effects, making it a vital tool for analyzing time series data across domains like neuroscience and economics.

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: The paper introduces a hybrid framework combining traditional XAI and generative AI to provide multimodal, personalized explanations for AI-driven education systems.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency in AI-driven learning systems and the inadequacy of current explainable AI techniques to address diverse user roles and comprehension challenges in education.

Method: Proposes a hybrid framework that integrates traditional XAI techniques with generative AI and user personalization to create tailored, multimodal explanations.

Result: The framework addresses key challenges in accuracy, fairness, and personalization in education-oriented AI systems, while advancing the dynamic communication-based approach to explainability.

Conclusion: Redefining explainability as user-tailored communication provides a pathway to more transparent, user-centered AI systems for education.

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: The paper proposes a framework for personalized, context-aware AI explanation systems on social media, addressing both expert and lay user needs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of user-specific and context-aware explainability in AI recommendations on social media, which undermines user trust and decision-making.

Method: A visual explanation system with diverse methods tailored to user needs, offering explanations in forms ranging from technical (for AI experts) to simplified (for lay users), including visual and numeric styles.

Result: The proposed system will undergo public validation with 30 users to assess its effectiveness in improving trust and decision-making.

Conclusion: The framework uniquely adapts both explanation style and granularity in a single pipeline, potentially setting a new standard for AI explainability in social media.

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: This paper proposes leveraging large pre-trained multi-modal models and their latent codes to build universal fake detectors that generalize across different generative content modalities.


<details>
  <summary>Details</summary>
Motivation: The need to combat misinformation and deepfakes generated using advanced generative models, which current fake detectors fail to generalize effectively across different models and data modalities.

Method: They utilize the latent representations from large pre-trained multi-modal models to train linear classifiers for detecting fake content.

Result: The approach performs state-of-the-art across various data modalities, surpassing or matching existing baseline methods in audio and image fake detection.

Conclusion: Latent codes of pre-trained multi-modal models inherently differentiate real from fake content, enabling the creation of computationally efficient, generalizable fake detectors.

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer](https://arxiv.org/abs/2508.00475)
*Yunhao Ma,Yanyu Lin,Mingjing Li,Puli Quan,Chenlin Zhou,Wenyue Zhang,Zhiwei Zhong,Wanyi Jia,Xueke Zhu,Qingyan Meng,Huihui Zhou,Fengwei An*

Main category: cs.AR

TL;DR: The abstract only provides affiliations of the contributors, lacking substantive content to analyze.


<details>
  <summary>Details</summary>
Motivation: No insights into the research problem or motivation are provided.

Method: The methodology is absent; no information on approaches or techniques used.

Result: This abstract does not describe any outcomes or findings.

Conclusion: Conclusions cannot be drawn due to the absence of relevant content.

Abstract: (1) Pengcheng Laboratory, (2) Southern University of Science and Technology,
(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
(4) University of Chinese Academy of Sciences

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: The paper evaluates Large Language Models (LLMs) in solving physics problems, introduces techniques to enhance their performance, applies a multi-agent verification framework, and presents a new benchmark dataset, PhysicsEval.


<details>
  <summary>Details</summary>
Motivation: Physics is a foundational intellectual field, and solving physics problems is a key domain of natural language reasoning. Evaluating and enhancing LLMs' performance in this domain will contribute to understanding and solving complex issues.

Method: The paper uses inference-time techniques and a multi-agent framework to verify and enhance model-proposed solutions. A benchmark dataset, PhysicsEval, featuring 19,609 physics problems, is introduced for evaluation.

Result: Significant performance improvements were observed using multi-agent frameworks, especially in areas where the models initially struggled.

Conclusion: Multi-agent frameworks and inference techniques enhance LLM capabilities in solving physics problems, and the PhysicsEval benchmark serves as a valuable resource for further research in this domain.

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [28] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: The paper evaluates optimization methods for long-context large language models (LLMs) in terms of memory, latency, throughput, and text generation quality. It highlights potential issues like compounded errors in larger models.


<details>
  <summary>Details</summary>
Motivation: To address resource challenges and limited context windows in LLMs and explore the impacts of optimizations like pruning, quantization, and token dropping on performance and quality.

Method: The study benchmarks various optimization techniques applied to two LLM architectures, evaluates their combinations, and examines scalability effects on a 70 billion-parameter model, integrating system profiling with task-specific insights.

Result: Findings show that naive combinations of optimization methods can harm larger models and that F1 metrics may mislead by concealing precision-recall trade-offs.

Conclusion: The work guides practitioners in balancing efficiency, accuracy, and scalability across tasks and systems, cautioning against oversimplified optimization approaches.

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [29] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: This study evaluates lexical diversity in texts generated by ChatGPT models versus human writers, concluding that LLM-generated texts are notably different and less human-like.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM-generated texts exhibit human-like lexical diversity and to assess differences across LLM models and human writing.

Method: The study analyzed text from ChatGPT models (-3.5, -4, -o4 mini, -4.5) and human writers using six lexical diversity measures. Statistical tests and Support Vector Machines identified differences.

Result: Texts generated by newer ChatGPT models (e.g., -4.5) displayed less human-like lexical diversity, with significant variation across all dimensions compared to human texts. Human writing showed consistency regardless of education or language background.

Conclusion: LLMs, especially newer iterations, produce texts that diverge significantly from human lexical diversity patterns, impacting their use in language pedagogy and related fields.

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [30] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: This paper critiques current computational humanities methods for oversimplifying complex cultural and linguistic data, introducing the concept of semiotic complexity to address interpretive challenges and offering solutions for better practices.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to advance the computational humanities field by addressing the lack of theoretical frameworks for understanding translation processes between cultural/linguistic and computational/mathematical domains.

Method: The authors introduce the concept of semiotic complexity and critique dominant computational modeling practices, particularly in evaluation methods, for inadequately handling interpretive variance in text meaning.

Result: The paper identifies a major flaw in current computational practices: the treatment of semiotically complex data as simple. This oversight leads to epistemological issues and superficial interpretive clarity.

Conclusion: The authors recommend computational humanists develop theories addressing semiotic complexity, ensuring consistency, accuracy, and transparency in their interpretive work.

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [31] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: The paper introduces FACTORY, a human-verified prompt set for evaluating long-form factuality of language models, revealing significant factual inaccuracies in state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for long-form factuality evaluation lack human verification, leading to unreliable quality assessments.

Method: FACTORY was created through a model-in-the-loop process followed by human refinement, ensuring challenging and fact-seeking prompts.

Result: FACTORY shows 40% factual inaccuracies in state-of-the-art model outputs compared to only 10% in older benchmarks, highlighting its difficulty.

Conclusion: FACTORY is a reliable and robust benchmark that underscores the need for improved factual reasoning in language models.

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [32] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: Neural semantic parsers, despite achieving high scores overall, struggle significantly with context-sensitive phenomena like English verb phrase ellipsis.


<details>
  <summary>Details</summary>
Motivation: To investigate whether neural semantic parsers can effectively handle strongly context-sensitive linguistic phenomena, such as English verb phrase ellipsis.

Method: The authors created a corpus of 120 ellipsis cases with their resolved semantic representations and evaluated various neural semantic parsers on this challenge set.

Result: While parsers performed well on standard tests, they exhibited poor performance on the ellipsis challenge set.

Conclusion: Neural semantic parsers currently lack robustness in addressing strongly context-sensitive phenomena such as verb phrase ellipsis.

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [33] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: This paper addresses the complex landscape of Large Language Models (LLMs) by providing a comparative list to help users select the most suitable model based on features like licensing, hardware needs, and release year.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution and proliferation of LLMs have made it challenging for researchers and organizations to choose the right model amidst varying licensing and hardware demands.

Method: The authors compiled and published a comparative list of foundational and fine-tuned LLMs with key features and specifications on GitLab, ensuring regular updates to guide optimal model selection.

Result: The study culminates in a comprehensive resource offering detailed comparisons of LLMs, making it easier to navigate the model selection process in a dynamic AI landscape.

Conclusion: By creating an adaptable and detailed public repository, this paper contributes to simplifying the selection process for LLMs, enabling informed decision-making for AI practitioners and researchers.

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [34] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: This paper addresses challenges in table understanding for large language models (LLMs and MLLMs), proposes a taxonomy of table representations, and identifies critical gaps in the field.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the complex and diverse nature of tabular data representation in LLMs and MLLMs, which makes it difficult to generalize tasks and methods.

Method: The approach includes introducing a taxonomy of tabular input types, defining table understanding tasks, and identifying key research gaps in the field.

Result: The paper identifies three critical gaps: over-focus on retrieval tasks, difficulties with complex and large table structures, and limited model generalization across tabular formats.

Conclusion: There is a pressing need for further research to advance table understanding capabilities in LLMs and MLLMs given the field's current limitations.

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [35] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: The paper explores the use of Discrete Wavelet Transforms (DWT) to compress and analyze word/sentence embeddings without losing semantic quality, achieving reduced dimensionality and improved performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate the ability of wavelet transforms, specifically DWT, to enhance word and sentence embeddings in NLP tasks by compressing data and extracting semantic information.

Method: The authors applied DWT to embeddings, reducing their dimensionality while preserving their quality, and tested the effectiveness on semantic similarity and other downstream tasks using various embedding models.

Result: DWT reduced embedding dimensionality by 50-93%, maintaining performance in semantic similarity tasks and improving accuracy in most downstream NLP tasks.

Conclusion: DWT is an effective tool for improving embeddings in NLP, offering compression and enhanced performance, leading to better data representation in NLP applications.

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [36] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: This study analyzed lexical trends in conversational podcasts pre- and post-2022, observing increased usage of words linked with Large Language Models (LLMs), raising questions about AI's impact on human language evolution.


<details>
  <summary>Details</summary>
Motivation: To explore whether the increasing adoption of AI-generated text is influencing broader changes in human language use.

Method: Analyzed a dataset of 22.1 million words from conversational science and technology podcasts before and after ChatGPT's 2022 release, focusing on LLM-linked word trends.

Result: Identified a moderate but significant rise in the use of LLM-associated words post-2022, while baseline synonyms showed no notable change.

Conclusion: This study highlights the initial signs of language shifts potentially driven by AI-LLM exposure, leaving open questions about their nature and ethical implications for society.

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [37] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: The paper introduces a framework to improve diagnostic accuracy and reasoning in LLMs for complex clinical scenarios, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in understanding medical texts, but their diagnostic reliability in complex clinical scenarios needs enhancement.

Method: An Etiology-Aware Attention Steering Framework is implemented, which integrates Clinical Reasoning Scaffolding and employs fine-tuning techniques targeting critical etiology reasoning heads.

Result: Diagnostic accuracy improved by 15.65% and reasoning focus scores by 31.6%. External validations confirm reliability in complex scenarios.

Conclusion: The framework aligns model attention with structured reasoning, offering a promising approach to creating more interpretable and reliable AI diagnostic systems.

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [38] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: This paper proposes MCSEO, a method enhancing multimodal sentence embeddings by incorporating object-phrase alignment alongside image-caption alignment, yielding improved results on STS tasks.


<details>
  <summary>Details</summary>
Motivation: To address the problem of noise in image-caption pairs used in multimodal sentence embeddings, which often include irrelevant or redundant information.

Method: The MCSEO method uses segmentation and object detection models to extract object-phrase pairs, optimizing a contrastive learning objective specifically for object-phrase correspondence.

Result: Experimental results show that MCSEO consistently outperforms strong baselines on semantic textual similarity (STS) tasks with different backbone models.

Conclusion: Precise object-phrase alignment significantly improves multimodal sentence embeddings, making it an effective approach for enhancing representation learning.

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [39] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: The paper introduces AdaPlan, a paradigm for LLM agents using global plans for long-horizon tasks, and PilotRL, a training framework with reinforcement learning. It delivers notable performance improvements over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent paradigms struggle with complex tasks requiring long-term plans and generalization to novel problems, as they often rely on single-step reasoning and supervised fine-tuning.

Method: The authors propose AdaPlan, which integrates global planning with execution for long-term decision-making, and PilotRL, a reinforcement learning framework that progressively improves planning and execution coordination.

Result: PilotRL achieves state-of-the-art performance, surpassing GPT-4o by 3.60% and outperforming GPT-4o-mini by 55.78% at a comparable parameter scale.

Conclusion: AdaPlan and PilotRL address limitations of current LLM paradigms by effectively combining global planning and adaptive training, highlighting their superior performance for agent-based tasks.

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [40] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: The study introduces a method allowing small language models (SLMs) to perform knowledge-intensive tasks competitively by dynamically constructing task vectors during reasoning.


<details>
  <summary>Details</summary>
Motivation: SLMs have limitations in knowledge-intensive tasks due to their constrained capacity, necessitating innovative approaches to reasoning and task optimization.

Method: A dynamic reasoning paradigm is proposed, where the model's reasoning (marked by special tags) refines task vectors on the fly, using RLVR optimization and MCP integration.

Result: Lucy, a newly developed 1.7B-parameter SLM utilizing this method, achieved 78.3% accuracy on the SimpleQA benchmark, rivaling much larger models.

Conclusion: SLMs can effectively compete with larger models if equipped with structured and dynamic reasoning mechanisms.

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [41] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: The paper proposes EdgeInfinite-Instruct to optimize Transformer-based large language models for resource-constrained edge devices by addressing computational and memory challenges while improving performance for long-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying LLMs on edge devices include high computational costs, memory constraints, and the need for mobile-specific optimizations. Existing methods often compromise performance or require full retraining.

Method: EdgeInfinite-Instruct uses Segmented Supervised Fine-Tuning (S-SFT) tailored for tasks like summarization and QA, along with post-training quantization and fixed-shape computation graph optimizations for NPUs.

Result: The approach shows improved domain-specific performance on benchmarks and real-world tasks while maintaining computational efficiency on edge devices.

Conclusion: EdgeInfinite-Instruct offers a practical and efficient solution for deploying LLMs on edge devices, retaining quality while optimizing resource usage and domain-specific capabilities.

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [42] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: This paper explores demonstration effectiveness in in-context learning and proposes a novel method called GradS for optimizing demonstration selection.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand why some demonstrations in in-context learning fail to enhance model performance and aims to address this limitation.

Method: The authors analyze demonstration effectiveness via gradient flow and linear self-attention models, proposing GradS, which selects demonstrations based on their gradient flow magnitude with respect to a query.

Result: GradS improved performance by an average of 6.8% over the strongest baselines across multiple datasets and large language models.

Conclusion: The paper establishes that demonstration effectiveness disparity increases with model depth and emphasizes that GradS effectively improves in-context learning by selecting impactful demonstrations.

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [43] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: This paper introduces SA-GCS, a training framework combining Curriculum Learning with RL to enhance UAV vision-language navigation performance.


<details>
  <summary>Details</summary>
Motivation: Improve UAV navigation in complex environments using language instructions by addressing inefficiencies in existing RL methods.

Method: SA-GCS integrates Semantic-Aware Difficulty Estimator and Gaussian Curriculum Scheduler to optimize RL training.

Result: Experiments on the CityNav benchmark showed superior performance, faster convergence, and better generalization compared to baselines.

Conclusion: SA-GCS effectively improves UAV navigation, showcasing robustness and scalability, with its implementation publicly accessible.

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [44] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: This paper explores the use of wavelets, particularly Discrete Wavelet Transforms (DWT), for enhancing and compressing word and sentence embeddings in NLP, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Wavelets have shown success in Image and Signal processing, prompting the study of their applicability in capturing linguistic structures for NLP tasks.

Method: The authors apply Discrete Wavelet Transforms (DWT) and combine it with Discrete Cosine Transform (DCT) to develop a model for compressing sentence embeddings into dense, fixed-size vectors.

Result: The proposed methods demonstrate efficacy across various NLP tasks, achieving results comparable to or exceeding original embeddings in some cases.

Conclusion: Wavelet-based transformations offer a promising direction for NLP by efficiently consolidating and compressing linguistic information without sacrificing performance.

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [45] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN introduces an agent-based approach to graph neural networks to improve informativeness and semantic relationship handling, achieving good performance without fine-tuning large language models.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of fixed aggregation schemes in GNNs which cannot handle imbalanced node informativeness and ignore global semantic relationships.

Method: Proposed an agent-based framework enabling each node to autonomously decide its actions and integrate retrieval-augmented generation for building global semantic relationships.

Result: ReaGAN achieved competitive performance in few-shot settings using a frozen large language model (LLM).

Conclusion: The results showcase the potential of agentic planning and retrieval augmentation to enhance graph learning capabilities.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [46] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: The paper introduces a new method for evaluating dialogue quality using a single model that aggregates knowledge from multiple language models, balancing efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for dialogue quality in large language models face issues with biases, inefficiency, and computational overhead while trying to ensure reliability.

Method: The authors propose a single evaluator model that integrates preference knowledge from multiple LLM judges to capture diverse feedback at low computational cost.

Result: Their approach outperforms existing methods on seven evaluation benchmarks, maintaining efficiency, robustness, and reliability across different scenarios.

Conclusion: The proposed method successfully mitigates biases and reduces computational costs while preserving the advantages of diverse feedback in dialogue quality evaluation.

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [47] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP's paper for the Automatic Minuting Shared Task at SIGDial 2025 explores combining retrieval-augmented generation (RAG) and Abstract Meaning Representations (AMR) to improve question-answering on meeting transcripts.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of automated question-answering systems by leveraging novel techniques like AMR for tasks that require detailed understanding, such as distinguishing between participants in conversation.

Method: Three systems based on retrieval-augmented generation (RAG) and Abstract Meaning Representations (AMR) were developed and tested on Task B of the Automatic Minuting Shared Task.

Result: Incorporating AMR improved response quality for about 35% of the questions, particularly benefiting questions involving the identification of different participants.

Conclusion: The proposed approach shows promise, especially in scenarios requiring nuanced understanding, but further work is needed to improve performance across a broader range of questions.

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [48] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: The study introduces half-truth detection as a challenge within fact verification, creating the PolitiFact-Hidden benchmark and proposing the TRACER framework to address omissions and implied intent.


<details>
  <summary>Details</summary>
Motivation: Current fact verification systems fail to detect misleading claims that are factually correct but omit critical context. This paper aims to address this limitation.

Method: Developed the PolitiFact-Hidden benchmark with annotations and proposed TRACER, a modular framework that aligns evidence, infers implied intent, and evaluates hidden content's impact.

Result: TRACER significantly strengthens half-truth detection by enhancing F1 scores in classification tasks by up to 16 points across various baselines.

Conclusion: Addressing omissions in fact-checking is critical for improving truthfulness evaluation, and TRACER effectively achieves this goal within existing pipelines.

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [49] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: The paper introduces Flat-LoRA and its efficient version EFlat-LoRA to improve the generalization of LoRA, achieving superior or comparable performance in tasks by targeting flat minima.


<details>
  <summary>Details</summary>
Motivation: To explore the correlation between the expressive and generalization abilities of LoRA, and to address the lack of tools to analyze sharpness and its connection to generalization in LoRA.

Method: The paper develops Flat-LoRA and EFlat-LoRA, theoretically demonstrating that perturbations in the full parameter space can be transferred to low-rank subspace, avoiding multi-matrix interference and achieving flat minima efficiently.

Result: EFlat-LoRA demonstrates performance improvements, outperforming both LoRA and full fine-tuning on datasets like GLUE and vision-language benchmarks such as SQA and VizWiz.

Conclusion: EFlat-LoRA achieves efficient optimization and improved model generalization, closely linking sharpness to LoRA's generalization and addressing previously overlooked aspects.

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [50] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: This study explores how emojis influence speech prosody and whether listeners can interpret emojis through prosodic cues.


<details>
  <summary>Details</summary>
Motivation: Emojis play a key role in text-based communication by adding emotional and pragmatic depth, but their connection to prosody in speech remains underexplored.

Method: The research uses human speech data collected via structured production and perception tasks to analyze the impact of emoji semantics on prosody.

Result: Speakers adapt their prosody based on emoji context, listeners accurately identify emojis through prosodic signals, and greater semantic differences between emojis lead to more distinct prosodic patterns.

Conclusion: Emojis significantly shape spoken communication by influencing prosodic delivery and interpretation, highlighting their role as meaningful tools in digital communication contexts.

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [51] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: The paper introduces 'PaPaformer,' a transformer-based architecture to train large language models faster by leveraging parallel paths that can be trained individually and combined efficiently.


<details>
  <summary>Details</summary>
Motivation: Modern language models demand significant computation resources and time for training, prompting the need for more efficient methodologies.

Method: The authors propose a parallel-path transformer model ('PaPaformer'), where smaller sub-networks (parallel paths) can be trained independently and merged into a unified model.

Result: 'PaPaformer' demonstrates the ability to substantially reduce training time while offering improved performance and allowing task-specific customization through individualized paths.

Conclusion: The architecture enables faster training, parameter reduction, and adaptability to specific tasks, making it a promising approach for the efficient development of large-language models.

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [52] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: NusaAksara introduces a benchmark for Indonesian languages and scripts, addressing a gap in NLP research by covering various tasks across text and image modalities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the underrepresentation of Indonesian scripts and languages in existing NLP benchmarks, highlighting the importance of preserving linguistic diversity.

Method: Human experts rigorously constructed the dataset, encompassing text and image data for 8 scripts across 7 languages and tested it across multiple models.

Result: Most models underperform, achieving near-zero results when applied to tasks involving Indonesia's local scripts, showing limitations in current NLP technologies.

Conclusion: NusaAksara emphasizes the need for improved NLP systems to support diverse and low-resource languages, advocating for further research on Indonesia’s rich linguistic heritage.

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [53] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: The paper introduces SynAdapt, a framework that synthesizes and adaptively uses continuous Chain-of-Thought (CCoT) reasoning for efficient and accurate outcomes, addressing challenges in complex question answering.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning improves model performance but is time-consuming. Continuous CoT methods are faster but face challenges such as alignment inefficiencies and inconsistencies.

Method: The authors propose SynAdapt, which synthesizes CCoT for better alignment, includes a difficulty classifier to identify hard questions, and adaptively prompts rethinking on challenging queries.

Result: SynAdapt delivers strong results across multiple benchmarks, achieving an optimal accuracy-efficiency balance.

Conclusion: SynAdapt can efficiently guide language models using continuous reasoning, adaptively addressing hard questions, and therefore improves both efficiency and accuracy.

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [54] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: This paper introduces CRUX, a novel framework for confidence estimation in large language models (LLMs) which evaluates both context faithfulness and consistency for reliable deployment.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable confidence estimation in LLMs, especially in scenarios involving contextual information, as current methods fail to account for this aspect.

Method: CRUX employs two metrics: contextual entropy reduction (contrastive sampling with and without context) and unified consistency examination (global consistency analysis with and without context).

Result: Experiments on five datasets, including CoQA, SQuAD, QuAC, BioASQ, and EduQG, demonstrate CRUX's superior performance in confidence estimation, achieving the highest AUROC compared to other methods.

Conclusion: CRUX provides a robust framework for confidence estimation, enabling trustworthy and safety-critical applications of LLMs by integrating context-aware metrics effectively.

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [55] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: This paper presents a novel topic modeling approach, GHTM, utilizing Graph Convolutional Networks and NMF for Bengali text datasets, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of topic modeling in Bengali texts, which has been largely understudied due to the language's morphological complexity and limited resources.

Method: The proposed Graph-Based Hybrid Topic Model (GHTM) leverages Graph Convolutional Networks to generate semantic embeddings of documents and employs Non-negative Matrix Factorization to extract latent topics.

Result: GHTM shows superior performance in terms of topic coherence and diversity compared to traditional and contemporary techniques across three Bengali datasets.

Conclusion: The study advances topic modeling in Bengali language processing by offering a more effective model and introducing a new dataset, "NCTBText," to diversify existing corpora.

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [56] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: This paper investigates the impact of tipping and threatening prompts on AI performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate the validity of widely held beliefs about prompting strategies, such as tipping and threatening, in improving AI model performance.

Method: The study uses rigorous testing on GPQA and MMLU-Pro benchmarks to assess the effects of different prompting techniques.

Result: Threatening or tipping prompts have no significant overall effect on benchmark performance. However, prompt variations can have an unpredictable effect on individual questions.

Conclusion: Simple prompting strategies are less effective than previously assumed for complex problems, though they can meaningfully change performance on specific questions.

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [57] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: Current AI-generated (AIG) text detectors are not robust in real-world settings, particularly against one-shot/few-shot and domain-specific model outputs. A new dataset (DACTYL) and classifiers using deep X-risk optimization (DXO) are proposed, proving more effective for out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is to address the limitations of existing AIG text detectors that fail in real-world scenarios, especially when encountering diverse and challenging text inputs such as one-shot/few-shot generations and outputs from domain-specific models.

Method: The authors developed the DACTYL dataset, focusing on challenging one-shot/few-shot and domain-specific model outputs. Two classification training approaches were tested: binary cross-entropy (BCE) optimization and deep X-risk optimization (DXO). A mock deployment scenario for detecting AI-generated student essays was used to assess model effectiveness.

Result: The results showed that most existing AIG text detectors struggled with texts from the challenging DACTYL dataset. While BCE classifiers outperformed slightly on the DACTYL test set, DXO classifiers excelled on out-of-distribution (OOD) texts and surpassed BCE classifiers significantly in a mock student essay detection scenario.

Conclusion: Deep X-risk optimization (DXO) demonstrates more effective generalization capabilities for AIG text detection on OOD data without overfitting. The findings emphasize the need for improved datasets and robust methods to advance AIG text detection technologies.

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [58] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: This paper reviews emerging techniques to enhance reasoning capabilities in Large Language Models (LLMs) tailored for medical applications, offering a taxonomy of these methods and evaluating advancements in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs in medicine lack systematic, transparent, and verifiable reasoning capabilities essential for clinical practice.

Method: The paper proposes a taxonomy of reasoning enhancement techniques, distinguishing between training-time strategies (e.g., fine-tuning, reinforcement learning) and test-time methods (e.g., prompt engineering). It reviews 60 studies concerning their application in various data modalities and clinical tasks.

Result: The review shows progress in reasoning quality and the emergence of advanced evaluation benchmarks. Key findings include gaps in faithfulness and multimodal reasoning capabilities.

Conclusion: Critical challenges remain, including achieving faithful reasoning, enabling native multimodal reasoning, and designing socially responsible and efficient medical AI systems.

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [59] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: The paper introduces 19 new Persian language datasets to benchmark 41 LLMs on topics like Iranian law, Persian grammar, and idioms, addressing the evaluation gap for non-European languages and cultural contexts.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive evaluation resources for non-English languages and non-Western cultural contexts in large language model assessments motivated this research, particularly focusing on Persian and Iranian cultural evaluations.

Method: The study developed 19 Persian-specific evaluation datasets covering diverse topics and applied these to benchmark 41 prominent LLMs.

Result: The research successfully created new datasets and benchmarked numerous LLMs, allowing for the evaluation of their performance in Persian language and Iranian cultural contexts.

Conclusion: This work bridges the gap in linguistic and cultural evaluation resources for LLMs, showcasing the importance of diversifying benchmarks beyond Western-oriented contexts.

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [60] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: This paper addresses the complex task of detecting sentence-level writing style changes within documents, using a Sequential Sentence Pair Classifier (SSPC) model that leverages contextual information effectively. The approach demonstrates strong performance across datasets with varying challenges.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenging problem of sentence-level style change detection, a crucial aspect of computational authorship analysis. This work is motivated by the need to address stylistically short and shallow sentences in the PAN-2025 shared task benchmark.

Method: The paper proposes a Sequential Sentence Pair Classifier (SSPC) model. It uses a pre-trained language model (PLM) for sentence representation, followed by a BiLSTM for contextualization within documents. Concatenated BiLSTM outputs are processed using a multi-layer perceptron for detecting adjacent style changes.

Result: The proposed model achieves macro-F1 scores of 0.923, 0.828, and 0.724 on EASY, MEDIUM, and HARD datasets of the PAN-2025 task, respectively, outperforming baselines and a zero-shot model.

Conclusion: The SSPC model offers a lightweight yet effective approach for detecting writing style changes in short, stylistically shallow sentences. The results demonstrate its potential as a strong solution for sentence-level style change detection.

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [61] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever enhances precedent retrieval by focusing on partial case information and rhetorical significance, offering scalable solutions for increasing legal document volumes.


<details>
  <summary>Details</summary>
Motivation: The research aims to address challenges in legal precedent retrieval caused by the overwhelming complexity and volume of legal documents, while maintaining consistency in judicial decisions under stare decisis.

Method: TraceRetriever combines BM25, Vector Database, and Cross-Encoder models with Reciprocal Rank Fusion for initial results and Hierarchical BiLSTM CRF classifier for rhetorical annotations.

Result: Evaluated on IL-PCR and COLIEE 2025 datasets, the system proves effective in retrieving legal precedents even with incomplete case details.

Conclusion: TraceRetriever provides a reliable and scalable method for practical legal precedent search, benefiting legal research when full case documentation is unavailable.

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [62] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: The paper evaluates zero-shot capabilities of cutting-edge large language models (LLMs) on detecting stylistic changes in sentences, establishing a challenging baseline outperforming existing ones while highlighting their sensitivity to style over semantics.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the complex task of sentence-level style change detection and evaluate whether state-of-the-art LLMs can identify granular stylistic variations effectively.

Method: The authors benchmarked four advanced LLMs on official datasets from the PAN 2024 and 2025 competitions, analyzing performance and exploring the influence of semantics and stylistic signals.

Result: Generative LLMs showed sensitivity to sentence-level stylistic changes, outperforming recommended baselines from the PAN competitions, and demonstrated heightened responsiveness to stylistic rather than semantic cues.

Conclusion: Advanced LLMs establish a robust baseline for style change detection and may exhibit greater sensitivity to purely stylistic signals, paving the way for deeper understanding and applications in authorship analysis.

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [63] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: This paper introduces NyayaRAG, a Retrieval-Augmented Generation framework tailored to legal judgment prediction in India's common law system, leveraging factual descriptions, statutes, and precedents for improved decision prediction and explanation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in leveraging statutory provisions and judicial precedents in legal judgment prediction within India's common law system.

Method: NyayaRAG combines factual case inputs, legal statutes, and semantically retrieved precedents using a domain-specific pipeline. It assesses predictive accuracy and explanation quality through standard and LLM-based metrics.

Result: Augmenting factual inputs with structured legal knowledge markedly improves predictive accuracy and legal explanation quality.

Conclusion: Integrating structured legal knowledge enhances LJP models' ability to predict court decisions and generate nuanced explanations, showcasing NyayaRAG's effectiveness in Indian legal contexts.

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [64] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: The paper introduces DAMR, a Monte Carlo Tree Search-based framework supported by LLM-based planning for efficient Knowledge Graph Question Answering, addressing adaptability and computational cost issues.


<details>
  <summary>Details</summary>
Motivation: Existing KGQA methods either suffer from limited adaptability due to their static reasoning approaches or incur high computational costs with dynamic generation strategies.

Method: The proposed DAMR integrates symbolic search with adaptive context-aware evaluation via an MCTS backbone, guided by an LLM-based planner, and employs a lightweight Transformer-based scorer for plausibility estimation.

Result: DAMR achieves superior performance compared to current state-of-the-art methods across multiple KGQA benchmarks.

Conclusion: DAMR offers a more efficient and accurate solution for KGQA, overcoming the limitations of previous approaches through a novel symbolic search and adaptive path evaluation mechanism.

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [65] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: The paper investigates whether large language models (LLMs) like GPT-4o can infer plausible explanations for observations using their training data.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs can perform reasoning about their training information and exhibit situational awareness, which can impact AI safety.

Method: Experiments training LLMs with fictitious chatbots' names and behavior descriptions and testing inference abilities based on provided example responses.

Result: GPT-4o inferred chatbot names correctly and displayed behavior characteristic of specific chatbots after targeted iterative training.

Conclusion: LLMs exhibit potential for situational awareness, raising implications for AI safety and reasoning capabilities.

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [66] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: The paper evaluates the validity of persona-based AI agents in replicating human personality traits by conducting a HEXACO personality test with GPT-4 agents.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI-powered generative agents can serve as effective substitutes for human participants in social science research, particularly in exploring human personality structures.

Method: The researchers conducted the HEXACO personality inventory experiment on 310 GPT-4 agents, performed factor analysis on their responses, and compared the data with original human-based findings.

Result: They found coherent personality structures recoverable from agents with partial alignment to the HEXACO framework. Additionally, personality dimensions were reliable when using a curated population of GPT-4 agents, though variations across models revealed biases.

Conclusion: Generative agents exhibit notable but imperfect alignment with human personality models, offering insights into their potential and limitations for social science research, while emphasizing the need for careful persona design to ensure representativeness.

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [67] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: This study introduces an agentic retrieval-augmented generation (RAG) framework for radiology QA that improves diagnostic accuracy and factual grounding, especially in mid-sized and small-scale language models, while reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Traditional radiology QA systems rely on single-step retrieval, which limits their ability to handle complex clinical reasoning tasks, prompting the need for improved frameworks.

Method: The paper proposes an agentic RAG framework that autonomously decomposes radiology questions, retrieves targeted evidence from Radiopaedia, and synthesizes evidence-based responses. It evaluates 24 LLMs with diverse parameters and training paradigms on 104 radiology questions from established datasets.

Result: Agentic RAG improved mean diagnostic accuracy (from 64% up to 73%) over zero-shot prompting and traditional RAG across models of varying sizes, with significant gains in mid-sized and small-scale LLMs. Hallucinations reduced by 9.4%, and clinically relevant context retrieval increased to 46%.

Conclusion: Agentic RAG enhances radiology QA performance, especially for mid-sized models, offering a promising avenue for improving clinical decision-making and warranting further validation studies.

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [68] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: The paper introduces GLiDRE, a new model for document-level relation extraction that achieves state-of-the-art results, especially in few-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in document-level relation extraction, particularly in zero-shot and few-shot settings where existing architectures like ATLOP fall short.

Method: Building on the compact and effective design of the GLiNER model, the paper presents GLiDRE, leveraging its principles for document-level relation extraction.

Result: GLiDRE achieves state-of-the-art performance in few-shot scenarios on the Re-DocRED dataset, outperforming existing models.

Conclusion: GLiDRE effectively addresses the complexity of document-level relation extraction and enhances performance in low-data scenarios, broadening the potential applications of compact NLP models.

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [69] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: This paper introduces MMBERT, a BERT-based multimodal framework for Chinese hate speech detection, addressing challenges like cloaking techniques and integrating modalities using Mixture-of-Experts architecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in hate speech detection on Chinese social networks, which often involve cloaking techniques, and improve detection capabilities beyond traditional methods primarily focused on text.

Method: The method involves designing MMBERT, a multimodal framework combining textual, speech, and visual modalities via Mixture-of-Experts architecture, supported by a three-stage progressive training paradigm.

Result: MMBERT outperformed fine-tuned BERT models, fine-tuned LLMs, and LLMs utilizing in-context learning on various Chinese hate speech datasets.

Conclusion: The proposed MMBERT advances the robustness and efficiency of multimodal hate speech detection in Chinese social networks and successfully mitigates challenges like adversarial perturbations.

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [70] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: The paper outlines a zero-shot system for question answering on tabular data using LLM-based Python code generation strategies, excelling in generating executable Pandas code.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of question answering over tabular data from diverse domains, leveraging state-of-the-art code generation techniques.

Method: The authors developed a Python code generation system that utilizes optimized prompting strategies in open-source LLMs to generate executable Pandas code.

Result: The experiments demonstrate varied effectiveness of LLMs in Python code generation, showing that Python code generation outperforms other approaches in tabular question answering. The system ranked eighth in Subtask I and sixth in Subtask II among 30 qualified systems.

Conclusion: Python-based code generation frameworks powered by LLMs provide a competitive and effective means of handling question answering tasks over tabular data in zero-shot settings.

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [71] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: This study expands upon prior work on language models' handling of gender-neutral and neopronouns with a new benchmark, MISGENDERED+, and evaluates five modern LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is to address the challenges of fairness and inclusivity in LLMs, especially regarding gender pronouns, and to improve limitations in prior evaluations.

Method: The paper introduces the updated MISGENDERED+ benchmark and evaluates five LLMs on pronoun accuracy in zero-shot, few-shot, and gender identity inference scenarios.

Result: The study finds improved accuracy in handling binary and gender-neutral pronouns compared to previous benchmarks but highlights ongoing issues with neopronouns and reverse inference.

Conclusion: The study concludes that while progress has been made, further work is required to address persistent gaps in inclusive AI, especially for complex pronoun usage.

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [72] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: The paper presents DAEDAL, a training-free method to overcome the static predefined generation length limitation in Diffusion Large Language Models (DLLMs), enabling adaptive length expansion for beneficial performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: DLLMs suffer from a rigid architecture requiring predefined generation lengths, leading to issues with performance degradation or computational inefficiency.

Method: DAEDAL method works in two phases: first, iteratively expanding text length before denoising based on a sequence completion metric; second, dynamically adjusting insufficient regions during denoising via mask token insertion.

Result: DAEDAL achieves comparable or better performance to fixed-length DLLMs while being computationally efficient with a higher effective token ratio.

Conclusion: DAEDAL resolves the static length constraint in DLLMs, bridging the gap with Autoregressive models and unlocking new efficiency and capability for DLLMs.

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [73] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: The paper proposes a novel method, QME, to enhance whole-body biometric recognition by introducing a quality-guided score-fusion framework.


<details>
  <summary>Details</summary>
Motivation: To address limitations of unimodal systems and improve whole-body biometric recognition performance by integrating data from different modalities like face, gait, and body.

Method: The paper develops a framework called QME, which uses a quality estimation mechanism (Quality Estimator) and a learnable score-fusion strategy leveraging the Mixture of Experts (MoE). It introduces pseudo-quality loss and score triplet loss to enhance performance.

Result: The proposed QME method achieves state-of-the-art results on various whole-body biometric datasets, outperforming traditional score-fusion methods.

Conclusion: QME effectively addresses challenges like score distribution variations and data quality variability in multimodal whole-body biometric recognition, demonstrating its significance over baseline methods.

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [74] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: The study evaluates action recognition models' ability to generalize motion concepts in varied contexts, introducing new datasets and benchmarks to assess transferability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to determine whether action recognition models can effectively generalize high-level motion concepts, even in unfamiliar contexts with similar distributions.

Method: The researchers introduced a motion transferability framework with three datasets—one synthetic and two adapted from real-world video datasets. They then evaluated 13 state-of-the-art models on their ability to transfer high-level motion concepts.

Result: The study found significant performance drops in recognizing high-level actions in novel contexts. Multimodal models faced challenges with fine-grained actions, and even bias-free synthetic datasets proved challenging. Larger models showed better transferability with dominant spatial cues but struggled with temporal reasoning.

Conclusion: Disentangling coarse and fine motions could help improve performance in temporally challenging datasets. The study establishes a key benchmark for motion transferability in action recognition research.

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [75] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: The paper introduces the Monado SLAM dataset to address limitations in existing visual-inertial odometry (VIO) and SLAM tracking systems under challenging real-world settings of head-mounted use cases.


<details>
  <summary>Details</summary>
Motivation: Existing VIO and SLAM systems struggle in challenging scenarios like intense motion, dynamic occlusions, and low-textured or poorly lit environments. Current datasets fail to cover these real-world challenges adequately, limiting system robustness and advancements.

Method: The authors developed the Monado SLAM dataset, containing real-world sequences captured using multiple virtual reality headsets. They provide it under the CC BY 4.0 license to promote open research in the field.

Result: Created a publicly accessible dataset that highlights real-world challenges of head-mounted tracking, aiming to improve tracking systems by providing data that better aligns with real-world conditions.

Conclusion: By releasing the Monado SLAM dataset, the paper seeks to encourage advancements in VIO and SLAM technologies, addressing persistent limitations in real-world tracking scenarios.

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [76] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: This paper presents a CNN model for gender classification using color images of the periocular region, achieving 99% accuracy on CVBL dataset and 96% on the (Female and Male) dataset.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of gender classification, particularly when such systems are affected by factors like cosmetics and disguise, by using visual cues in the periocular region.

Method: The study employs a Convolutional Neural Network (CNN) trained on color images of the periocular region and tested on two datasets (CVBL and Female/Male), with a focus on optimizing accuracy and keeping the model size efficient.

Result: The CNN achieved 99% accuracy on the CVBL dataset and 96% accuracy on the Female/Male dataset, with a small parameter count of 7,235,089.

Conclusion: The proposed CNN model effectively classifies gender using the periocular region, outperforming state-of-the-art methods and demonstrating potential for practical applications in security and surveillance.

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [77] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: This paper introduces the World Consistency Score (WCS), a metric to evaluate generative video models focusing on internal world consistency using interpretable sub-components.


<details>
  <summary>Details</summary>
Motivation: Existing video evaluation metrics focus predominantly on visual fidelity or prompt alignment, leaving gaps in assessing temporal and physical coherence.

Method: WCS utilizes a combination of four submetrics (object permanence, relation stability, causal compliance, and flicker penalty) integrated through a learned weighted formula and open-source tools. Human preference data trains the weightings.

Result: The proposed WCS metric demonstrated alignment with human judgments, outperforming traditional metrics like FVD and CLIPScore in evaluating generative video models.

Conclusion: WCS offers a comprehensive and interpretable evaluation framework for video generation, emphasizing temporal and physical coherence beyond existing metrics.

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [78] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: This paper introduces GeoExplorer, an AGL agent using curiosity-driven, goal-agnostic exploration for improved localization across diverse and unfamiliar environments.


<details>
  <summary>Details</summary>
Motivation: Traditional AGL methods rely on distance-based rewards, which limits exploration and generalization effectiveness in unseen environments or challenging distance estimation scenarios.

Method: The authors propose GeoExplorer, which incorporates curiosity-driven exploration using intrinsic, goal-agnostic rewards to enhance environment understanding and facilitate robust goal localization.

Result: GeoExplorer demonstrated improved robustness, diversity, and generalization in localizing targets across four AGL benchmark datasets, particularly in unfamiliar environments.

Conclusion: Curiosity-driven, goal-agnostic exploration enhances performance and generalization of AGL systems, as evidenced by GeoExplorer's success in diverse and challenging scenarios.

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [79] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: This paper introduces Probabilistic Point Clouds (PPC) to enhance 3D scene understanding using LiDAR by incorporating measurement uncertainty directly into point cloud data, leading to improved object detection under challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Modern LiDAR systems struggle in challenging scenarios, like detecting distant or low-albedo objects, resulting in sparse and erroneous point clouds that compromise downstream perception models due to neglect of uncertainty information.

Method: The authors propose Probabilistic Point Clouds (PPC), which augment each point with a probability attribute to represent measurement uncertainty, and present inference approaches that act as lightweight modules for robust 3D object detection.

Result: Using PPC, the proposed inference methods achieve superior performance in 3D object detection compared to baselines, under various challenging conditions in both indoor and outdoor settings, and for both LiDAR and camera-LiDAR fusion models.

Conclusion: PPC enhances LiDAR-based 3D object detection by addressing uncertainty in raw data, offering a more robust and accurate representation for challenging real-world scenarios.

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [80] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: The paper discusses the introduction of Selective Modality Shifting (SMS), a perturbation-based approach, to evaluate and expose biases in Vision-Language Models (VLMs) toward text over images in medical decision-making systems.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making requires a balanced analysis of both medical images and clinical reports. Existing Vision-Language Models (VLMs), while promising for this task, show a strong tendency to favor textual over visual information, potentially overlooking critical visual cues.

Method: The SMS approach systematically swaps images or text in binary classification tasks to quantify and expose biases in VLMs. The authors assess six VLMs (both generalist and medically fine-tuned ones) on two datasets (MIMIC-CXR and FairVLMed) under both unperturbed and perturbed conditions.

Result: Findings reveal a pronounced reliance on text by all evaluated models, even in the presence of relevant visual data. Attention-based qualitative analyses further confirm the overshadowing of image information by textual data.

Conclusion: The research emphasizes the need to develop and evaluate multimodal medical models that truly integrate visual and textual data, as current models tend to disproportionately depend on single modalities.

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [81] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: The paper introduces structured graph "lineages" that grow hierarchically and defines algebraic operations tailored for these graded graphs. It applies this concept to deep neural networks and multigrid numerical methods.


<details>
  <summary>Details</summary>
Motivation: Graphs and hierarchical graphs are foundational in defining architectures for machine learning models and computational science. The paper aims to create structured approaches to optimize operations and scalability for these models.

Method: The authors propose structured graph lineages with exponential growth, bipartite connections, graded graph categorizations, skeletal algebraic operations, and scalable unary operators suited for hierarchical modeling.

Result: The paper derives an algebraic type theory for graded graphs, enabling efficient skeletized operations, hierarchical architectures, and local adaptive algorithms. Applications to neural networks and numerical methods showcase the utility.

Conclusion: The proposed framework provides robust structures for hierarchical graph modeling and operations, opening avenues for advanced applications in computational and machine learning domains.

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [82] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: This paper proposes a new approach for automatic real personality recognition (RPR) by simulating personalized internal cognition from short audio-visual behaviors.


<details>
  <summary>Details</summary>
Motivation: Most existing RPR solutions rely on external observations, which fail to accurately capture real personalities and lead to inferior results.

Method: The approach builds a personalized cognition model represented as network weights that simulate individual-specific facial reactions. These are encoded into a novel graph structure, analyzed with a proposed 2D Graph Neural Network (2D-GNN), and jointly trained end-to-end.

Result: The approach efficiently simulates internal cognition and infers real personality traits from external expressive behaviors using the proposed 2D-GNN.

Conclusion: The paper highlights the effectiveness of simulating internal cognition for more accurate personality recognition, offering a superior alternative to traditional methods reliant on external impressions.

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [83] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: The paper introduces SAM-PTx, a method to enhance the Segment Anything Model (SAM) by integrating CLIP-derived text embeddings for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the underutilized potential of semantic text prompts in SAM's prompt-based segmentation, which traditionally relies on spatial prompts.

Method: They design a lightweight adapter (Parallel-Text) to inject frozen text embeddings into SAM's image encoder, modifying only the MLP-parallel branch of transformer blocks while keeping the attention pathway for spatial reasoning intact.

Result: Experiments on COD10K, COCO, and ADE20K datasets demonstrate improved segmentation performance over spatial prompt baselines, proving the effectiveness of text embeddings.

Conclusion: Integrating semantic text prompts into SAM's architecture is a promising and efficient approach for enhancing segmentation performance with minimal computational complexity.

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [84] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: The study improves Few-Shot Image Classification by using object localization to address image ambiguities.


<details>
  <summary>Details</summary>
Motivation: Few-Shot Image Classification struggles with ambiguities caused by multiple objects or complex backgrounds.

Method: Incorporates object localization using Segment Anything Model or unsupervised foreground extraction.

Result: Marked improvement in classification performance across benchmarks with minimal input.

Conclusion: Object localization significantly enhances few-shot classification, even with minimal object identification input.

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [85] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: The paper introduces the MSF-UM model for guided depth map super-resolution, combining state-space modeling, convolutional layers, and cross-modal fusion strategies to enhance accuracy while reducing model complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional convolutional neural networks struggle with global contextual modeling in depth maps, and transformers face computational and memory inefficiencies when handling high-resolution data.

Method: The MSF-UM model integrates a Mamba state-space module with a residual dense channel attention block in a guided multi-scale U-shaped structure. The model also uses cross-modal fusion to incorporate texture details from color images.

Result: The MSF-UM model achieves better reconstruction accuracy and fewer model parameters than existing methods, validated through experiments on several public datasets.

Conclusion: MSF-UM demonstrates strong generalization capability for large-scale depth map super-resolution while addressing inefficiencies in traditional and transformer-based approaches.

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [86] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: The paper introduces PointGauss, a real-time multi-object segmentation framework for Gaussian Splatting using a point cloud-driven pipeline, paired with a new dataset, DesktopObjects-360, for improved 3D segmentation evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D segmentation in Gaussian Splatting suffer from slow initialization and lack of multi-view consistency. The authors target these inefficiencies and propose a solution that enables faster and more consistent segmentation methods.

Method: PointGauss employs a point cloud-based Gaussian primitive decoder to efficiently generate 3D instance masks within 1 minute and uses a GPU-accelerated 2D mask rendering system for ensuring multi-view consistency.

Result: PointGauss outperforms state-of-the-art methods with significant multi-view mIoU gains (1.89 to 31.78%) and greater computational efficiency. The new benchmark dataset DesktopObjects-360 also facilitates better 3D segmentation evaluation.

Conclusion: The proposed PointGauss framework significantly enhances real-time 3D segmentation performance in Gaussian Splatting. The introduction of DesktopObjects-360 addresses limitations of existing benchmarks, offering a more comprehensive dataset for 3D segmentation research.

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [87] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: The paper introduces a novel continual learning framework for generative vision-language models, utilizing a mixture of visual projectors and expert recommendation/pruning strategies to improve task adaptation and instruction-following responses.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current generative VLMs that prioritize visual inputs over language instructions when learning tasks with repetitive textual instructions.

Method: The proposed framework uses specialized visual projectors as experts to ground visual-to-language translations based on instruction contexts, along with expert recommendation for reusability and expert pruning to avoid interference.

Result: Experiments confirmed better adaptability and performance compared to existing continual learning methods, achieving improved instruction-following responses across diverse tasks.

Conclusion: The approach demonstrates effectiveness in continual learning scenarios for generative VLMs, ensuring balanced learning of visual and textual instructions to tackle new tasks efficiently.

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [88] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper surveys multimodal referring segmentation, which segments target objects in visual scenes based on text or audio inputs. It highlights methods, datasets, and challenges, while offering extensive benchmark comparisons.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance the field of multimodal referring segmentation, crucial for applications needing precise object perception from user instructions, driven by developments in neural networks and multimodal capabilities.

Method: The paper adopts a comprehensive survey approach, introducing problem definitions, summarizing a meta architecture, reviewing methods for images, videos, and 3D scenes, discussing GREx methods for real-world challenges, and providing benchmark performance comparisons.

Result: The paper compiles and organizes existing knowledge in multimodal referring segmentation, including datasets, meta architectures, representative methods, and comparative analysis on standard benchmarks.

Conclusion: The survey underscores progress, challenges, and practical uses of multimodal referring segmentation, serving as a resource for researchers and practitioners.

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [89] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: This paper introduces a benchmark to evaluate semantic correspondence robustness in challenging scenarios, revealing performance drops and insights into improvement strategies.


<details>
  <summary>Details</summary>
Motivation: The robustness of semantic correspondence under challenging scenarios is underexplored despite its critical role in numerous computer vision tasks.

Method: The authors created a dataset with 14 challenging scenarios and evaluated semantic correspondence methods, comparing various models and robustness strategies.

Result: Existing methods decline in performance under adverse conditions, large-scale models help but fine-tuning reduces robustness, fusion models achieve better robustness, and general data augmentations are ineffective.

Conclusion: Improving semantic correspondence robustness requires task-specific enhancement strategies rather than broad data augmentations, highlighting the need for tailored approaches.

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [90] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: The paper proposes a framework using Spatial Self-Attention and LSTM, along with federated learning mechanisms, to enhance drowsiness detection accuracy and privacy in decentralized, diverse real-world settings.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of accurately detecting driver drowsiness in real-world, decentralized, and heterogeneous data environments.

Method: Spatial Self-Attention (SSA) integrated with LSTM for feature extraction, Gradient Similarity Comparison (GSC) to improve model aggregation in federated learning, and data preprocessing with video frame analysis and augmentation.

Result: Achieved a detection accuracy of 89.9%, outperforming existing methods in federated learning settings.

Conclusion: The approach demonstrates strong potential for reliable drowsiness detection in intelligent transportation systems, aiding in road safety improvements.

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [91] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: The paper introduces TITAN-Guide, a method for Text-to-Video diffusion models that reduces memory usage and enhances performance without requiring supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Training-free guidance for diffusion models faces challenges with high memory usage and sub-optimal control, which affect computation-heavy tasks like Text-to-Video diffusion.

Method: The authors proposed TITAN-Guide, which leverages forward gradient descent and eliminates the need for backpropagation during discriminative guidance, allowing for efficient memory usage and optimal latent optimization.

Result: TITAN-Guide outperforms prior approaches in memory efficiency and improves Text-to-Video diffusion model performance across benchmarks.

Conclusion: The approach offers a practical and efficient solution to optimize memory and control in Text-to-Video diffusion tasks, making it highly applicable without requiring additional fine-tuning.

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [92] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: The paper introduces AniMer+, a unified Vision Transformer framework with Mixture-of-Experts for reconstructing the pose and shape of mammals and birds using a limited, novel synthetic dataset.


<details>
  <summary>Details</summary>
Motivation: To enable accurate multi-species pose and shape reconstruction to advance biological research, addressing the current lack of comprehensive datasets and limited network capacity in existing methods.

Method: The authors present AniMer+, a high-capacity Vision Transformer employing a Mixture-of-Experts for taxa-specific and shared anatomical feature learning. They also generate two synthetic datasets, CtrlAni3D for mammals and CtrlAVES3D for birds, using a diffusion-based image generation pipeline.

Result: AniMer+ achieves superior performance compared to existing methods across several benchmarks, including the challenging Animal Kingdom dataset, showcasing strong generalization capabilities.

Conclusion: AniMer+ and its synthetic datasets significantly advance the modeling of animals' pose and shape, providing a unified, efficient, and scalable framework for multi-species reconstruction challenges.

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [93] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: This paper proposes a novel framework for editing pedestrian elements in multi-view driving videos, enhancing data augmentation for autonomous driving models.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems struggle with pedestrian detection due to limited dangerous pedestrian data in training datasets.

Method: The system identifies pedestrian regions across camera views, applies pose sequence control for editing, and ensures spatiotemporal coherence and cross-view consistency.

Result: Experiments validate the framework's ability to generate visually realistic and coherent pedestrian scenarios for use in autonomous driving.

Conclusion: The method provides a robust tool for multi-view pedestrian video generation, aiding in simulation and dataset enhancement.

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [94] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: This paper presents AerialCSP, a synthetic dataset designed to address the challenges of analyzing aerial imagery from Concentrated Solar Power plants, improving fault detection and reducing the reliance on manual labeling.


<details>
  <summary>Details</summary>
Motivation: The paper addresses machine learning challenges in analyzing aerial images of CSP plants, which are difficult due to their unique reflective surfaces and domain-specific elements. Generic datasets fail to generalize without costly and extensive retraining.

Method: The authors create AerialCSP, a synthetic dataset that simulates aerial imagery of CSP plants. They use this dataset to pretrain models for object detection and image segmentation, benchmarking them against CSP-related vision tasks.

Result: Models pretrained on AerialCSP demonstrate improved performance in detecting real-world faults in CSP plants, especially rare and small defects, reducing the need for manual labeling.

Conclusion: The study shows that synthetic datasets like AerialCSP can effectively improve machine learning model performance in specialized domains, offering a cost-effective solution for industrial applications and making datasets publicly available.

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [95] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: The paper proposes a two-stage pipeline for low-light image enhancement using event cameras: visibility restoration with amplitude-phase entanglement, and structure refinement with dynamic alignment.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture intensity changes with high temporal resolution, offering potential for better low-light image enhancement. Existing methods fail to fully utilize the benefits of both modalities, limiting performance.

Method: 1. Visibility restoration network using amplitude-phase entanglement in Fourier space.
2. Structure refinement with dynamic alignment to manage spatial mismatch.
3. Spatial-frequency interpolations used to create diverse negative samples for contrastive loss.

Result: The proposed method outperformed state-of-the-art models in experiments.

Conclusion: The approach successfully exploits modality-specific advantages, offering superior low-light image enhancement by leveraging event cameras.

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [96] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: The paper introduces DocTron-Formula, a unified framework for OCR of mathematical formulas, which surpasses specialized models in accuracy and robustness, and CSFormula, a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: OCR for mathematical content is challenging due to structural diversity, complexity, and variability, and current models struggle to address these issues effectively.

Method: The authors propose DocTron-Formula, a framework built on general vision-language models, combined with a novel large-scale dataset CSFormula for supervised fine-tuning.

Result: DocTron-Formula achieves state-of-the-art accuracy across various styles, domains, and layouts, outperforming specialized models.

Conclusion: The presented approach establishes a new paradigm for understanding complex scientific documents, showing robust and accurate performance.

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [97] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: The paper presents a novel framework for identifying rare human behavior scenarios in autonomous driving datasets by combining motion sequences, video frames, and natural language for retrieval.


<details>
  <summary>Details</summary>
Motivation: Safety-critical scenarios in autonomous driving require reliable systems that can efficiently identify rare or complex behavior exhibited by vulnerable road users.

Method: The authors propose a context-aware motion retrieval framework using SMPL-based motion sequences, corresponding video frames, and embedding them in a multimodal space aligned with natural language.

Result: The proposed method achieves up to 27.5% higher accuracy in motion-context retrieval compared to state-of-the-art models, when tested on the WayMoCo dataset.

Conclusion: The framework facilitates scalable retrieval of human behavior scenarios for better evaluation of autonomous driving systems, supported by the newly introduced WayMoCo dataset.

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [98] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: This paper proposes a generative framework, GV-VAD, to augment video anomaly detection datasets by generating synthetic videos using text-conditioned models for improved training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly detection approaches are hindered by limited datasets due to high anomaly unpredictability, rarity, and annotation costs.

Method: GV-VAD utilizes text-conditioned video generation to create synthetic training videos and introduces a loss scaling strategy to optimize their contribution during training.

Result: The framework demonstrates superior performance compared to state-of-the-art methods on the UCF-Crime dataset.

Conclusion: Synthetic video generation is an effective low-cost strategy to augment VAD datasets, enhancing model performance and generalization.

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [99] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: The paper introduces a novel personalization guidance technique for text-to-image diffusion models to improve balancing between text alignment and target distribution fidelity without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning pre-trained text-to-image models with limited data often faces trade-offs between subject fidelity and preserving the model's generality, which current sampling methods fail to balance effectively.

Method: The proposed personalization guidance employs an unlearned weak model conditioned on a null prompt and dynamically adjusts unlearning through weight interpolation between pre-trained and fine-tuned models during inference.

Result: Experimental results show the method improves text alignment and target distribution fidelity and works well with multiple fine-tuning strategies.

Conclusion: Personalization guidance addresses the limitations of existing sampling methods and provides a balanced, effective, and computationally efficient approach for adapting text-to-image diffusion models.

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [100] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: IGL-Nav introduces an efficient 3D-aware framework for visual navigation using image goals, outperforming state-of-the-art methods and enabling robust image localization even in challenging settings.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of visual navigation where the goal is defined as an image, aiming to improve upon existing methods that struggle to model geometric relationships effectively in 3D space.

Method: The proposed IGL-Nav framework incrementally updates scene representation using monocular prediction and leverages geometric information for coarse localization through efficient 3D convolution, followed by fine target pose optimization using differentiable rendering.

Result: IGL-Nav significantly surpasses state-of-the-art methods in performance across diverse experiments and effectively handles free-view image-goal tasks. It can also be used with real robotics platforms utilizing cellphone-captured goal images.

Conclusion: Incremental updates and geometric reasoning enable efficient and accurate navigation for image-goal tasks, with promising applications in robotic systems and challenging visual scenarios.

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


### [101] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: Introduces a practical and accurate method for calibrating camera spectral sensitivity using a diffraction grating, without needing specialized equipment.


<details>
  <summary>Details</summary>
Motivation: To improve calibration of camera spectral sensitivity for better performance in computer vision tasks like color correction and material analysis.

Method: Utilizes images of direct illumination and its diffraction pattern through an uncalibrated grating sheet, estimating camera sensitivity and grating parameters in a closed form.

Result: Demonstrates superior performance compared to conventional reference target-based methods on synthetic and real-world datasets.

Conclusion: The method is accurate, practical, and avoids the need for specialized equipment.

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [102] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: Collaborative framework with language and vision agents for multi-image reasoning across diverse tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of multimodal reasoning across multiple images and diverse datasets.

Method: Employs a dual-agent system: a PromptEngineer and a VisionReasoner, using LVLMs for automated, modular reasoning without training.

Result: Achieved near-ceiling performance on tasks like TQA, DocVQA, and MMCoQA across 18 datasets from the 2025 MIRAGE Challenge.

Conclusion: LVLMs, coupled with informative prompts, enable effective multi-image reasoning and task generalization.

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [103] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: The paper presents a Speed-Guided Learnable Kalman Filter (SG-LKF) to handle dynamic tracking scenarios in autonomous vehicle multi-object tracking (MOT).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of conventional tracking-by-detection methods, which ignore ego-vehicle speed variations that degrade tracking stability and accuracy in dynamic high-speed scenarios.

Method: The proposed SG-LKF dynamically adapts uncertainty modeling to ego-vehicle speed using MotionScaleNet (MSNet), a specialized MLP architecture. Additionally, a self-supervised trajectory consistency loss is introduced to improve inter-frame association and trajectory continuity.

Result: SG-LKF achieved state-of-the-art performance on benchmarks like KITTI 2D MOT (79.59% HOTA), KITTI 3D MOT (82.03% HOTA), and outperformed SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.

Conclusion: This work demonstrates the critical importance of ego-vehicle speed in MOT and provides an effective solution for achieving higher tracking accuracy and stability in dynamic scenarios.

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [104] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: This paper introduces CoST, a collaborative perception approach that integrates multi-agent and multi-time data simultaneously in a spatio-temporal space for better perception performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations in previous collaborative perception methods, particularly their separation of multi-agent fusion and multi-time fusion into sequential steps.

Method: Proposes CoST, which aggregates observations from multiple agents and times into a unified spatio-temporal space, optimizing feature transmission and fusion.

Result: CoST improves both efficiency (less redundant data transmission) and accuracy, and is compatible with most existing methods, enhancing them while reducing bandwidth usage.

Conclusion: The CoST method provides a superior framework for collaborative perception by simultaneously incorporating spatio-temporal data, offering benefits in efficiency, accuracy, and compatibility.

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [105] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: This paper proposes a machine learning method for classifying honey origins, achieving high accuracy using hyperspectral imaging data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately identifying honey's botanical origins through efficient data processing and analysis.

Method: The approach involves class transformation for dataset preparation, LDA for feature extraction, and classification using SVM and KNN.

Result: The methodology achieved state-of-the-art classification accuracy: 95.13% for image-based and 92.80% for instance-based honey classification.

Conclusion: The method effectively classifies honey origins with excellent accuracy, demonstrating the strength of combining hyperspectral imaging and machine learning.

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [106] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon is a new neural implicit reconstruction approach for 3D surfaces from a small number of images, leveraging volume rendering-based feature consistency and depth constraints to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods either generalize poorly to unseen views or have limited quality due to insufficient geometric clues, requiring a method to handle sparse views effectively.

Method: The method introduces volume rendering-based feature consistency loss for smoothness, supplemented by uncertainty-guided depth constraints to improve detail recovery in ambiguous or occluded areas.

Result: SparseRecon surpasses state-of-the-art performance by achieving high-quality 3D reconstruction results, even with minimal overlapping views between input images.

Conclusion: The SparseRecon method effectively addresses limitations of prior techniques, delivering smooth and detailed reconstructions from sparse-view inputs with improved generalization capabilities.

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [107] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: The paper addresses the inefficiency of traditional self-attention in Transformers by introducing Representation Shift, a training-free metric that enables effective token compression compatible with FlashAttention, providing computational speedups in tasks like video-text retrieval and video QA.


<details>
  <summary>Details</summary>
Motivation: Transformers require high computational cost due to their scaling in task complexity and self-attention mechanism. The inefficiency is further exacerbated by memory overhead, making it essential to find methods to reduce cost while maintaining performance.

Method: The paper proposes Representation Shift, a training-free, model-agnostic metric to measure token representation changes. This metric facilitates token compression without relying on attention maps, enabling compatibility with optimized attention kernels like FlashAttention.

Result: Representation Shift showed significant computational improvements, achieving speedups of up to 5.5% and 4.4% for video-text retrieval and video QA tasks respectively, while retaining compatibility across model types, including Transformers, CNNs, and state space models.

Conclusion: Representation Shift provides an innovative, efficient, and generalizable method for token compression. It seamlessly integrates with modern attention kernels such as FlashAttention, enabling computational efficiency and broad practical application across various models.

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [108] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt combines forward and backward video prediction using a language model, outperforming traditional methods in action anticipation.


<details>
  <summary>Details</summary>
Motivation: Current methods for video-based action anticipation are unidirectional and fail to capture distinct sub-actions, limiting their effectiveness.

Method: BiAnt utilizes a bidirectional framework (forward and backward prediction) enhanced with a large language model to improve action anticipation.

Result: The BiAnt method demonstrated superior performance in edit distance metrics when tested on the Ego4D dataset, surpassing baseline methods.

Conclusion: Incorporating bidirectional prediction with language models shows promise for more accurate video-based long-term action anticipation.

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [109] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: This paper introduces the Adapt-WeldNet framework to enhance weld defect detection in offshore environments by combining optimized machine learning models with a novel interpretability framework.


<details>
  <summary>Details</summary>
Motivation: Traditional NDT methods and existing neural network-based approaches often fail to detect subtle or internal weld defects and lack interpretability, leading to safety and reliability concerns in critical operations.

Method: The Adapt-WeldNet framework systematically evaluates pretrained architectures, transfer learning strategies, and adaptive optimizers to optimize defect detection. It incorporates the DDIA framework, which uses XAI techniques like Grad-CAM and LIME for expert-validated transparency and integrates a Human-in-the-Loop approach.

Result: The improved defect detection system combines high performance with enhanced interpretability, addressing safety and reliability concerns in challenging marine and offshore environments.

Conclusion: This work provides a robust, interpretable, and trustworthy approach to welding defect detection, building confidence in automated systems for critical offshore operations.

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [110] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: This paper introduces $MV_{Hybrid}$, a novel hybrid architecture combining state space models (SSMs) and Vision Transformer (ViT) for improved spatial gene expression prediction from histopathology images.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of current Vision Transformer (ViT)-based pathology models, which underperform in clinical settings, by introducing architectural innovations that better capture subtle morphological patterns correlating with molecular phenotypes.

Method: The researchers developed the $MV_{Hybrid}$ backbone architecture by integrating state space models with ViT, leveraging the low-frequency bias enabled by negative eigenvalue initialization. The models were pretrained using DINOv2 on colorectal cancer datasets and assessed through random split and leave-one-study-out evaluations.

Result: $MV_{Hybrid}$ significantly outperformed ViT in spatial gene expression prediction, showing 57% higher correlation in LOSO evaluation and demonstrating robustness with 43% less performance degradation compared to random split. It also performed equally or better in classification, patch retrieval, and survival prediction tasks.

Conclusion: The proposed $MV_{Hybrid}$ architecture demonstrates superior performance and robustness in spatial transcriptomics tasks, establishing its potential as a next-generation pathology vision foundation model (VFM).

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [111] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: The paper introduces Cued-Agent, a collaborative multi-agent system designed for Automatic Cued Speech Recognition (ACSR) that effectively integrates hand gestures and lip movements to convert visual data into text. It outperforms previous methods and handles data limitations effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work stems from the limited training data and insufficient performance of existing methods to effectively handle the temporal asynchrony between hand gestures and lip movements in Automatic Cued Speech Recognition (ACSR). The authors aim to leverage the capabilities of multi-agent systems to address these challenges.

Method: The method involves developing Cued-Agent, a system composed of four specialized sub-agents: (1) a Multimodal Large Language Model-based Hand Recognition agent for decoding hand movements, (2) a pretrained Transformer-based Lip Recognition agent for extracting lip features, (3) a Hand Prompt Decoding agent that integrates hand and lip features dynamically, and (4) a Self-Correction Phoneme-to-Word agent for semantic refinement and direct text conversion. The system incorporates a training-free manner for certain tasks and collects an expanded dataset of 14 Mandarin CS subjects to support the study.

Result: Experimental results show that Cued-Agent significantly outperforms state-of-the-art methods in both normal and hearing-impaired scenarios. This underscores its effectiveness in handling multimodal inputs under data-constrained circumstances.

Conclusion: Cued-Agent is a robust and efficient solution for Automatic Cued Speech Recognition, offering improved performance over existing methods. It illustrates the potential of collaborative multi-agent systems in overcoming challenges like data scarcity and multimodal fusion in ACSR tasks.

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [112] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: This paper introduces DAPT, a new prompt tuning framework to address information asymmetry issues in vision-language models, achieving better cross-modal alignment and improved performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the information asymmetry in prompt tuning for vision-language models, where biased alignment leads to skewed attention favoring visual contexts over textual objects.

Method: The method involves decoupling visual modality into foreground and background components and symmetrically aligning them with textual representations. Additionally, a visual pull-push regularization is used to enhance unbiased visual attention.

Result: The DAPT framework demonstrates significant performance improvements in few-shot learning, base-to-novel generalization, and data-efficient learning benchmarks.

Conclusion: DAPT is effective and architecture-free, successfully solving the modality alignment issue and enhancing transferability in vision-language tasks.

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [113] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: The paper introduces a video forgery detection method that combines RGB appearance features and optical flow residuals to detect spatial and temporal inconsistencies in AI-generated videos.


<details>
  <summary>Details</summary>
Motivation: The emergence of realistic synthetic videos created by diffusion-based models poses challenges for existing video forgery detection methods, especially in capturing subtle temporal inconsistencies.

Method: The paper presents a detection framework with a dual-branch architecture: one branch analyzes RGB frames while the other examines optical flow residuals to detect temporal synthesis artifacts.

Result: Using experiments on text-to-video and image-to-video tasks across ten generative models, the proposed approach demonstrated robustness and superior generalization capabilities.

Conclusion: The framework effectively detects video forgeries across diverse AI generative models by leveraging spatial-temporal consistencies.

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [114] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: The paper introduces iSafetyBench, a video-language benchmark designed to evaluate models in industrial settings, especially focusing on recognizing hazardous activities. It reveals the limitations of current models in safety-critical tasks.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to evaluate and improve video-language models for high-stakes industrial applications where recognizing routine operations and hazardous scenarios is essential.

Method: The authors created iSafetyBench, a benchmark consisting of 1,100 real-world industrial video clips annotated with multi-label action tags across routine and hazardous categories, paired with multiple-choice questions for granular assessments.

Result: Eight state-of-the-art video-language models were tested under zero-shot conditions and exhibited significant performance gaps, particularly in hazardous activity recognition and multi-label tasks.

Conclusion: The findings highlight the inadequacies of current video-language models for safety-critical industrial applications and emphasize the importance of developing more robust, safety-aware systems. iSafetyBench is introduced as a tool to stimulate advancement in this area.

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [115] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox is a 3D simulation for training embodied agents in shopping tasks, with human benchmarks and vision-language AI integrations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of retail-specific simulation environments for embodied agent training.

Method: Developed a high-fidelity 3D retail store simulation, coupled with VR and VLM-powered agents, incorporating a dataset of human demonstrations.

Result: Created a platform for agents to interact with retail environments and established benchmarks against human performance.

Conclusion: Sari Sandbox enhances embodied agent development by providing tools and benchmarks to improve their realism and scalability for retail tasks.

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [116] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: This paper addresses the problem of geometric distortions and blurring in videos caused by atmospheric turbulence by introducing the Dynamic Efficiency Index (DEI) and a novel Physical Model-Driven Multi-Stage Video Restoration (PMR) framework.


<details>
  <summary>Details</summary>
Motivation: Restoring edge details and eliminating mixed distortions in videos severely affected by strong atmospheric turbulence and complex dynamics is highly challenging for existing methods.

Method: The paper introduces DEI for quantifying video dynamics under turbulence and proposes PMR, a three-stage framework involving de-tilting, motion segmentation enhancement, and de-blurring, optimized with lightweight backbones and stage-wise joint training.

Result: Experiments show that the PMR framework effectively reduces motion artifacts, restores edge details, and performs well in high-turbulence and dynamic real-world scenarios.

Conclusion: The proposed method is efficient, achieves high-quality restoration in challenging conditions, and shows strong generalization capability. Code and datasets will be made publicly available.

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [117] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock is a novel framework designed to accelerate inference in Diffusion Transformers (DiTs) by selectively skipping redundant computations, achieving over 2x speedup with minimal quality degradation.


<details>
  <summary>Details</summary>
Motivation: To address the high inference latency in Diffusion Transformers (DiTs), which limits their use in real-time applications due to their sequential denoising process.

Method: The paper introduces Sortblock, a training-free framework that dynamically caches Transformer block-wise features based on their similarity across timesteps. It ranks residual evolution to adaptively determine a recomputation ratio and uses a lightweight linear prediction mechanism to reduce errors from skipped computations.

Result: Sortblock delivers over a 2x inference speedup with minimal degradation in the quality of generated outputs, as demonstrated through extensive experiments across various tasks and DiT architectures.

Conclusion: Sortblock offers an effective, generalizable solution to accelerating diffusion-based generative models, balancing computational efficiency with output fidelity.

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [118] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5 improves high-resolution diffusion models by restructuring latent space and augmenting diffusion training, boosting convergence speed and image quality.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in using autoencoders with higher latent channel numbers for latent diffusion models, which improve reconstruction quality but reduce diffusion model convergence and generation quality.

Method: Introduced a structured latent space to separate object structure and image details, and implemented augmented diffusion training with additional objectives to enhance convergence speed.

Result: DC-AE 1.5 achieves faster convergence and superior image generation quality compared to its predecessor, delivering 4x faster performance on ImageNet 512x512 with improved results.

Conclusion: DC-AE 1.5's innovations make it a more efficient and effective choice for latent diffusion models, significantly improving model training and performance quality.

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [119] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: This paper focuses on addressing the problem of video outpainting by modifying video inpainting models. The authors use a hierarchical discriminator and a specialized loss function to achieve improved results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve video outpainting techniques, which have traditionally relied on generating background contents, by employing video inpainting models that handle object flow and consistency better.

Method: The authors propose a hierarchical discriminator with global and local objectives, alongside a specialized outpainting loss function derived from discriminator features. They fine-tune existing inpainting models on this adversarial loss.

Result: The approach achieves superior results in video outpainting tasks, both quantitatively and qualitatively, outperforming state-of-the-art methods.

Conclusion: The hierarchical discriminator and specialized outpainting loss significantly enhance the generator's performance, producing visually and globally coherent extended scenes. The work advances video outpainting techniques and offers publicly accessible code and demos for reproducibility.

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [120] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: This paper introduces UIS-Mamba, a novel underwater instance segmentation model, incorporating two new modules to enhance segmentation in complex underwater environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges with underwater instance segmentation, such as color distortion, blurred boundaries, and complex backgrounds, and adapt the Mamba state space model to this specialized scenario.

Method: They designed UIS-Mamba with two modules: Dynamic Tree Scan (DTS) for maintaining feature continuity in instances and Hidden State Weaken (HSW) for reducing background interference using an Ncut-based mechanism.

Result: UIS-Mamba achieves superior performance on the UIIS and USIS10K datasets while maintaining low parameters and computational complexity.

Conclusion: The proposed UIS-Mamba model, leveraging DTS and HSW modules, effectively adapts Mamba to underwater segmentation tasks, offering state-of-the-art results and resource efficiency.

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [121] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: This paper proposes a novel method for amodal completion in complex human-object interactions using physical priors and a multi-regional inpainting strategy, improving over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods struggle with dynamic scenarios in Human-Object Interaction (HOI) tasks due to limited understanding of physical relationships and constraints.

Method: The authors utilize physical priors related to human topology and contact information to define two distinct regions (primary and secondary). They propose a customized multi-regional inpainting strategy using denoising techniques within a diffusion model.

Result: The proposed method outperforms current techniques in terms of accuracy and realism in both shape and visual detail of occluded object completions. It is also robust even without ground-truth annotations.

Conclusion: This work advances machine perception of dynamic HOI environments by integrating physical constraints, making it applicable for tasks such as 3D reconstruction and novel view/pose synthesis.

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [122] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: The paper introduces TopoTTA, a novel test-time adaptation framework for tubular structure segmentation (TSS) that enhances segmentation under domain shifts by improving topological representation and continuity.


<details>
  <summary>Details</summary>
Motivation: TSS is highly sensitive to domain shifts, as topological changes and local feature variations degrade segmentation performance. Addressing these sensitivities is crucial for improving TSS robustness in unseen target domains.

Method: The proposed TopoTTA framework operates in two stages: Stage 1 adapts to cross-domain topological discrepancies using TopoMDCs to enhance topological representation. Stage 2 strengthens topological continuity via the TopoHG strategy and prediction alignment on pseudo-labels in pseudo-break regions.

Result: Experiments conducted on ten datasets across four scenarios show that TopoTTA achieves a significant average improvement of 31.81% in clDice, effectively mitigating topological distribution shifts.

Conclusion: TopoTTA proves to be a highly effective plug-and-play solution for CNN-based TSS models, demonstrating superior performance in handling domain shifts and improving topological integrity.

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [123] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: The paper introduces SDMatte, an interactive matting model leveraging diffusion model capabilities for detailed edge extraction and region focus through visual prompts.


<details>
  <summary>Details</summary>
Motivation: Existing interactive matting methods struggle with extracting fine-grained edge details, and diffusion models offer powerful data distribution modeling and interaction abilities.

Method: The authors develop SDMatte by utilizing diffusion models for visual prompt-driven interaction, integrating coordinate and opacity embeddings into U-Net, and introducing a masked self-attention mechanism.

Result: Experiments across multiple datasets show SDMatte outperforms other solutions in interactive matting tasks.

Conclusion: The innovative techniques introduced in SDMatte validate its effectiveness in interactive matting, addressing key limitations in precision and edge detail extraction.

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [124] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias is a framework to mitigate biases in Text-to-Image models by detecting biased patterns and generating inclusive guides, achieving high accuracy and reducing biased outputs significantly.


<details>
  <summary>Details</summary>
Motivation: Address issues of unintended gender and racial biases in Text-to-Image models that often appear even when such attributes are unmentioned.

Method: Leverages vision-language models to detect bias and generates fairness guides through alternative inclusive prompts. Used in a CLIP-guided training process.

Result: Achieved 91.6% accuracy in detecting harmful patterns and reduced biased outputs from 90% to negligible levels while maintaining image quality and diversity.

Conclusion: AutoDebias effectively mitigates both subtle and overlapping biases in Text-to-Image models, outperforming existing methods and ensuring fair representation without sacrificing model performance.

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [125] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: The paper introduces CLIPTime, a vision-language model for predicting fungal growth stages and associated timestamps from visual and textual data, addressing challenges in capturing temporal progression.


<details>
  <summary>Details</summary>
Motivation: Accurately capturing temporal dynamics in biological growth is essential for fields like agriculture and microbiology, but existing vision-language models struggle with time-aware reasoning.

Method: The authors build upon the CLIP architecture, designing a multimodal, multitask model that jointly predicts growth stages and timestamps without requiring explicit temporal inputs during testing. A synthetic dataset with annotated timestamps and stage labels is introduced for training and evaluation.

Result: CLIPTime achieves strong performance in both classification and regression tasks, offering interpretable and temporally precise outputs. Custom metrics demonstrate its effectiveness in modeling biological progression.

Conclusion: CLIPTime broadens the capability of vision-language models for temporal reasoning, showing promise for applications in biological monitoring and similar real-world domains.

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [126] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: The paper introduces PIF-Net for fusing multispectral and hyperspectral images using a framework that explicitly targets the ill-posed nature of the challenge by incorporating priors and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Multispectral and hyperspectral image fusion is challenging due to the trade-off between spectral and spatial detail and data misalignment, which leads to ill-posed problems.

Method: The authors propose PIF-Net, which combines an invertible Mamba architecture for balanced global spectral modeling and a novel Fusion-Aware Low-Rank Adaptation module to calibrate features while ensuring model efficiency.

Result: Experiments on multiple benchmark datasets show that PIF-Net outperforms state-of-the-art methods in image restoration while maintaining lightweight model efficiency.

Conclusion: PIF-Net proves to be an effective and efficient solution to the ill-posed problem of multispectral and hyperspectral image fusion by maintaining a balance between spectral richness and spatial detail.

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [127] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: This paper introduces SeTe-VSR, a novel video super-resolution model incorporating semantic and temporal-spatial guidance, addressing fidelity and temporal consistency issues.


<details>
  <summary>Details</summary>
Motivation: The need to improve video super-resolution models to better align with low-resolution inputs while maintaining temporal consistency and enhancing fidelity.

Method: SeTe-VSR utilizes semantic information and spatial-temporal integration within latent diffusion spaces to recover video details effectively.

Result: Experiments show superior performance of SeTe-VSR in detail recovery and perceptual quality compared to existing methods.

Conclusion: SeTe-VSR effectively balances intricate detail recovery and temporal coherence, advancing video super-resolution technology.

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [128] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: This paper introduces HyPCV-Former, a hyperbolic spatio-temporal transformer for video anomaly detection in 3D point cloud videos. The approach achieves state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods that use Euclidean representations struggle to capture hierarchical event structures and spatio-temporal continuity, necessitating a better geometric approach for video anomaly detection.

Method: HyPCV-Former extracts spatial features from point cloud sequences, embeds them in Lorentzian hyperbolic space, and employs a hyperbolic multi-head self-attention mechanism to model temporal dynamics in non-Euclidean geometry.

Result: HyPCV-Former demonstrates significant improvements in anomaly detection, with a 7% performance boost on the TIMo dataset and 5.6% on the DAD dataset compared to benchmarks.

Conclusion: The use of hyperbolic geometry in anomaly detection tasks enhances the ability to capture hierarchical structures and temporal dependencies, making HyPCV-Former effective for video surveillance applications.

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [129] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC is a new framework for multi-image composition that enhances image generation using layout control without additional training.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of spatially aware, coherent, and consistent image synthesis from multiple references, leveraging a training-free method.

Method: Two novel attention mechanisms (Group Isolation Attention and Region-Modulated Attention) were introduced into the MMDiT model, without requiring additional training.

Result: LAMIC outperformed existing methods in multiple evaluation metrics, including layout control, identity preservation, and background consistency, showcasing strong zero-shot generalization.

Conclusion: The framework demonstrates a new training-free paradigm for multi-image composition, combining strengths of single-reference models while scaling with evolving foundation models.

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [130] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0 improves hyperspectral medical image segmentation by combining spectral similarity with spatial cues, outperforming previous methods without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Enhance segmentation in hyperspectral medical imaging by overcoming the limitations of spatial-only cues and improving model adaptability to low-data scenarios.

Method: Integrates spectral angle prompting into the Segment Anything Model (SAM) for early fusion of spectral and spatial information.

Result: Achieved up to +3.8% better Dice scores over RGB-only models and +3.1% over earlier spectral fusion methods, showing improved few-shot and zero-shot generalization.

Conclusion: SAMSA 2.0 offers a robust method for accurate segmentation in clinical imaging, especially in scenarios with limited data or noise, without requiring model retraining.

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [131] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: The paper proposes LesiOnTime, a 3D segmentation approach for accurately detecting small lesions in breast DCE-MRI by leveraging longitudinal imaging and clinical BI-RADS scores, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of small lesions in breast DCE-MRI is crucial for early cancer diagnosis in high-risk patients. Current methods neglect longitudinal and clinical data often utilized by radiologists.

Method: LesiOnTime integrates temporal imaging data using a Temporal Prior Attention block and incorporates clinical BI-RADS scores through BI-RADS Consistency Regularization loss for segmentation training.

Result: LesiOnTime improves segmentation performance by 5% Dice score compared to existing methods, with ablation studies confirming the individual contributions of its components.

Conclusion: Integrating temporal and clinical data enhances lesion segmentation and mimics real-world diagnostic workflows, helping improve early cancer detection accuracy.

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [132] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: This paper introduces an unsupervised method for labeling Sentinel-2 satellite imagery using segmentation and graph neural networks, avoiding the need for pre-labeled data.


<details>
  <summary>Details</summary>
Motivation: Labeling remote sensing imagery requires significant expertise and resources, and existing tools depend on pre-labeled data. This creates a need for methods that can infer labels without prior annotations.

Method: The proposed pipeline uses convolutional and graph neural networks for segmentation, grouping pixels based on color and spatial similarities, and encoding their local neighborhood features. The goal is to achieve robust feature representations without relying on pre-labeled data.

Result: The method produces feature representations that reduce outliers, support fine-grained labeling, and provide rotationally invariant semantic relationships in the encoded space.

Conclusion: This approach offers a more autonomous and efficient way to label remote sensing imagery, enhancing the robustness and usability of the resulting labels.

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [133] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: This paper proposes a benchmark and training dataset to improve egocentric video understanding for spatiotemporal grounding.


<details>
  <summary>Details</summary>
Motivation: Egocentric videos are crucial for applications like augmented reality and robotics, but challenges like shorter object durations and larger positional shifts remain unaddressed compared to exocentric videos.

Method: The authors analyze egocentric video challenges and introduce EgoMask, a benchmark dataset with pixel-level annotations constructed using an automatic pipeline. They also create EgoMask-Train, a large-scale training dataset.

Result: State-of-the-art spatiotemporal grounding models perform poorly on EgoMask initially, but fine-tuning using EgoMask-Train significantly boosts model performance while maintaining effectiveness on exocentric datasets.

Conclusion: EgoMask and EgoMask-Train provide vital tools to address egocentric video challenges, advancing research in spatiotemporal video grounding and egocentric video understanding.

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [134] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: EPANet is proposed for underwater fish detection, offering improved accuracy and efficiency over existing methods by integrating features effectively and maintaining lightweight model complexity.


<details>
  <summary>Details</summary>
Motivation: Underwater fish detection faces challenges like low resolution and background interference, with existing methods often introducing inefficiencies through complex attention mechanisms.

Method: EPANet uses an efficient path aggregation feature pyramid network (EPA-FPN) with long-range skip connections and cross-layer fusion, and a multi-scale short path bottleneck (MS-DDSP bottleneck) for diverse feature representation.

Result: EPANet demonstrates superior detection accuracy and inference speed on benchmark underwater fish detection datasets, outperforming state-of-the-art methods.

Conclusion: The proposed EPANet advances underwater fish detection by balancing accuracy, speed, and model complexity, addressing limitations of conventional methods.

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [135] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: The paper introduces a diffusion model-based framework for automatic reference-based video color grading, employing look-up tables (LUTs) for structural preservation and fast inference, and enabling user-specific adjustments via textual prompts.


<details>
  <summary>Details</summary>
Motivation: Video color grading is a complex process predominantly handled by professional colorists due to the artistic and storytelling elements involved. The paper aims to simplify and democratize this process using an automated, reference-based solution.

Method: The approach involves generating a look-up table (LUT) that aligns the color attributes of the input video with reference scenes using a diffusion model. Additionally, user preferences can be incorporated for low-level feature enhancements via text prompts.

Result: Experimental results, including user studies, indicate that the method effectively achieves video color grading, maintaining structural details and enabling artistic control.

Conclusion: The proposed framework offers an efficient and user-friendly solution for video color grading that balances artistic expression with automation, making the process accessible to non-professionals.

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [136] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: The paper evaluates Vision-Language Models (VLMs) on medical images for determining relative anatomical positions, identifies performance shortcomings, explores visual prompts as potential aids, and introduces the MIRP benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve clinical decision-making by enabling VLMs to effectively determine relative positions of anatomical structures in medical images.

Method: The researchers evaluated state-of-the-art VLMs and tested visual prompts like alphanumeric or colored markers to enhance performance, followed by introducing the MIRP dataset for systematic evaluation.

Result: All VLMs evaluated showed poor performance in determining relative anatomical positions on medical images, with only moderate improvements from visual prompts.

Conclusion: VLMs currently rely more on prior anatomical knowledge than image content for relative positioning in medical imaging, highlighting significant challenges and the need for further research using the MIRP benchmark dataset.

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [137] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: The paper introduces "Diffusion Bridge Distillation for Purification" (DBLP), a fast, diffusion-based method for defending neural networks against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks exploit the vulnerabilities of DNNs, and existing purification methods are too computationally intensive for real-time use.

Method: The authors propose DBLP, combining noise bridge distillation (aligning adversarial noise with clean data) with adaptive semantic enhancement (using edge maps as extra guidance).

Result: DBLP outperforms existing methods in adversarial robustness, image quality, and efficiency, needing only about 0.2 seconds per inference.

Conclusion: DBLP represents a significant advancement in efficient, real-time adversarial purification, making it more practical for deployment.

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [138] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune is a hierarchical attention token pruning framework for vision-language models (VLMs) that requires no training and significantly reduces computational overhead while maintaining high task accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the excessive computational overhead and limited inference efficiency in Vision-Language Models, which encode images into long sequences of visual tokens. Existing approaches to mitigate this often require task-specific training or special tokens, limiting scalability.

Method: HiPrune is a training-free, model-agnostic token pruning framework that leverages hierarchical attention structures in vision encoders. It selects three types of informative tokens: Anchor tokens (high attention in object-centric layers), Buffer tokens (adjacent to anchors for spatial continuity), and Register tokens (strong global context from deep layers).

Result: HiPrune achieves state-of-the-art token pruning performance, preserving up to 99.3% task accuracy with only 33.3% of tokens and 99.5% accuracy with just 11.1% tokens. It also reduces inference FLOPs and latency by up to 9 times, demonstrating generalization across various models and tasks.

Conclusion: HiPrune offers an efficient way to reduce computational costs in VLMs without retraining, enabling widespread adaptability across architectures while maintaining impressive task accuracy.

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [139] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: The paper introduces FreeCP, a training-free framework for open-vocabulary semantic segmentation (OVSS), addressing issues of class redundancy and visual-language ambiguity.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges and performance issues in OVSS caused by class redundancy and visual-language ambiguity in existing training-free approaches.

Method: FreeCP, a training-free class purification framework, purifies semantic categories and rectifies class activation errors, facilitating improved segmentation predictions.

Result: Experiments on eight benchmarks demonstrate that FreeCP significantly enhances segmentation performance when integrated with other OVSS methods.

Conclusion: FreeCP presents a resource-efficient and effective solution to improve OVSS without additional training, showing strong potential as a plug-and-play module.

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [140] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP is a diffusion model-based method that generates articulated objects aligned with partial point clouds, ensuring physical plausibility through constraints.


<details>
  <summary>Details</summary>
Motivation: Everyday environments contain interactable articulated objects requiring realistic generation and alignment to partial point clouds for practical applications.

Method: PhysNAP uses signed distance functions (SDFs) for part shapes, guides the diffusion process with alignment loss, incorporates non-penetration and mobility constraints, and uses category-aware techniques.

Result: Tests using the PartNet-Mobility dataset show PhysNAP improves constraint consistency when compared to an unguided baseline while maintaining generative ability.

Conclusion: PhysNAP offers a tradeoff between constraint consistency and generative ability, providing a robust approach to generating physically plausible articulated objects.

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [141] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: This paper proposes a weakly supervised object detection method using image-level annotations instead of expensive bounding box annotations, focusing on virus detection in images.


<details>
  <summary>Details</summary>
Motivation: Current object detection techniques rely heavily on annotated bounding boxes which are expensive and labor-intensive to acquire, especially when domain expertise is required.

Method: The authors use a pre-trained model to generate pseudo-labels for virus detection from image-level annotations, employing an optimization strategy with a shrinking receptive field, bypassing the need for specific network architectures.

Result: The proposed approach produces pseudo-labels that are easier to obtain and surpass the performance of existing weak labeling methods and even ground truth labels under time-constrained annotation conditions.

Conclusion: The algorithm offers a cost-effective, efficient solution for domain-specific object detection, reducing reliance on costly annotations while maintaining or enhancing detection performance.

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [142] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: The paper addresses issues in unsupervised Visual Odometry methods caused by dynamic objects by introducing a novel uncertainty modeling approach, CoProU-VO, which considers uncertainties across temporal frames.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of unsupervised Visual Odometry methods when dealing with dynamic objects that violate the static scene assumption, which leads to errors in pose estimation.

Method: Proposed CoProU-VO, a probabilistic model that combines uncertainties between consecutive frames using a vision transformer backbone to estimate depth, uncertainties, and camera poses simultaneously.

Result: Experiments on KITTI and nuScenes datasets demonstrated improved performance and robustness in dynamic scenes compared to previous methods, especially in complex highway environments.

Conclusion: The proposed cross-frame uncertainty propagation method enhances the reliability of unsupervised Visual Odometry, particularly in dynamic and challenging scenarios.

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [143] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: This paper introduces an uncertainty-aware method for detecting unknown objects in semantic segmentation, achieving significant improvements in accuracy and false positive rates.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models misclassify unknown objects in real-world scenarios, particularly in cases where rare classes are mistaken for truly unknown ones.

Method: The approach employs an evidential classifier in a likelihood ratio test to classify pixel features while integrating uncertainty through probability distributions.

Result: The proposed method achieves a low 2.5% average false positive rate, high average precision of 90.91%, and minimal computational overhead, evaluated over five benchmark datasets.

Conclusion: Incorporating uncertainty into semantic segmentation outlier detection improves the effectiveness of identifying unknown objects and reduces errors significantly.

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [144] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: The study addresses gaps in artificial night-time light remote sensing by proposing a novel framework to extend VIIRS-like data sets backwards to 1986 for China, improving accuracy and reliability in long-term human activity analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing VIIRS-like NTL reconstruction methods, which suffer from underestimation of light intensity and omission of structural details, and to enable reliable long-term time-series studies beyond the VIIRS sensor's starting year of 2012.

Method: The authors introduce a two-stage reconstruction framework composed of a Hierarchical Fusion Decoder (HFD) for initial NTL reconstruction fidelity enhancement and a Dual Feature Refiner (DFR) utilizing high-resolution impervious surface masks for fine-grained structural detail refinement.

Result: The new EVAL product for China extends VIIRS-like NTL data back to 1986, achieving higher accuracy (boosting R² from 0.68 to 0.80 and reducing RMSE from 1.27 to 0.99) and demonstrating strong temporal consistency and correlations with socioeconomic indicators.

Conclusion: The EVAL dataset is substantially more accurate and temporally consistent than previous approaches, making it a valuable publicly available resource for long-term analysis of human activities and socioeconomic studies.

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [145] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: The paper introduces Wukong, an intermediate-output-based NSFW detection framework, to address challenges in Text-to-Image generation systems.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and shortcomings of current NSFW filtering mechanisms for AI-based image generation, which either analyze text prompts inadequately or add latency in analyzing the final output images.

Method: Introduces Wukong, a transformer-based framework that detects NSFW content during the iterative denoising process of diffusion models, using insights from early denoising steps and reusing U-Net's pre-trained cross-attention parameters.

Result: Evaluation demonstrates Wukong's superior performance over text-based safeguards and comparable accuracy to image filters, with reduced computational overhead.

Conclusion: Wukong enhances NSFW detection efficiency in text-to-image generation systems, providing a practical balance of accuracy and computational speed compared to existing methods.

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [146] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: GeoMoE proposes a novel framework that utilizes Mixture-of-Experts (MoE) for motion field modeling, effectively addressing heterogeneous motion patterns in complex scenes.


<details>
  <summary>Details</summary>
Motivation: To improve motion field modeling in two-view geometry, particularly for complex real-world scenes with diverse motion patterns and depth discontinuities.

Method: GeoMoE employs a Probabilistic Prior-Guided Decomposition for structured decomposition of motion fields and an MoE-Enhanced Bi-Path Rectifier to refine modeling and reduce interference.

Result: GeoMoE surpasses previous methods in relative pose and homography estimation, demonstrating strong generalization abilities.

Conclusion: The framework enables more accurate and fine-grained motion field estimation, paving the way for better modeling of heterogeneous motion patterns in real-world scenarios.

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [147] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces DPoser-X, a diffusion-based model for 3D whole-body human pose modeling, addressing challenges with complexity and dataset scarcity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a robust and versatile model for full-body human poses, overcoming challenges like complex pose structures and limited high-quality data.

Method: It uses a diffusion model approach with novel techniques like truncated timestep scheduling and masked training to unify various pose-related tasks and handle dataset challenges.

Result: DPoser-X shows superior robustness and performance across multiple benchmarks, outperforming state-of-the-art models in body, hand, face, and full-body pose modeling.

Conclusion: DPoser-X establishes itself as a new benchmark for whole-body human pose prior modeling, demonstrating its effectiveness and versatility.

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [148] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: The paper examines vulnerabilities in face detection systems used for recognition in challenging environments and introduces novel attack methods while proposing countermeasures.


<details>
  <summary>Details</summary>
Motivation: Face detection systems are crucial for alignment in face recognition but are vulnerable to attacks, prompting the need for studying these security risks.

Method: The study introduces Face Generation Attacks and Landmark Shift Attack to compromise face detection systems and explores mitigation strategies.

Result: The paper demonstrates the effectiveness of the proposed attacks against face detection and provides measures to counter these vulnerabilities.

Conclusion: Face detection systems can be compromised, but the paper offers insights and strategies for improving security against such attacks.

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [149] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: The paper presents a generative model approach to synthesize annotated medical image data, improving the explainability and accuracy of classification models used in medical diagnoses.


<details>
  <summary>Details</summary>
Motivation: Enhancing explainability and alignment of AI models with clinical reasoning by integrating pathology-related visual attributes in medical image analysis, while addressing the lack of large-scale annotated datasets.

Method: A diffusion-based generative model is enhanced with attribute conditioning to synthesize annotated medical image data, utilizing 20 attribute-labeled samples from LIDC-IDRI as training data.

Result: Incorporating synthetic images in training improved attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to using only real annotated datasets.

Conclusion: Synthetic data offers a promising solution to the challenges posed by limited annotated datasets, making explainable AI models more viable and effective for medical image diagnosis.

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [150] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: This paper revisits and benchmarks 11 patch defenses, analyzing 94 patch types across 94,000 images to provide a unified evaluation framework, uncovering key insights for improvement.


<details>
  <summary>Details</summary>
Motivation: The inconsistent and incomplete evaluations of existing defenses against adversarial patch attacks necessitate a unified benchmarking framework to strengthen defenses.

Method: The study evaluates 11 defenses under 2 attack goals, 13 patch attacks, 11 object detectors, and 4 metrics, creating a dataset with 94 patch types and 94,000 images for comprehensive analysis.

Result: Key findings include the influence of patch distribution on defense difficulty, the correlation of defense performance with average precision rather than patch detection accuracy, and the relative robustness of defenses using stochastic models or universal patch properties.

Conclusion: The analyses offer guidance to improve patch defense evaluation and design, while the dataset and code repository aim to advance future research by integrating new methods.

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [151] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: The paper explores using pretrained depth features for image dehazing, introducing a plug-and-play RGB-D fusion module that works with various architectures.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of haze variability in real-world image dehazing and enhance adaptability for diverse accuracy and efficiency needs.

Method: Investigates pretrained depth representations, evaluates their consistency across haze levels, and develops a universal RGB-D fusion module compatible with different architectures.

Result: Proposed method showed effectiveness and adaptability, validated through experiments across multiple benchmarks.

Conclusion: The RGB-D fusion module leverages depth representations to improve performance in dehazing and exhibits broad applicability to diverse architectures.

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [152] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: The paper introduces a novel, training-free method called Detection by Difference of Differences (D3) for detecting AI-generated videos. It achieves significant improvement in detection accuracy by analyzing second-order temporal discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods for AI-generated videos fail to adequately account for temporal artifacts. This gap creates a need for more effective approaches to distinguish synthetic content from real videos, especially given the growing concern over the misuse of high-fidelity AI-generated media.

Method: The authors developed a second-order dynamical analysis grounded in Newtonian mechanics and extended Second-order Central Difference features to detect temporal artifacts. They used these features to design D3, a novel detection method based on analyzing divergences in second-order temporal feature distributions, without requiring training.

Result: D3 was validated across 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM) and 40 subsets. For instance, it achieved a 10.39% absolute improvement in mean Average Precision on the GenVideo dataset over the previous state-of-the-art. The method also demonstrated high computational efficiency and robustness against post-processing effects.

Conclusion: D3 offers a significant advancement in detecting AI-generated videos by leveraging second-order temporal discrepancies without the need for training, providing an effective and efficient solution to address growing concerns over synthetic media dissemination.

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [153] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: This study introduces MIHBench, the first benchmark to systematically evaluate hallucinations in Multi-Image Multimodal Large Language Models (MLLMs), focusing on object-related semantic understanding.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive research investigating hallucination in MLLMs using multiple images, as existing studies predominantly focus on single-image scenarios.

Method: The study proposes MIHBench, a benchmark with three tasks addressing object existence, count reasoning, and cross-view identity consistency. It also introduces a Dynamic Attention Balancing mechanism to manage inter-image attention distributions.

Result: The benchmark revealed critical factors contributing to hallucinations in multi-image contexts, and the proposed Dynamic Attention Balancing mechanism reduced hallucination occurrences in various state-of-the-art MLLMs.

Conclusion: MIHBench fills a gap in the field by enabling systematic evaluation, and the introduced method strengthens stability and reasoning in multi-image scenarios for MLLMs.

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [154] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count is a differentiable model for object counting and quantity control in text-to-image generation, using a novel 'cardinality' map.


<details>
  <summary>Details</summary>
Motivation: To address challenges in open-vocabulary object counting and enable precise quantity control in text-to-image (T2I) generation.

Method: Proposes a 'cardinality' map for counting variations, integrates representation alignment, and uses a hybrid strong-weak supervision scheme within a fully differentiable architecture.

Result: YOLO-Count achieves state-of-the-art object counting accuracy and enhances quantity control capabilities for T2I generative systems.

Conclusion: YOLO-Count successfully integrates open-vocabulary counting with T2I generation, offering accurate counting and robust quantity control.

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [155] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper introduces Dense Backbone, a lightweight backbone for LiDAR-based 3D object detection, which reduces model complexity while maintaining robust object detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Although LiDAR-based 3D object detection has advanced, most frameworks use complex backbones like VGG or ResNet, leading to high computational costs. There is a gap in lightweight backbone design specifically for 3D object detection.

Method: The proposed Dense Backbone leverages a dense-layer-based lightweight design for high-speed processing and strong detection accuracy. It is integrated with existing SoTA models like PillarNet without modifying other components.

Result: DensePillarNet, created by integrating Dense Backbone with PillarNet, reduces model parameters by 29% and latency by 28%, with only a 2% drop in detection accuracy on the nuScenes test set.

Conclusion: Dense Backbone demonstrates that lightweight architectures can preserve detection accuracy while significantly lowering computational costs, making it highly deployable in autonomous driving applications.

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [156] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: The paper introduces GECO, a framework for learning geometrically coherent and semantically discriminative features in self-supervised vision models.


<details>
  <summary>Details</summary>
Motivation: Self-supervised vision models often lack an understanding of 3D geometry despite capturing semantic features. GECO aims to integrate geometric coherence into these models.

Method: The GECO framework uses optimal transport for training and supervision, even accounting for occlusions and disocclusions. It employs a lightweight architecture achieving faster processing speeds.

Result: GECO operates at 30 fps, 98.2% faster than previous methods, and delivers state-of-the-art performance, improving PCK by 6.0%, 6.2%, and 4.1% on benchmark datasets.

Conclusion: GECO demonstrates improved geometric feature learning and introduces new metrics to enhance geometry-awareness in self-supervised learning. It outperforms prior methods in both speed and accuracy.

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [157] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: The paper addresses the emerging threat of avatar-based impersonation in communication systems and explores using facial motion patterns as biometric verification to identify individuals.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address security risks associated with avatar-mediated communication, particularly impersonation, where attackers replicate a user's avatar to pose as them fraudulently.

Method: The authors develop a new dataset with avatar videos (genuine and impostor) and propose a spatio-temporal Graph Convolutional Network architecture using facial landmarks to model facial gestures for identity verification.

Result: Experiments show that facial motion cues can achieve meaningful identity verification with AUC values approaching 80%.

Conclusion: Facial motion patterns have potential as a behavioral biometric for identity verification in avatar-mediated systems, emphasizing the need for advanced defenses against such security risks.

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [158] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: The paper introduces SU-ESRGAN, a novel framework for super-resolution in satellite imagery, integrating ESRGAN with segmentation loss and pixel-wise uncertainty maps, achieving credible and domain-aware results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of GANs in remote sensing applications, such as lack of semantic consistency and per-pixel confidence, which are critical for applications like disaster response, urban planning, and agriculture.

Method: Introduces SU-ESRGAN, combining ESRGAN with DeepLabv3 for segmentation loss and Monte Carlo dropout for uncertainty mapping. Also, tests cross-domain adaptability using fine-tuned models on drone datasets.

Result: Evaluations show SU-ESRGAN achieves comparable SR performance (PSNR, SSIM, LPIPS) with baseline ESRGAN on aerial imagery and adapts better to datasets with domain-aligned imaging characteristics.

Conclusion: SU-ESRGAN enhances satellite and UAV imagery via modular and domain-aware SR processing, emphasizing the importance of training for specific imaging domains for improved performance.

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [159] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: The paper presents a novel Test-Time Adaptation (TTA) framework for medical image-to-image translation, addressing limitations in handling out-of-distribution samples. It achieves robust performance by dynamically adapting to test samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations in current image-to-image translation models, which struggle with out-of-distribution samples and risk performance degradation in real-world medical imaging tasks.

Method: The proposed method includes a Reconstruction Module to assess domain shifts and a Dynamic Adaptation Block to adjust features in a pretrained translation model, selectively modifying the adaptation process based on each test sample's characteristics.

Result: The approach showed consistent improvements on low-dose CT denoising and T1 to T2 MRI translation tasks compared to baseline models and prior TTA methods.

Conclusion: Dynamic, sample-specific adjustments enhance the resiliency of translation models in real-world medical imaging scenarios, making it a promising path forward for handling domain shifts.

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [160] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: This paper addresses the challenges of zero-shot anomaly detection (ZSAD) under domain shifts by introducing a framework called PILOT, which combines dynamic prompt learning and label-free test-time adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the limitations of existing zero-shot anomaly detection methods that struggle with domain shifts due to restricted training domains, failing to generalize to unseen distributions.

Method: PILOT employs a dual-branch prompt learning mechanism for adaptive anomaly cues and a label-free test-time adaptation strategy that updates parameters using high-confidence pseudo-labels.

Result: Experiments on 13 industrial and medical benchmarks show that PILOT achieves state-of-the-art performance for anomaly detection and localization under domain shift.

Conclusion: PILOT effectively overcomes the challenges of ZSAD under domain shifts, demonstrating strong adaptability and generalization capabilities.

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [161] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: This paper examines semantic segmentation of heterogeneously labeled point-cloud datasets for public safety applications, finding performance issues with smaller, safety-critical features.


<details>
  <summary>Details</summary>
Motivation: To improve public safety applications, such as pre-incident planning systems, by addressing semantic segmentation challenges with differently labeled 3D point-cloud data.

Method: Employed graded schemas and KPConv architecture, evaluating semantic segmentation performance using IoU metrics on safety-relevant features in the NIST Point Cloud City dataset.

Result: Performance variability was observed, with larger objects like stairs/windows performing well, but smaller, safety-critical features showed lower segmentation accuracy due to class imbalance and limited geometric distinction.

Conclusion: Reliable semantic segmentation for public safety requires standardized annotation protocols and advanced labeling methods to overcome data heterogeneity and improve detection of small, critical elements.

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [162] [Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT](https://arxiv.org/abs/2508.00341)
*Shengheng Liu,Ningning Fu,Zhonghao Zhang,Yongming Huang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: This paper integrates over-the-air computation with federated learning to improve model aggregation efficiency, focusing on user scheduling and receive beam steering for better communication and training performance.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address privacy concerns in IoT systems while improving the efficiency of federated learning, especially under constraints of large-scale networks with limited radio resources.

Method: The authors propose an optimization framework using the difference-of-convex technique for user scheduling and receive beam steering, and complement it with a low-complexity, non-iterative policy based on wireless channel analysis.

Result: Experiments confirm the advantages of the proposed methods, showing improved aggregation error rates and learning performance in comparison to existing techniques.

Conclusion: The integrated approach enhances federated learning's communication efficiency and accuracy, making it practical for large-scale IoT systems with stringent resource constraints.

Abstract: The rising popularity of Internet of things (IoT) has spurred technological
advancements in mobile internet and interconnected systems. While offering
flexible connectivity and intelligent applications across various domains, IoT
service providers must gather vast amounts of sensitive data from users, which
nonetheless concomitantly raises concerns about privacy breaches. Federated
learning (FL) has emerged as a promising decentralized training paradigm to
tackle this challenge. This work focuses on enhancing the aggregation
efficiency of distributed local models by introducing over-the-air computation
into the FL framework. Due to radio resource scarcity in large-scale networks,
only a subset of users can participate in each training round. This highlights
the need for effective user scheduling and model transmission strategies to
optimize communication efficiency and inference accuracy. To address this, we
propose an integrated approach to user scheduling and receive beam steering,
subject to constraints on the number of selected users and transmit power.
Leveraging the difference-of-convex technique, we decompose the primal
non-convex optimization problem into two sub-problems, yielding an iterative
solution. While effective, the computational load of the iterative method
hampers its practical implementation. To overcome this, we further propose a
low-complexity user scheduling policy based on characteristic analysis of the
wireless channel to directly determine the user subset without iteration.
Extensive experiments validate the superiority of the proposed method in terms
of aggregation error and learning performance over existing approaches.

</details>


### [163] [Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services](https://arxiv.org/abs/2508.00426)
*Rohan Gandhi,Ankur Mallick,Ken Sueda,Rui Liang*

Main category: cs.DC

TL;DR: This paper addresses the challenge of efficiently packing conference calls across servers within datacenters, proposing the Tetris framework to reduce high CPU utilization and hosting costs.


<details>
  <summary>Details</summary>
Motivation: Current conference services experience performance degradation and increased costs due to uneven CPU usage among servers hosting calls, intensified by variability and bursty call arrivals.

Method: The Tetris framework includes (a) optimized initial call assignments using historical data and (b) periodic migration of calls from hot servers through linear optimization.

Result: Evaluation on real-world data showed that Tetris reduces the number of participants on hot servers by at least 2.5 times, enhancing efficiency significantly.

Conclusion: Tetris effectively mitigates hot server issues in conferencing services, improving performance and reducing costs through smarter call placement mechanisms.

Abstract: Conference services like Zoom, Microsoft Teams, and Google Meet facilitate
millions of daily calls, yet ensuring high performance at low costs remains a
significant challenge. This paper revisits the problem of packing calls across
Media Processor (MP) servers that host the calls within individual datacenters
(DCs). We show that the algorithm used in Teams -- a large scale conferencing
service as well as other state-of-art algorithms are prone to placing calls
resulting in some of the MPs becoming hot (high CPU utilization) that leads to
degraded performance and/or elevated hosting costs. The problem arises from
disregarding the variability in CPU usage among calls, influenced by
differences in participant numbers and media types (audio/video), compounded by
bursty call arrivals. To tackle this, we propose Tetris, a multi-step framework
which (a) optimizes initial call assignments by leveraging historical data and
(b) periodically migrates calls from hot MPs using linear optimization, aiming
to minimize hot MP usage. Evaluation based on a 24-hour trace of over 10
million calls in one DC shows that Tetris reduces participant numbers on hot
MPs by at least 2.5X.

</details>


### [164] [SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments](https://arxiv.org/abs/2508.00622)
*Kapel Dev,Yash Madhwal,Sofia Shevelo,Pavel Osinenko,Yury Yanovich*

Main category: cs.DC

TL;DR: SwarnRaft is a blockchain-inspired positioning framework for UAV swarms to maintain coordination in GNSS-denied conditions using the Raft consensus algorithm.


<details>
  <summary>Details</summary>
Motivation: The reliability of UAV swarms in critical applications is challenged by disruptions in GNSS signals caused by interference, environmental conditions, or attacks.

Method: The paper introduces SwarnRaft, which utilizes the Raft consensus algorithm to achieve state updates (e.g., location, heading) for drones even in GNSS-denied scenarios, using GNSS, local sensing, and WiFi communication.

Result: The system exhibited robustness in preserving swarm coherence and fault tolerance, with an efficient and scalable communication model demonstrated in simulations.

Conclusion: SwarnRaft provides a secure and practical solution for ensuring decentralized UAV swarm operations in challenging, unpredictable environments.

Abstract: Unmanned aerial vehicle (UAV) swarms are increasingly used in critical
applications such as aerial mapping, environmental monitoring, and autonomous
delivery. However, the reliability of these systems is highly dependent on
uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,
which can be disrupted in real-world scenarios due to interference,
environmental conditions, or adversarial attacks, causing disorientation,
collision risks, and mission failure. This paper proposes SwarnRaft, a
blockchain-inspired positioning and consensus framework for maintaining
coordination and data integrity in UAV swarms operating under GNSS-denied
conditions. SwarnRaft leverages the Raft consensus algorithm to enable
distributed drones (nodes) to agree on state updates such as location and
heading, even in the absence of GNSS signals for one or more nodes. In our
prototype, each node uses GNSS and local sensing, and communicates over WiFi in
a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify
the position of the failed node based on its last known state and trajectory.
Our system demonstrates robustness in maintaining swarm coherence and fault
tolerance through a lightweight, scalable communication model. This work offers
a practical and secure foundation for decentralized drone operation in
unpredictable environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [165] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: This paper introduces a scalable spatiotemporal Transformer (ScaleSTF) for predicting urban network dynamics with improved efficiency and interpretability, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Urban networks require efficient and interpretable models to handle their complex spatiotemporal dynamics for decision-making, but current methods struggle with scalability in large systems.

Method: The authors propose ScaleSTF, a Transformer-based neural diffusion model that simplifies architectures by using principles inspired by physical laws. This reduces computational complexity while maintaining effectiveness.

Result: ScaleSTF demonstrates state-of-the-art performance and scalability in applications like traffic flow, solar power, and smart meters.

Conclusion: The study provides a new perspective on modeling and predicting spatiotemporal dynamics in large-scale urban systems, balancing efficiency, interpretability, and performance.

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [166] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: The paper introduces a hybrid deep learning framework to efficiently and safely measure high-profile Highway Railway Grade Crossing (HRGC) profiles using IMU, GPS, and ground truth data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of costly, time-intensive, and disruptive conventional methods for measuring HRGC profiles, which are essential for highway and railroad safety.

Method: A hybrid deep learning framework combining LSTM and Transformer architectures was developed. Data was collected using IMU, GPS-equipped vehicles, and a walking profiler, then tested in the field using three deep learning models.

Result: The LSTM-Transformer sequential and parallel models outperformed others, enabling the generation of 2D/3D HRGC profiles with high accuracy and efficiency based on field data from the Red Rock Railroad Corridor in Oklahoma.

Conclusion: The proposed deep learning models show strong potential to improve safety at HRGCs by allowing rapid, cost-effective, and accurate assessment of these profiles.

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [167] [DO-EM: Density Operator Expectation Maximization](https://arxiv.org/abs/2507.22786)
*Adit Vishnu,Abhay Shastry,Dhruva Kashyap,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: This paper introduces an Expectation-Maximization (EM) framework for training Density Operator Models (DOMs) on classical hardware, making them scalable to real-world data, and achieving superior performance on tasks like image generation.


<details>
  <summary>Details</summary>
Motivation: Density Operator Models (DOMs) are promising in generative modeling due to their quantum foundations, but existing training methods struggle to scale to real-world datasets like MNIST. This motivates the development of scalable and efficient training algorithms.

Method: The authors develop the Density Operator Expectation Maximization (DO-EM) algorithm by reformulating the quantum Expectation step as a quantum information projection (QIP) problem, solved using the Petz Recovery Map. DO-EM optimizes a quantum evidence lower bound iteratively and ensures non-decreasing log-likelihood for a broad class of models.

Result: The proposed DO-EM algorithm successfully trains a novel Quantum Interleaved Deep Boltzmann Machine (QiDBM) on MNIST. The QiDBM outperforms classical Deep Boltzmann Machines (DBMs) in image generation, reducing Fréchet Inception Distance by 40–60%.

Conclusion: The DO-EM framework bridges quantum latent variable models and classical scalability, delivering better performance and opening new avenues for applying quantum-inspired generative models to real-world datasets.

Abstract: Density operators, quantum generalizations of probability distributions, are
gaining prominence in machine learning due to their foundational role in
quantum computing. Generative modeling based on density operator models
(\textbf{DOMs}) is an emerging field, but existing training algorithms -- such
as those for the Quantum Boltzmann Machine -- do not scale to real-world data,
such as the MNIST dataset. The Expectation-Maximization algorithm has played a
fundamental role in enabling scalable training of probabilistic latent variable
models on real-world datasets. \textit{In this paper, we develop an
Expectation-Maximization framework to learn latent variable models defined
through \textbf{DOMs} on classical hardware, with resources comparable to those
used for probabilistic models, while scaling to real-world data.} However,
designing such an algorithm is nontrivial due to the absence of a well-defined
quantum analogue to conditional probability, which complicates the Expectation
step. To overcome this, we reformulate the Expectation step as a quantum
information projection (QIP) problem and show that the Petz Recovery Map
provides a solution under sufficient conditions. Using this formulation, we
introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an
iterative Minorant-Maximization procedure that optimizes a quantum evidence
lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing
log-likelihood across iterations for a broad class of models. Finally, we
present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a
\textbf{DOM} that can be trained with the same resources as a DBM. When trained
with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms
larger classical DBMs in image generation on the MNIST dataset, achieving a
40--60\% reduction in the Fr\'echet Inception Distance.

</details>


### [168] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: The paper proposes a novel electricity price prediction model by combining Bayesian regime detection and conditional neural processes and evaluates its use in various operational frameworks, finding it balanced and preferable over other models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the accuracy and operational utility of 24-hour electricity price predictions in the German market by addressing prediction and operational outcome trade-offs.

Method: They employ a disentangled sticky hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) for regime detection and use independent conditional neural processes (CNP) for modeling hourly price trajectories, combining these into regime-weighted predictions. The approach is benchmarked against deep neural networks (DNN) and Lasso estimated auto-regressive (LEAR) models in battery storage optimization tasks, supplemented by a multi-criteria evaluation using TOPSIS.

Result: The R-NP model showed a balanced trade-off in operational outcomes, outperforming both DNN and LEAR in comprehensive evaluations for 2021, 2022, and 2023, though LEAR excelled in certain specific tasks.

Conclusion: The proposed R-NP model proves to be the most balanced and versatile solution for electricity price prediction, bridging the gap between prediction accuracy and operational utility in multiple scenarios.

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [169] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT proposes a resource-efficient federated fine-tuning approach for Large Language Models by leveraging cognitive development-inspired progressive training stages, enhancing model efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Developing efficient federated fine-tuning methods for LLMs, to address resource constraints and privacy concerns while improving adaptability for edge devices.

Method: Introduced a developmental strategy for fine-tuning LLMs, dividing the process into stages. Utilized techniques like deconfliction-guided groupings and differential-based layer fusion for efficient submodel construction.

Result: DevFT achieved faster convergence (up to 4.59x), reduced communication overhead (10.67x), and improved performance (9.07% average) compared to state-of-the-art methods.

Conclusion: DevFT offers a cognitively inspired, resource-efficient federated fine-tuning approach for LLMs, improving model training efficiency, communication economy, and task adaptability.

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [170] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: This study compared two spatial constraints in topographic convolutional neural networks, finding that Weight Similarity (WS) offers advantages like better representation robustness, higher input sensitivity, and stronger spatial organization compared to Activation Similarity (AS) and standard CNNs.


<details>
  <summary>Details</summary>
Motivation: Neural networks can benefit from spatial and functional constraints to mirror brain-like organization, yet systematic comparisons of different topographic implementations are lacking.

Method: The paper analyzed topographic CNN models trained with two spatial constraints, Weight Similarity (WS) and Activation Similarity (AS), comparing them on classification accuracy, noise robustness, input sensitivity, and spatial organization.

Result: WS provided increased noise robustness, higher accuracy under weight corruption, greater activation variance, and stronger functional localization compared to AS and standard CNNs.

Conclusion: WS constraints in topographic neural networks yield more robust and organized feature representations, highlighting their potential to enhance biophysically inspired models.

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [171] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: This paper introduces the Bias-Corrected Exponential Moving Average (BEMA), which aims to stabilize fine-tuning of language models by reducing variance without introducing optimization bias. It outperforms standard EMA and vanilla training in convergence rates and final performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of instability in language model fine-tuning caused by stochasticity, often due to small batch sizes. While EMA smooths training, its bias introduces optimization lag that needs correction.

Method: The authors propose BEMA, an enhancement of EMA that eliminates the bias from old iterates while maintaining the variance-reduction benefits. This is supported by a theoretical model showing its acceleration advantage.

Result: Experiments demonstrate that BEMA improves convergence rates and final performance across various standard benchmarks when compared with both standard EMA and vanilla training.

Conclusion: BEMA offers a theoretically sound and practical solution for stabilizing and improving the efficiency of language model fine-tuning, making it advantageous over existing EMA and vanilla approaches.

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [172] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: This paper introduces POBAX, a benchmark for evaluating reinforcement learning algorithms under various forms of partial observability, focusing on memory-improvable environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing benchmarks in representing diverse forms of partial observability encountered in real-world reinforcement learning scenarios.

Method: The authors propose best-practice guidelines for benchmarking and develop POBAX, a comprehensive library featuring diverse environments and tasks designed to test algorithms under partial observability.

Result: POBAX provides representative environments with challenging memory-improvable tasks, along with recommended hyperparameters and implementations to streamline experimentation.

Conclusion: POBAX serves as a robust benchmarking framework, aiding the development of reinforcement learning algorithms that effectively tackle partial observability.

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [173] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: The paper introduces a new actionable definition of interpretability addressing current gaps in interpretability research. It proposes a blueprint and provides an open-source library.


<details>
  <summary>Details</summary>
Motivation: Existing definitions of interpretability are not actionable, making current research ill-posed and failing to provide clear guidelines for interpretable model design.

Method: The authors propose a new definition of interpretability and outline a general blueprint for designing interpretable models, supplemented by an open-source library.

Result: Their definition reveals key properties and principles needed for interpretable model design. The library supports interpretable data structures and processes.

Conclusion: The paper provides both theoretical and practical contributions, enabling robust and well-defined interpretability in AI model design.

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [174] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: This paper investigates the impact of instruction tuning on confidence calibration in large language models and proposes a solution using label smoothing, addressing challenges such as reduced effectiveness with large vocabulary LLMs and memory inefficiencies.


<details>
  <summary>Details</summary>
Motivation: To examine the effects of instruction tuning on the calibration of confidence in large language models, which is crucial for ensuring reliable output.

Method: The study evaluates the role of label smoothing as a regularization method during supervised fine-tuning of LLMs, conducting theoretical and experimental analysis to identify its limitations, especially in large vocabulary settings. Additionally, a custom kernel is designed to overcome memory constraints in the loss computation.

Result: The authors found significant calibration degradation in instruction-tuned LLMs and demonstrated the potential and limitations of label smoothing. The customized kernel successfully reduces memory consumption without compromising performance.

Conclusion: Label smoothing can improve calibration in fine-tuned LLMs, but careful consideration is required for large vocabulary models. The proposed memory-efficient solution addresses computational challenges effectively.

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [175] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: The paper proposes TriP-LLM, a novel unsupervised time-series anomaly detection framework using large language models (LLMs) to efficiently encode temporal features and perform anomaly detection with strong performance and lower memory consumption.


<details>
  <summary>Details</summary>
Motivation: The growing scale and complexity of time-series data due to IoT and smart manufacturing have exposed the limitations of traditional statistical methods in handling such data effectively.

Method: TriP-LLM employs a tri-branch design (Patching, Selection, Global) to encode time-series data into patch-wise tokens for processing by a pretrained large language model. A lightweight patch-wise decoder reconstructs the input data, generating anomaly scores. It utilizes a unified open-source framework and evaluates performance using the PATE metric.

Result: TriP-LLM outperforms recent state-of-the-art anomaly detection methods on multiple public benchmark datasets, as confirmed through extensive experiments and ablation studies.

Conclusion: TriP-LLM demonstrates superior anomaly detection capabilities with reduced memory consumption, making it suitable for GPU-constrained environments. The use of LLMs significantly enhances its performance. Code and models are available for transparency and further research.

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [176] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: The paper introduces Adacc, a memory management framework that uses adaptive compression and checkpointing to reduce GPU memory usage, improving training speed by up to 37% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Training large language models with recomputation to manage memory brings significant overhead, motivating a need for efficient memory management solutions.

Method: Adacc integrates layer-specific compression, optimal memory scheduling using MILP, and an adaptive policy mechanism to adjust memory optimization dynamically during training.

Result: Experiments demonstrate Adacc accelerates training by 1.01x to 1.37x compared to existing frameworks while preserving model performance.

Conclusion: Adacc is an effective framework for improving the efficiency of large language model training, balancing reduced memory usage with training speed and model accuracy.

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


### [177] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: This paper introduces a machine learning-based method for seismic design focusing on optimizing frame member properties to minimize costs.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and precision of performance-based seismic design by overcoming computational challenges in existing methods.

Method: An explainable machine learning model is used to directly map design variables to performance metrics, which is integrated into a genetic optimization algorithm to solve the inverse problem.

Result: The model achieved high accuracy (R2>90%) in diverse test cases and effectively optimized properties of frame members while aligning with engineering principles.

Conclusion: The proposed methodology provides an accurate and computationally efficient approach for designing earthquake-resilient structures that minimize repair costs.

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [178] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: The paper integrates a LightGBM model and genetic algorithm (GA) to assess the role of COVID-19 data in Bitcoin return predictions, showing significant predictive improvements, especially during market extremes.


<details>
  <summary>Details</summary>
Motivation: To investigate if COVID-19 pandemic-related health data enhances Bitcoin return prediction accuracy.

Method: A framework combining LightGBM regression and genetic algorithm optimization was applied to models with and without COVID-19 indicators, using a dataset of Bitcoin returns and COVID-19 metrics.

Result: COVID-19 indicators significantly improved predictive performance, with R2 values increasing by 40% and key vaccination metrics emerging as dominant predictors.

Conclusion: Pandemic-related health data, particularly vaccination metrics, enrich financial analytics, aiding investors and policymakers during systemic crises.

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [179] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: The paper presents "Stress-Aware Learning," a neural training framework inspired by material science, using adaptive noise to escape sharp minima and enhance model generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address the challenge of optimization stagnation in deep neural networks by introducing a concept rooted in structural fatigue, improving training robustness and generalization.

Method: The method integrates a "Plastic Deformation Optimizer" that injects adaptive noise into model parameters based on a stress signal indicating training difficulty.

Result: Experiments across six architectures, four optimizers, and seven vision benchmarks showed enhanced robustness and generalization with minimal computational cost.

Conclusion: The approach effectively enhances model training dynamics, promoting convergence to flatter and more generalizable loss landscape regions, with minimal resource overhead.

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [180] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: The paper introduces StackLiverNet, an interpretable stacked ensemble model for liver disease detection, achieving near-perfect performance with advanced preprocessing and interpretability features.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from improving upon limitations in existing liver disease classification models, such as high error rates, poor interpretability, and computational inefficiencies.

Method: The study develops StackLiverNet, which uses advanced preprocessing, random undersampling, hyperparameter-optimized base classifiers, and a LightGBM meta-model, complemented by LIME, SHAP, and sensitivity analysis for interpretability.

Result: StackLiverNet achieved a testing accuracy of 99.89%, Cohen Kappa of 0.9974, and AUC of 0.9993, with minimal misclassifications and fast training/inference speeds.

Conclusion: The proposed StackLiverNet is highly effective and interpretable, making it suitable for clinical applications in liver disease diagnosis while addressing key limitations in previous models.

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [181] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: This paper introduces a reformulated approach to layer transformations in neural networks, emphasizing stability, interpretability, and robust training dynamics.


<details>
  <summary>Details</summary>
Motivation: Current neural networks often lack stability and interpretability in their structure and training processes.

Method: The authors decompose layer transformations into a structured linear operator and a residual corrective component to improve signal propagation and training dynamics while maintaining compatibility with typical objectives.

Result: Through experiments, the authors demonstrate enhanced gradient conditioning, reduced sensitivity to changes, and increased robustness across layers.

Conclusion: The findings suggest that this structured approach contributes to more stable and transparent neural architectures, while maintaining expressive capacity.

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [182] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: This study enhances ECG analysis for deep learning by developing novel Variational Autoencoders (VAEs) to reduce data complexity and improve predictive capabilities, achieving competitive results with lower computational demands.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using high-dimensional, variable ECG data in deep learning models, especially with limited training datasets.

Method: The study developed three VAE variants (SAE, A beta-VAE, and C beta-VAE) for feature generation and compression, and evaluated them using a Light Gradient Boost Machine (LGBM) on prediction tasks.

Result: The A beta-VAE yielded superior signal fidelity with a low MAE of 15.7 µV, and the SAE combined with traditional features achieved AUROC 0.901 for LVEF prediction, close to state-of-the-art models but with reduced computational requirements.

Conclusion: The VAE-based feature extraction pipeline simplifies ECG data, enhances predictive tasks, avoids overfitting, and is effective even with small datasets, presenting a practical solution for ECG analysis in constrained settings.

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [183] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN, combines Reinforcement Learning with Graph Neural Networks to strategically place bicycle sensors and improve volume estimation in sparse data environments.


<details>
  <summary>Details</summary>
Motivation: The need for accurate link-level bicycling volume estimation in urban transport planning is hindered by sparse sensor data coverage.

Method: INSPIRE-GNN integrates Graph Convolutional Networks, Graph Attention Networks, and a Deep Q-Network RL agent for optimized sensor placement and volume estimation.

Result: Tested on Melbourne's bicycle network, INSPIRE-GNN significantly improved estimation accuracy and outperformed traditional sensor placement strategies and competing models.

Conclusion: The framework equips transport planners with enhanced tools for sensor network expansion, optimization, and data-informed planning decisions.

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [184] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: The paper presents a novel interpretability method for fine-tuned LLMs by analyzing weight differences instead of activations, allowing the detection of new behaviors and defenses against threats like backdoors without needing similar training data. 


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods for LLMs largely rely on distributionally similar data to study model behavior, limiting their ability to detect novel threats such as backdoors, which are out-of-distribution behaviors.

Method: The method proposes analyzing the top singular vectors of the weight difference between the fine-tuned and base models. By monitoring cosine similarities in activations along these directions, the method identifies behaviors introduced during fine-tuning.

Result: The approach successfully detected backdoor attacks with up to 100% success and a false positive rate below 1.2%, identified unlearned behaviors with 95.42% accuracy, and inferred fine-tuning focuses of commercial models during audits.

Conclusion: The method provides a reliable way to monitor and audit fine-tuned LLMs without requiring data similar to the original training data. Its applications include defense against malicious backdoors, understanding erasure outcomes, and pre-deployment model inspections.

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [185] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: This paper introduces a diffusion-based semantic communication framework (DiSC-Med) for transmitting medical images efficiently and robustly over noisy channels.


<details>
  <summary>Details</summary>
Motivation: The need for timely and efficient transmission of medical data through noisy, bandwidth-limited channels in remote healthcare applications.

Method: The DiSC-Med framework employs medical-enhanced compression for bandwidth efficiency and denoising blocks for robustness. It captures key semantic information rather than using pixel-based communication.

Result: DiSC-Med achieved superior reconstruction performance and ultra-high bandwidth efficiency in experiments on real-world medical datasets, demonstrating robustness even in noisy conditions.

Conclusion: The proposed framework shows potential for robust and efficient applications in telehealth by addressing data transmission challenges effectively.

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [186] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: This paper explores framing regression as a Reinforcement Learning (RL) problem to allow more flexible learning objectives.


<details>
  <summary>Details</summary>
Motivation: Standard regression techniques are limited by predefined loss functions that may not align with complex or non-differentiable objectives.

Method: The authors model regression as an RL problem by treating predictions as actions and designing custom reward signals. They develop an RL agent using techniques like Actor-Critic, Prioritized Experience Replay, and positional encoding.

Result: The RL framework successfully solves the regression task on a noisy sine wave example and provides greater flexibility in defining objectives.

Conclusion: Using RL for regression shows promise, offering a more adaptable approach to modeling objectives and guiding system learning.

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [187] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind is a simulator-based reinforcement learning framework for improving session-based goals at web-scale recommendation systems, demonstrating significant improvements in user engagement metrics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of optimizing long-term user satisfaction in web-scale recommendation systems, which is difficult to achieve with traditional supervised learning methods.

Method: RecoMind creates a simulation environment using existing recommendation models and bootstraps reinforcement learning policies to optimize immediate and session-based user interactions.

Result: In both offline simulations and online A/B tests on a video streaming platform, RecoMind significantly enhanced user engagement, with metrics such as videos watched for more than 10 seconds increasing by 15.81% and session depth improving by 4.71%.

Conclusion: RecoMind efficiently incorporates RL into web-scale recommendation systems, demonstrating its potential to improve session-based user satisfaction in a scalable and systematic manner.

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [188] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: The paper presents a two-stage framework for robust classification with noisy labels, leveraging geometry information and the non-negative kernel (NNK) neighborhood construction.


<details>
  <summary>Details</summary>
Motivation: To enhance classification robustness when fine-tuning foundation models on noisy labeled data, addressing limitations in existing k-NN-based methods.

Method: The two-stage framework includes reliability estimation followed by reliability-weighted inference, using NNK neighborhood construction and novel reliability estimation techniques.

Result: Evaluation on CIFAR-10 and DermaMNIST dataset demonstrates improved robustness to label noise compared to standard k-NN and recent approaches.

Conclusion: The proposed methodology effectively ensures better classification performance in noisy label conditions using geometry and advanced reliability estimation methods.

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [189] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: The paper introduces KRAdapter, a new parameter-efficient fine-tuning (PEFT) method that improves upon LoRA's limitations in approximating matrices with high effective ranks, achieving better performance without additional memory or computational cost.


<details>
  <summary>Details</summary>
Motivation: LoRA, a popular PEFT method, faces limitations when dealing with matrices with flat spectrums or high effective ranks, particularly in multimodal and large language models, highlighting the need for a more robust method.

Method: The authors introduce KRAdapter, a novel algorithm leveraging the Khatri-Rao product to generate weight updates that inherently produce matrices with high effective ranks, addressing LoRA's shortcomings.

Result: KRAdapter achieves superior performance on vision-language models (up to 1B parameters) and large language models (up to 8B parameters), especially in unseen common-sense reasoning tasks, while maintaining the efficiency of LoRA.

Conclusion: KRAdapter serves as a more effective and efficient alternative to LoRA for fine-tuning large-scale models, offering advantages in handling high effective ranks without sacrificing computational or memory efficiency.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [190] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: This paper introduces an online tutoring system that improves student learning by optimizing feedback using data-driven strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a system that provides adaptive and effective feedback to students, enhancing their learning outcomes during practice sessions.

Method: The paper uses a multi-armed bandit (MAB) framework combined with offline policy evaluation to optimize assistance actions for questions and evaluates personalization through contextual bandit (CB) policies.

Result: MAB policies significantly improve student outcomes across 166,000 sessions. While CB policies show potential for personalization based on individual differences, their added benefits over MAB are often minimal due to small effect sizes.

Conclusion: Optimized MAB policies effectively enhance overall student learning for the population, with limited advantages observed from CB policies in personalizing feedback. The system demonstrates scalability and practical impact by supporting thousands of students daily.

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [191] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: The paper introduces GOODFormer, a graph transformer that generalizes well under distribution shifts, using invariant learning principles.


<details>
  <summary>Details</summary>
Motivation: Existing graph transformers struggle to generalize under distribution shifts, necessitating a method that captures invariant graph patterns across distributions.

Method: GOODFormer employs an entropy-guided subgraph disentangler, a dynamic positional and structural encoder, and an invariant learning module to enable generalized graph representations.

Result: The proposed method demonstrates superior performance compared to state-of-the-art baselines on benchmark datasets under distribution shifts.

Conclusion: GOODFormer effectively addresses generalization under distribution shifts by leveraging invariant learning and demonstrates significant empirical effectiveness.

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [192] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA is a novel data assimilation approach combining gradient-based updates with generative priors for improved forecasting in complex, chaotic systems.


<details>
  <summary>Details</summary>
Motivation: To address error accumulation and limitations of conventional data assimilation methods, particularly regarding their assumption of Gaussian error statistics in chaotic systems.

Method: PnP-DA alternates between gradient-based analysis updates using Mahalanobis-distance misfits and incorporating a generative prior conditioned on background forecasts via conditional Wasserstein coupling.

Result: Experiments show that PnP-DA reduces forecast errors effectively across varying sparsities and noise levels, outperforming classical variational data assimilation techniques.

Conclusion: PnP-DA is a promising approach for improving earth system modeling through flexible statistical assumptions and leveraging historical data, offering superior performance in chaotic systems.

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [193] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: This paper introduces a method to visualize and understand the development of language models using susceptibility matrices and UMAP.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance understanding of how language models develop their computational structures during training by using tools from statistical physics.

Method: The authors apply UMAP (Uniform Manifold Approximation and Projection) to susceptibility matrices to visualize structural evolution in language models throughout the training process.

Result: Visualizations revealed clear developmental patterns, including known features like the induction circuit and novel structures like the 'spacing fin' for counting space tokens.

Conclusion: Susceptibility analysis combined with UMAP provides a holistic way to visualize and uncover novel mechanisms in language models, advancing methodologies for studying neural network development.

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [194] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD leverages diffusion models to synthesize latent space features near class boundaries, generating high-quality OOD data for better detection.


<details>
  <summary>Details</summary>
Motivation: Previous methods for OOD detection struggle to extract latent features outside the in-distribution boundary due to difficulty in identifying decision boundaries.

Method: BOOD selects latent features near decision boundaries, perturbs them to create OOD features, and uses diffusion models to decode these features back into pixel-space images.

Result: BOOD achieves notable improvements in OOD detection performance metrics: 29.64% decrease in average FPR95 and 7.27% improvement in average AUROC on CIFAR-100.

Conclusion: This framework provides a more efficient and precise mechanism for distinguishing OOD data, demonstrating superior performance over state-of-the-art methods.

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [195] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: The paper addresses over-smoothing in GNNs using a novel framework SGPC which incorporates cellular-sheaf message passing and PAC-Bayes regularization, achieving improved performance and guarantees.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to resolve the over-smoothing problem in GNNs, especially for heterophilic graphs, where node features overly homogenize and lose distinctiveness due to incompatible adjacency structures.

Method: This study proposes SGPC, combining cellular-sheaf message passing, optimal transport-based lifting, variance-reduced diffusion, and PAC-Bayes spectral regularization. It ensures robust semi-supervised node classification and computational efficiency.

Result: The proposed SGPC achieves superior performance on nine homophilic and heterophilic benchmarks, outperforming state-of-the-art GNNs. It also provides certified confidence intervals for unseen nodes.

Conclusion: SGPC effectively mitigates over-smoothing, maintains model scalability, and ensures theoretical performance bounds, offering both enhanced accuracy and stability for graph-based learning tasks.

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [196] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: The paper introduces OID-PPO, a reinforcement learning framework for interior design that addresses issues in existing methods by integrating design guidelines and offering computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in residential interior design such as unstructured layouts, computational costs, and dependency on expert knowledge, which are inadequately addressed by current methods.

Method: A novel RL framework, OID-PPO, is proposed, which incorporates expert-defined guidelines into its reward function. It uses a diagonal Gaussian policy for flexible furniture placement and adapts to partial environmental observability.

Result: OID-PPO outperforms state-of-the-art methods in layout quality and computational efficiency. Ablation studies highlight the importance of guideline integration and assess individual design constraints' contributions.

Conclusion: The framework offers a structured and computationally efficient solution for interior design challenges, validating the integration of functional and visual guidelines within reinforcement learning approaches.

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [197] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: The paper proposes universal algorithms for adaptive regret minimization under online convex optimization by introducing a dual adaptivity approach that accounts for function types and environmental changes.


<details>
  <summary>Details</summary>
Motivation: Current adaptive regret minimization algorithms lack universality, requiring prior knowledge of function types and parameters, limiting their practical application.

Method: The authors devised a meta-expert framework that dynamically employs multiple experts and aggregates their performance using a meta-algorithm capable of second-order bounds. This framework addresses varying function types and environmental changes.

Result: The proposed algorithms minimize adaptive regret across various convex function classes while allowing transitions between function types during rounds.

Conclusion: The paper introduces a practical and theoretical enhancement in online learning through universal algorithms, expanding application scenarios and improving adaptive optimization performance.

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [198] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib simplifies ML pipeline creation for users with minimal expertise by leveraging knowledge graphs and a user-friendly graphical interface.


<details>
  <summary>Details</summary>
Motivation: Domain experts lack the ML knowledge and training needed to build high-quality ML pipelines, despite their pressing need for analytics.

Method: ExeKGLib provides a Python library with a graphical interface, using knowledge graphs to simplify the development and understanding of ML pipelines.

Result: ExeKGLib enhances usability, transparency, and reusability of ML pipelines, demonstrated through real-world examples.

Conclusion: ExeKGLib bridges the gap between ML practitioners and domain experts, enabling non-ML experts to construct executable ML workflows.

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [199] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: The paper introduces Co-Reward, a self-supervised reinforcement learning framework for large language models (LLMs) that uses contrastive agreement across semantically analogical questions to generate rewards, addressing issues like the scaling-up dilemma and collapse in reasoning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the reasoning capabilities of LLMs without relying on human-annotated labels, which are hard to scale for complex tasks. They address the limitations of existing self-reward approaches that suffer from collapse issues.

Method: The method involves creating similar but unlabeled questions for each training sample, synthesizing surrogate labels through rollout voting, and constructing rewards using a cross-referencing mechanism to enforce reasoning consistency across question pairs. This approach is inspired by self-supervised learning.

Result: Co-Reward demonstrates superior performance on multiple reasoning benchmarks and surpasses ground-truth labeled rewards, achieving up to a +6.8% improvement on the MATH500 dataset for the Llama-3.2-3B-Instruct model.

Conclusion: The proposed framework not only overcomes the drawbacks of self-reward systems but also enhances reasoning in LLMs through a novel self-supervised reward-shaping technique. Its success suggests potential scalability and reduced dependency on human-annotated data for reasoning tasks.

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [200] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: This paper introduces a new machine learning model, ResE-BiLSTM, for predicting post-loan default using financial anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Enhancing credit risk management through improved post-loan default predictions using advanced machine learning techniques.

Method: The study employs a ResE-BiLSTM model with a sliding window technique, evaluates it on the Freddie Mac mortgage data, and compares it to five baseline models across various metrics while using an ablation study and SHAP analysis for interpretation.

Result: ResE-BiLSTM performs better than baseline models in terms of Accuracy, Precision, Recall, F1, and AUC metrics.

Conclusion: ResE-BiLSTM is a highly effective model for predicting post-loan defaults, offering superior performance and practical utility in credit risk scenarios.

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [201] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: ctdGAN is introduced to address class imbalance in tabular data using conditional GANs with cluster-based sampling and loss functions, which improve data fidelity and classification performance.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in tabular data leads to poor machine learning performance, and current GAN-based solutions fail to consider data subspaces and conditional sampling efficiency.

Method: The proposed ctdGAN method uses space partitioning to assign cluster labels, generating samples in meaningful subspaces with a novel probabilistic sampling strategy and a loss function that penalizes both cluster and class mispredictions.

Result: In tests on 14 imbalanced datasets, ctdGAN generated high-fidelity samples that improved classification accuracy.

Conclusion: ctdGAN effectively tackles class imbalances in tabular data, improving the quality of generated samples and enhancing downstream machine learning tasks.

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [202] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: The paper introduces CoLL, a framework combining LLMs and GNNs for graph anomaly detection in text-attributed graphs (TAGs).


<details>
  <summary>Details</summary>
Motivation: Existing methods for graph anomaly detection focus on graph domain optimization but neglect textual modality's potential, especially leveraging semantic understanding and reasoning capabilities of large language models (LLMs).

Method: CoLL integrates multi-LLM collaboration for evidence-augmented anomaly detection and combines it with a GNN equipped with a gating mechanism to fuse high-order structural and textual features.

Result: CoLL achieves an average improvement of 13.37% in AP over existing methods, demonstrating superiority in detecting anomalies.

Conclusion: CoLL effectively combines LLMs and GNNs for enhanced graph anomaly detection, leveraging complementary strengths and opening new avenues for using LLMs in advancing graph anomaly detection methods.

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [203] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: The paper introduces CMUCL, a new method for anomaly detection in text-attributed graphs that integrates text and graph structures to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Graph data is increasingly used in high-risk domains, requiring effective anomaly detection methods. Current techniques often fail to integrate textual information effectively with graph topology in such scenarios.

Method: The proposed CMUCL model jointly trains text and graph encoders using cross-modal and uni-modal multi-scale consistency, aiming to better uncover anomaly-specific information. An anomaly score estimator identifies node-specific anomalies.

Result: CMUCL improves text-attributed graph anomaly detection by achieving an 11.13% higher average accuracy (AP) than previous methods. Additionally, 8 benchmark datasets for TAG anomaly detection were released.

Conclusion: CMUCL represents a significant step forward in leveraging text and graph topology for anomaly detection, addressing existing limitations and enabling better performance in this domain.

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [204] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: The paper develops new algorithms for online nonsubmodular optimization in the delayed feedback bandit setting, achieving improved regret bounds that address issues with irregular delays.


<details>
  <summary>Details</summary>
Motivation: To improve regret bounds in online nonsubmodular optimization with delayed feedback under bandit settings, overcoming limitations associated with dependency on maximum delay and coupling between delays and bandit feedback effects.

Method: The paper introduces two algorithms: DBGD-NF, which uses all available gradient estimations per round to depend on average delays, and an extension employing a blocking update mechanism to decouple delay-bandit dependencies.

Result: DBGD-NF achieves a regret bound of $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$, while the extended algorithm achieves $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$, offering more robust performance under certain delay conditions.

Conclusion: The methods provide improved regret bounds over prior work, especially under irregular or smaller delays, and experimental results validate their practical effectiveness in structured sparse learning scenarios.

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [205] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: The paper introduces a two-stage framework for improving mineral detection using hyperspectral imaging in the Cuprite mining district.


<details>
  <summary>Details</summary>
Motivation: Mineral detection in hyperspectral imaging faces challenges due to noise and redundant bands masking weak mineral signals.

Method: Stage one enhances SNR using band thresholding and Savitzky-Golay filtering; Stage two uses KMeans clustering and NNLS for abundance unmixing.

Result: The approach shows improved unmixing accuracy and enhanced detection of weak mineral zones using metrics like cosine similarity and RMSE.

Conclusion: This framework offers a reproducible way to reduce dimensionality and improve geological hyperspectral imaging applications.

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [206] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: This study focuses on enhancing hydrogen atom transfer (HAT) reaction simulations using machine-learned potentials compared to conventional methods. Among three graph neural network architectures, MACE demonstrates the most accuracy in predicting potential energy surfaces and reaction barriers.


<details>
  <summary>Details</summary>
Motivation: HAT reactions are crucial in biological processes, but their simulation is limited by the need for quantum-level accuracy at biologically relevant scales, where classical methods fail. The study aims to use machine-learned potentials to overcome these limitations.

Method: The paper systematically generates HAT configurations in peptides using semiempirical methods and DFT to create large datasets. Three graph neural network architectures (SchNet, Allegro, and MACE) are benchmarked for their ability to learn HAT potential energy surfaces and predict reaction barriers, focusing on their accuracy and transferability.

Result: MACE consistently outperforms SchNet and Allegro across energy, force, and barrier prediction tasks, achieving a mean absolute error of 1.13 kcal/mol in out-of-distribution DFT barrier predictions. This level of accuracy allows for the integration of machine-learned potentials into larger simulations.

Conclusion: The findings highlight the potential of machine-learned potentials, especially MACE, for quantum-accurate simulations of HAT processes in peptides. The work broadens the applicability of these techniques to other biomolecular systems, improving our understanding of chemical reactivity in complex environments.

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [207] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: Active Learning (AL) is computationally expensive and yields modest improvements (1-4%) compared to other methods such as data augmentation (DA) and semi-supervised learning (SSL), which achieve up to 60% improvement.


<details>
  <summary>Details</summary>
Motivation: Investigate why Active Learning is less utilized outside its literature and explore better solutions for low-data scenarios.

Method: Study the performance of different methods, including AL, DA, and SSL, in addressing low-data problems and analyze their combinations.

Result: DA and SSL outperform AL significantly in low-data scenarios. AL, when combined with strong DA and SSL methods, still provides marginal improvements.

Conclusion: AL should be seen as a complementary tool to optimize performance further after applying DA and SSL methods.

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [208] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: The paper introduces a graph-based model, SBSCGM, and hybrid Graph Neural Network (HybridGraphMedGNN) for predicting ICU patient criticalness and mortality risk with state-of-the-art accuracy using multi-modal EHR data.


<details>
  <summary>Details</summary>
Motivation: To improve prediction of ICU patient criticalness and mortality risk using the relational structure in Electronic Health Records (EHR) instead of analyzing patients in isolation.

Method: The authors propose SBSCGM, a model that dynamically creates a patient similarity graph using a hybrid similarity measure, and HybridGraphMedGNN, a hybrid Graph Neural Network that combines GCN, GraphSAGE, and GAT layers to analyze these graphs.

Result: SBSCGM and HybridGraphMedGNN achieved state-of-the-art performance with an AUC-ROC of 0.94 on the MIMIC-III dataset, along with enhanced precision/recall and interpretable attention mechanisms.

Conclusion: The framework provides a scalable, interpretable, and effective solution for ICU critical care risk prediction, with strong potential for real-world clinical application.

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [209] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP, a QGIS plugin, simplifies remote sensing with AI by enabling feature extraction, clustering, and prediction without requiring expertise, GPUs, or extensive datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges that limit the broad application of deep learning in remote sensing, such as large dataset needs, high computing requirements, and specialized coding skills.

Method: The authors developed IAMAP, a QGIS plugin that uses self-supervised learning strategies and foundation models to streamline image analysis tasks for non-specialists in remote sensing.

Result: IAMAP successfully provides tools for extracting features, dimensionality reduction, clustering, similarity mapping, and supervised model calibration, enabling access to robust deep learning functionalities.

Conclusion: IAMAP facilitates energy-efficient and accessible deep learning for non-specialists, democratizing its use in remote sensing image analysis.

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [210] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: The paper proposes Separated-Variable Spectral Neural Networks (SV-SNN) to address spectral bias in solving high-frequency oscillatory PDEs, achieving significant improvements in accuracy, efficiency, and parameter reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-informed neural networks (PINNs) struggle to effectively model high-frequency components in partial differential equations (PDEs) due to spectral bias, which poses challenges in fields like fluid mechanics, quantum mechanics, and wave propagation.

Method: The SV-SNN approach integrates separation of variables, adaptive Fourier spectral features with learnable frequency parameters, and a theoretical framework based on singular value decomposition to tackle the spectral bias issue. It decomposes multivariate functions into univariate products and uses adaptive spectral methods for high-frequency accuracy.

Result: The proposed SV-SNN framework achieves 1-3 orders of magnitude improvement in accuracy, a reduction of over 90% in parameter count, and a 60% decrease in training time, outperforming benchmarks on multiple PDE problems.

Conclusion: SV-SNN effectively addresses spectral bias in neural PDE solving, offering a more efficient and accurate alternative to traditional PINNs. The implementation will be made publicly available.

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [211] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: The paper introduces KFS, an architecture inspired by Kolmogorov-Arnold Networks and Parseval's theorem, to improve time series forecasting by addressing cross-scale noise and heterogeneous information distribution.


<details>
  <summary>Details</summary>
Motivation: Real-world time series forecasting faces challenges due to cross-scale noise interference and varied information distribution among frequency components.

Method: The KFS framework uses a FreK module for energy-based frequency selection, KAN for complex pattern modeling, and timestamp embedding alignment to synchronize temporal features.

Result: Extensive experiments demonstrate that KFS achieves state-of-the-art performance on multiple datasets.

Conclusion: KFS is a simple and effective architecture for time series forecasting, tackling challenges in noise and pattern complexity.

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [212] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: The paper addresses the challenge of kamikaze drone swarms attacks using reinforcement learning to optimize interception decisions, showing superior results compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The growing threat of low-cost kamikaze drone swarms necessitates efficient decision-making to protect high-value targets and infrastructure.

Method: A high-fidelity simulation environment was created where a reinforcement learning agent coordinates multiple effectors to intercept drones based on state features, evaluated against a rule-based baseline.

Result: The reinforcement learning policy achieved lower average damage and greater efficiency in defending critical zones across hundreds of simulated attack scenarios compared to the handcrafted baseline.

Conclusion: Reinforcement learning offers promising advantages as a strategic layer in defense systems, enhancing resilience alongside existing control mechanisms.

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [213] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: The paper introduces DINOZAUR, a diffusion-based neural operator approach with improved scalability and uncertainty quantification for solving PDEs.


<details>
  <summary>Details</summary>
Motivation: To address the scalability and lack of intrinsic uncertainty quantification in Fourier Neural Operators (FNOs), which are widely used but over-parameterized and unreliable for scientific applications requiring UQ.

Method: The authors replaced the dense tensor multiplier in FNOs with a diffusion multiplier structured like the heat kernel, featuring a single learnable time parameter per channel, and introduced Bayesian priors over these parameters.

Result: DINOZAUR significantly reduced parameter count and memory footprint while maintaining or outperforming existing methods across PDE benchmarks, and provided efficient spatially-correlated uncertainty estimates.

Conclusion: DINOZAUR offers a scalable, memory-efficient neural operator solution with calibrated uncertainty quantification, making it reliable for scientific and engineering applications.

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [214] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv is a novel model leveraging neural controlled differential equations and contrastive learning to analyze survival predictions using irregularly sampled EHR data. It prioritizes both accuracy and transparency by modeling clinical progression and linking it to survival outcomes.


<details>
  <summary>Details</summary>
Motivation: There is a need for trustworthy survival prediction in clinical decision-making, particularly using longitudinal EHR data that captures patient progression but poses challenges due to irregular sampling and complexity.

Method: TrajSurv integrates neural controlled differential equations for continuous latent trajectory modeling, time-aware contrastive learning to align latent states with patient states, and a two-step interpretation process to explain clinical progression's influence on survival outcomes.

Result: TrajSurv demonstrated competitive prediction accuracy and superior transparency compared to existing methods, validated on two datasets, MIMIC-III and eICU.

Conclusion: TrajSurv addresses survival prediction challenges by accurately modeling clinical progression and transparently linking it to outcomes, establishing a promising method for reliable and interpretable predictions in healthcare.

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [215] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper introduces a model, DP-DGAD, for identifying anomalies in dynamic graphs by leveraging evolving representations of normal and anomalous patterns stored in a memory buffer.


<details>
  <summary>Details</summary>
Motivation: Existing generalist GAD models struggle to handle evolving anomalies in dynamic graphs and face challenges in addressing new domains and limited labeled data.

Method: The proposed DP-DGAD model extracts dynamic prototypes from temporal ego-graphs, utilizes a memory buffer for storing and updating patterns, and uses confidence-based pseudo-labeling for domain adaptation.

Result: DP-DGAD achieves state-of-the-art performance in dynamic graph anomaly detection across ten real-world datasets from different domains.

Conclusion: The proposed method effectively detects domain-specific and domain-agnostic anomalies in evolving graphs and adapts across diverse real-world domains using dynamic prototypes.

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [216] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: This paper combines generalized dynamic factor models (GDFM) and generative adversarial networks (GANs) for generating plausible long-term wind power scenarios while representing spatio-temporal correlations.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of synthesizing wind power scenarios that capture both spatial and temporal correlations among geographically distributed wind farms for resource adequacy studies.

Method: The authors integrate GANs and GDFMs, using GANs as a filter for extracting dynamic factors with temporal information and applying these in the GDFM to represent spatio-temporal correlations. This method combines the advantages of both approaches.

Result: Numerical tests demonstrated that the combined method outperforms alternatives in synthesizing wind power scenarios with realistic statistical characteristics. It accurately captures spatio-temporal correlations and waveforms from Australian wind farms.

Conclusion: The combined GDFM and GAN approach is effective in generating wind power scenarios that better reflect real-world characteristics, offering an improved tool for resource adequacy studies.

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [217] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: Various AI models were tested to classify mental health diagnostic categories, achieving notable accuracy, especially with hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To explore efficient AI models for classifying mental health conditions like Anxiety and Adjustment Disorder, enhancing diagnostic processes.

Method: Traditional ML methods like Random Forest and SVM were compared with deep learning models like SciBERT, using oversampling strategies and hyperparameter tuning.

Result: Decision Tree, eXtreme Gradient Boost, DistilBERT, and SciBERT all reached 96% accuracy. Oversampling, except SMOTE, had minimal impact, but hyperparameter tuning significantly improved results.

Conclusion: Hyperparameter tuning proved more impactful than oversampling, emphasized as crucial for model performance. Findings advance AI tools for mental health diagnostics.

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [218] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: This paper proposes an advanced Graph Neural Network method called MIND, which eliminates the need for handcrafted features and efficiently solves complex network science problems like Network Dismantling.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and biases introduced by handcrafted features traditionally used in Graph Neural Networks for network science problems.

Method: Developing the MIND framework that utilizes an attention mechanism, message-iteration profiles, and synthetic network generation.

Result: MIND outperformed state-of-the-art methods in solving the NP-hard Network Dismantling problem, even on real-world networks with millions of nodes, while being trained solely on synthetic networks.

Conclusion: The MIND framework increases efficiency and generalizability, making it applicable to a wide range of complex network problems beyond dismantling.

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [219] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: This paper proposes factored state-space methods for robust Markov decision processes (r-MDPs) to enhance sample efficiency and robust policy synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving r-MDPs require significant samples to robustly address epistemic uncertainties. The paper seeks to overcome inefficiencies by leveraging factored state-space representations to enhance sample efficiency and policy robustness.

Method: The paper reformulates the hard, non-convex optimization in solving factored r-MDPs into linear programs to make them tractable. Additionally, it proposes methods to directly learn factored representations of the model.

Result: The results show that exploiting the factored structure improves sample efficiency, enabling the synthesis of robust policies with tighter performance guarantees compared to state-of-the-art methods.

Conclusion: Factored representations in r-MDPs can effectively address epistemic uncertainties, delivering robust and sample-efficient policy solutions that outperform existing approaches in terms of performance guarantees.

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [220] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: This paper proposes the JSON Bag-of-Tokens (JSON-Bag) method to represent game trajectories using JSON tokenization and Jensen-Shannon distance for classification tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the need for a generalized representation of game trajectories that is both effective and efficient in classifying diverse gaming attributes.

Method: The method introduces JSON-Bag to tokenize JSON game descriptions, combines this with Jensen-Shannon distance for trajectory comparison, and evaluates performance using a prototype-based nearest-neighbor search and Random Forest for feature extraction.

Result: JSON-Bag outperformed hand-crafted baselines across the majority of tasks with better accuracy in N-shot classification and improved classification through tokenized feature extraction.

Conclusion: The JSON-Bag model is an effective, sample-efficient method for representing game trajectories and correlates well with agent policy differences.

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [221] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: The paper introduces NeGPR, a framework for graph-level domain adaptation, addressing challenges posed by noisy labels in the source data, enhancing adaptation across domains.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Domain Adaptation methods struggle with annotation noise, impairing their ability to align features and adapt effectively under domain shifts.

Method: The authors propose NeGPR, which pretrains two branches (semantic and topology) to ensure consistency in feature space, employs nested refinement for cross-domain learning, and incorporates noise-aware regularization to handle pseudo-label noise.

Result: NeGPR achieves notable performance improvements, outperforming state-of-the-art methods with up to 12.7% accuracy improvements, especially under severe label noise.

Conclusion: NeGPR offers a robust solution for graph domain adaptation under noisy label conditions, demonstrating its effectiveness in real-world applications.

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [222] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: The paper introduces the MOSTLY AI Synthetic Data SDK, an open-source tool for creating high-quality tabular synthetic data, addressing challenges like privacy and accessibility.


<details>
  <summary>Details</summary>
Motivation: To overcome barriers to data accessibility caused by increasing privacy, proprietary, and ethical restrictions.

Method: Development of an SDK using the TabularARGN framework, featuring differential privacy, fairness-aware data generation, and automated quality checks.

Result: The SDK successfully handles diverse, complex data types, offering speed and usability improvements, and has been widely adopted in real-world settings.

Conclusion: The SDK promotes data democratization by enabling secure, efficient synthetic data generation, addressing key challenges in data access.

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [223] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: The paper introduces a multi-fidelity stratified sampling scheme to efficiently estimate small failure probabilities in complex systems, achieving significant computational savings.


<details>
  <summary>Details</summary>
Motivation: Existing methods for rare event analysis require excessive computational resources, creating challenges for nonlinear finite element modeling under stochastic conditions.

Method: The proposed method uses stratified sampling to generate high-fidelity data, trains adaptive deep-learning metamodels for low-fidelity modeling, and integrates results using a multi-fidelity Monte Carlo framework.

Result: Application to a high-rise steel building under stochastic wind excitation demonstrated accurate estimation of exceedance probabilities with reduced computational costs.

Conclusion: The multi-fidelity scheme offers a practical solution for uncertainty propagation and rare event estimation in computationally intensive scenarios.

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [224] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: This paper proposes a single deterministic model for effective uncertainty quantification, using feature space density to detect distributional shifts and out-of-distribution samples.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of Bayesian neural networks and deep ensemble methods for uncertainty quantification, which require significant computational power and storage.

Method: A single deterministic model quantifies uncertainty by leveraging feature space density derived from kernel density estimation. The method compares training set density with test sample feature space representation to detect distributional shifts.

Result: Experiments on 2D synthetic datasets and OOD detection tasks (CIFAR-10 vs. SVHN) showed that the proposed method exceeds the performance of baseline models.

Conclusion: The proposed method effectively identifies distributional shifts and OOD samples, offering a computationally efficient alternative to Bayesian neural networks and deep ensemble methods.

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [225] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: The paper introduces the Diffusion-Scheduled Denoising Autoencoder (DDAE), which combines diffusion-based noise scheduling and contrastive learning to enhance anomaly detection in tabular data.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in tabular data is challenging due to intricate feature interactions and limited anomalous examples. Existing methods struggle to adapt to diverse data distributions and efficient reconstruction mapping is lacking.

Method: The DDAE framework integrates diffusion noise scheduling and contrastive learning into the encoding process. It systematically explores noise levels and scheduling strategies in both semi-supervised and unsupervised settings, evaluated across 57 datasets.

Result: DDAE outperforms existing models in semi-supervised settings and achieves competitive results in unsupervised scenarios. In performance metrics, DDAE boosts PR-AUC by up to 65% and ROC-AUC by 16% over top baseline models.

Conclusion: Optimizing noise levels and scheduling strategies significantly enhances anomaly detection in tabular data. The findings highlight DDAE's effectiveness and the importance of principled noise strategies.

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [226] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: This paper examines the impact of different encoding methods (Amplitude-encoding and Angle-encoding) and rotational gate choices on the classification performance of Variational Quantum Circuits (VQCs).


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of how different encoding mechanisms and rotational gates in Variational Quantum Circuits affect classification performance in quantum machine learning.

Method: The authors compared Amplitude-encoding and Angle-encoding models by training them on Wine and Diabetes datasets. They analyzed the effects of applying various rotational gates and assessed performance variations under the same model topology.

Result: The study found varying classification performances between encoding methods and rotational gates, with accuracy differences ranging from 10% to 41%.

Conclusion: The choice of encoding method and rotational gates is a crucial hyperparameter for VQC models, having a significant impact on their classification performance.

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [227] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: The study investigates socio-academic and financial factors influencing CGPA, using surveys, regression, and classification models, and develops a web tool for personalized student insights.


<details>
  <summary>Details</summary>
Motivation: This study aims to understand how various socio-academic and financial factors impact students' CGPA to create strategies for academic improvement.

Method: A literature review, online survey with 1,050 participants, causal analysis, regression, and classification models like Ridge Regression and Random Forest were used, alongside explainable AI tools like SHAP and LIME.

Result: Ridge Regression achieved a Mean Absolute Error of 0.12 and a Mean Squared Error of 0.023 in predictions, while Random Forest achieved 98.68% accuracy in classification. Critical factors like study hours and scholarships were identified.

Conclusion: The study developed a web-based application enabling students to predict and improve their CGPA by highlighting critical factors through data-driven insights and model interpretability.

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [228] [Reinitializing weights vs units for maintaining plasticity in neural networks](https://arxiv.org/abs/2508.00212)
*J. Fernando Hernandez-Garcia,Shibhansh Dohare,Jun Luo,Rich S. Sutton*

Main category: cs.NE

TL;DR: This paper addresses 'loss of plasticity' in neural networks trained on non-stationary data and compares two reinitialization strategies: selective weight reinitialization vs unit reinitialization.


<details>
  <summary>Details</summary>
Motivation: The need to address loss of plasticity, which hampers a neural network's ability to learn continually when exposed to non-stationary data.

Method: The authors propose a new algorithm termed 'selective weight reinitialization' that reinitializes the least useful weights in the network, and compare it with existing methods like continual backpropagation and ReDo.

Result: Experiments reveal that weight reinitialization is more effective than unit reinitialization in maintaining network plasticity in setups with a small number of units or networks employing layer normalization.

Conclusion: Reinitializing weights is advantageous across a broader range of scenarios compared to reinitializing units, except when the network is sufficiently large and lacks layer normalization.

Abstract: Loss of plasticity is a phenomenon in which a neural network loses its
ability to learn when trained for an extended time on non-stationary data. It
is a crucial problem to overcome when designing systems that learn continually.
An effective technique for preventing loss of plasticity is reinitializing
parts of the network. In this paper, we compare two different reinitialization
schemes: reinitializing units vs reinitializing weights. We propose a new
algorithm, which we name \textit{selective weight reinitialization}, for
reinitializing the least useful weights in a network. We compare our algorithm
to continual backpropagation and ReDo, two previously proposed algorithms that
reinitialize units in the network. Through our experiments in continual
supervised learning problems, we identify two settings when reinitializing
weights is more effective at maintaining plasticity than reinitializing units:
(1) when the network has a small number of units and (2) when the network
includes layer normalization. Conversely, reinitializing weights and units are
equally effective at maintaining plasticity when the network is of sufficient
size and does not include layer normalization. We found that reinitializing
weights maintains plasticity in a wider variety of settings than reinitializing
units.

</details>


### [229] [Sequential, Parallel and Consecutive Hybrid Evolutionary-Swarm Optimization Metaheuristics](https://arxiv.org/abs/2508.00229)
*Piotr Urbańczyk,Aleksandra Urbańczyk,Magdalena Król,Leszek Rutkowski,Marek Kisiel-Dorohinicki*

Main category: cs.NE

TL;DR: This paper investigates hybrid evolutionary-swarm metaheuristics combining PSO and GA, introducing sequential, parallel, and consecutive approaches, and evaluates them against benchmark functions.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization performance in higher-dimensional search spaces by combining the strengths of PSO and GA.

Method: The study tests sequential, parallel, and consecutive hybrid PSO-GA algorithms on benchmark functions, and introduces a novel consecutive hybrid approach with explicit information transfer.

Result: Hybrid PSO-GA metaheuristics showed superior convergence and consistency, especially in high-dimensional search spaces.

Conclusion: Combining PSO and GA yields better optimization outcomes, and the proposed consecutive hybrid approach effectively ensures information transfer between steps.

Abstract: The goal of this paper is twofold. First, it explores hybrid
evolutionary-swarm metaheuristics that combine the features of PSO and GA in a
sequential, parallel and consecutive manner in comparison with their standard
basic form: Genetic Algorithm and Particle Swarm Optimization. The algorithms
were tested on a set of benchmark functions, including Ackley, Griewank, Levy,
Michalewicz, Rastrigin, Schwefel, and Shifted Rotated Weierstrass, across
multiple dimensions. The experimental results demonstrate that the hybrid
approaches achieve superior convergence and consistency, especially in
higher-dimensional search spaces. The second goal of this paper is to introduce
a novel consecutive hybrid PSO-GA evolutionary algorithm that ensures
continuity between PSO and GA steps through explicit information transfer
mechanisms, specifically by modifying GA's variation operators to inherit
velocity and personal best information.

</details>


### [230] [Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning](https://arxiv.org/abs/2508.00380)
*Kebin Sun,Tao Jiang,Ran Cheng,Yaochu Jin,Kay Chen Tan*

Main category: cs.NE

TL;DR: EvoGO is a data-driven evolutionary optimization framework using generative learning to improve adaptability and performance in optimization tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing evolutionary algorithms that rely on handcrafted heuristics, improving generality and automation.

Method: EvoGO introduces a three-stage framework: data preparation (pairwise dataset generation), model training (generative model transforms inferior to superior solutions), and population generation (replacing traditional reproduction with scalable generative mechanisms).

Result: The experiments show EvoGO consistently converges within 10 generations and substantially outperforms various traditional optimization methods across diverse tasks.

Conclusion: EvoGO presents an innovative, fully automated, and efficient optimization approach, indicating a significant step forward for data-driven evolutionary algorithms.

Abstract: Recent advances in data-driven evolutionary algorithms (EAs) have
demonstrated the potential of leveraging data to improve optimization accuracy
and adaptability. Nevertheless, most existing approaches remain dependent on
handcrafted heuristics, which limits their generality and automation. To
address this challenge, we propose Evolutionary Generative Optimization
(EvoGO), a fully data-driven framework empowered by generative learning. EvoGO
streamlines the evolutionary optimization process into three stages: data
preparation, model training, and population generation. The data preparation
stage constructs a pairwise dataset to enrich training diversity without
incurring additional evaluation costs. During model training, a tailored
generative model learns to transform inferior solutions into superior ones. In
the population generation stage, EvoGO replaces traditional reproduction
operators with a scalable and parallelizable generative mechanism. Extensive
experiments on numerical benchmarks, classical control problems, and
high-dimensional robotic tasks demonstrate that EvoGO consistently converges
within merely 10 generations and significantly outperforms a wide spectrum of
optimization approaches, including traditional EAs, Bayesian optimization, and
reinforcement learning based methods. Source code will be made publicly
available.

</details>


### [231] [STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers](https://arxiv.org/abs/2508.00387)
*Zeqi Zheng,Zizheng Zhu,Yingchao Yu,Yanchen Huang,Changze Lv,Junfeng Tang,Zhaofei Yu,Yaochu Jin*

Main category: cs.NE

TL;DR: This paper proposes Shallow-level Temporal Feedback (STF), a lightweight module to improve Transformer-based Spiking Neural Networks (SNNs).


<details>
  <summary>Details</summary>
Motivation: Transformer-based SNNs underperform compared to floating-point ANNs due to binary spike train processing. Existing solutions to address this require costly deep-level feedback loops, which increase energy consumption and latency.

Method: The proposed STF module includes Temporal-Spatial Position Embedding (TSPE) and Temporal Feedback (TF). It is designed to improve performance with minimal overhead by working at shallow encoding levels.

Result: STF improves Transformer-based SNN performance on datasets such as CIFAR-10, CIFAR-100, and ImageNet-1K. It also enhances spike diversity and outperforms existing methods in adversarial robustness and temporal sensitivity.

Conclusion: STF offers an efficient and effective spike encoding scheme for static scenarios, achieving higher performance with reduced complexity.

Abstract: Transformer-based Spiking Neural Networks (SNNs) suffer from a great
performance gap compared to floating-point Artificial Neural Networks (ANNs)
due to the binary nature of spike trains. Recent efforts have introduced
deep-level feedback loops to transmit high-level semantic information to narrow
this gap. However, these designs often span multiple deep layers, resulting in
costly feature transformations, higher parameter overhead, increased energy
consumption, and longer inference latency. To address this issue, we propose
Shallow-level Temporal Feedback (STF), a lightweight plug-and-play module for
the encoding layer, which consists of Temporal-Spatial Position Embedding
(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF
consistently improves performance across various Transformer-based SNN
backbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,
under different spike timestep settings. Further analysis reveals that STF
enhances the diversity of the spike patterns, which is key to performance gain.
Moreover, evaluations on adversarial robustness and temporal sensitivity
confirm that STF outperforms direct coding and its variants, highlighting its
potential as a new spike encoding scheme for static scenarios. Our code will be
released upon acceptance.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [232] [DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme](https://arxiv.org/abs/2508.00441)
*Daichi Mukunoki*

Main category: cs.PF

TL;DR: The paper explores efficient matrix multiplication using low-precision operations such as FP8 and FP64 emulation on new AI hardware.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of leveraging low-precision operations in scientific computations, especially given the performance focus of AI hardware.

Method: The paper revisits the Ozaki scheme, applies FP8 and FP6 Tensor Cores, and introduces new blocking strategies for computational efficiency.

Result: The study demonstrates enhanced DGEMM performance on GPUs using FP64 emulation and FP8 Tensor Cores.

Conclusion: The proposed techniques are effective for achieving high-performance matrix multiplication tailored to modern AI hardware capabilities.

Abstract: Since AI computations require low-precision matrix multiplications,
processors with enhanced performance for these operations are increasing along
with the growing demand for AI computations. However, it is difficult to use
these operations directly for scientific computations. The Ozaki scheme, an
accurate matrix multiplication method proposed by Ozaki et al. in 2012, enables
FP64 matrix multiplication (DGEMM) using low-precision floating-point
operations such as FP16. The method was subsequently extended to utilize
integer arithmetic. The use of integer operations reduces computational cost
compared to the floating-point based approach. It has also demonstrated higher
performance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores
for AI workloads. However, the latest hardware tends to enhance low-precision
floating-point operation performance such as FP8 instead of INT8. This study
revisits the utilization of low-precision floating-point operations in the
Ozaki scheme, considering the latest AI hardware. Specifically, we consider the
use of FP6 and FP8 Tensor Cores. Moreover, for processors that support very
slow FP64 operations or do not support them at all, we consider the use of the
FP64 emulation based on integer arithmetic. We also examine a new blocking
strategy. We demonstrate the effectiveness of these methods by evaluating the
performance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell
architecture GPU.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [233] [Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/abs/2508.00005)
*Tilman Hinnerichs,Bart Swinkels,Jaap de Jong,Reuben Gardos Reid,Tudor Magirescu,Neil Yorke-Smith,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: The paper introduces BART, a solver that uses syntactic constraints to effectively narrow down program spaces, reducing enumeration time and eliminating up to 99% of unwanted programs.


<details>
  <summary>Details</summary>
Motivation: Program synthesis involves navigating a vast space of possible programs, creating difficulties in efficiently searching for viable and useful solutions.

Method: This paper employs syntactic constraints to limit program spaces by incorporating them into the solver, allowing efficient propagation and solving without execution of programs.

Result: BART demonstrates that syntactic constraints reduce program spaces by up to 99% and significantly decrease enumeration time.

Conclusion: Modeling program spaces through syntactic constraints not only improves program synthesis search but also offers a promising direction for eliminating computational overhead.

Abstract: A core challenge in program synthesis is taming the large space of possible
programs. Since program synthesis is essentially a combinatorial search, the
community has sought to leverage powerful combinatorial constraint solvers.
Here, constraints are used to express the program semantics, but not as a
potentially potent tool to remove unwanted programs. Recent inductive logic
programming approaches introduce constraints on the program's syntax to be
synthesized. These syntactic constraints allow for checking and propagating a
constraint without executing the program, and thus for arbitrary operators. In
this work, we leverage syntactic constraints to model program spaces, defining
not just solutions that are feasible, but also ones that are likely useful. To
demonstrate this idea, we introduce BART, a solver that efficiently propagates
and solves these constraints. We evaluate BART on program space enumeration
tasks, finding that the constraints eliminate up to 99 percent of the program
space, and that modeling program spaces significantly reduces enumeration time.

</details>


### [234] [From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms](https://arxiv.org/abs/2508.00013)
*Zurabi Kobaladze,Anna Arnania,Tamar Sanikidze*

Main category: cs.PL

TL;DR: This paper reviews the evolution of program synthesis paradigms from formal logic methods to neural models, analyzing approaches, principles, systems, and challenges.


<details>
  <summary>Details</summary>
Motivation: To examine and compare paradigms and advancements in program synthesis, addressing its evolution, trade-offs, and challenges.

Method: A comparative literature review analyzing five program synthesis approaches: deductive, inductive, schema-based, large language model-based, and neuro-symbolic hybrid synthesis.

Result: A detailed narrative highlighting principles, systems, applications, and trade-offs in program synthesis, from formally verified tools to probabilistic models.

Conclusion: The evolution is marked by a transition from symbolic to neuro-symbolic methods, with future directions aimed at improving reliability and scalability for program synthesis.

Abstract: Program synthesis--the automated generation of executable code from
high-level specifications--has been a central goal of computer science for over
fifty years. This thesis provides a comparative literature review of the main
paradigms that have shaped the field, tracing its evolution from formal logic
based methods to recent advances using large scale neural models. We examine
five key approaches: logic based (deductive) synthesis, inductive (example
based) synthesis, sketch/schema based synthesis, large language model based
synthesis, and neuro-symbolic hybrids. For each, we analyze foundational
principles, notable systems, and practical applications, highlighting trade
offs between correctness guarantees, specification requirements, search
complexity, and expressive power. By reviewing developments from formally
verified synthesis tools such as KIDS and Coq to data driven models generating
probabilistic code from natural language like Codex, we present a comprehensive
narrative of progress and ongoing challenges. This work emphasizes the
transition from symbolic to hybrid neuro-symbolic methods and outlines future
directions for reliable and scalable program synthesis.

</details>


### [235] [Extended Abstract: Mutable Objects with Several Implementations](https://arxiv.org/abs/2508.00016)
*Matt Kaufmann,Yahya Sohail,Warren A. Hunt Jr*

Main category: cs.PL

TL;DR: The paper introduces 'attach-stobj,' a feature in ACL2 Version 8.6, enabling various executable operations for a given abstract stobj without recertifying associated content.


<details>
  <summary>Details</summary>
Motivation: To address the need for flexible operations on abstract stobjs in ACL2 without compromising recertification of books and theorems, thus enhancing usability and efficiency.

Method: The paper introduces and explains the 'attach-stobj' feature with user-level guidance and implementation insights.

Result: The feature supports diverse executable operations for abstract stobjs, maintaining integrity of dependent books and theorems.

Conclusion: The 'attach-stobj' feature significantly improves the manageability and functionality of abstract stobjs in ACL2, exemplifying practical enhancement in the tool.

Abstract: This extended abstract outlines an ACL2 feature, attach-stobj, that first
appeared in ACL2 Version 8.6 (October, 2024). This feature supports different
executable operations for a given abstract stobj, without requiring
recertification of the book that introduces that stobj or theorems about it.
The paper provides background as well as a user-level overview and some
implementation notes.

</details>


### [236] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: This paper explores using large language models (LLMs) to automate Python type annotations with a generate-check-repair pipeline and demonstrates effectiveness without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Manual type annotations in Python are error-prone and labor-intensive, while traditional automation methods face challenges like limited vocabularies and reliance on large labeled datasets.

Method: The authors propose a generate-check-repair pipeline where LLMs generate type annotations, MyPy verifies their correctness, and errors are iteratively corrected.

Result: Four LLM variants were tested on a Python benchmark. General-purpose and reasoning-optimized LLMs achieved up to 88.6% consistency and 70.5% exact-match accuracy, requiring fewer iterations than traditional methods.

Conclusion: LLMs can effectively automate type annotations in Python, rivaling traditional approaches without additional task-specific fine-tuning, and the pipeline may generalize to other imperative languages like Ruby.

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


### [237] [Semantic Subtyping for Maps in Erlang](https://arxiv.org/abs/2508.00482)
*Erdem Yildirim,Albert Schimpf,Stefan Wehr,Annette Bieniusa*

Main category: cs.PL

TL;DR: The paper creates a set-theoretic model for types, introduces semantic subtyping, and innovates subtyping for parameteric map types.


<details>
  <summary>Details</summary>
Motivation: There is a need to formalize types and subtyping, especially for complex type structures like map types in languages such as Erlang.

Method: The authors construct a set-theoretic model encompassing various types, including base types, type variables, set-theoretic types, and map types. They define subtyping using semantic relations based on set containment.

Result: The study successfully defines semantic subtyping across parameteric map types, demonstrating a novel approach.

Conclusion: This work provides a new conceptual framework for handling subtyping in parameteric map types, advancing theoretical understanding and practical application in programming languages.

Abstract: In this paper we will construct a set-theoretic model of types featuring type
variables, base types, set-theoretic types and map types. Syntax of map types
spans all the map types available in Erlang. The model of types is used to
define a semantic subtyping relation based on set containment. The novelty of
this work is the definition of subtyping over parameteric map types.

</details>


### [238] [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
*Mikel Vandeloise*

Main category: cs.PL

TL;DR: This paper conducts a systematic review on programming paradigms' formal foundations, identifying limitations in existing classifications and advocating for compositional reconstruction using mathematical frameworks.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges posed by multi-paradigm languages, which create practical software engineering issues like interoperability defects and expose the limitations of traditional classification methods for programming paradigms.

Method: The authors conducted a systematic literature review (SLR) of 74 primary studies, synthesizing their findings to assess the state of classifications and explore formal mathematical frameworks.

Result: The paper finds that existing taxonomies lack granularity, a unified formal basis, and fail to address hybrid languages effectively. It highlights a convergence towards compositional reconstruction using concepts like Type theory, Category theory, and Unifying Theories of Programming (UTP).

Conclusion: The study identifies a shift in focus from traditional classification to reconstructive approaches and provides a research map for further unifying these frameworks to address the challenges of multi-paradigm programming.

Abstract: The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [239] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: The paper introduces XRoboToolkit, a framework for extended reality-based robot teleoperation using OpenXR standard, improving scalability and data quality for Vision-Language-Action model training.


<details>
  <summary>Details</summary>
Motivation: To address limitations in scalability, setup complexity, and data quality in teleoperation approaches for collecting large-scale robot demonstration datasets.

Method: The study utilized XRoboToolkit, which offers low-latency visual feedback, optimization-based inverse kinematics, and diverse tracking modalities, enabling integration across various robotic platforms and simulation environments.

Result: The framework demonstrated effectiveness in precision manipulation tasks and improved data quality for Vision-Language-Action models, leading to robust autonomous performance.

Conclusion: XRoboToolkit advances teleoperation methods by enhancing data collection scalability and quality, contributing significantly to Vision-Language-Action model development.

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [240] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: This paper introduces CHILD, a compact teleoperation system that enables joint-level control for humanoid robots.


<details>
  <summary>Details</summary>
Motivation: Existing teleoperation systems rarely support whole-body joint-level control for humanoid robots, limiting task diversity.

Method: A reconfigurable system named CHILD, enabling full-body control via joint-level mapping and adaptive force feedback, fitted in a standard baby carrier.

Result: Validated with loco-manipulation tasks on humanoid robots and dual-arm systems, showcasing its capabilities.

Conclusion: CHILD enhances operator control and experience while being accessible and reproducible due to open-sourcing hardware design.

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [241] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: The paper introduces a descriptor combining pseudo-rigid-body modeling and Morse theory for characterizing and controlling soft continuum robot morphologies.


<details>
  <summary>Details</summary>
Motivation: To enable precise and quantitative assessment, classification, and control of soft continuum robots' morphologies, which have applications in complex medical scenarios like surgery.

Method: The authors developed a morphology descriptor based on critical points in directional projections, using a pseudo-rigid-body model and Morse theory, integrating these into taxonomy and optimization in robot configuration.

Result: The descriptor achieved discrete morphology representation and classification and could be applied to optimize actuation parameters for desired robot shapes.

Conclusion: This framework fosters unified morphology handling in soft continuum robots and has potential in enhancing precision and adaptability for medical applications.

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [242] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: The paper introduces UAV-ON, a benchmark for UAV object-goal navigation in open-world environments using high-level semantic goals instead of detailed instructions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of Vision-and-Language Navigation (VLN), which relies on sequential linguistic instructions, by advancing scalable and autonomous navigation methods for aerial agents.

Method: The authors developed UAV-ON, consisting of 14 Unreal Engine environments with 1270 annotated target objects and instance-level instructions, and tested baseline methods for semantic goal-driven aerial navigation.

Result: Baseline methods, including the Aerial ObjectNav Agent (AOA), performed poorly, indicating the significant difficulty of the tasks posed by UAV-ON.

Conclusion: UAV-ON provides a challenging and realistic benchmark to push the boundaries of scalable autonomy in UAV navigation using semantic goal descriptions.

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [243] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser is a framework for trajectory prediction, integrating topometric maps into a diffusion-based model to produce accurate and road-compliant trajectories.


<details>
  <summary>Details</summary>
Motivation: Improving trajectory prediction accuracy and compliance with road geometry using multimodal data and removing reliance on explicit constraints.

Method: TopoDiffuser fuses LiDAR, historical motion, and route data into a BEV format and incorporates topometric map cues into the denoising process of a diffusion model.

Result: TopoDiffuser achieves superior performance on the KITTI benchmark with enhanced geometric consistency compared to state-of-the-art methods.

Conclusion: TopoDiffuser is effective for road-compliant trajectory prediction and provides valuable insights through feature ablation studies; its public code release supports further research.

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [244] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: The paper introduces Omni-Scan, a framework that uses a bi-manual robot for creating 3D Gaussian Splat models with high-quality 360-degree object views.


<details>
  <summary>Details</summary>
Motivation: Creating accurate 3D models from multi-view images without relying on traditional bulky or workspace-limited setups like multi-camera arrays or precise scanners.

Method: A bi-manual robot is used to rotate and re-grasp objects for complete capturing, leveraging models like DepthAnything and SegmentAnything for object segmentation and gripper/background removal in the pipeline.

Result: The Omni-Scan pipeline achieved an 83% accuracy in identifying visual or geometric defects across 12 industrial and household objects.

Conclusion: Omni-Scan demonstrates an effective and flexible solution for producing omni-directional 3D models, suitable for applications like defect inspection using fewer specialized tools.

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [245] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: This paper introduces a novel time optimization policy (TOP) to improve humanoid robot stability and precision during standing manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving both robustness and accuracy in humanoid robot standing controllers, especially under fast upper-body motions.

Method: The authors utilize motion prior with a variational autoencoder (VAE) for upper-body movements, decouple whole-body control into upper-body PD controllers and lower-body RL controllers, and train using the TOP method.

Result: Both simulation and real-world experiments validate the approach, showing enhanced stability and accuracy in standing manipulation tasks.

Conclusion: The paper demonstrates a successful integration of time optimization techniques and decoupled controllers, improving humanoid robots' capability for stable and precise manipulation tasks.

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [246] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: This paper proposes a framework enabling humanoid robots to better imitate human motion by addressing kinematics and balance challenges using advanced control methods.


<details>
  <summary>Details</summary>
Motivation: To make humanoid robots capable of imitating complex and human-like movements despite differences in kinematics and dynamics between humans and robots.

Method: The approach combines contact-aware whole-body motion retargeting with a non-linear centroidal model predictive controller for precise motion and balance maintenance.

Result: Experiments, conducted in simulation and on real humanoid robots, showed accurate and adaptable human motion imitation.

Conclusion: The proposed framework effectively improves a humanoid robot's ability to imitate human motion while addressing real-time balance and adaptability issues.

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [247] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: The authors introduce NIVA, a probabilistic framework for multi-agent traffic simulation that utilizes a hierarchical Bayesian model to enhance scenario diversity and interactivity, achieving competitive results on the Waymo dataset.


<details>
  <summary>Details</summary>
Motivation: The need for realistic and scalable traffic simulations for evaluating autonomous vehicles' performance is increasing, especially for handling diverse and interactive scenarios.

Method: The authors propose NIVA, which is driven by a hierarchical Bayesian model and employs autoregressive sampling from latent Gaussian distributions for closed-loop simulation.

Result: NIVA unifies various trajectory prediction models, achieves competitive results on the Waymo Open Motion Dataset, and offers enhanced control over driving intentions and styles.

Conclusion: NIVA provides a scalable and effective solution for multi-agent simulation, bridging traditional prediction models and closed-loop approaches while improving control and performance.

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [248] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: The paper introduces SubCDM, a strategy for making decisions in robot swarms using only a subset of robots, maintaining accuracy while being resource-efficient.


<details>
  <summary>Details</summary>
Motivation: Current robot swarm systems require the participation of all robots for collective decision-making, which is resource-intensive and limits multitasking.

Method: The SubCDM method dynamically and decentrally constructs a subset of the swarm based on local information, adapting its size for consensus accuracy.

Result: Simulations with 100 robots demonstrate that SubCDM achieves similar decision-making accuracy as full swarm participation while significantly reducing the involved robots.

Conclusion: SubCDM provides an efficient alternative for swarm robotics, enabling collective decision-making with fewer resources and retaining accuracy.

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [249] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: The study introduces HannesImitationPolicy, an imitation learning approach for controlling a prosthetic hand, Hannes, in unstructured environments. It also releases HannesImitationDataset for training and achieves superior results compared to traditional methods in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the dexterity and autonomous operation of prosthetic hands, particularly in unstructured environments, by leveraging imitation learning—an area largely unexplored in prosthetic control but successful in robotics.

Method: The method involves creating HannesImitationPolicy, an imitation learning framework, and HannesImitationDataset, which includes diverse grasping demonstrations. Using this dataset, a diffusion policy is trained to predict wrist orientations and hand closures for effective grasping.

Result: The trained policy enables successful object grasping across various conditions and scenarios. It outperforms a segmentation-based visual servo controller in challenging and unstructured environments.

Conclusion: Imitation learning is a viable and superior approach for controlling prosthetic hands, offering significant performance improvements in unstructured scenarios. The study presents a novel pathway for dexterity restoration using task demonstrations rather than manual annotations.

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [250] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: This paper introduces OmniUnet, a transformer-based neural network for semantic segmentation using RGB, depth, and thermal imagery, tested in a Martian-like semi-desert environment.


<details>
  <summary>Details</summary>
Motivation: Enhancing robot navigation in Martian-like terrain using multimodal perception systems to integrate complementary sensor data.

Method: OmniUnet, a RGB-D-T powered transformer model, was trained on labeled multimodal data collected by a custom sensor mounted on MaRTA, a Martian rover prototype.

Result: The model achieved 80.37% pixel accuracy and operated effectively on resource-limited systems, demonstrating its feasibility for robotic deployment.

Conclusion: OmniUnet improves terrain segmentation for planetary robotics and offers open-source tools to advance research in multimodal perception.

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [251] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: The paper proposes a control scheme for human-robot collaborative object transportation using a quadruped robot with an advanced suction cup system. Experimental evaluations validate its performance.


<details>
  <summary>Details</summary>
Motivation: Enable effective collaboration between humans and quadruped robots in object transportation by reducing human effort and improving system safety.

Method: Admittance control with variable damping for controllability and reduced human effort, coupled with a barrier artificial potential signal to keep objects attached to the suction cup.

Result: Experimental tests using the Unitree Go1 robot with MIGHTY suction cup confirm the scheme's passivity and effective performance.

Conclusion: The proposed control scheme facilitates safe and effort-efficient human-robot collaboration in object transportation, demonstrating the feasibility of this approach.

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [252] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: The paper discusses updates to OpenScout, a mobile robot for research and industry, emphasizing hardware improvements and enhanced simulation capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a more accessible, robust, and efficient open-source mobile robot for both research and industrial applications.

Method: The authors upgraded OpenScout to version 1.1 with simplified, cost-effective, and powerful onboard computing hardware alongside implementing a ROS2 interface and Gazebo simulation.

Result: The changes in OpenScout led to improved functionality, ease of use, and reduction in costs, all while maintaining open-source accessibility.

Conclusion: OpenScout v1.1 serves as a valuable case study demonstrating the benefits of evolving OSH projects for increased usability, performance, and affordability.

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [253] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: This paper presents a novel data-driven method for controlling exoskeletons in post-stroke walking, demonstrating its feasibility through a prototype and real-time testing.


<details>
  <summary>Details</summary>
Motivation: The work addresses the difficulty of applying adaptive exoskeleton control to post-stroke populations, which face challenges like population heterogeneity, gait variability, and a lack of training datasets.

Method: The researchers trained a multi-task Temporal Convolutional Network (TCN) using IMU data from post-stroke and healthy participants for adaptive torque control.

Result: The model achieved an $R^2$ of $0.74 \pm 0.13$ for post-stroke walking data, and their wearable prototype demonstrated real-time implementation with one post-stroke participant.

Conclusion: Data-driven torque estimation shows promise for adaptive exoskeleton control in post-stroke rehabilitation, opening pathways for real-world deployment despite current challenges.

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [254] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP is a framework to optimize Diffusion Policies for real-time robotic manipulation on mobile devices by reducing computational inefficiency and memory usage through network compression and sampling step reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to adapt Diffusion Policies for mobile devices where computational resources and memory are constrained, making current approaches inefficient.

Method: The method involves computational analysis to identify inefficiencies, employing a combination of network pruning, retraining for recoverability, and consistency distillation to accelerate inference and reduce memory usage.

Result: Experimental results show that LightDP achieves real-time performance on mobile devices with competitive action prediction accuracy, tested across standard datasets and real-world experiments.

Conclusion: LightDP presents a lightweight and effective alternative for deploying diffusion-based policies on mobile platforms without significant performance loss, broadening their practical application potential.

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [255] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: This paper introduces Video Policy, a framework that uses video generation to enhance robot policy learning, addressing data limitations and robustness challenges.


<details>
  <summary>Details</summary>
Motivation: Current visuomotor policies face challenges with behavioral/perceptual shifts and are constrained by limited human demonstration data.

Method: The paper proposes a modular framework combining video and action generation, leveraging end-to-end training.

Result: The method improves robustness and sample efficiency, generalizes well to unseen scenarios, and demonstrates task performance benefits tied to video generation.

Conclusion: Leveraging video generative models significantly enhances robot policy learning, enabling scalable and data-efficient solutions.

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [256] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: The paper introduces Git-Context-Controller (GCC), a structured context management framework inspired by Git for enabling large language model agents to be effective in long-term workflows.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of context management faced by large language model agents in long-term workflows such as extended coding projects.

Method: The paper proposes a Git-like structured context management system called GCC, which introduces operations like COMMIT, BRANCH, MERGE, and CONTEXT enabling agents to handle memory hierarchies effectively in long-term tasks.

Result: Agents using GCC achieve state-of-the-art performance on the SWE-Bench-Lite benchmark and significantly outperform non-GCC systems in both bug resolution (48.00 vs competitors) and a self-replication task (40.7% vs 11.7%).

Conclusion: The GCC approach significantly improves context management and long-term task resolution for agents, elevating their capability in complex workflows.

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [257] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: The paper benchmarks various large language models (LLMs) for their ability to generate functional Python code for scientific tasks without examples in the prompt. GPT-4.1 performs the best.


<details>
  <summary>Details</summary>
Motivation: To analyze how well LLMs can automate code generation for scientific experiments, particularly with unfamiliar Python APIs.

Method: Zero-shot prompting with structured requirements for code generation tasks, followed by quantitative and qualitative evaluation of outputs.

Result: GPT-4.1 consistently succeeded in generating correct and executable Python code, while other models often failed. Issues with third-party library documentation were also noted.

Conclusion: LLMs exhibit limitations in scientific automation, requiring improved prompt design, library documentation, and model capabilities.

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [258] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: The paper reviews machine learning pipelines in software engineering and identifies best practices, highlighting their importance in improving software quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the growing complexity of software engineering systems and the inefficiency of traditional approaches in tasks like defect detection and debugging.

Method: A systematic literature review (SLR) of state-of-the-art machine learning pipelines for software engineering, focusing on their components, techniques, and evaluation measures.

Result: Robust preprocessing techniques and ensemble methods, such as SMOTE for data balancing and Gradient Boosting, dominate in performance and reliability, with new evaluation metrics emerging.

Conclusion: Well-designed ML pipelines are crucial for addressing software engineering challenges, and the findings provide actionable insights for improving software quality and fostering innovation in complex environments.

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [259] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: This paper provides a comprehensive review of code generation agents powered by large language models (LLMs), exploring their development, techniques, applications, benchmarks, tools, and future challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid development and transformative potential of LLM-powered code generation agents have revolutionized software development, necessitating a comprehensive understanding of their evolution, methodologies, and practical applications.

Method: The paper systematically surveys the field by categorizing techniques, analyzing single-agent and multi-agent architectures, exploring full SDLC applications, summarizing evaluation metrics, and cataloging representative tools. It also identifies primary challenges and proposes future research directions.

Result: The survey presents a detailed categorization of LLM-based techniques, highlights their application in software development lifecycle stages, compiles evaluation benchmarks and tools, and provides insights into challenges and opportunities.

Conclusion: LLM-based code generation agents are reshaping software development with their autonomy, task scope expansion, and engineering practicality enhancement. The survey provides a firm foundation for future research and technological advancements.

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [260] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: This paper explores how quantization, a model compression technique, affects both performance and privacy risk in large language models for code.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the privacy risks of sensitive training data in LLMs4Code and investigates whether quantization impacts their ability to expose private information.

Method: The study applies widely used quantization techniques to three model families (Pythia, CodeGen, GPTNeo) and evaluates task performance and privacy risk.

Result: Quantization reduces privacy risks significantly and reveals a positive correlation between task performance and privacy risk. Larger quantized models may offer better trade-offs.

Conclusion: Quantization provides a practical approach to balancing performance and privacy risk, offering insights applicable across architectures, model sizes, and MI methods.

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [261] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: This paper investigates how software systems using large language models (LLMs) are tested by analyzing student project reports.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on testing strategies for LLM-powered systems in software engineering.

Method: The authors conducted thematic analysis of 99 student-written reports on their experiences building LLM-powered applications.

Result: Testing combined manual and automated strategies, with challenges including unpredictable outputs, integration failures, and prompt sensitivity.

Conclusion: Testing LLM-powered systems requires blending traditional methods with new approaches adapted for model behavior.

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [262] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: This study compares object-oriented programming (OOP) and functional programming (FP) by analyzing the design and implementation of a Digital Wallet system in Kotlin (OOP) and Scala (FP).


<details>
  <summary>Details</summary>
Motivation: To explore how OOP and FP paradigms influence the architectural characteristics of software systems, addressing their impact for developers and organizations.

Method: The study conducts a self-ethnographic qualitative analysis and a survey-based quantitative analysis, comparing implementations of a Digital Wallet system created using Kotlin and Scala.

Result: The study provides insights on differences between OOP and FP in system architecture, both from the perspective of developers writing the code and those evaluating it.

Conclusion: The findings aim to help developers and organizations decide which programming paradigm better suits their project needs.

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [263] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: The paper introduces GenLoc, a Large Language Model-based approach to improve bug localization by addressing vocabulary mismatch issues between bug reports and source code. Experiments showcase significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of bug localization techniques is limited due to vocabulary mismatches between bug reports and source code. This paper aims to overcome these limitations using advanced AI methods.

Method: GenLoc uses an iterative code analysis process powered by a Large Language Model, supplemented optionally with vector embeddings to gather context and semantically relevant files.

Result: Evaluated on more than 9,000 real-world bug reports from six large-scale Java projects, GenLoc achieved an average improvement of over 60% in Accuracy@1, outperforming five state-of-the-art techniques.

Conclusion: GenLoc demonstrates significant advancements in bug localization by leveraging LLMs and code exploration, setting a new benchmark in its domain.

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [264] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan is a platform that integrates value-flow and symbolic reasoning, merging high-performance data-flow analysis with efficient symbolic execution.


<details>
  <summary>Details</summary>
Motivation: To address the gap caused by the lack of a unified platform for combining value-flow and symbolic analysis approaches in program analysis.

Method: Desyan expands the Soufflé Datalog engine with industry-standard SMT solving capabilities, allowing flexible and efficient integration of reasoning paradigms.

Result: Desyan offers best-in-class performance for value-flow analysis, supports complex and lightweight symbolic evaluations, demonstrating significant execution time reduction compared to traditional approaches.

Conclusion: Desyan effectively bridges the gap between symbolic analysis and value-flow analysis, enabling seamless and efficient integration for diverse program analysis use cases.

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [265] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: The paper addresses issues in using large language models (LLMs) for generating graph models from natural language descriptions, proposing a framework to enhance consistency and quality.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for graph model generation are prone to syntax violations, constraint inconsistencies, and inaccuracies, with limited solutions for addressing these flaws.

Method: The paper introduces an abstraction-concretization framework where candidate outputs from an LLM are aggregated into a probabilistic partial model, which is then refined to a final conforming model.

Result: Experiments on datasets with various LLMs showed that the framework significantly improves the quality and consistency of graph model generation.

Conclusion: The proposed framework effectively addresses significant issues in LLMs, advancing their utility for generating graph models in software engineering tasks.

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [266] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: The paper develops a machine learning tool to predict Codeforces users' software engineering job readiness using their competitive programming data.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating job readiness among programmers based on coding performance.

Method: Extracted user activity data from the Codeforces API and used a Random Forest classifier to categorize job readiness. Deployed the model through Flask and Render for real-time predictions.

Result: The model effectively differentiates user employability levels, demonstrating its utility in evaluating coding-based job readiness.

Conclusion: This study establishes a foundational framework for using machine learning to assess career readiness, with potential for applicability in broader technical fields.

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


### [267] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: Introduces ULT, a benchmark aimed at enhancing the evaluation of LLMs in unit test generation for realistic and complex Python functions. It identifies weaknesses in existing benchmarks and explores reasoning versus memorization.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating LLMs in unit test generation suffer from data contamination and oversimplified function code, leading to potentially biased and unrepresentative results.

Method: Designed ULT via multi-stage curation to ensure high cyclomatic complexity and eliminate contamination. Created a paired benchmark, PLT, with leaked test cases for controlled analysis of LLM behavior.

Result: ULT demonstrated lower scores in generating accurate and effective test cases compared to simpler benchmarks like TestEval and PLT, highlighting its higher complexity and realism.

Conclusion: ULT provides a robust metric for evaluating LLMs on realistic, challenging unit test generation scenarios, addressing shortcomings in earlier benchmarks and aiding deeper analysis.

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [268] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: The study examines power dynamics in professional pair programming sessions, introducing the concept of 'Power Gap,' which affects outcomes like knowledge transfer and code quality. It offers specific advice to practitioners to avoid hierarchical behaviors and promote equalizing behaviors.


<details>
  <summary>Details</summary>
Motivation: The research addresses the need to understand power dynamics in pair programming within industry to improve the overall practice and outcomes.

Method: The study applies Grounded Theory Methodology to analyze 22 pair programming sessions and conducts a survey of 292 participants to validate the observations and concepts derived from the theory.

Result: The key result is the identification of the 'Power Gap' phenomenon, its behaviors, and its negative impact on knowledge transfer, code quality, and process efficiency. The survey confirms the prevalence and relevance of the identified concepts.

Conclusion: Avoiding Power Gaps is essential for effective pair programming. Practitioners should avoid hierarchical behaviors and focus on equalizing behaviors to enhance collaboration and outcomes.

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [269] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: The paper presents SPENCER, a framework combining dual-encoder and cross-encoder models for improving code retrieval efficiency and accuracy, leveraging self-adaptive model distillation techniques.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of dual-encoder models in code retrieval tasks, which lack interaction between code snippets and natural language queries during training.

Method: SPENCER employs a dual-encoder to narrow search space and a cross-encoder for accuracy improvements, alongside a novel model distillation technique with adaptive teaching assistant selection.

Result: SPENCER demonstrates superior performance compared to solely dual-encoder-based approaches, achieving over 98% of performance while significantly reducing inference time by 70%.

Conclusion: The combination of dual-encoder and cross-encoder frameworks, coupled with adaptive model distillation, enhances both accuracy and efficiency in code retrieval tasks.

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [270] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: User feedback is abundant but often irrelevant for detecting severe issues in large-scale online service systems. Filtering irrelevant feedback and adopting machine learning can improve issue detection.


<details>
  <summary>Details</summary>
Motivation: Large-scale service systems face challenges in identifying severe issues due to the vast amount of user feedback, most of which is irrelevant.

Method: Conduct an empirical study analyzing 50,378,766 user feedback items across six real-world services within an online service with over a billion users.

Result: The study reveals that most feedback is unrelated to system issues, certain feedback features are insufficient for detecting severe issues, and feedback topic distributions over time are stable, indicating suitability for machine learning analysis.

Conclusion: Findings establish an empirical basis for designing machine learning-based issue detection approaches, aiming for practical applications in large-scale service systems.

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [271] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: The paper introduces MCeT, an automated tool for evaluating the correctness of sequence diagrams against textual requirements using a fine-grained multi-perspective approach enabled by Large Language Models (LLMs). It significantly enhances issue identification precision and coverage compared to directly using LLMs.


<details>
  <summary>Details</summary>
Motivation: With the increased use of LLMs for designing behavioral model diagrams, automated correctness evaluation tools are essential to assess these AI-generated models and provide feedback for refinement, aiding human engineers and enabling AI assistants to self-improve.

Method: The authors propose MCeT, which uses a fine-grained, multi-perspective approach, splitting diagrams and requirements text into atomic components for detailed comparison. They utilize LLMs and introduce a self-consistency checking mechanism to mitigate hallucinations.

Result: The method improves precision from 0.58 to 0.81 compared to direct use of LLMs and detects 90% more issues than experienced engineers, identifying 6 additional issues per diagram.

Conclusion: MCeT substantially enhances the accuracy and reliability of model correctness evaluations, providing a better solution than current LLM capabilities for analyzing sequence diagrams against textual requirements.

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [272] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: The paper evaluates the quality of code generated by Large Language Models (LLMs) compared to human-written code using metrics like reliability and maintainability. It finds LLM-generated code generally exhibits fewer bugs but introduces critical issues in complex coding tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are increasingly used for code generation, but the quality and reliability of their code compared to traditional human coding remain unclear.

Method: The study analyzes Python code across three difficulty levels using different LLM configurations and SonarQube for quality metrics assessment, focusing on maintainability, reliability, and effort to fix code issues.

Result: LLM-generated code has fewer bugs and requires less effort to address them but can introduce critical issues and structural flaws, especially in complex coding tasks.

Conclusion: LLMs show promise in simplifying coding tasks but require systematic validation and improvement, particularly for high-complexity scenarios.

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [273] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: This paper introduces an automated method for checking if concrete process models adhere to reference models by using causal dependency analysis, addressing shortcomings in expressiveness and automation in current methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring adherence to reference models is critical for maintaining quality and consistency in processes, yet current approaches lack automation and semantic comparison for process model conformance.

Method: The paper proposes an algorithm and integrates it into a semantic framework for conformance checking using causal dependency analysis of tasks and events.

Result: The algorithm was evaluated through a case study and demonstrated improved accuracy and flexibility in verifying conformance to reference models.

Conclusion: The research offers a tool-assisted solution to enhance the verification process for maintaining alignment with reference models while noting areas for improvement.

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [274] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: This paper explores Dynamic Symbolic Execution (DSE) for analyzing semantic differences in MontiArc models, enhancing model-driven system analysis, though scalability challenges persist.


<details>
  <summary>Details</summary>
Motivation: Current model-driven development lacks robust methods for ensuring correctness and consistency as models evolve. Semantic difference analysis using DSE is investigated as a solution.

Method: Enhanced MontiArc-to-Java generator is utilized to collect symbolic and concrete runtime execution data for analysis, along with evaluating strategies for runtime efficiency and completeness.

Result: The study establishes a framework for DSE applicability in semantic analysis, noting promising insights into system behavior but identifying scalability as a key challenge.

Conclusion: DSE offers potential in semantic difference analysis for component-and-connector architectures but requires further research to address scalability issues for larger systems.

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [275] [On the utility of toy models for theories of consciousness](https://arxiv.org/abs/2508.00190)
*Larissa Albantakis*

Main category: q-bio.NC

TL;DR: The paper explores the usefulness of toy models in evaluating and developing consciousness theories, focusing on Integrated Information Theory (IIT) and Global Workspace Theory (GWT).


<details>
  <summary>Details</summary>
Motivation: To investigate how toy models can simplify abstract concepts and help clarify, test, and develop scientific theories of consciousness.

Method: The study draws primarily on examples from IIT and GWT, utilizing toy models to analyze theoretical consistency and explore experiential features of consciousness.

Result: The use of toy models demonstrates their potential to make abstract frameworks tangible, address specific experiential features, and sharpen philosophical debates within consciousness research.

Conclusion: Toy models are effective tools for bridging theoretical abstraction and empirical research, offering valuable insights for constructing comprehensive theories of consciousness.

Abstract: Toy models are highly idealized and deliberately simplified models that
retain only the essential features of a system in order to explore specific
theoretical questions. Long used in physics and other sciences, they have
recently begun to play a more visible role in consciousness research. This
chapter examines the potential utility of toy models for developing and
evaluating scientific theories of consciousness in terms of their ability to
clarify theoretical frameworks, test assumptions, and illuminate philosophical
challenges. Drawing primarily on examples from Integrated Information Theory
(IIT) and Global Workspace Theory (GWT), I show how these simplified systems
could make abstract concepts more tangible, enabling researchers to probe the
coherence, consistency, and implications of competing frameworks. In addition
to supporting theory development, toy models can also address specific features
of experience, as exemplified by the account of spatial extendedness and
temporal flow provided by integrated information theory (IIT) and recent
theory-independent structural approaches. Moreover, toy models bring
philosophical debates into sharper focus, such as the distinction between
functional and structural theories of consciousness. By bridging abstract
claims and empirical inquiry, toy models provide essential insights into the
challenges of building comprehensive theories of consciousness.

</details>


### [276] [State-switching navigation strategies in C. elegans are beneficial for chemotaxis](https://arxiv.org/abs/2508.00191)
*Kevin S. Chen,Andrew M. Leifer,Jonathan W. Pillow*

Main category: q-bio.NC

TL;DR: The study investigates how C. elegans navigate sensory environments by analyzing their state-switching behaviors, revealing that navigation involves persistent internal states guiding their sensory and motor strategies.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms and principles guiding sensory-driven navigation in animals, particularly focusing on when and how behavioral strategies such as 'steering' and 'turning' are determined and influenced by internal states and external sensory inputs.

Method: The authors used detailed measurements of sensory-guided navigation in C. elegans, developed a novel statistical model of state-dependent navigation, and applied genetic perturbations and reinforcement learning frameworks to examine underlying mechanisms and performance effects.

Result: The study identified two persistent internal states in C. elegans navigation: one enriched for steering and the other for turning. Transitions between these states, driven by sensory inputs, enhance gradient-climbing success. These findings challenge prior assumptions about static sensory-driven behaviors.

Conclusion: C. elegans employ a hierarchical, state-switching navigation strategy that integrates both internal states and external sensory inputs, enabling adaptive and efficient behaviors. This principle is likely applicable across different species and contexts.

Abstract: Animals employ different strategies for relating sensory input to behavioral
output to navigate sensory environments, but what strategy to use, when to
switch and why remain unclear. In C. elegans, navigation is composed of
'steering' and 'turns', corresponding to small heading changes and large
reorientation events, respectively. It is unclear whether transitions between
these elements are driven solely by sensory input or are influenced by internal
states that persist over time. It also remains unknown how worms accomplish
seemingly surprising feats of navigation--for example, worms appear to exit
turns correctly oriented toward a goal, despite their presumed lack of spatial
awareness during the turn. Here, we resolve these questions using detailed
measurements of sensory-guided navigation and a novel statistical model of
state-dependent navigation. We show that the worm's navigation is well
described by a sensory-driven state-switching model with two distinct states,
each persisting over many seconds and producing different mixtures of
sensorimotor relations. One state is enriched for steering, while the other is
enriched for turning. This hierarchical, temporal organization of strategies
challenges the previous assumption that strategies are static over time and
driven solely by immediate sensory input. Sensory input causally drives
transitions between these persistent internal states, and creates the
appearance of 'directed turns.' Genetic perturbations and a data-constrained
reinforcement learning model demonstrate that state-switching enhances
gradient-climbing performance. By combining measurement, perturbation, and
modeling, we show that state-switching plays a functionally beneficial role in
organizing behavior over time--a principle likely to generalize across species
and contexts.

</details>


### [277] [The Repeated-Stimulus Confound in Electroencephalography](https://arxiv.org/abs/2508.00531)
*Jack A. Kilgallen,Barak A. Pearlmutter,Jeffrey Mark Siskind*

Main category: q-bio.NC

TL;DR: The paper identifies the repeated-stimulus confound in neural-decoding studies and finds that decoding accuracy in affected studies is overestimated due to evaluation biases.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the growing use of deep-learning models in neural-decoding studies, which rely on large datasets potentially introducing biases due to repeated stimuli.

Method: The authors examined a susceptible dataset, reviewed 16 publications, and conducted experiments to quantify the extent of accuracy overestimation caused by the repeated-stimulus confound.

Result: Decoding accuracies were found to be overestimated by 4.46-7.42%, and a 1% increase in accuracy under the confound raises overestimation by 0.26%.

Conclusion: The repeated-stimulus confound not only inflates performance estimates but also undermines the validity of claims in affected studies, potentially enabling pseudoscientific assertions like extrasensory perception.

Abstract: In neural-decoding studies, recordings of participants' responses to stimuli
are used to train models. In recent years, there has been an explosion of
publications detailing applications of innovations from deep-learning research
to neural-decoding studies. The data-hungry models used in these experiments
have resulted in a demand for increasingly large datasets. Consequently, in
some studies, the same stimuli are presented multiple times to each participant
to increase the number of trials available for use in model training. However,
when a decoding model is trained and subsequently evaluated on responses to the
same stimuli, stimulus identity becomes a confounder for accuracy. We term this
the repeated-stimulus confound. We identify a susceptible dataset, and 16
publications which report model performance based on evaluation procedures
affected by the confound. We conducted experiments using models from the
affected studies to investigate the likely extent to which results in the
literature have been misreported. Our findings suggest that the decoding
accuracies of these models were overestimated by between 4.46-7.42%. Our
analysis also indicates that per 1% increase in accuracy under the confound,
the magnitude of the overestimation increases by 0.26%. The confound not only
results in optimistic estimates of decoding performance, but undermines the
validity of several claims made within the affected publications. We conducted
further experiments to investigate the implications of the confound in
alternative contexts. We found that the same methodology used within the
affected studies could also be used to justify an array of pseudoscientific
claims, such as the existence of extrasensory perception.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [278] [funOCLUST: Clustering Functional Data with Outliers](https://arxiv.org/abs/2508.00110)
*Katharine M. Clark,Paul D. McNicholas*

Main category: stat.ML

TL;DR: The paper introduces an extension of the OCLUST algorithm for robust clustering of functional data, focusing on both curve clustering and outlier identification.


<details>
  <summary>Details</summary>
Motivation: Clustering functional data is challenging due to their infinite-dimensional nature and susceptibility to outliers, necessitating more robust methods.

Method: The researchers adapt the OCLUST framework to the functional data setting, making it resistant to outliers while clustering and trimming data effectively.

Result: The proposed methodology shows strong performance in clustering and outlier identification across both simulated and real-world datasets.

Conclusion: The extended OCLUST algorithm provides a robust and effective approach for clustering functional data and dealing with outliers.

Abstract: Functional data present unique challenges for clustering due to their
infinite-dimensional nature and potential sensitivity to outliers. An extension
of the OCLUST algorithm to the functional setting is proposed to address these
issues. The approach leverages the OCLUST framework, creating a robust method
to cluster curves and trim outliers. The methodology is evaluated on both
simulated and real-world functional datasets, demonstrating strong performance
in clustering and outlier identification.

</details>


### [279] [Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.00247)
*Sergei Gleyzer,Hanh Nguyen,Dinesh P. Ramakrishnan,Eric A. F. Reinhardt*

Main category: stat.ML

TL;DR: The paper introduces a new variant of Kolmogorov-Arnold Networks (KANs) using weighted sinusoidal functions to model multivariable functions, proving its validity and showing competitive performance to multilayer perceptrons (MLPs).


<details>
  <summary>Details</summary>
Motivation: To explore novel architectures beyond traditional multilayer perceptrons, leveraging the Kolmogorov-Arnold representation theorem to innovate activation functions and improve multivariable function approximations.

Method: The proposed method replaces inner and outer functions in the Kolmogorov-Arnold representation with learnable weighted sinusoidal functions, fixing the phases to constant values and proving the framework's theoretical validity. Numerical experiments were conducted comparing its performance to MLPs and fixed-frequency Fourier transform methods.

Result: The proposed method outperformed fixed-frequency Fourier transform techniques and performed comparably to multilayer perceptrons across a range of multivariable functions in numerical evaluations.

Conclusion: The sinusoidal-function-based Kolmogorov-Arnold Networks present a theoretically valid and practically effective alternative to traditional MLPs, offering a new avenue for multivariable function approximation.

Abstract: The Kolmogorov-Arnold representation theorem states that any continuous
multivariable function can be exactly represented as a finite superposition of
continuous single variable functions. Subsequent simplifications of this
representation involve expressing these functions as parameterized sums of a
smaller number of unique monotonic functions. These developments led to the
proof of the universal approximation capabilities of multilayer perceptron
networks with sigmoidal activations, forming the alternative theoretical
direction of most modern neural networks.
  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an
alternative to multilayer perceptrons. KANs feature learnable nonlinear
activations applied directly to input values, modeled as weighted sums of basis
spline functions. This approach replaces the linear transformations and
sigmoidal post-activations used in traditional perceptrons. Subsequent works
have explored alternatives to spline-based activations. In this work, we
propose a novel KAN variant by replacing both the inner and outer functions in
the Kolmogorov-Arnold representation with weighted sinusoidal functions of
learnable frequencies. Inspired by simplifications introduced by Lorentz and
Sprecher, we fix the phases of the sinusoidal activations to linearly spaced
constant values and provide a proof of its theoretical validity. We also
conduct numerical experiments to evaluate its performance on a range of
multivariable functions, comparing it with fixed-frequency Fourier transform
methods and multilayer perceptrons (MLPs). We show that it outperforms the
fixed-frequency Fourier transform and achieves comparable performance to MLPs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [280] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: FedGuard is a federated learning mechanism that uses server-specified mini-batch data to identify and exclude poisoned models, outperforming other schemes in mitigating Byzantine attacks under non-IID data and high malicious client ratios.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning frameworks are vulnerable to Byzantine attacks, especially with highly non-IID datasets or when over half of the clients are malicious.

Method: FedGuard uses server-specified mini-batch data to evaluate client models' bias and exclude poisoned models based on their dropped confidence.

Result: Comprehensive evaluations show FedGuard's superior performance under non-IID datasets, 90% malicious clients, and diverse attack types.

Conclusion: FedGuard effectively addresses limitations of prior defense methods by leveraging model bias sensitivity, ensuring robustness against a wide range of Byzantine attacks.

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [281] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: The paper reviews Dynamic Binary Instrumentation (DBI), its techniques, and tools, compares their capabilities, and evaluates them across different scenarios.


<details>
  <summary>Details</summary>
Motivation: DBI is crucial for monitoring and modifying compiled binaries during run-time and is widely used in both academia and industry. The paper aims to clarify the strengths and weaknesses of various DBI techniques.

Method: The paper categorizes DBI techniques, compares their instrumentation capabilities across different primitives and runtime events, and evaluates their performance using benchmarks.

Result: The findings reveal that no single DBI technique is consistently superior in all use cases. Different approaches excel under specific circumstances.

Conclusion: The paper concludes that DBI approaches must be chosen based on the context of use since their efficacy varies depending on the requirements and environment.

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


### [282] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: The paper introduces a two-stage jailbreak framework called AGILE to improve attack success against language models while overcoming flaws in existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing jailbreak techniques, such as incoherence, poor transferability, and reliance on manual effort, when analyzing language model vulnerabilities.

Method: The proposed framework combines scenario-based context generation with rephrased malicious queries and fine-grained edits informed by the model's hidden states, effectively steering malicious queries toward benign ones.

Result: The method achieves up to 37.74% greater attack success over baselines and demonstrates strong transferability to black-box models, maintaining effectiveness against prominent defense mechanisms.

Conclusion: AGILE highlights the limitations of current defense measures, providing a more robust method for testing and mitigating vulnerabilities, with insights for future safeguard development.

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [283] [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
*Xinzhang Chen,Hassan Ali,Arash Shaghaghi,Salil S. Kanhere,Sanjay Jha*

Main category: cs.CR

TL;DR: The paper introduces TOSense, a Chrome extension for real-time natural language queries on Terms of Service documents, using cutting-edge NLP tools and novel evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Users struggle to interpret lengthy and opaque Terms of Service documents, resulting in legal risks and information imbalance. The paper aims to address this issue via an accessible NLP-based tool.

Method: The proposed system, TOSense, comprises a crawler ('tos-crawl') that extracts ToS content and a lightweight NLP pipeline using MiniLM for semantic retrieval and BART-encoder for relevance verification. A novel Question Answering Evaluation Pipeline (QEP) generates synthetic questions for testing.

Result: Experiments conducted on platforms like Apple, Google, and Netflix show the system achieves up to 44.5% accuracy in retrieving answers across diverse topics.

Conclusion: TOSense effectively enables users to interactively query ToS documents, mitigating information asymmetry while reducing manual annotation costs through synthetic question generation.

Abstract: Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

</details>


### [284] [CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization](https://arxiv.org/abs/2508.00478)
*Yuning Jiang,Nay Oo,Qiaoran Meng,Lu Lin,Dusit Niyato,Zehui Xiong,Hoon Wei Lim,Biplab Sikdar*

Main category: cs.CR

TL;DR: CyGATE introduces a game-theoretic approach enhancing cyber defense against dynamic threats by leveraging large language models with retrieval-augmented generation (RAG).


<details>
  <summary>Details</summary>
Motivation: Address gaps in current cyber defense systems that rely on static assumptions and fail to integrate real-time threat intelligence.

Method: Utilize a partially observable stochastic game (POSG) framework modeled using LLMs with RAG, applied to dynamic cyber conflict scenarios, enabling adaptive patch prioritization.

Result: CyGATE achieves efficient prioritization of high-risk vulnerabilities through dynamic threat integration, strategic anticipation, and optimized resource use.

Conclusion: By combining advanced modeling with real-time intelligence, CyGATE demonstrates enhanced adaptability and resource efficiency in dynamic cyber defense scenarios.

Abstract: Modern cyber attacks unfold through multiple stages, requiring defenders to
dynamically prioritize mitigations under uncertainty. While game-theoretic
models capture attacker-defender interactions, existing approaches often rely
on static assumptions and lack integration with real-time threat intelligence,
limiting their adaptability. This paper presents CyGATE, a game-theoretic
framework modeling attacker-defender interactions, using large language models
(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection
and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber
conflicts as a partially observable stochastic game (POSG) across Cyber Kill
Chain stages. Both agents use belief states to navigate uncertainty, with the
attacker adapting tactics and the defender re-prioritizing patches based on
evolving risks and observed adversary behavior. The framework's flexible
architecture enables extension to multi-agent scenarios involving coordinated
attackers, collaborative defenders, or complex enterprise environments with
multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE
effectively prioritizes high-risk vulnerabilities, enhancing adaptability
through dynamic threat integration, strategic foresight by anticipating
attacker moves under uncertainty, and efficiency by optimizing resource use.

</details>


### [285] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: The paper proposes a cybersecurity framework using Evidential Deep Learning (EDL) to infer attack stages, emphasizing robust detection of out-of-distribution inputs.


<details>
  <summary>Details</summary>
Motivation: Traditional binary classification for malicious activity lacks an attack-stage perspective essential for dynamic and effective countermeasures.

Method: The study employs EDL to model uncertainty by generating Dirichlet distributions for potential attack stages, enhancing stage inference and identifying OOD inputs.

Result: Experiments show the model effectively detects attack stages with calibrated confidence and identifies OOD inputs that signal tactic shifts.

Conclusion: Uncertainty-aware models using EDL are promising for dynamic and adversarial staged threat detection.

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [286] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: This paper examines security threats in LLMs, focusing on jailbreak attacks and sensitive information leakage, and introduces methodologies for forensic insights and active defenses.


<details>
  <summary>Details</summary>
Motivation: Concerns around the security vulnerabilities of LLMs, including jailbreak attacks and data leakage exacerbated by RAG, necessitate robust defensive measures.

Method: The authors propose a two-part approach: analyzing LLM interaction data to map usage patterns and introducing LeakSealer, a model-agnostic framework combining static analysis with dynamic defenses.

Result: LeakSealer shows high precision and recall in detecting adversarial prompts in ToxicChat dataset and achieves an AUPRC of 0.97 in detecting PII leakage, outperforming existing systems.

Conclusion: LeakSealer provides effective defense against jailbreaking and sensitive data leakage, proving its utility through empirical tests in both static and dynamic scenarios.

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [287] [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/abs/2508.00398)
*Sunjae Yoon,Gwanhyeong Koo,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: The paper introduces an Occlusion-robust Stylization Framework (OSF) for creating drawing-based 3D animations, addressing challenges like style degradation under occlusions.


<details>
  <summary>Details</summary>
Motivation: To address issues of style property degradation, such as contour flickering and stroke blurring, caused by occlusions in drawing-based 3D animation systems.

Method: They propose OSF which leverages optical flow for occlusion-robust edge guidance, enabling consistent stylization even with occlusions, and operates as a single-pass system.

Result: OSF achieves higher stylization consistency, 2.4x faster inference, and 2.1x reduced memory usage compared to previous two-stage methods.

Conclusion: OSF effectively addresses the stylization pose gap, enhancing visual quality and efficiency for drawing-based 3D animation systems, particularly under complex occlusion scenarios.

Abstract: 3D animation aims to generate a 3D animated video from an input image and a
target 3D motion sequence. Recent advances in image-to-3D models enable the
creation of animations directly from user-hand drawings. Distinguished from
conventional 3D animation, drawing-based 3D animation is crucial to preserve
artist's unique style properties, such as rough contours and distinct stroke
patterns. However, recent methods still exhibit quality deterioration in style
properties, especially under occlusions caused by overlapping body parts,
leading to contour flickering and stroke blurring. This occurs due to a
`stylization pose gap' between training and inference in stylization networks
designed to preserve drawing styles in drawing-based 3D animation systems. The
stylization pose gap denotes that input target poses used to train the
stylization network are always in occlusion-free poses, while target poses
encountered in an inference include diverse occlusions under dynamic motions.
To this end, we propose Occlusion-robust Stylization Framework (OSF) for
drawing-based 3D animation. We found that while employing object's edge can be
effective input prior for guiding stylization, it becomes notably inaccurate
when occlusions occur at inference. Thus, our proposed OSF provides
occlusion-robust edge guidance for stylization network using optical flow,
ensuring a consistent stylization even under occlusions. Furthermore, OSF
operates in a single run instead of the previous two-stage method, achieving
2.4x faster inference and 2.1x less memory.

</details>


### [288] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: The paper introduces SpA2V, a framework for generating realistic videos from audio by incorporating spatial auditory cues alongside semantic ones.


<details>
  <summary>Details</summary>
Motivation: Human capability to visualize based on sound includes understanding spatial attributes (e.g., location, movement), which is often ignored by existing audio-to-video generation methods.

Method: SpA2V employs a two-stage approach: (1) Audio-guided Video Planning to extract spatial and semantic cues in the form of Video Scene Layouts (VSLs), and (2) Layout-grounded Video Generation to conditionally guide pre-trained diffusion models.

Result: Experiments show that SpA2V generates videos with a high degree of semantic and spatial alignment to the input audio, outperforming existing methods.

Conclusion: By explicitly incorporating spatial auditory cues, SpA2V advances the state-of-the-art in audio-driven video generation by achieving improved realism and alignment between audio and video.

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [289] [Formal Power Series Representations in Probability and Expected Utility Theory](https://arxiv.org/abs/2508.00294)
*Arthur Paul Pedersen,Samuel Allen Alexander*

Main category: math.PR

TL;DR: The paper introduces a general theory of coherent preferences, relaxing standard conditions like transitivity and continuity, and presents its representation by utility within ordered field extensions of the reals.


<details>
  <summary>Details</summary>
Motivation: Current preference theories impose strict conditions (like transitivity and continuity) which may not align with how preferences operate in practice. The paper seeks to develop a more generalized and flexible framework.

Method: The authors establish a coherence requirement akin to de Finetti's framework for probability, and prove a key result extending H"older's Theorem and strengthening Hahn's Embedding Theorem.

Result: The new theory allows any coherent preference system to admit extension to a complete system. It also demonstrates that any complete coherent preference can be represented by utility in an ordered field extension of the real numbers.

Conclusion: The proposed framework broadens the scope of preference theory by removing traditional restrictions while maintaining coherence, providing a robust utility representation framework.

Abstract: We advance a general theory of coherent preference that surrenders
restrictions embodied in orthodox doctrine. This theory enjoys the property
that any preference system admits extension to a complete system of
preferences, provided it satisfies a certain coherence requirement analogous to
the one de Finetti advanced for his foundations of probability. Unlike de
Finetti's theory, the one we set forth requires neither transitivity nor
Archimedeanness nor boundedness nor continuity of preference. This theory also
enjoys the property that any complete preference system meeting the standard of
coherence can be represented by utility in an ordered field extension of the
reals. Representability by utility is a corollary of this paper's central
result, which at once extends H\"older's Theorem and strengthens Hahn's
Embedding Theorem.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [290] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: The paper investigates decentralized secure aggregation (DSA) in federated learning, characterizing the optimal communication and key usage rates required to ensure data security in the absence of a central aggregator.


<details>
  <summary>Details</summary>
Motivation: To address the limited understanding of information-theoretic limits and optimal communication and key usage in decentralized secure aggregation systems for federated learning.

Method: The study examines a network of K users in a DSA framework, each with private inputs, aiming to securely compute the sum of inputs while preventing information leakage under collusion. It characterizes the rate region representing minimal communication and secret key requirements.

Result: The results demonstrate that securely computing one input symbol requires each user to transmit at least one symbol, hold one symbol of a secret key, and collectively possess at least K-1 independent key symbols.

Conclusion: This work establishes the foundational limits for DSA, aiding in the development of secure and communication-efficient protocols for distributed systems.

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


### [291] [Towards a Measure Theory of Semantic Information](https://arxiv.org/abs/2508.00525)
*George M. Coghill*

Main category: cs.IT

TL;DR: This paper critiques Floridi's theory on semantic information and proposes a new model based on the unit circle to resolve its issues, including the Bar-Hillel-Carnap paradox.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address shortcomings in Floridi's theory of strongly semantic information, specifically its failure to resolve the Bar-Hillel-Carnap paradox while adhering to his stipulated requirements.

Method: The author critiques Floridi's approach using its own framework, and develops a new model leveraging the unit circle and quantum probability concepts to measure informativeness.

Result: The new unit circle-based approach successfully removes the Bar-Hillel-Carnap paradox, aligns with Floridi's informativeness requirements, and demonstrates practical utility where contradictory messages are equally informative.

Conclusion: The paper concludes that this novel unit circle-based model overcomes both the paradox and critical limitations of Floridi's theory, offering a more robust measure of informativeness.

Abstract: A classic account of the quantification of semantic information is that of
Bar-Hiller and Carnap. Their account proposes an inverse relation between the
informativeness of a statement and its probability. However, their approach
assigns the maximum informativeness to a contradiction: which Floridi refers to
as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a
distance metric and parabolic relation, designed to remove this paradox.
Unfortunately is approach does not succeed in that aim.
  In this paper I critique Floridi's theory of strongly semantic information on
its own terms and show where it succeeds and fails. I then present a new
approach based on the unit circle (a relation that has been the basis of
theories from basic trigonometry to quantum theory). This is used, by analogy
with von Neumann's quantum probability to construct a measure space for
informativeness that meets all the requirements stipulated by Floridi and
removes the paradox. In addition, while contradictions and tautologies have
zero informativeness, it is found that messages which are contradictory to each
other are equally informative. The utility of this is explained by means of an
example.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [292] [Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation](https://arxiv.org/abs/2508.00017)
*Nikolai Sergeev*

Main category: cs.LO

TL;DR: This paper introduces Generative Logic (GL), a deterministic system for exploring and proving mathematical laws starting from axiomatic definitions. GL systematically generates proofs that are machine-checkable and includes navigable exports to HTML.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and verifiable system for automatically exploring and proving mathematical theorems, starting with basic axioms and generating independently inspectable proofs.

Method: The authors developed a system (GL) where axiomatic definitions are written in Mathematical Programming Language (MPL) and interpreted by a distributed grid of Logic Blocks (LBs). The system generates proofs by deducing new facts, applies normalization and type checks, and outputs proofs that are fully auditable in HTML format.

Result: GL successfully reconstructs proofs for foundational arithmetic laws starting from Peano axioms. These include addition associativity, multiplication commutativity, and distributivity. The proofs are machine-checkable and accessible as HTML files.

Conclusion: The GL framework provides a scalable, deterministic solution for mathematical exploration and proof generation. It offers potential for applications in massively parallel hardware and could integrate with probabilistic models for conjecture and theorem generation.

Abstract: We present Generative Logic (GL), a deterministic architecture that begins
from user-supplied axiomatic definitions -- written in a minimalist
Mathematical Programming Language (MPL) -- and systematically explores their
deductive neighborhood. Definitions are compiled into a distributed grid of
simple Logic Blocks (LBs) that exchange messages; any time several expressions
unify under an inference rule, a new fact is emitted with full provenance to
its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order
Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate
implications, applies normalization and type filters, and automatically
reconstructs machine-checkable proofs of foundational arithmetic laws including
associativity and commutativity of addition, associativity and commutativity of
multiplication, and distributivity. Generated proofs export to navigable HTML
so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel
realizations and describe prospective integration with probabilistic models
(e.g., Large Language Models (LLMs)) for autoformalization and conjecture
seeding. The Python and MPL code to reproduce the Peano experiments, along with
the full HTML proof graphs, are available in the project's GitHub repository at
https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724
and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We
invite community feedback and collaboration.

</details>


### [293] [Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers](https://arxiv.org/abs/2508.00419)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.LO

TL;DR: The paper investigates whether reasoning-optimized large language models (LLMs) integrated with SMT solvers can improve loop invariant synthesis. The framework achieves state-of-the-art results on a benchmark, proving the latent logical reasoning strengths of LLMs.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing loop invariants for program correctness is significant, with existing methods still falling short for arbitrary programs.

Method: The paper employs reasoning-optimized LLMs in combination with an SMT solver (Z3) in a generate-and-check framework. Counterexamples are used iteratively to refine loop invariants.

Result: Using the Code2Inv benchmark, the proposed framework achieves 100% task coverage (133/133), outperforming prior approaches (107/133 tasks) with minimal model proposals per instance and reasonable compute time.

Conclusion: The study demonstrates that LLMs have untapped logical reasoning capabilities which can advance loop invariant synthesis, with potential generalizability to other imperative programming languages.

Abstract: Loop invariants are essential for proving the correctness of programs with
loops. Developing loop invariants is challenging, and fully automatic synthesis
cannot be guaranteed for arbitrary programs. Some approaches have been proposed
to synthesize loop invariants using symbolic techniques and more recently using
neural approaches. These approaches are able to correctly synthesize loop
invariants only for subsets of standard benchmarks. In this work, we
investigate whether modern, reasoning-optimized large language models can do
better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled
generate-and-check pipeline with the Z3 SMT solver, using solver
counterexamples to iteratively guide invariant refinement. We use Code2Inv
benchmark, which provides C programs along with their formal preconditions and
postconditions. On this benchmark of 133 tasks, our framework achieves 100%
coverage (133 out of 133), outperforming the previous best of 107 out of 133,
while requiring only 1-2 model proposals per instance and 14-55 seconds of
wall-clock time. These results demonstrate that LLMs possess latent logical
reasoning capabilities which can help automate loop invariant synthesis. While
our experiments target C-specific programs, this approach should be
generalizable to other imperative languages.

</details>


### [294] [Analysing Temporal Reasoning in Description Logics Using Formal Grammars](https://arxiv.org/abs/2508.00575)
*Camille Bourgaux,Anton Gnatenko,Michaël Thomazo*

Main category: cs.LO

TL;DR: This paper explores the connection between a temporal description logic ($\mathcal{TEL}^\bigcirc$) and specific formal grammars, leading to insights about its decidability and periodicity properties.


<details>
  <summary>Details</summary>
Motivation: The authors aim to close an open question regarding the decidability of query answering in $\mathcal{TEL}^\bigcirc$ and explore its periodicity properties.

Method: They establish correspondence between $\mathcal{TEL}^\bigcirc$ and conjunctive grammars, leveraging grammar-based tools to analyze $\mathcal{TEL}^\bigcirc$.

Result: The findings show that $\mathcal{TEL}^\bigcirc$ lacks ultimate periodicity and query answering is undecidable. However, decidability is proven for specific fragments of $\mathcal{TEL}^\bigcirc$.

Conclusion: This work advances understanding of $\mathcal{TEL}^\bigcirc$ by establishing key properties and enabling decidability analysis for certain fragments using existing grammar tools.

Abstract: We establish a correspondence between (fragments of)
$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$
description logic with the LTL operator $\bigcirc^k$, and some specific kinds
of formal grammars, in particular, conjunctive grammars (context-free grammars
equipped with the operation of intersection). This connection implies that
$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicity
of models, and further leads to undecidability of query answering in
$\mathcal{TEL}^\bigcirc$, closing a question left open since the introduction
of $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidability
of query answering for some new interesting fragments of
$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools and
algorithms for conjunctive grammars.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [295] [Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning](https://arxiv.org/abs/2508.00024)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mario Bifulco,Carlos Andrés Durán,Cristian Bosch,Ricardo Simón Carbajo*

Main category: quant-ph

TL;DR: This paper proposes a quantum-classical pipeline combining Vision Transformer (ViT) embeddings and quantum Support Vector Machines (SVMs) to improve scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Quantum Support Vector Machines (SVMs) are constrained by scalability issues due to high-dimensional quantum states and hardware limitations.

Method: The authors create a quantum-classical approach using class-balanced k-means distillation and pretrained Vision Transformer (ViT) embeddings, evaluated via tensor network simulations with cuTensorNet.

Result: The proposed system achieves accuracy improvements of 8.02% on Fashion-MNIST and 4.42% on MNIST compared to classical SVMs. It shows that ViT embeddings uniquely enhance quantum advantage, unlike CNN features.

Conclusion: This study demonstrates that the synergy between transformer embeddings and quantum kernel methods provides a pathway for scalable quantum machine learning leveraging modern architectures.

Abstract: Quantum Support Vector Machines face scalability challenges due to
high-dimensional quantum states and hardware limitations. We propose an
embedding-aware quantum-classical pipeline combining class-balanced k-means
distillation with pretrained Vision Transformer embeddings. Our key finding:
ViT embeddings uniquely enable quantum advantage, achieving up to 8.02%
accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,
while CNN features show performance degradation. Using 16-qubit tensor network
simulation via cuTensorNet, we provide the first systematic evidence that
quantum kernel advantage depends critically on embedding choice, revealing
fundamental synergy between transformer attention and quantum feature spaces.
This provides a practical pathway for scalable quantum machine learning that
leverages modern neural architectures.

</details>


### [296] [Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems](https://arxiv.org/abs/2508.00027)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi,Yongli Ren,Jiayang Niu*

Main category: quant-ph

TL;DR: The paper proposes a hybrid machine learning algorithm for quantum recommenders that reduces qubit requirements drastically, making them feasible for current NISQ devices, while maintaining state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current quantum recommender systems are inefficient as they map one qubit per tag, requiring over a hundred qubits which is impractical for today's NISQ devices and leads to error-prone circuits.

Method: The authors introduce a three-stage hybrid algorithm: compress tag profiles using SVD sketching and k-means, optimize feature selection via QAOA under a fixed qubit constraint, and construct a Quantum semi-Random Forest (QsRF) model on just 5 qubits.

Result: The compressed tag representation achieves over 97% variance, and the algorithm effectively selects 5 optimal features using depth-3 QAOA for a 2020-QUBO problem. A 100-tree QsRF trained on these features matches the performance of baseline methods on datasets like ICM-150/500.

Conclusion: The approach significantly reduces the quantum resource demand while maintaining performance parity with advanced classical methods, making quantum recommenders feasible for current hardware standards.

Abstract: Modern recommenders describe each item with hundreds of sparse semantic tags,
yet most quantum pipelines still map one qubit per tag, demanding well beyond
one hundred qubits, far out of reach for current noisy-intermediate-scale
quantum (NISQ) devices and prone to deep, error-amplifying circuits. We close
this gap with a three-stage hybrid machine learning algorithm that compresses
tag profiles, optimizes feature selection under a fixed qubit budget via QAOA,
and scores recommendations with a Quantum semi-Random Forest (QsRF) built on
just five qubits, while performing similarly to the state-of-the-art methods.
Leveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \%
variance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A
100-tree QsRF trained on these codes matches full-feature baselines on
ICM-150/500.

</details>


### [297] [Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins](https://arxiv.org/abs/2508.00029)
*Azadeh Alavi,Sanduni Jayasinghe,Mojtaba Mahmoodian,Sam Mazaheri,John Thangarajah,Sujeeva Setunge*

Main category: quant-ph

TL;DR: This paper introduces a quantum-classical hybrid method for structural health monitoring (SHM) to improve efficiency in large-scale structures using quantum-enhanced neural networks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing inefficiencies in computational costs and challenges of inverse finite element (FE) analysis in structural health monitoring for critical civil structures, aiming for better real-time problem-solving.

Method: A hybrid quantum-classical multilayer perceptron (QMLP) framework integrates symmetric positive definite (SPD) matrices, polynomial features, parameterized quantum circuits (PQC), and classical neural networks for enhanced data representation and inference.

Result: Experiments demonstrate that the QMLP achieves extremely low mean squared error (MSE, 0.0000000000316), significantly improving performance compared to classical approaches in structural health monitoring of a bridge.

Conclusion: The QMLP framework showcases quantum-enhanced capabilities for more efficient and scalable real-time SHM, paving the way for robust digital twins in structural applications.

Abstract: Large-scale civil structures, such as bridges, pipelines, and offshore
platforms, are vital to modern infrastructure, where unexpected failures can
cause significant economic and safety repercussions. Although finite element
(FE) modeling is widely used for real-time structural health monitoring (SHM),
its high computational cost and the complexity of inverse FE analysis, where
low dimensional sensor data must map onto high-dimensional displacement or
stress fields pose ongoing challenges. Here, we propose a hybrid quantum
classical multilayer perceptron (QMLP) framework to tackle these issues and
facilitate swift updates to digital twins across a range of structural
applications.
  Our approach embeds sensor data using symmetric positive definite (SPD)
matrices and polynomial features, yielding a representation well suited to
quantum processing. A parameterized quantum circuit (PQC) transforms these
features, and the resultant quantum outputs feed into a classical neural
network for final inference. By fusing quantum capabilities with classical
modeling, the QMLP handles large scale inverse FE mapping while preserving
computational viability.
  Through extensive experiments on a bridge, we demonstrate that the QMLP
achieves a mean squared error (MSE) of 0.0000000000316, outperforming purely
classical baselines with a large margin. These findings confirm the potential
of quantum-enhanced methods for real time SHM, establishing a pathway toward
more efficient, scalable digital twins that can robustly monitor and diagnose
structural integrity in near real time.

</details>


### [298] [Dimension reduction with structure-aware quantum circuits for hybrid machine learning](https://arxiv.org/abs/2508.00048)
*Ammar Daskin*

Main category: quant-ph

TL;DR: The paper uses quantum circuits based on Schmidt decomposition for vector compression, enabling effective data approximation and hybrid machine learning model design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage tensor network decomposition for data compression and improve machine learning model efficiency by reducing parameters.

Method: The method involves using quantum circuits based on Schmidt decomposition to achieve $k$-rank approximations of data, followed by integration with a classical neural network.

Result: The experiments on scikit-learn datasets demonstrated that quantum circuits effectively compress data and provide $k$-rank approximations for machine learning.

Conclusion: Quantum circuits based on Schmidt decomposition can compress and approximate data, enabling efficient hybrid machine learning with reduced learnable parameters.

Abstract: Schmidt decomposition of a vector can be understood as writing the singular
value decomposition (SVD) in vector form. A vector can be written as a linear
combination of tensor product of two dimensional vectors by recursively
applying Schmidt decompositions via SVD to all subsystems. Given a vector
expressed as a linear combination of tensor products, using only the $k$
principal terms yields a $k$-rank approximation of the vector. Therefore,
writing a vector in this reduced form allows to retain most important parts of
the vector while removing small noises from it, analogous to SVD-based
denoising.
  In this paper, we show that quantum circuits designed based on a value $k$
(determined from the tensor network decomposition of the mean vector of the
training sample) can approximate the reduced-form representations of entire
datasets. We then employ this circuit ansatz with a classical neural network
head to construct a hybrid machine learning model. Since the output of the
quantum circuit for an $2^n$ dimensional vector is an $n$ dimensional
probability vector, this provides an exponential compression of the input and
potentially can reduce the number of learnable parameters for training
large-scale models. We use datasets provided in the Python scikit-learn module
for the experiments. The results confirm the quantum circuit is able to
compress data successfully to provide effective $k$-rank approximations to the
classical processing component.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [299] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: This paper introduces GEPAR3D, a new unified model improving tooth segmentation in CBCT scans, focusing on fine anatomical structures like root apices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in CBCT tooth segmentation, especially for complex structures such as root apices, which are critical for orthodontic root resorption assessment.

Method: The method combines instance detection and segmentation using GEPAR3D, integrating a Statistical Shape Model for anatomical context and a deep watershed approach for 3D energy basins of each tooth.

Result: GEPAR3D achieved an average Dice Similarity Coefficient of 95.0% (+2.8% over the second-best method) and recall of 95.2% (+9.5%) on external test datasets.

Conclusion: The paper concludes that GEPAR3D significantly improves root segmentation quality, enabling better assessment of root resorption for improved orthodontic decision-making, with tools available for public use.

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [300] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: The study proposes a weakly supervised 3D multi-task UNet integrating vesselness priors for improved intracranial aneurysm detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: Accurate detection and morphological analysis of intracranial aneurysms are crucial but challenging due to their small size, soft contrast in medical images, and lack of voxel-wise annotated data.

Method: The researchers used a weakly supervised 3D multi-task UNet model that incorporates Frangi's vesselness filter as priors for segmentation and detection tasks. The model was trained on coarse annotations in the Lausanne dataset and validated externally on the ADAM dataset.

Result: The method outperformed state-of-the-art techniques, achieving a Dice score of 0.614 and 95% Hausdorff Distance of 1.38mm for segmentation, and a false positive rate of 1.47 with a sensitivity of 92.9% for detection.

Conclusion: The proposed technique effectively improves both detection and segmentation of intracranial aneurysms, demonstrating strong generalizability and superior performance compared to existing approaches.

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [301] [CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography](https://arxiv.org/abs/2507.22953)
*Murong Xu,Tamaz Amiranashvili,Fernando Navarro,Maksym Fritsak,Ibrahim Ethem Hamamci,Suprosanna Shit,Bastian Wittmann,Sezgin Er,Sebastian M. Christ,Ezequiel de la Rosa,Julian Deseoe,Robert Graf,Hendrik Möller,Anjany Sekuboyina,Jan C. Peeken,Sven Becker,Giulia Baldini,Johannes Haubold,Felix Nensa,René Hosch,Nikhil Mirajkar,Saad Khalid,Stefan Zachow,Marc-André Weber,Georg Langs,Jakob Wasserthal,Mehmet Kemal Ozdemir,Andrey Fedorov,Ron Kikinis,Stephanie Tanadini-Lang,Jan S. Kirschke,Stephanie E. Combs,Bjoern Menze*

Main category: eess.IV

TL;DR: This paper introduces CADS, a groundbreaking AI framework for whole-body CT segmentation, leveraging a massive dataset with comprehensive anatomical coverage.


<details>
  <summary>Details</summary>
Motivation: Current AI segmentation models for CT scans are fragmented, focusing on individual structures, lacking performance consistency, and insufficient anatomical coverage.

Method: CADS integrates heterogeneous data sources into a large-scale dataset of 22,022 CT volumes, develops segmentation models using existing architectures, and validates performance against public datasets and real-world hospital data.

Result: CADS demonstrates superior segmentation accuracy over state-of-the-art models, proving its utility for clinical interventions in radiation oncology.

Conclusion: The open-source release of CADS, including its dataset, models, and tools, aims to enable robust AI-based anatomical analysis in radiology for clinicians and researchers.

Abstract: Accurate delineation of anatomical structures in volumetric CT scans is
crucial for diagnosis and treatment planning. While AI has advanced automated
segmentation, current approaches typically target individual structures,
creating a fragmented landscape of incompatible models with varying performance
and disparate evaluation protocols. Foundational segmentation models address
these limitations by providing a holistic anatomical view through a single
model. Yet, robust clinical deployment demands comprehensive training data,
which is lacking in existing whole-body approaches, both in terms of data
heterogeneity and, more importantly, anatomical coverage. In this work, rather
than pursuing incremental optimizations in model architecture, we present CADS,
an open-source framework that prioritizes the systematic integration,
standardization, and labeling of heterogeneous data sources for whole-body CT
segmentation. At its core is a large-scale dataset of 22,022 CT volumes with
complete annotations for 167 anatomical structures, representing a significant
advancement in both scale and coverage, with 18 times more scans than existing
collections and 60% more distinct anatomical targets. Building on this diverse
dataset, we develop the CADS-model using established architectures for
accessible and automated full-body CT segmentation. Through comprehensive
evaluation across 18 public datasets and an independent real-world hospital
cohort, we demonstrate advantages over SoTA approaches. Notably, thorough
testing of the model's performance in segmentation tasks from radiation
oncology validates its direct utility for clinical interventions. By making our
large-scale dataset, our segmentation models, and our clinical software tool
publicly available, we aim to advance robust AI solutions in radiology and make
comprehensive anatomical analysis accessible to clinicians and researchers
alike.

</details>


### [302] [Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection](https://arxiv.org/abs/2508.00438)
*Sumin Seo,In Kyu Lee,Hyun-Woo Kim,Jaesik Min,Chung-Hwan Jung*

Main category: eess.IV

TL;DR: The paper proposes a novel diffusion model-based augmentation technique for improving automated detection and classification of coronary stenosis, particularly in datasets with limited samples or class imbalance.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like limited labeled data and class imbalance in automated methods for detecting coronary stenosis using deep learning.

Method: The authors developed a diffusion model for lesion inpainting to create realistic synthetic lesions with user-guided control of severity as a data augmentation strategy.

Result: The method achieved superior performance in lesion detection and severity classification across both in-house and public datasets, even with limited training data.

Conclusion: The proposed approach enhances the reliability and efficiency of coronary stenosis evaluation, especially under conditions of data scarcity, thereby supporting more robust clinical decision-making.

Abstract: Coronary stenosis is a major risk factor for ischemic heart events leading to
increased mortality, and medical treatments for this condition require
meticulous, labor-intensive analysis. Coronary angiography provides critical
visual cues for assessing stenosis, supporting clinicians in making informed
decisions for diagnosis and treatment. Recent advances in deep learning have
shown great potential for automated localization and severity measurement of
stenosis. In real-world scenarios, however, the success of these competent
approaches is often hindered by challenges such as limited labeled data and
class imbalance. In this study, we propose a novel data augmentation approach
that uses an inpainting method based on a diffusion model to generate realistic
lesions, allowing user-guided control of severity. Extensive evaluation on
lesion detection and severity classification across various synthetic dataset
sizes shows superior performance of our method on both a large-scale in-house
dataset and a public coronary angiography dataset. Furthermore, our approach
maintains high detection and classification performance even when trained with
limited data, highlighting its clinical importance in improving the assessment
of severity of stenosis and optimizing data utilization for more reliable
decision support.

</details>


### [303] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug is a new plug-in framework that improves foundation flow-matching (FM) priors for solving ill-posed inverse problems, achieving superior performance in tasks like image super-resolution and Gaussian deblurring.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address challenges in leveraging domain-agnostic FM priors for ill-posed inverse problems, aiming to unlock their potential by enhancing their capabilities.

Method: FMPlug introduces two key enhancements: a time-adaptive warm-up strategy and sharp Gaussianity regularization, leveraging similarities between observed and desired objects and the Gaussian characteristics of generative flows.

Result: FMPlug significantly outperforms state-of-the-art methods utilizing foundation FM priors, particularly in image super-resolution and Gaussian deblurring tasks.

Conclusion: FMPlug demonstrates that domain-agnostic FM priors can be effectively adapted and optimized to solve complex inverse problems through its novel design approach.

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


### [304] [AI-Driven Collaborative Satellite Object Detection for Space Sustainability](https://arxiv.org/abs/2508.00755)
*Peng Hu,Wenxuan Zhang*

Main category: eess.IV

TL;DR: The paper proposes a novel satellite clustering framework using collaborative deep learning-based space object detection (SOD) to tackle the rising issues of in-orbit satellite collision risks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional ground-based tracking systems in mitigating in-orbit collision risks due to increasing satellite density in low-Earth orbit (LEO).

Method: The authors developed a satellite clustering framework that uses collaborative deep learning for space object detection (SOD). They created a high-fidelity dataset simulating satellite imaging and introduced a distance-aware viewpoint selection strategy to optimize detection.

Result: The proposed method achieved competitive detection accuracy comparable to alternatives while maintaining low size, weight, and power constraints.

Conclusion: The clustering-based approach shows its potential to enhance space situational awareness and promote space sustainability through distributed, AI-enabled in-orbit systems.

Abstract: The growing density of satellites in low-Earth orbit (LEO) presents serious
challenges to space sustainability, primarily due to the increased risk of
in-orbit collisions. Traditional ground-based tracking systems are constrained
by latency and coverage limitations, underscoring the need for onboard,
vision-based space object detection (SOD) capabilities. In this paper, we
propose a novel satellite clustering framework that enables the collaborative
execution of deep learning (DL)-based SOD tasks across multiple satellites. To
support this approach, we construct a high-fidelity dataset simulating imaging
scenarios for clustered satellite formations. A distance-aware viewpoint
selection strategy is introduced to optimize detection performance, and recent
DL models are used for evaluation. Experimental results show that the
clustering-based method achieves competitive detection accuracy compared to
single-satellite and existing approaches, while maintaining a low size, weight,
and power (SWaP) footprint. These findings underscore the potential of
distributed, AI-enabled in-orbit systems to enhance space situational awareness
and contribute to long-term space sustainability.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [305] [Constructive Disintegration and Conditional Modes](https://arxiv.org/abs/2508.00617)
*Nathaël Da Costa,Marvin Pförtner,Jon Cockayne*

Main category: math.ST

TL;DR: The paper delves into the complexities of disintegration in Bayesian statistics, introducing mathematical tools to construct disintegrations, correcting misconceptions, and outlining its implications in practice.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a common misunderstanding in Bayesian statistics, where disintegration of measures is conflated with restricting probability density functions to subsets, and aims to clarify and improve these methods for better applications.

Method: The authors developed mathematical tools to construct disintegrations and analyzed their applications on differentiable manifolds. They provided examples illustrating the disagreement between restricted densities and disintegration densities.

Result: The paper demonstrates that restricted densities and disintegration densities can differ significantly, and evaluates the modes of disintegrations to highlight discrepancies in conditional modes. They also explore the practical implications of these differences, especially in Bayesian inference and inverse problems.

Conclusion: The authors advocate for leveraging both restricted and disintegration measures, determined by the context, and emphasize the importance of carefully distinguishing between the two in theoretical and practical applications.

Abstract: Conditioning, the central operation in Bayesian statistics, is formalised by
the notion of disintegration of measures. However, due to the implicit nature
of their definition, constructing disintegrations is often difficult. A
folklore result in machine learning conflates the construction of a
disintegration with the restriction of probability density functions onto the
subset of events that are consistent with a given observation. We provide a
comprehensive set of mathematical tools which can be used to construct
disintegrations and apply these to find densities of disintegrations on
differentiable manifolds. Using our results, we provide a disturbingly simple
example in which the restricted density and the disintegration density
drastically disagree. Motivated by applications in approximate Bayesian
inference and Bayesian inverse problems, we further study the modes of
disintegrations. We show that the recently introduced notion of a "conditional
mode" does not coincide in general with the modes of the conditional measure
obtained through disintegration, but rather the modes of the restricted
measure. We also discuss the implications of the discrepancy between the two
measures in practice, advocating for the utility of both approaches depending
on the modelling context.

</details>


### [306] [Local Poisson Deconvolution for Discrete Signals](https://arxiv.org/abs/2508.00824)
*Shayan Hundrieser,Tudor Manole,Danila Litskevich,Axel Munk*

Main category: math.ST

TL;DR: This paper addresses the challenge of recovering a discrete atomic signal from a binned Poisson convolution model, analyzing its statistical properties and practical applications in super-resolution microscopy.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from applications such as super-resolution microscopy, where accurate estimation of the discrete signal provides critical insights into cellular formations.

Method: The authors develop a framework to analyze the local minimax risk of signal recovery under a multiscale loss function, emphasizing the recovery rates depending on the signal's local geometric structure.

Result: The study finds that accurate recovery is possible under broader signal classes than previously understood, with rates dependent on local geometry. Results are also applied to Gaussian mixture models and real-world super-resolution microscopy data.

Conclusion: The findings indicate a more optimistic perspective on Poisson deconvolution, enabling accurate recovery for broader signal classes, with practical utility demonstrated through applications to microscopy data and runtime analysis.

Abstract: We analyze the statistical problem of recovering an atomic signal, modeled as
a discrete uniform distribution $\mu$, from a binned Poisson convolution model.
This question is motivated, among others, by super-resolution laser microscopy
applications, where precise estimation of $\mu$ provides insights into spatial
formations of cellular protein assemblies. Our main results quantify the local
minimax risk of estimating $\mu$ for a broad class of smooth convolution
kernels. This local perspective enables us to sharply quantify optimal
estimation rates as a function of the clustering structure of the underlying
signal. Moreover, our results are expressed under a multiscale loss function,
which reveals that different parts of the underlying signal can be recovered at
different rates depending on their local geometry. Overall, these results paint
an optimistic perspective on the Poisson deconvolution problem, showing that
accurate recovery is achievable under a much broader class of signals than
suggested by existing global minimax analyses. Beyond Poisson deconvolution,
our results also allow us to establish the local minimax rate of parameter
estimation in Gaussian mixture models with uniform weights.
  We apply our methods to experimental super-resolution microscopy data to
identify the location and configuration of individual DNA origamis. In
addition, we complement our findings with numerical experiments on runtime and
statistical recovery that showcase the practical performance of our estimators
and their trade-offs.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [307] [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
*Li Zhao,Rui Sun,Zuoyou Jiang,Bo Yang,Yuxiao Bai,Mengting Chen,Xinyang Wang,Jing Li,Zuo Bai*

Main category: q-fin.TR

TL;DR: The paper proposes a multi-agent system inspired by corporate management structures to improve LLM-based trading systems' robustness and performance in financial trading.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLM-based trading systems, which suffer from high sensitivity to market noise, undermining their effectiveness.

Method: The system consists of a Data Team for processing market data into text factors and a Research Team for parallelized trading decisions, with a real-time evaluation and scoring mechanism based on market feedback.

Result: The proposed system adapts dynamically to market changes, showing improved robustness and delivering superior trading performance compared to existing methods.

Conclusion: This multi-agent system offers a competitive edge by leveraging market feedback for dynamic adjustments, enhancing the reliability and success of LLM-based financial trading systems.

Abstract: In financial trading, large language model (LLM)-based agents demonstrate
significant potential. However, the high sensitivity to market noise undermines
the performance of LLM-based trading systems. To address this limitation, we
propose a novel multi-agent system featuring an internal competitive mechanism
inspired by modern corporate management structures. The system consists of two
specialized teams: (1) Data Team - responsible for processing and condensing
massive market data into diversified text factors, ensuring they fit the
model's constrained context. (2) Research Team - tasked with making
parallelized multipath trading decisions based on deep research methods. The
core innovation lies in implementing a real-time evaluation and ranking
mechanism within each team, driven by authentic market feedback. Each agent's
performance undergoes continuous scoring and ranking, with only outputs from
top-performing agents being adopted. The design enables the system to
adaptively adjust to dynamic environment, enhances robustness against market
noise and ultimately delivers superior trading performance. Experimental
results demonstrate that our proposed system significantly outperforms
prevailing multiagent systems and traditional quantitative investment methods
across diverse evaluation metrics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [308] [Data-Driven Motion Planning for Uncertain Nonlinear Systems](https://arxiv.org/abs/2508.00154)
*Babak Esmaeili,Hamidreza Modares,Stefano Di Cairano*

Main category: eess.SY

TL;DR: The paper discusses a data-driven approach to motion planning for nonlinear systems, constructing invariant polytopes for safe control.


<details>
  <summary>Details</summary>
Motivation: To provide a safe and dynamically feasible motion-planning approach for nonlinear systems without reliance on traditional system dynamics models.

Method: The framework uses data-driven algorithms to identify admissible convex regions, compute invariant sets with linear matrix inequalities, and interpolate state-feedback gains for real-time control.

Result: Through simulations, the proposed framework achieves safe and effective motion planning for complex nonlinear systems.

Conclusion: Using a data-driven approach expands the capability of motion planning by ensuring safety and dynamic feasibility without requiring explicit system dynamics models.

Abstract: This paper proposes a data-driven motion-planning framework for nonlinear
systems that constructs a sequence of overlapping invariant polytopes. Around
each randomly sampled waypoint, the algorithm identifies a convex admissible
region and solves data-driven linear-matrix-inequality problems to learn
several ellipsoidal invariant sets together with their local state-feedback
gains. The convex hull of these ellipsoids, still invariant under a
piece-wise-affine controller obtained by interpolating the gains, is then
approximated by a polytope. Safe transitions between nodes are ensured by
verifying the intersection of consecutive convex-hull polytopes and introducing
an intermediate node for a smooth transition. Control gains are interpolated in
real time via simplex-based interpolation, keeping the state inside the
invariant polytopes throughout the motion. Unlike traditional approaches that
rely on system dynamics models, our method requires only data to compute safe
regions and design state-feedback controllers. The approach is validated
through simulations, demonstrating the effectiveness of the proposed method in
achieving safe, dynamically feasible paths for complex nonlinear systems.

</details>


### [309] [Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems](https://arxiv.org/abs/2508.00724)
*Boyu Li,Zhengchen Li,Weimin Wu,Mengchu Zhou*

Main category: eess.SY

TL;DR: The paper addresses scheduling challenges in systems with attachable heterogeneous AGVs like carriers and shuttles, proposing Petri nets and a metaheuristic algorithm to enhance efficiency and prevent deadlocks.


<details>
  <summary>Details</summary>
Motivation: To improve automated material transportation systems' efficiency and avoid deadlock issues caused by the synchronization of attachable heterogeneous AGVs.

Method: The paper introduces Petri nets for modeling AGV schedules, develops a firing-driven decoding method, integrates deadlock detection/prevention strategies, and designs a metaheuristic within an adaptive search framework.

Result: Numerical experiments validate the proposed approach's effectiveness compared to current practices, exact solvers, and advanced metaheuristics, showing enhanced computational efficiency and deadlock-free schedules.

Conclusion: Using Petri nets and metaheuristic algorithms ensures better operational efficiency and managerial insights in industrial settings involving attachable heterogeneous AGVs.

Abstract: The increasing demand for automation and flexibility drives the widespread
adoption of heterogeneous automated guided vehicles (AGVs). This work intends
to investigate a new scheduling problem in a material transportation system
consisting of attachable heterogeneous AGVs, namely carriers and shuttles. They
can flexibly attach to and detach from each other to cooperatively execute
complex transportation tasks. While such collaboration enhances operational
efficiency, the attachment-induced synchronization and interdependence render
the scheduling coupled and susceptible to deadlock. To tackle this challenge,
Petri nets are introduced to model AGV schedules, well describing the
concurrent and sequential task execution and carrier-shuttle synchronization.
Based on Petri net theory, a firing-driven decoding method is proposed, along
with deadlock detection and prevention strategies to ensure deadlock-free
schedules. Furthermore, a Petri net-based metaheuristic is developed in an
adaptive large neighborhood search framework and incorporates an effective
acceleration method to enhance computational efficiency. Finally, numerical
experiments using real-world industrial data validate the effectiveness of the
proposed algorithm against the scheduling policy applied in engineering
practice, an exact solver, and four state-of-the-art metaheuristics. A
sensitivity analysis is also conducted to provide managerial insights.

</details>


### [310] [Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms](https://arxiv.org/abs/2508.00775)
*Andrea Martin,Ian R. Manchester,Luca Furieri*

Main category: eess.SY

TL;DR: The paper explores ways to modify linearly convergent algorithms to improve average-case performance for specific problems while maintaining worst-case guarantees.


<details>
  <summary>Details</summary>
Motivation: The need to balance between strict worst-case guarantees and improved performance on commonly occurring specific problem instances in high-stakes engineering applications.

Method: The authors derive mathematical characterizations and allowable modifications to preserve linear convergence properties when optimizing for average-case performance.

Result: They successfully augmented algorithms like gradient descent and Nesterov's method, demonstrating effectiveness in scenarios like ill-conditioned linear equations and model predictive control.

Conclusion: The findings provide a framework to customize optimization algorithms for specific applications while retaining their reliability on broader problem classes.

Abstract: In high-stakes engineering applications, optimization algorithms must come
with provable worst-case guarantees over a mathematically defined class of
problems. Designing for the worst case, however, inevitably sacrifices
performance on the specific problem instances that often occur in practice. We
address the problem of augmenting a given linearly convergent algorithm to
improve its average-case performance on a restricted set of target problems -
for example, tailoring an off-the-shelf solver for model predictive control
(MPC) for an application to a specific dynamical system - while preserving its
worst-case guarantees across the entire problem class. Toward this goal, we
characterize the class of algorithms that achieve linear convergence for
classes of nonsmooth composite optimization problems. In particular, starting
from a baseline linearly convergent algorithm, we derive all - and only - the
modifications to its update rule that maintain its convergence properties. Our
results apply to augmenting legacy algorithms such as gradient descent for
nonconvex, gradient-dominated functions; Nesterov's accelerated method for
strongly convex functions; and projected methods for optimization over
polyhedral feasibility sets. We showcase effectiveness of the approach on
solving optimization problems with tight iteration budgets in application to
ill-conditioned systems of linear equations and MPC for linear systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [311] [Towards Efficient Certification of Maritime Remote Operation Centers](https://arxiv.org/abs/2508.00543)
*Christian Neurohr,Marcel Saager,Lina Putze,Jan-Patrick Osterloh,Karina Rothemann,Hilko Wiards,Eckard Böde,Axel Hahn*

Main category: cs.CY

TL;DR: This paper proposes a hazard database concept to enhance safety in shore-based remote operation centers for automated ships, derived from a generic functional architecture.


<details>
  <summary>Details</summary>
Motivation: The increasing automation in ships is shifting crew responsibilities from onboard to shore-based remote operation centers, necessitating robust mechanisms for monitoring, control, and certification.

Method: The authors design a hazard database concept by categorizing hazard sources using a generic functional architecture and conduct a preliminary suitability analysis to identify effective methods for hazard analysis and risk assessment.

Result: The paper identifies a framework for constructing hazard databases and suitable methodologies for addressing potential risks in remote operation centers.

Conclusion: The concept contributes to safer and better-certified remote operation centers, facilitating the continued adoption of automated ships.

Abstract: Additional automation being build into ships implies a shift of crew from
ship to shore. However, automated ships still have to be monitored and, in some
situations, controlled remotely. These tasks are carried out by human operators
located in shore-based remote operation centers. In this work, we present a
concept for a hazard database that supports the safeguarding and certification
of such remote operation centers. The concept is based on a categorization of
hazard sources which we derive from a generic functional architecture. A
subsequent preliminary suitability analysis unveils which methods for hazard
analysis and risk assessment can adequately fill this hazard database.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [312] [LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources](https://arxiv.org/abs/2508.00654)
*Rodrigo Escobar Díaz Guerrero,Jamile Mohammad Jafari,Tobias Meyer-Zedler,Michael Schmitt,Juergen Popp,Thomas Bocklitz*

Main category: cs.CE

TL;DR: The study introduces LEO, a web-based platform to address the challenge of linking and integrating disparate data sources like ELNs and OMERO for microscopy research.


<details>
  <summary>Details</summary>
Motivation: The motivation of the study is to tackle the challenge of managing and linking large volumes of data from heterogeneous platforms in microscopy research, ensuring compliance with FAIR principles.

Method: LEO, a web-based platform initially designed to link ELNs and OMERO, utilizes a plugin-based architecture for extensibility and scalability in data integration.

Result: LEO successfully provides a scalable, extensible solution that links heterogeneous data sources and streamlines microscopy research workflows.

Conclusion: LEO bridges the gap in microscopy research data integration by linking distributed systems, enhancing Findability, Accessibility, Interoperability, and Reusability of data.

Abstract: In the interdisciplinary field of microscopy research, managing and
integrating large volumes of data stored across disparate platforms remains a
major challenge. Data types such as bioimages, experimental records, and
spectral information are often maintained in separate repositories, each
following different management standards. However, linking these data sources
across the research lifecycle is essential to align with the FAIR principles of
data management: Findability, Accessibility, Interoperability, and Reusability.
Despite this need, there is a notable lack of tools capable of effectively
integrating and linking data from heterogeneous sources. To address this gap,
we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based
platform designed to create and manage links between distributed data systems.
LEO was initially developed to link objects between Electronic Lab Notebooks
(ELNs) and OMERO, but its functionality has since been extended through a
plugin-based architecture, allowing the integration of additional data sources.
This extensibility makes LEO a scalable and flexible solution for a wide range
of microscopy research workflows.

</details>


### [313] [Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models](https://arxiv.org/abs/2508.00804)
*Julian Lemmel,Manuel Kranzl,Adam Lamine,Philipp Neubauer,Radu Grosu,Sophie Neubauer*

Main category: cs.CE

TL;DR: The paper presents a method for real-time tuning of structured state space models to improve predictions during inference.


<details>
  <summary>Details</summary>
Motivation: Structured state space models (SSMs) are efficient and capable of long-range modeling, but they lack adaptability after offline training. This paper aims to address this limitation.

Method: The proposed method uses real-time recurrent learning to update SSM parameters dynamically, enabling online adaptation during inference.

Result: Evaluation on a carbon emission dataset shows that the method reduces prediction error consistently in online settings.

Conclusion: The approach demonstrates potential for resource-constrained environments, making SSMs adaptable and better suited for dynamic scenarios.

Abstract: This paper introduces a new approach for fine-tuning the predictions of
structured state space models (SSMs) at inference time using real-time
recurrent learning. While SSMs are known for their efficiency and long-range
modeling capabilities, they are typically trained offline and remain static
during deployment. Our method enables online adaptation by continuously
updating model parameters in response to incoming data. We evaluate our
approach for linear-recurrent-unit SSMs using a small carbon emission dataset
collected from embedded automotive hardware. Experimental results show that our
method consistently reduces prediction error online during inference,
demonstrating its potential for dynamic, resource-constrained environments.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [314] [Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration](https://arxiv.org/abs/2508.00668)
*Raquel Coelho,Roy Pea,Christian Schunn,Jinglei Cheng,Junyu Liu*

Main category: physics.ed-ph

TL;DR: The paper advocates for interdisciplinary collaboration between quantum information science (QIS) and the learning sciences (LS) to effectively prepare young learners for QIS.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of preparing young learners for quantum information science, a radically different and complex field.

Method: The paper proposes leveraging design-based research and reshaping knowledge representations to inform QIS educational practices.

Result: Collaboration between QIS and LS could enhance understanding and teaching methods in complex domains while providing beneficial QIS learning experiences.

Conclusion: The authors call for a mutual partnership between QIS and LS to achieve theoretical and practical advancements in both fields.

Abstract: As quantum information science advances and the need for pre-college
engagement grows, a critical question remains: How can young learners be
prepared to participate in a field so radically different from what they have
encountered before? This paper argues that meeting this challenge will require
strong interdisciplinary collaboration with the Learning Sciences (LS), a field
dedicated to understanding how people learn and designing theory-guided
environments to support learning. Drawing on lessons from previous STEM
education efforts, we discuss two key contributions of the learning sciences to
quantum information science (QIS) education. The first is design-based
research, the signature methodology of learning sciences, which can inform the
development, refinement, and scaling of effective QIS learning experiences. The
second is a framework for reshaping how learners reason about, learn and
participate in QIS practices through shifts in knowledge representations that
provide new forms of engagement and associated learning. We call for a two-way
partnership between quantum information science and the learning sciences, one
that not only supports learning in quantum concepts and practices but also
improves our understanding of how to teach and support learning in highly
complex domains. We also consider potential questions involved in bridging
these disciplinary communities and argue that the theoretical and practical
benefits justify the effort.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [315] [Reimagining Voltage-Controlled Cryogenic Boolean Logic Paradigm with Quantum-Enhanced Josephson Junction FETs](https://arxiv.org/abs/2508.00295)
*Md Mazharul Islam,Diego Ferrer,Shamiul Alam,Juan P. Mendez,Denis Mamaluy,Wei Pan,Ahmedullah Aziz*

Main category: cs.ET

TL;DR: This paper explores advanced cryogenic electronics by leveraging improved Josephson Junction Field Effect Transistors (JJFETs) and demonstrates fundamental logic gate designs for scalable quantum-enhanced computing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses fundamental challenges in the cascadability of superconducting devices for logic architectures and aims to enhance their integration into complex systems.

Method: The authors utilized an advanced JJFET design using InAs and GaSb heterostructures combined with a Verilog A compact model. They also incorporated multilayered Heater Nanocryotron (nTron) for cascadability and employed simulations for logic circuit validation.

Result: Simulation analysis successfully demonstrated the implementation of fundamental logic gates such as NOT, NAND, and NOR, as well as a 3-input majority gate. The operation of a 2-input XOR gate was also validated using these components.

Conclusion: The paper establishes the feasibility of scalable logic architectures using quantum-enhanced JJFETs and multilayered nTrons, showcasing promise for future quantum and reversible computing designs.

Abstract: The growing demand for ultra low power computing and the emergence of quantum
technologies have intensified interest in cryogenic electronics, particularly
superconducting devices.Despite their promise, current controlled
superconducting components face fundamental challenges in cascadability,
limiting their effectiveness in complex logic architectures.To overcome this,
recent efforts have focused on developing gate tunable superconducting devices,
such as Josephson Junction Field Effect Transistors (JJFETs).However, achieving
robust control and sufficient supercurrent gain, both critical for
transistor-like performance in logic circuits remains a key challenge.A recent
advancement in JJFET design, based on InAs and GaSb heterostructures,
demonstrates enhanced gain and favorable device characteristics suitable for
circuit integration.Building on this innovation, we propose and analyze
fundamental voltage controlled logic topologies using the quantum enhanced
JJFET. We develop a Verilog A based circuit compatible compact model of the
quantum enhanced JJFET which accurately captures the experimentally observed
device characteristics.To ensure cascadability, our logic circuits incorporate
the multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based
thermal switch.Through simulation based analysis, we demonstrate the successful
implementation of fundamental logic gates, including NOT, NAND, and NOR.
Furthermore, we design a 3 input majority gate, which plays a pivotal role in
quantum and reversible computing due to its universality.Finally, to
demonstrate the cascadability of our proposed logic topology, we demonstrate
the operation of a 2 input XOR gate based on our designed JJFET based NOT,
NAND, and NOR gate.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [316] [When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation](https://arxiv.org/abs/2508.00450)
*Hongxiang Lin,Hao Guo,Zeshun Li,Erpeng Xue,Yongqian He,Xiangyu Hou,Zhaoyu Hu,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: This paper introduces CoEA, a method to overcome bias and static optimization flaws in LLM-enhanced recommendation systems, improving exploratory recommendations.


<details>
  <summary>Details</summary>
Motivation: Traditional recommendation systems overly emphasize historical preferences, causing content fatigue. Existing LLM-models cannot address group-driven preferences or adjust dynamically with new data.

Method: The authors propose the Co-Evolutionary Alignment (CoEA) method, which includes the Dual-Stable Interest Exploration (DSIE) module for addressing bias and the Periodic Collaborative Optimization (PCO) mechanism for dynamic closed-loop optimization.

Result: Extensive online and offline experiments demonstrate that the CoEA model enhances the capability of exploratory recommendations.

Conclusion: CoEA effectively resolves limitations in LLM-based recommendation systems by addressing interest modeling bias and static optimization issues, enabling dynamic and more balanced recommendations.

Abstract: Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

</details>


### [317] [M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation](https://arxiv.org/abs/2508.00452)
*Chuan He,Yongchao Liu,Qiang Li,Wenliang Zhong,Chuntao Hong,Xinwei Yao*

Main category: cs.IR

TL;DR: The paper introduces the Multi-Modal Multi-View Variational AutoEncoder (M^2VAE), a generative model that leverages multi-modal features and user preferences to address the cold-start item recommendation issue, validated successfully on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Cold-start recommendation remains a challenge, especially when new items lack historical data. Existing methods inadequately address the multi-view nature of modalities and shared-modality-specific features.

Method: The M^2VAE model generates type-specific latent variables for multiple feature types, uses the Product-of-Experts for shared representation, applies a contrastive loss for disentangling views, and employs a preference-guided Mixture-of-Experts for user representation fusion.

Result: Experiments on real-world datasets demonstrate the approach's efficacy in improving cold-start recommendations.

Conclusion: The study confirms that M^2VAE effectively disentangles shared and unique modality views, leverages multi-modal features, and models user preferences for enhanced cold-start recommendations.

Abstract: Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

</details>


### [318] [Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking](https://arxiv.org/abs/2508.00751)
*Qing Zhang,Alex Deng,Michelle Du,Huiji Gao,Liwei He,Sanjeev Katariya*

Main category: cs.IR

TL;DR: The paper develops interleaving and counterfactual evaluation methods to enhance online ranking algorithm assessments, making them faster and more sensitive compared to traditional A/B testing.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving efficient, accurate, and cost-effective evaluations for ranking algorithms in search and recommender systems, particularly overcoming the limitations of traditional A/B testing.

Method: The study introduces interleaving and counterfactual evaluation techniques to rapidly and effectively identify candidates for A/B testing, while increasing experiment sensitivity.

Result: The proposed methods boosted sensitivity by up to 100 times compared to traditional A/B testing and streamlined the experimental process for ranking algorithm evaluations.

Conclusion: The developed evaluation techniques offer a practical and efficient means for businesses to enhance their ranking algorithm experiments, with production insights applicable to similar contexts.

Abstract: Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [319] [Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization](https://arxiv.org/abs/2508.00307)
*Belman Jahir Rodriguez,Sergio F. Chevtchenko,Marcelo Herrera Martinez,Yeshwant Bethy,Saeed Afshar*

Main category: eess.AS

TL;DR: The paper proposes a U-net model for spherical acoustic source localization, which segments beamformed audio maps to identify sound presence regions rather than estimating discrete direction-of-arrival (DoA) angles. Tested on real-world drone field recordings, the model shows improved localization precision.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve sound source localization by leveraging a spherical segmentation approach, addressing issues with traditional directional regression models and enabling dense spatial audio understanding.

Method: A modified U-Net is introduced to segment beamformed audio maps generated by a custom 24-microphone array. The Tversky loss tackles class imbalance, and a post-processing step calculates centroids for sound source directions.

Result: The model successfully generalizes across diverse environments, showing enhanced angular precision in acoustic source localization when applied to real-world drone recordings.

Conclusion: The proposed approach advances spatial audio understanding by offering a robust and array-independent method with improved precision, representing a paradigm shift in sound source localization.

Abstract: We introduce a U-net model for 360{\deg} acoustic source localization
formulated as a spherical semantic segmentation task. Rather than regressing
discrete direction-of-arrival (DoA) angles, our model segments beamformed audio
maps (azimuth and elevation) into regions of active sound presence. Using
delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate
signals aligned with drone GPS telemetry to create binary supervision masks. A
modified U-Net, trained on frequency-domain representations of these maps,
learns to identify spatially distributed source regions while addressing class
imbalance via the Tversky loss. Because the network operates on beamformed
energy maps, the approach is inherently array-independent and can adapt to
different microphone configurations without retraining from scratch. The
segmentation outputs are post-processed by computing centroids over activated
regions, enabling robust DoA estimates. Our dataset includes real-world
open-field recordings of a DJI Air 3 drone, synchronized with 360{\deg} video
and flight logs across multiple dates and locations. Experimental results show
that U-net generalizes across environments, providing improved angular
precision, offering a new paradigm for dense spatial audio understanding beyond
traditional Sound Source Localization (SSL).

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [320] [Composable OS Kernel Architectures for Autonomous Intelligence](https://arxiv.org/abs/2508.00604)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.OS

TL;DR: The paper proposes transforming OS kernels into adaptive, AI-integrated platforms for intelligent systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance OS kernel architecture to support intelligent systems capable of sensory and cognitive processing efficiently in edge devices, cloud infrastructures, and real-time environments.

Method: The method involves three contributions: using Loadable Kernel Modules (LKMs) as AI units, expanding the Linux kernel for AI-native environments, and introducing Neurosymbolic kernel designs using advanced mathematical frameworks.

Result: The proposed architecture enables operating systems to anticipate and adapt to cognitive needs of intelligent applications.

Conclusion: This research creates a foundation for OS kernels that integrate AI, adaptive mechanisms, and advanced reasoning abilities, fostering intelligent system evolution.

Abstract: As intelligent systems permeate edge devices, cloud infrastructure, and
embedded real-time environments, this research proposes a new OS kernel
architecture for intelligent systems, transforming kernels from static resource
managers to adaptive, AI-integrated platforms. Key contributions include: (1)
treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for
fast sensory and cognitive processing in kernel space; (2) expanding the Linux
kernel into an AI-native environment with built-in deep learning inference,
floating-point acceleration, and real-time adaptive scheduling for efficient ML
workloads; and (3) introducing a Neurosymbolic kernel design leveraging
Category Theory and Homotopy Type Theory to unify symbolic reasoning and
differentiable logic within OS internals. Together, these approaches enable
operating systems to proactively anticipate and adapt to the cognitive needs of
autonomous intelligent applications.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [321] [Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method](https://arxiv.org/abs/2508.00101)
*Alena Kopaničáková,Youngkyu Lee,George Em Karniadakis*

Main category: math.NA

TL;DR: The paper introduces a novel deflation strategy leveraging a neural network, DeepONet, to improve the convergence of the Preconditioned Conjugate Gradient method while solving large-scale linear systems.


<details>
  <summary>Details</summary>
Motivation: Enhance the convergence of the preconditioned conjugate gradient method for parametric large-scale linear systems using innovative deflation approaches.

Method: The proposed method uses DeepONet to construct deflation subspaces, introducing two approaches for deflation operators: approximating near-null space vectors via basis functions and using predicted solutions directly.

Result: Numerical experiments on various types of problems show improvements in convergence and scalability, highlighting the method's adaptability across models and resolutions.

Conclusion: The DeepONet-based deflation strategy improves efficiency and generalization of the PCG method, confirming its promise for diverse applications.

Abstract: We propose a new deflation strategy to accelerate the convergence of the
preconditioned conjugate gradient(PCG) method for solving parametric
large-scale linear systems of equations. Unlike traditional deflation
techniques that rely on eigenvector approximations or recycled Krylov
subspaces, we generate the deflation subspaces using operator learning,
specifically the Deep Operator Network~(DeepONet). To this aim, we introduce
two complementary approaches for assembling the deflation operators. The first
approach approximates near-null space vectors of the discrete PDE operator
using the basis functions learned by the DeepONet. The second approach directly
leverages solutions predicted by the DeepONet. To further enhance convergence,
we also propose several strategies for prescribing the sparsity pattern of the
deflation operator. A comprehensive set of numerical experiments encompassing
steady-state, time-dependent, scalar, and vector-valued problems posed on both
structured and unstructured geometries is presented and demonstrates the
effectiveness of the proposed DeepONet-based deflated PCG method, as well as
its generalization across a wide range of model parameters and problem
resolutions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [322] [Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process](https://arxiv.org/abs/2508.00816)
*Youssef Ait El Mahjoub,Jean-Michel Fourneau,Salma Alouah*

Main category: math.OC

TL;DR: The paper introduces the Single-Input Superstate Decomposable MDP (SISDMDP) concept, enabling efficient policy evaluation for certain structured MDPs using decomposition techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational challenges of policy evaluation in Markov Decision Processes, especially in infinite-horizon settings like average-reward or discounted-reward formulations.

Method: The authors define SISDMDP by combining Chiu's single-input decomposition with Robertazzi's single-cycle recurrence property. They develop a decomposition-based policy evaluation method leveraging this structure.

Result: The proposed approach yields an exact and efficient policy evaluation method for SISDMDPs, making it applicable to average and discounted reward scenarios.

Conclusion: The paper demonstrates that exploiting structural properties in MDPs can significantly enhance scalability and efficiency in solving sequential decision-making problems with large state spaces.

Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in
sequential decision-making, especially when dealing with large state spaces and
long-term optimization criteria. A key step in Bellman dynamic programming
algorithms is the policy evaluation, which becomes computationally demanding in
infinite-horizon settings such as average-reward or discounted-reward
formulations. In the context of Markov chains, aggregation and disaggregation
techniques have for a long time been used to reduce complexity by exploiting
structural decompositions. In this work, we extend these principles to a
structured class of MDPs. We define the Single-Input Superstate Decomposable
Markov Decision Process (SISDMDP), which combines Chiu's single-input
decomposition with Robertazzi's single-cycle recurrence property. When a policy
induces this structure, the resulting transition graph can be decomposed into
interacting components with centralized recurrence. We develop an exact and
efficient policy evaluation method based on this structure. This yields a
scalable solution applicable to both average and discounted reward MDPs.

</details>


### [323] [Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence](https://arxiv.org/abs/2508.00091)
*Chandler Smith,HanQin Cai,Abiy Tasissa*

Main category: math.OC

TL;DR: The paper introduces a Riemannian optimization approach for the Euclidean Distance Geometry (EDG) problem by framing it as low-rank matrix completion. It achieves notable theoretical and empirical results for efficient recovery with partial pairwise distance data.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the problem of recovering geometric configurations from incomplete pairwise distances, which is vital in applications like sensor networks, molecular modeling, and machine learning.

Method: The paper utilizes Riemannian gradient descent on rank-$r$ positive semi-definite matrices and develops an initialization approach using hard thresholding. It ensures geometric consistency and leverages the restricted isometry property analysis.

Result: Theoretical guarantees are provided for convergence under specific sampling probabilities. Empirical evaluations show competitive performance against advanced methods and robustness tailored for EDG-specific matrix incoherence.

Conclusion: The framework proves effective both theoretically and practically for solving the EDG problem, with robustness enhancements suited to the task's unique requirements.

Abstract: The problem of recovering a configuration of points from partial pairwise
distances, referred to as the Euclidean Distance Geometry (EDG) problem, arises
in a broad range of applications, including sensor network localization,
molecular conformation, and manifold learning. In this paper, we propose a
Riemannian optimization framework for solving the EDG problem by formulating it
as a low-rank matrix completion task over the space of positive semi-definite
Gram matrices. The available distance measurements are encoded as expansion
coefficients in a non-orthogonal basis, and optimization over the Gram matrix
implicitly enforces geometric consistency through the triangle inequality, a
structure inherited from classical multidimensional scaling. Under a Bernoulli
sampling model for observed distances, we prove that Riemannian gradient
descent on the manifold of rank-$r$ matrices locally converges linearly with
high probability when the sampling probability satisfies $p \geq
\mathcal{O}(\nu^2 r^2 \log(n)/n)$, where $\nu$ is an EDG-specific incoherence
parameter. Furthermore, we provide an initialization candidate using a one-step
hard thresholding procedure that yields convergence, provided the sampling
probability satisfies $p \geq \mathcal{O}(\nu r^{3/2} \log^{3/4}(n)/n^{1/4})$.
A key technical contribution of this work is the analysis of a symmetric linear
operator arising from a dual basis expansion in the non-orthogonal basis, which
requires a novel application of the Hanson--Wright inequality to establish an
optimal restricted isometry property in the presence of coupled terms.
Empirical evaluations on synthetic data demonstrate that our algorithm achieves
competitive performance relative to state-of-the-art methods. Moreover, we
propose a novel notion of matrix incoherence tailored to the EDG setting and
provide robustness guarantees for our method.

</details>


### [324] [Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks](https://arxiv.org/abs/2508.00267)
*Molly Noel,Gabriel Mancino-Ball,Yangyang Xu*

Main category: math.OC

TL;DR: The paper introduces Adam-type stochastic methods with neighbor-sampling for efficient GCN training, ensuring optimal convergence rates and improved performance on large graph datasets.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in current GCN training methods that lack theoretical guarantees and miss key deep learning features like adaptivity and momentum.

Method: The authors propose Adam-type methods using neighbor-sampling and a control variate technique to minimize sampling-induced stochastic errors. Theoretical assumptions guarantee the optimal convergence rate.

Result: Extensive experiments on benchmark node classification tasks show the proposed methods outperform existing neighbor-sampling-based SGD strategies, especially on large-scale graph datasets.

Conclusion: Adam-type neighbor-sampling methods improve GCN training efficiency and performance, providing both theory-backed guarantees and practical deep learning strengths.

Abstract: Graph convolutional networks (GCNs) are a powerful tool for graph
representation learning. Due to the recursive neighborhood aggregations
employed by GCNs, efficient training methods suffer from a lack of theoretical
guarantees or are missing important practical elements from modern deep
learning algorithms, such as adaptivity and momentum. In this paper, we present
several neighbor-sampling (NS) based Adam-type stochastic methods for solving a
nonconvex GCN training problem. We utilize the control variate technique
proposed by [1] to reduce the stochastic error caused by neighbor sampling.
Under standard assumptions for Adam-type methods, we show that our methods
enjoy the optimal convergence rate. In addition, we conduct extensive numerical
experiments on node classification tasks with several benchmark datasets. The
results demonstrate superior performance of our methods over classic NS-based
SGD that also uses the control-variate technique, especially for large-scale
graph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [325] [Design, Simulation, and Fabrication of a Hexagonal Microfluidic Platform for Culturing Neurons](https://arxiv.org/abs/2508.00425)
*Maxx Yung*

Main category: physics.flu-dyn

TL;DR: This study developed a microfluidic device with hexagonal wells and channels for neural organoid computing, validated through computational fluid dynamics and photolithography.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for stable and controlled environments for in vitro neuronal organoid computing.

Method: Design, simulation (CFD modeling), and fabrication via photolithography of a hexagonal honeycomb array microfluidic device.

Result: The device supports ideal flow regimes for cell viability, validated through CFD modeling—targeting flows of 0.1-1 μL/min and pressure ranges of 177-329 kPa.

Conclusion: The research presents a validated and fabricated microfluidic architecture, enabling further experimental flow analysis and neural integration.

Abstract: Developing an organoid computing platform from neurons in vitro demands
stable, precisely controlled microenvironments. To address this requirement, we
designed, simulated, and fabricated a microfluidic device featuring hexagonal
wells ($34.64\,\mathrm{\mu m}$ side length) in a honeycomb array connected by
$20\,\mathrm{\mu m}$ channels. Computational fluid dynamics (CFD) modeling,
validated by high mesh quality ($0.934$ orthogonal quality) and robust
convergence, confirmed the architecture supports flow regimes ideal for
ensuring cell viability. At target flow rates of $0.1$ - $1\,\mathrm{\mu
L/min}$, simulations revealed the extrapolated pressure differential across the
full $50{,}000\,\mathrm{\mu m}$ device remains within stable operating limits
at $177\,\mathrm{kPa}$ (average) and $329\,\mathrm{kPa}$ (maximum).
Photolithography successfully produced this architecture, with only minor
corner rounding observed at feature interfaces. This work therefore establishes
a computationally validated and fabricated platform, paving the way for
experimental flow characterization and subsequent neural integration.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [326] [AdapDISCOM: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors](https://arxiv.org/abs/2508.00120)
*Abdoul O. Diakité,Claudia Moreau,Gleb Bezgin,Nikhil Bhagwat,Pedro Rosa-Neto,Jean-Baptiste Poline,Simon Girard,Amadou Barry,for the Alzheimers Disease Neuroimaging Initiative*

Main category: stat.ME

TL;DR: The paper introduces AdapDISCOM, an advanced regression method for multimodal biomedical data plagued by missingness and measurement errors, with proven theoretical robustness and enhanced performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in statistical inference and prediction involving multimodal high-dimensional data, which often suffer from block-wise missingness and measurement errors.

Method: AdapDISCOM combines modality-specific weighting schemes into the DISCOM framework to manage heterogeneity in data structures and errors. It includes robust variants like AdapDISCOM-Huber and Fast-AdapDISCOM for improved computational efficiency.

Result: Simulation results show AdapDISCOM surpasses existing methods like DISCOM, SCOM, and CoCoLasso, especially under heterogeneous contamination and heavy-tailed distributions. Its application to ADNI data improved prediction accuracy and reliably identified biomarkers.

Conclusion: AdapDISCOM offers a powerful, scalable framework for analyzing imperfect multimodal high-dimensional data in biomedical research, ensuring robust predictions and consistent model selections.

Abstract: Multimodal high-dimensional data are increasingly prevalent in biomedical
research, yet they are often compromised by block-wise missingness and
measurement errors, posing significant challenges for statistical inference and
prediction. We propose AdapDISCOM, a novel adaptive direct sparse regression
method that simultaneously addresses these two pervasive issues. Building on
the DISCOM framework, AdapDISCOM introduces modality-specific weighting schemes
to account for heterogeneity in data structures and error magnitudes across
modalities. We establish the theoretical properties of AdapDISCOM, including
model selection consistency and convergence rates under sub-Gaussian and
heavy-tailed settings, and develop robust and computationally efficient
variants (AdapDISCOM-Huber and Fast-AdapDISCOM). Extensive simulations
demonstrate that AdapDISCOM consistently outperforms existing methods such as
DISCOM, SCOM, and CoCoLasso, particularly under heterogeneous contamination and
heavy-tailed distributions. Finally, we apply AdapDISCOM to Alzheimers Disease
Neuroimaging Initiative (ADNI) data, demonstrating improved prediction of
cognitive scores and reliable selection of established biomarkers, even with
substantial missingness and measurement errors. AdapDISCOM provides a flexible,
robust, and scalable framework for high-dimensional multimodal data analysis
under realistic data imperfections.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [327] [Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach](https://arxiv.org/abs/2508.00629)
*Francisco Crespo,Javier Villegas,Carlos Baena,Eduardo Baena,Sergio Fortes,Raquel Barco*

Main category: cs.NI

TL;DR: The paper introduces a lightweight, O-RAN compliant distributed application to optimize CPU resource management in softwarized RANs, improving energy efficiency without compromising performance.


<details>
  <summary>Details</summary>
Motivation: The transition to softwarized RANs under the O-RAN framework creates challenges in efficiently managing CPU resources due to the complex interplay between latency-sensitive workloads and OS schedulers.

Method: The study proposes a distributed application (dApp) deployed at the Distributed Unit (DU) that leverages thread-level telemetry to dynamically adjust CPU settings, such as thread affinity and core isolation, in real-time.

Result: Experiments with a commercial-grade srsRAN setup showed consistent power savings and better CPU utilization without negatively affecting latency or real-time processing.

Conclusion: This lightweight dApp can optimize CPU resource control in O-RAN networks, improving energy efficiency and utilization while being infrastructure-agnostic and introducing negligible overhead.

Abstract: The transition toward softwarized Radio Access Networks (RANs), driven by the
Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through
disaggregation and virtualization of base station functions. However, this
shift introduces new challenges in managing CPU resources efficiently under
strict real-time constraints. In particular, the interplay between
latency-sensitive RAN workloads and general-purpose Operating System (OS)
schedulers often leads to sub-optimal performance and unnecessary energy
consumption. This work proposes a lightweight, programmable distributed
application (dApp) deployed at the Distributed Unit (DU) level to dynamically
orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging
thread-level telemetry like context switches, Instructions Per Cycle (IPC), and
cache metrics, to adapt CPU thread affinity, core isolation, and frequency
scaling in real time. Unlike existing solutions, it requires no access to
proprietary RAN software, hardware-specific features, or kernel modifications.
Fully compliant with the O-RAN architecture and agnostic to the underlying RAN
stack, the proposed solution introduces negligible overhead while improving
energy efficiency and CPU utilization. Experimental results using a
commercial-grade srsRAN deployment demonstrate consistent power savings without
compromising real-time processing performance, highlighting the potential of
low-latency dApps for fine-grained resource control in next-generation networks

</details>


### [328] [Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts](https://arxiv.org/abs/2508.00234)
*Jin Yang,Qiong Wu,Zhiying Feng,Zhi Zhou,Deke Guo,Xu Chen*

Main category: cs.NI

TL;DR: The paper addresses the challenge of routing user requests to edge-deployed LLMs to ensure quality-of-service (QoS) by proposing a deep reinforcement learning (DRL)-based framework.


<details>
  <summary>Details</summary>
Motivation: There is increasing demand for LLM services with challenges in network latency, data privacy, and varying response quality at the edge, particularly for IoT and mobile applications.

Method: The authors developed a DRL-based framework incorporating dynamic state abstraction with a Heterogeneous Graph Attention Network, an action impact estimator, and a tailored reward function.

Result: Their framework demonstrated significant enhancements in QoS and computing resource efficiency under both Poisson and real-world workloads, outperforming existing solutions.

Conclusion: The proposed DRL-based routing system effectively addresses key QoS challenges for edge-deployed LLM services, making it suitable for dynamic and heterogeneous application scenarios.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
leading to a significant increase in user demand for LLM services. However,
cloud-based LLM services often suffer from high latency, unstable
responsiveness, and privacy concerns. Therefore, multiple LLMs are usually
deployed at the network edge to boost real-time responsiveness and protect data
privacy, particularly for many emerging smart mobile and IoT applications.
Given the varying response quality and latency of LLM services, a critical
issue is how to route user requests from mobile and IoT devices to an
appropriate LLM service (i.e., edge LLM expert) to ensure acceptable
quality-of-service (QoS). Existing routing algorithms fail to simultaneously
address the heterogeneity of LLM services, the interference among requests, and
the dynamic workloads necessary for maintaining long-term stable QoS. To meet
these challenges, in this paper we propose a novel deep reinforcement learning
(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM
services. Due to the dynamic nature of the global state, we propose a dynamic
state abstraction technique to compactly represent global state features with a
heterogeneous graph attention network (HAN). Additionally, we introduce an
action impact estimator and a tailored reward function to guide the DRL agent
in maximizing QoS and preventing latency violations. Extensive experiments on
both Poisson and real-world workloads demonstrate that our proposed algorithm
significantly improves average QoS and computing resource efficiency compared
to existing baselines.

</details>


### [329] [Agent Network Protocol Technical White Paper](https://arxiv.org/abs/2508.00007)
*Gaowei Chang,Eidan Lin,Chengxuan Yuan,Rizhao Cai,Binbin Chen,Xuan Xie,Yin Zhang*

Main category: cs.NI

TL;DR: The paper introduces the Agent Network Protocol (ANP) to address connectivity and collaboration challenges for autonomous agents on the internet.


<details>
  <summary>Details</summary>
Motivation: Existing internet infrastructure is optimized for human interaction, causing inefficiencies and limitations for agent-based interconnection and collaboration.

Method: The ANP framework proposes a three-layer protocol system: an identity and encrypted communication layer, a meta-protocol negotiation layer, and an application protocol layer.

Result: ANP helps solve key issues related to agent authentication, negotiation, and capability discovery. Its design aligns with emerging trends in internet transformation.

Conclusion: ANP is positioned as the foundational communication framework for the Agentic Web, offering a modular, AI-native solution that bridges current internet capabilities with the needs of scalable agent interconnection.

Abstract: With the development of large models and autonomous decision-making AI,
agents are rapidly becoming the new entities of the internet, following mobile
apps. However, existing internet infrastructure is primarily designed for human
interaction, creating data silos, unfriendly interfaces, and high collaboration
costs among agents, making it difficult to support the needs for large-scale
agent interconnection and collaboration. The internet is undergoing a profound
transformation, showing four core trends: agents replacing traditional
software, universal agent interconnection, native protocol-based connections,
and autonomous agent organization and collaboration. To align with these
trends, Agent Network Protocol (ANP) proposes a new generation of communication
protocols for the Agentic Web. ANP adheres to AI-native design, maintains
compatibility with existing internet protocols, adopts a modular composable
architecture, follows minimalist yet extensible principles, and enables rapid
deployment based on existing infrastructure. Through a three-layer protocol
system--identity and encrypted communication layer, meta-protocol negotiation
layer, and application protocol layer--ANP. systematically solves the problems
of agent identity authentication, dynamic negotiation, and capability discovery
interoperability.

</details>


### [330] [Enabling Immersive XR Collaborations over FTTR Networks (Invited)](https://arxiv.org/abs/2508.00009)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: The paper investigates predictive bandwidth allocation and seamless handover mechanisms over Fiber-To-The-Room (FTTR) to enable high-quality immersive experiences for extended reality collaboration within premises.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the immersive experience for in-premise extended reality collaborations by addressing network challenges like bandwidth allocation and handover within Fiber-To-The-Room setups.

Method: The study develops and evaluates predictive bandwidth allocation and seamless handover schemes tailored for FTTR networks.

Result: The proposed methods demonstrate the viability of achieving high-quality immersive experiences for in-premise collaboration applications over FTTR.

Conclusion: Fiber-To-The-Room, combined with predictive bandwidth allocation and seamless handover, presents a viable solution for enabling immersive extended reality collaborations within premises.

Abstract: Fiber-To-The-Room is a potential solution to achieve in-premise extended
reality collaborations. This paper explores predictive bandwidth allocation and
seamless handover schemes over FTTR, showing high-quality immersive experience
for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s).

</details>


### [331] [AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks](https://arxiv.org/abs/2508.00011)
*Ahmet Melih Ince,Ayse Elif Canbilen,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: The paper proposes using reinforcement learning with deep deterministic policy gradient (DDPG) to optimize the age-of-information (AoI) in HAPS-enabled 6G networks for autonomous driving, improving reliability and information freshness.


<details>
  <summary>Details</summary>
Motivation: The need for hyper-reliable and low-latency communication (HRLLC) for safety-critical applications like autonomous driving motivates exploring resource allocation strategies in 6G networks incorporating non-terrestrial elements like HAPS.

Method: The study employs reinforcement learning techniques, specifically deep deterministic policy gradient (DDPG), to dynamically optimize AoI in high-altitude platform station (HAPS)-based vehicle-to-everything (V2X) networks.

Result: The proposed DDPG-based approach enhances information freshness and overall network reliability by facilitating AoI-aware resource allocation without requiring centralized control.

Conclusion: Integrating HAPS with DDPG-based learning offers a promising solution for efficient resource allocation in 6G networks, crucial for autonomous vehicle platoons, improving communication reliability and information timeliness.

Abstract: Sixth-generation (6G) networks are designed to meet the hyper-reliable and
low-latency communication (HRLLC) requirements of safety-critical applications
such as autonomous driving. Integrating non-terrestrial networks (NTN) into the
6G infrastructure brings redundancy to the network, ensuring continuity of
communications even under extreme conditions. In particular, high-altitude
platform stations (HAPS) stand out for their wide coverage and low latency
advantages, supporting communication reliability and enhancing information
freshness, especially in rural areas and regions with infrastructure
constraints. In this paper, we present reinforcement learning-based approaches
using deep deterministic policy gradient (DDPG) to dynamically optimize the
age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.
The proposed method improves information freshness and overall network
reliability by enabling independent learning without centralized coordination.
The findings reveal the potential of HAPS-supported solutions, combined with
DDPG-based learning, for efficient AoI-aware resource allocation in
platoon-based autonomous vehicle systems.

</details>


### [332] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: This paper develops a scalable framework for predicting availability of unused spectrum for secondary users, combining a Markov model with ITU-R propagation techniques to enhance accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The inefficient utilization of spectrum resources across time and space necessitates strategies for secondary user access to unused frequencies, motivating predictive models for spectrum availability.

Method: The proposed method integrates a two-state Markov chain for primary user activity modeling with ITU-R Recommendations P.528 and P.2108 to account for propagation effects. Temporal and spatial predictions are derived using this combination.

Result: The framework accurately identifies spectrum availability in real-time with low computational cost, demonstrating scalability and flexibility across frequency bands and scenarios.

Conclusion: The system effectively enables real-time spectrum management and dynamic sharing, providing significant utility for cognitive radio networks and similar technologies.

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


### [333] [Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study](https://arxiv.org/abs/2508.00256)
*Chuang Zhang,Geng Sun,Jiacheng Wang,Yijing Lin,Weijie Yuan,Sinem Coleri,Dusit Niyato,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: The paper explores leveraging large artificial intelligence models (LAMs) to address security challenges in low-altitude wireless networks (LAWNs).


<details>
  <summary>Details</summary>
Motivation: LAWNs face unique and amplified security challenges due to their operational environment, and traditional solutions are insufficient to address these vulnerabilities.

Method: The authors introduce a LAM-based optimization framework using large language models (LLMs) to improve reinforcement learning for secure communication in LAWNs. This includes generating enhanced state features and designing intrinsic rewards.

Result: Simulation results through a case study validate the proposed LAM-based framework's effectiveness for securing LAWNs.

Conclusion: LAMs offer promising solutions for enhancing communication security in LAWNs. The paper concludes by proposing future research directions for integrating LAMs into similar applications.

Abstract: Low-altitude wireless networks (LAWNs) have the potential to revolutionize
communications by supporting a range of applications, including urban parcel
delivery, aerial inspections and air taxis. However, compared with traditional
wireless networks, LAWNs face unique security challenges due to low-altitude
operations, frequent mobility and reliance on unlicensed spectrum, making it
more vulnerable to some malicious attacks. In this paper, we investigate some
large artificial intelligence model (LAM)-enabled solutions for secure
communications in LAWNs. Specifically, we first explore the amplified security
risks and important limitations of traditional AI methods in LAWNs. Then, we
introduce the basic concepts of LAMs and delve into the role of LAMs in
addressing these challenges. To demonstrate the practical benefits of LAMs for
secure communications in LAWNs, we propose a novel LAM-based optimization
framework that leverages large language models (LLMs) to generate enhanced
state features on top of handcrafted representations, and to design intrinsic
rewards accordingly, thereby improving reinforcement learning performance for
secure communication tasks. Through a typical case study, simulation results
validate the effectiveness of the proposed framework. Finally, we outline
future directions for integrating LAMs into secure LAWN applications.

</details>


### [334] [Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network](https://arxiv.org/abs/2508.00042)
*Athanasios Tziouvaras,Carolina Fortuna,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Marko Grobelnik,Blaž Bertalanič*

Main category: cs.NI

TL;DR: This paper introduces two unsupervised, model-agnostic concept drift detection methods for AI-driven 6G networks to maintain model accuracy under non-stationary wireless environments.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the challenges posed by concept drift due to dynamic wireless environments like user mobility and infrastructure changes, which degrade machine-learning model performance.

Method: The authors propose two unsupervised, expected-utility score-based drift detection methods that identify concept drifts and assess model retraining needs without requiring ground-truth labels post-deployment.

Result: These detectors improve performance for wireless localization and link-anomaly detection tasks, outperforming classical methods (such as ADWIN, DDM, CUSUM) by 20-40 percentage points and achieving high F1-scores (0.94 and 1.00) in retraining alarm accuracy.

Conclusion: The methods effectively resolve concept drift challenges in AI-native 6G networks, offering better performance in detection and reduced false alarm rates compared to classical techniques.

Abstract: AI-native 6G networks promise unprecedented automation and performance by
embedding machine-learning models throughout the radio access and core segments
of the network. However, the non-stationary nature of wireless environments due
to infrastructure changes, user mobility, and emerging traffic patterns,
induces concept drifts that can quickly degrade these model accuracies.
Existing methods in general are very domain specific, or struggle with certain
type of concept drift. In this paper, we introduce two unsupervised,
model-agnostic, batch concept drift detectors. Both methods compute an
expected-utility score to decide when concept drift occurred and if model
retraining is warranted, without requiring ground-truth labels after
deployment. We validate our framework on two real-world wireless use cases in
outdoor fingerprinting for localization and for link-anomaly detection, and
demonstrate that both methods are outperforming classical detectors such as
ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an
F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus
reducing the false alarm rate by up to 20 percentage points compared to the
best classical detectors.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [335] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: This study investigates the impact of AI on software developers' productivity and experience using the SPACE framework, finding that AI generally improves routine tasks' efficiency and satisfaction but has limited effect on collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand how AI tools influence various aspects of software developers' work, addressing questions about productivity, satisfaction, and collaboration.

Method: The study employed a mixed-methods approach including surveys with over 500 developers, interviews, and observational studies to gather quantitative and qualitative insights.

Result: AI tools are widely adopted and perceived to enhance productivity for routine tasks. Positive impacts vary by task complexity, usage, and team-level adoption. Efficiency and satisfaction are improved, but collaboration shows limited benefits.

Conclusion: AI augments developers rather than replacing them. Its effective integration depends on team culture, organizational support, and peer learning. Recommendations are provided for better adoption in software engineering.

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [336] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: The paper proposes using classical hypertext principles to enhance user agency in algorithm-driven systems by contrasting user-driven and agent-driven interfaces.


<details>
  <summary>Details</summary>
Motivation: To address the issue of diminishing user agency in algorithm-dominant environments that prioritize engagement and efficiency.

Method: The authors performed a comparative analysis of user interfaces such as Wikipedia vs. Instagram Explore and Are.na vs. GenAI image tools.

Result: The study found that hypertext systems promote provenance, associative thinking, and user-driven meaning-making, unlike algorithmic systems that tend to obscure processes and reduce user participation.

Conclusion: Introducing hypertextual values like friction, traceability, and structure in design can help reclaim user agency in algorithmic systems.

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


### [337] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)
*Guilherme Guerino,Luiz Rodrigues,Luana Bianchiniand Mariana Alves,Marcelo Marinho,Thomaz Veloso,Valmir Macario,Diego Dermeval,Thales Vieira,Ig Bittencourt,Seiji Isotani*

Main category: cs.HC

TL;DR: The paper discusses using Augmented Intelligence (AuI) to tackle the challenges of Artificial Intelligence in Education (AIED), focusing on the development and testing of a teacher-centered Intelligent Tutoring System (ITS) called MathAIde for mathematics feedback.


<details>
  <summary>Details</summary>
Motivation: To address existing challenges in AIED, such as the role of teachers in system design, limitations of AI tools, and accessibility of resources, by emphasizing human-AI collaboration through AuI.

Method: Design and evaluation of MathAIde through brainstorming sessions, prototyping, A/B testing, and case studies in real classroom environments for both teachers and students.

Result: The study identified key design elements for integrating AuI in ITSs, focusing on teacher feedback and pre-defined remediation methods, and demonstrated successful real-world deployment.

Conclusion: A teacher-centered, user-focused design of ITSs boosts their utility and adoption, with practical contributions for resource-limited educational settings.

Abstract: Integrating Artificial Intelligence in Education (AIED) aims to enhance
learning experiences through technologies like Intelligent Tutoring Systems
(ITS), offering personalized learning, increased engagement, and improved
retention rates. However, AIED faces three main challenges: the critical role
of teachers in the design process, the limitations and reliability of AI tools,
and the accessibility of technological resources. Augmented Intelligence (AuI)
addresses these challenges by enhancing human capabilities rather than
replacing them, allowing systems to suggest solutions. In contrast, humans
provide final assessments, thus improving AI over time. In this sense, this
study focuses on designing, developing, and evaluating MathAIde, an ITS that
corrects mathematics exercises using computer vision and AI and provides
feedback based on photos of student work. The methodology included
brainstorming sessions with potential users, high-fidelity prototyping, A/B
testing, and a case study involving real-world classroom environments for
teachers and students. Our research identified several design possibilities for
implementing AuI in ITSs, emphasizing a balance between user needs and
technological feasibility. Prioritization and validation through prototyping
and testing highlighted the importance of efficiency metrics, ultimately
leading to a solution that offers pre-defined remediation alternatives for
teachers. Real-world deployment demonstrated the usefulness of the proposed
solution. Our research contributes to the literature by providing a usable,
teacher-centered design approach that involves teachers in all design phases.
As a practical implication, we highlight that the user-centered design approach
increases the usefulness and adoption potential of AIED systems, especially in
resource-limited environments.

</details>


### [338] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)
*Zhanna Kaufman,Madeline Endres,Cindy Xiong Bearfield,Yuriy Brun*

Main category: cs.HC

TL;DR: The study examines how explainability visualizations affect comprehension, bias perception, and trust in ML models, finding that increased comprehension often leads to reduced trust due to heightened bias awareness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of how explainability visualizations for ML models influence stakeholders' trust, comprehension, and bias perception, particularly since stakeholders' trust differs based on their backgrounds.

Method: The authors create a taxonomy of existing explainability visualizations and conduct user studies using five popular tools (LIME, SHAP, CP, Anchors, and ELI5) to explore the relationship between visualization characteristics, comprehension, bias perception, and trust.

Result: The study identifies an inverse relationship between trust and comprehension, showing that better comprehension often increases perceived bias, which, in turn, reduces trust. They verify these effects through controlled manipulation of visualization designs.

Conclusion: Improved visualization designs can facilitate responsible ML use by balancing the trade-offs between comprehension, bias perception, and trust. Reducing perceived bias—either through fairer models or visualization adjustments—boosts trust even when users' comprehension is high.

Abstract: Systems relying on ML have become ubiquitous, but so has biased behavior
within them. Research shows that bias significantly affects stakeholders' trust
in systems and how they use them. Further, stakeholders of different
backgrounds view and trust the same systems differently. Thus, how ML models'
behavior is explained plays a key role in comprehension and trust. We survey
explainability visualizations, creating a taxonomy of design characteristics.
We conduct user studies to evaluate five state-of-the-art visualization tools
(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how
taxonomy characteristics affect comprehension, bias perception, and trust for
non-expert ML users. Surprisingly, we find an inverse relationship between
comprehension and trust: the better users understand the models, the less they
trust them. We investigate the cause and find that this relationship is
strongly mediated by bias perception: more comprehensible visualizations
increase people's perception of bias, and increased bias perception reduces
trust. We confirm this relationship is causal: Manipulating explainability
visualizations to control comprehension, bias perception, and trust, we show
that visualization design can significantly (p < 0.001) increase comprehension,
increase perceived bias, and reduce trust. Conversely, reducing perceived model
bias, either by improving model fairness or by adjusting visualization design,
significantly increases trust even when comprehension remains high. Our work
advances understanding of how comprehension affects trust and systematically
investigates visualization's role in facilitating responsible ML applications.

</details>


### [339] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)
*Ziqing Xu,Nick Bryan-Kinns*

Main category: cs.HC

TL;DR: This paper introduces DeformTune, a tactile and accessible AI music system aimed at non-musicians, and explores user challenges and design opportunities.


<details>
  <summary>Details</summary>
Motivation: Many AI music tools are inaccessible to non-musicians due to text prompts, complex controls, or technical requirements.

Method: The researchers combined a tactile deformable interface with the MeasureVAE model and conducted a study with 11 non-musician participants, analyzing feedback through thematic analysis.

Result: The study revealed challenges like unclear controls, limited expressiveness, and the need for guidance. It also offered design opportunities, such as multimodal feedback and progressive interaction support.

Conclusion: DeformTune provides early insights into making AI music systems more understandable and empowering for non-musicians, contributing to explainable and accessible AI design.

Abstract: Many existing AI music generation tools rely on text prompts, complex
interfaces, or instrument-like controls, which may require musical or technical
knowledge that non-musicians do not possess. This paper introduces DeformTune,
a prototype system that combines a tactile deformable interface with the
MeasureVAE model to explore more intuitive, embodied, and explainable AI
interaction. We conducted a preliminary study with 11 adult participants
without formal musical training to investigate their experience with
AI-assisted music creation. Thematic analysis of their feedback revealed
recurring challenge--including unclear control mappings, limited expressive
range, and the need for guidance throughout use. We discuss several design
opportunities for enhancing explainability of AI, including multimodal feedback
and progressive interaction support. These findings contribute early insights
toward making AI music systems more explainable and empowering for novice
users.

</details>


### [340] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)
*Jacqueline Elise Bruen,Myounghoon Jeon*

Main category: cs.HC

TL;DR: The study explored perceptions of AI-generated art and found people attribute more artistic merit to AI-made works when unaware of its use.


<details>
  <summary>Details</summary>
Motivation: To address mixed opinions and evaluate perceptions of art created using generative AI tools.

Method: Analyzed audience perceptions of dance performances—with and without GenAI involvement—across two conditions (before/after knowing the creation process) with surveys.

Result: Participants showed higher artistic merit for AI-generated works when unaware of the tool's involvement in their creation.

Conclusion: Social context and user interpretation are crucial in shaping perceptions of AI-generated art, prompting further discussions for bridging understanding gaps.

Abstract: With the development of generative artificial intelligence (GenAI) tools to
create art, stakeholders cannot come to an agreement on the value of these
works. In this study we uncovered the mixed opinions surrounding art made by
AI. We developed two versions of a dance performance augmented by technology
either with or without GenAI. For each version we informed audiences of the
performance's development either before or after a survey on their perceptions
of the performance. There were thirty-nine participants (13 males, 26 female)
divided between the four performances. Results demonstrated that individuals
were more inclined to attribute artistic merit to works made by GenAI when they
were unaware of its use. We present this case study as a call to address the
importance of utilizing the social context and the users' interpretations of
GenAI in shaping a technical explanation, leading to a greater discussion that
can bridge gaps in understanding.

</details>


### [341] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: MetaExplainer is a neuro-symbolic framework that bridges the gap between model-generated explanations and user needs by employing advanced language models and an Explanation Ontology. It demonstrates high performance across explanation stages.


<details>
  <summary>Details</summary>
Motivation: There exists a gap between AI model explanations and user requirements, which hinders trustworthiness and interpretability of AI systems.

Method: The framework uses a three-stage process: user question formatting via LLMs, explainer method recommendations, and natural language synthesis guided by an Explanation Ontology.

Result: MetaExplainer achieves high metrics, including 59.06% F1-score for question reframing, 70% faithfulness for model explanations, and 67% context utilization in natural language synthesis. User studies validated the system's creativity and comprehensiveness.

Conclusion: MetaExplainer bridges the explanatory gap using structured processes and achieves broad applicability in diverse domains for enhancing AI trustworthiness and user-centered explanations.

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


### [342] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)
*Süeda Özkaya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: This paper reviews the use of Large Language Models (LLMs) in Virtual Reality (VR) games, highlighting their impact on NPC interactions, storytelling, and adaptive systems while addressing challenges such as ethics and scalability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how LLMs can transform VR games in terms of narrative, interaction, accessibility, and personalization, providing groundbreaking immersive experiences.

Method: The study draws on the analysis of 62 peer-reviewed papers published from 2018 to 2025 to evaluate the applications, challenges, and future potential of LLMs in VR environments.

Result: LLMs contribute significantly to enhancing realism, creativity, and user engagement in VR settings but require careful integration of hybrid AI designs, multimodal interactions, and ethical measures.

Conclusion: Robust strategies are essential for the responsible advancement of LLM-powered VR systems, emphasizing multimodal AI, reinforcement learning, and ethical safeguards for inclusive and intelligent design.

Abstract: The integration of Large Language Models (LLMs) into Virtual Reality (VR)
games marks a paradigm shift in the design of immersive, adaptive, and
intelligent digital experiences. This paper presents a comprehensive review of
recent research at the intersection of LLMs and VR, examining how these models
are transforming narrative generation, non-player character (NPC) interactions,
accessibility, personalization, and game mastering. Drawing from an analysis of
62 peer reviewed studies published between 2018 and 2025, we identify key
application domains ranging from emotionally intelligent NPCs and procedurally
generated storytelling to AI-driven adaptive systems and inclusive gameplay
interfaces. We also address the major challenges facing this convergence,
including real-time performance constraints, memory limitations, ethical risks,
and scalability barriers. Our findings highlight that while LLMs significantly
enhance realism, creativity, and user engagement in VR environments, their
effective deployment requires robust design strategies that integrate
multimodal interaction, hybrid AI architectures, and ethical safeguards. The
paper concludes by outlining future research directions in multimodal AI,
affective computing, reinforcement learning, and open-source development,
aiming to guide the responsible advancement of intelligent and inclusive VR
systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [343] [AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation](https://arxiv.org/abs/2508.00733)
*Le Wang,Jun Wang,Feng Deng,Chen Zhang,Kun Gai,Di Zhang*

Main category: cs.SD

TL;DR: AudioGen-Omni is a multimodal audio synthesis model capable of generating synchronized audio from video inputs using a unified training approach and novel techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a unified model that generates synchronized, high-quality audio, speech, and songs based on multimodal inputs, aiming to address semantic alignment and lip-sync accuracy challenges.

Method: A novel joint training paradigm integrating video-text-audio corpora, employing AdaLN-based joint attention mechanism, and PAAPI for precise multimodal alignment.

Result: AudioGen-Omni achieves state-of-the-art results for Text-to-Audio/Speech/Song tasks, improves audio quality, semantic alignment, and efficiency with 1.91-second inference for 8 seconds of audio.

Conclusion: AudioGen-Omni sets a new benchmark in multimodal audio generation tasks, showcasing advancements in semantic richness, acoustic diversity, and computational efficiency.

Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [344] [Anomaly detection with spiking neural networks for LHC physics](https://arxiv.org/abs/2508.00063)
*Barry M. Dillon,Jim Harkin,Aqib Javed*

Main category: hep-ph

TL;DR: This paper explores using Spiking Neural Network-based AutoEncoders for anomaly detection in particle collisions at the LHC, focusing on low-latency real-time applications.


<details>
  <summary>Details</summary>
Motivation: To improve real-time anomaly detection in particle physics experiments at the LHC while addressing the challenges of latency and computational efficiency.

Method: The paper designs and evaluates a simple Spiking Neural Network (SNN) AutoEncoder using the CMS ADC2021 dataset.

Result: The SNN AutoEncoders perform competitively with conventional AutoEncoders in detecting anomalies under all tested signal models.

Conclusion: SNN AutoEncoders show promise for efficient, low-latency anomaly detection in LHC experiments, with potential for further enhancements via neuromorphic hardware advancements.

Abstract: Anomaly detection offers a promising strategy for discovering new physics at
the Large Hadron Collider (LHC). This paper investigates AutoEncoders built
using neuromorphic Spiking Neural Networks (SNNs) for this purpose. One key
application is at the trigger level, where anomaly detection tools could
capture signals that would otherwise be discarded by conventional selection
cuts. These systems must operate under strict latency and computational
constraints. SNNs are inherently well-suited for low-latency, low-memory,
real-time inference, particularly on Field-Programmable Gate Arrays (FPGAs).
Further gains are expected with the rapid progress in dedicated neuromorphic
hardware development. Using the CMS ADC2021 dataset, we design and evaluate a
simple SNN AutoEncoder architecture. Our results show that the SNN AutoEncoders
are competitive with conventional AutoEncoders for LHC anomaly detection across
all signal models.

</details>


### [345] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: This paper introduces the use of diffusion models to generate jet images for proton-proton collisions at the LHC, showing that consistency models outperform score-based diffusion models in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To improve the generation accuracy and computational efficiency of jet image data used in High Energy Physics (HEP) research.

Method: The paper trains score-based diffusion models and consistency models on 2D jet image representations derived from kinematic variables in the JetNet dataset. Image fidelity is assessed using metrics like Frechet Inception Distance (FID).

Result: Consistency models are shown to produce jet images with higher fidelity and stability than score-based diffusion models.

Conclusion: The use of diffusion models, particularly consistency models, offers enhanced tools for HEP research by improving computational efficiency and jet image generation fidelity.

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>
