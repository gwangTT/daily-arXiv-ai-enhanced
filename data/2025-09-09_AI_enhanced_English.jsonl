{"id": "2509.05445", "pdf": "https://arxiv.org/pdf/2509.05445", "abs": "https://arxiv.org/abs/2509.05445", "authors": ["Grzegorz Sroka", "S\u0142awomir T. Wierzcho\u0144"], "title": "Robustness and Invariance of Hybrid Metaheuristics under Objective Function Transformations", "categories": ["cs.NE"], "comment": null, "summary": "This paper evaluates the robustness and structural invariance of hybrid\npopulation-based metaheuristics under various objective space transformations.\nA lightweight plug-and-play hybridization operator is applied to nineteen\nstate-of-the-art algorithms-including differential evolution (DE), particle\nswarm optimization (PSO), and recent bio-inspired methods-without modifying\ntheir internal logic. Benchmarking on the CEC-2017 suite across four dimensions\n(10, 30, 50, 100) is performed under five transformation types: baseline,\ntranslation, scaling, rotation, and constant shift. Statistical comparisons\nbased on Wilcoxon and Friedman tests, Bayesian dominance analysis, and\nconvergence trajectory profiling consistently show that differential-based\nhybrids (e.g., hIMODE, hSHADE, hDMSSA) maintain high accuracy, stability, and\ninvariance under all tested deformations. In contrast, classical\nalgorithms-especially PSO- and HHO-based variants-exhibit significant\nperformance degradation under non-separable or distorted landscapes. The\nfindings confirm the superiority of adaptive, structurally resilient hybrids\nfor real-world optimization tasks subject to domain-specific transformations.", "AI": {"tldr": "The paper evaluates the robustness of hybrid metaheuristics under various transformations, showing differential-based hybrids outperform classical methods.", "motivation": "To assess how robust and structurally invariant hybrid population-based metaheuristics are when faced with transformations in objective space.", "method": "The study applies a hybridization operator to 19 state-of-the-art metaheuristics and benchmarks them on CEC-2017 across different transformations and dimensions, using statistical tools for comparison.", "result": "Differential-based hybrids exhibit high accuracy and stability across transformations, while classical methods, especially PSO and HHO variants, suffer under distorted landscapes.", "conclusion": "Adaptive hybrids show resilience and are superior for optimization tasks involving domain-specific objective space transformations."}}
{"id": "2509.05858", "pdf": "https://arxiv.org/pdf/2509.05858", "abs": "https://arxiv.org/abs/2509.05858", "authors": ["Vedant Karia", "Abdullah Zyarah", "Dhireesha Kudithipudi"], "title": "Genesis: A Spiking Neuromorphic Accelerator With On-chip Continual Learning", "categories": ["cs.NE", "cs.SY", "eess.SY"], "comment": null, "summary": "Continual learning, the ability to acquire and transfer knowledge through a\nmodels lifetime, is critical for artificial agents that interact in real-world\nenvironments. Biological brains inherently demonstrate these capabilities while\noperating within limited energy and resource budgets. Achieving continual\nlearning capability in artificial systems considerably increases memory and\ncomputational demands, and even more so when deploying on platforms with\nlimited resources. In this work, Genesis, a spiking continual learning\naccelerator, is proposed to address this gap. The architecture supports\nneurally inspired mechanisms, such as activity-dependent metaplasticity, to\nalleviate catastrophic forgetting. It integrates low-precision continual\nlearning parametersand employs a custom data movement strategy to accommodate\nthe sparsely distributed spikes. Furthermore, the architecture features a\nmemory mapping technique that places metaplasticity parameters and synaptic\nweights in a single address location for faster memory access. Results show\nthat the mean classification accuracy for Genesis is 74.6% on a task-agnostic\nsplit-MNIST benchmark with power consumption of 17.08mW in a 65nm technology\nnode.", "AI": {"tldr": "This paper introduces Genesis, a spiking continual learning accelerator designed for efficient memory and energy use, achieving a classification accuracy of 74.6% on split-MNIST while operating at low power.", "motivation": "To enable continual learning in artificial systems, akin to biological brains, while addressing challenges of increased memory and computational demands, particularly on platforms with limited resources.", "method": "The study proposes Genesis, featuring neurally inspired mechanisms like activity-dependent metaplasticity to mitigate catastrophic forgetting, alongside innovations like low-precision parameters, a data movement strategy for sparsity, and optimized memory mapping.", "result": "Genesis achieves a mean classification accuracy of 74.6% on a split-MNIST task-agnostic benchmark, consuming only 17.08mW on a 65nm technology node.", "conclusion": "High accuracy and energy efficiency in Genesis demonstrate its potential to advance continual learning systems, making them viable for deployment in resource-constrained environments."}}
{"id": "2509.06272", "pdf": "https://arxiv.org/pdf/2509.06272", "abs": "https://arxiv.org/abs/2509.06272", "authors": ["Nitin Gupta", "Bapi Dutta", "Anupam Yadav"], "title": "An Explainable Framework for Particle Swarm Optimization using Landscape Analysis and Machine Learning", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Swarm intelligence algorithms have demonstrated remarkable success in solving\ncomplex optimization problems across diverse domains. However, their widespread\nadoption is often hindered by limited transparency in how algorithmic\ncomponents influence performance. This work presents a multi-faceted\ninvestigation of Particle Swarm Optimization (PSO) to further understand the\nkey role of different topologies for better interpretability and\nexplainability. To achieve this objective, we first develop a comprehensive\nlandscape characterization framework using Exploratory Landscape Analysis (ELA)\nto quantify problem difficulty and identify critical features affecting the\noptimization performance of PSO. Next, we conduct a rigorous empirical study\ncomparing three fundamental swarm communication architectures -- Ring, Star,\nand Von Neumann topologies -- analysing their distinct impacts on\nexploration-exploitation balance, convergence behaviour, and solution quality\nand eventually develop an explainable benchmarking framework for PSO, to decode\nhow swarm topologies affects information flow, diversity, and convergence.\nBased on this, a novel machine learning approach for automated algorithm\nconfiguration is introduced for training predictive models on extensive Area\nover the Convergence Curve (AOCC) data to recommend optimal settings based on\nproblem characteristics. Through systematic experimentation across twenty four\nbenchmark functions in multiple dimensions, we establish practical guidelines\nfor topology selection and parameter configuration. These findings advance the\ndevelopment of more transparent and reliable swarm intelligence systems. The\nsource codes of this work can be accessed at\nhttps://github.com/GitNitin02/ioh_pso.", "AI": {"tldr": "The paper investigates Particle Swarm Optimization (PSO) by studying swarm topologies and proposes an explainable benchmarking framework, alongside a machine learning-based method for automated configuration.", "motivation": "To address the lack of transparency in understanding how PSO algorithmic components contribute to performance and advance interpretability and explainability in swarm intelligence algorithms.", "method": "Uses Exploratory Landscape Analysis (ELA) to characterize problem landscapes, evaluates swarm topologies (Ring, Star, Von Neumann) using empirical studies, and introduces a machine learning approach for predicting optimal configurations based on Area over the Convergence Curve (AOCC) data.", "result": "The study identifies how different swarm communication topologies influence exploration, convergence, and performance metrics and provides practical guidelines for topology and parameter selection using experimental data.", "conclusion": "The work enhances transparency and interpretability in PSO, enabling improved algorithm configuration and development of more reliable swarm intelligence systems with publicly available source codes."}}
{"id": "2509.06636", "pdf": "https://arxiv.org/pdf/2509.06636", "abs": "https://arxiv.org/abs/2509.06636", "authors": ["Ismael Gomez", "Guangzhi Tang"], "title": "Full Integer Arithmetic Online Training for Spiking Neural Networks", "categories": ["cs.NE"], "comment": "Accepted at ICANN 2025", "summary": "Spiking Neural Networks (SNNs) are promising for neuromorphic computing due\nto their biological plausibility and energy efficiency. However, training\nmethods like Backpropagation Through Time (BPTT) and Real Time Recurrent\nLearning (RTRL) remain computationally intensive. This work introduces an\ninteger-only, online training algorithm using a mixed-precision approach to\nimprove efficiency and reduce memory usage by over 60%. The method replaces\nfloating-point operations with integer arithmetic to enable hardware-friendly\nimplementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,\nRSNNs), showing versatility across architectures. Evaluations on MNIST and the\nSpiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models\nachieve accuracy comparable to or better than full-precision baselines using\n16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in\nlow-precision and deeper models, performance remains robust. In conclusion, the\nproposed integer-only online learning algorithm presents an effective solution\nfor efficiently training SNNs, enabling deployment on resource-constrained\nneuromorphic hardware without sacrificing accuracy.", "AI": {"tldr": "The paper introduces an integer-only, online training algorithm for Spiking Neural Networks (SNNs) using a mixed-precision approach that improves efficiency and reduces memory usage.", "motivation": "SNNs are biologically plausible and energy-efficient but training methods like BPTT and RTRL are computationally intensive, leading to the need for improved methods.", "method": "The algorithm employs a mixed-precision approach, replacing floating-point operations with integer arithmetic for hardware-friendly implementation and generalizing to different architectures like CSNNs and RSNNs.", "result": "Evaluations on MNIST and SHD datasets show comparable or better accuracy to full-precision models while reducing memory usage by over 60%, with 16-bit shadow and 8- or 12-bit inference weights.", "conclusion": "The proposed method is efficient for training SNNs, addressing computational limitations while enabling deployment on resource-constrained hardware without compromising accuracy."}}
{"id": "2509.05365", "pdf": "https://arxiv.org/pdf/2509.05365", "abs": "https://arxiv.org/abs/2509.05365", "authors": ["Dingcui Yu", "Yunpeng Song", "Yiyang Huang", "Yumiao Zhao", "Yina Lv", "Chundong Wang", "Youtao Zhang", "Liang Shi"], "title": "Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs", "categories": ["cs.PF"], "comment": "Dingcui Yu, Yunpeng Song and Yiyang Huang have equal contributions to\n  this work. Liang Shi and Yina Lv are the corresponding authors", "summary": "Data compression is widely adopted for modern solid-state drives (SSDs) to\nmitigate both storage capacity and SSD lifetime issues. Researchers have\nproposed compression schemes at different system layers, including device-side\nsolutions like CCSDs ( c ompression-based c omputational SSDs) and compression\nsupported by host-side, like F2FS (flash-friendly file system). We conduct\nquantitative studies to understand how host-side and device-side compression\nschemes affect the temperature and performance of SSD-based storage systems.\nFrom our experiments, device-side compression, facilitated by a hardware\ncompression engine, can raise the temperature of CCSDs to intolerable levels,\nresulting in throttling and service shutdown. In contrast, host-side\ncompression causes software-stack overhead, which often results in large\nperformance degradation and resource consumption. To ensure efficient data\ncompression with high performance and better temperature control, we propose\nWaltz, a temperature-aware cooperative compression method that schedules\n(de)compression tasks at the host and device sides by monitoring device\ntemperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for\nspace and performance optimization, respectively. Waltz is implemented within\nF2FS, achieving high performance while extending SSD lifetime and preventing\noverheating-induced in-flight shutdowns.", "AI": {"tldr": "This paper evaluates the impact of host-side and device-side data compression on SSD temperature and performance, proposing temperature-aware cooperative compression solutions.", "motivation": "To address SSD storage and lifetime challenges using data compression while mitigating performance and overheating issues inherent in current approaches.", "method": "Quantitative experiments evaluate device-side and host-side compression impacts, followed by the introduction of Waltz, a cooperative compression algorithm integrated into F2FS.", "result": "Device-side compression overheats SSDs, while host-side compression reduces performance. Waltz is shown to optimize SSD performance, extend lifetime, and prevent overheating.", "conclusion": "Temperature-aware cooperative compression strategies like Waltz improve SSD efficiency, balance performance, and control overheating."}}
{"id": "2509.05451", "pdf": "https://arxiv.org/pdf/2509.05451", "abs": "https://arxiv.org/abs/2509.05451", "authors": ["Niansong Zhang", "Wenbo Zhu", "Courtney Golden", "Dan Ilan", "Hongzheng Chen", "Christopher Batten", "Zhiru Zhang"], "title": "Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device", "categories": ["cs.AR", "cs.PF"], "comment": "MICRO 2025", "summary": "Compute-in-SRAM architectures offer a promising approach to achieving higher\nperformance and energy efficiency across a range of data-intensive\napplications. However, prior evaluations have largely relied on simulators or\nsmall prototypes, limiting the understanding of their real-world potential. In\nthis work, we present a comprehensive performance and energy characterization\nof a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.\nWe compare the GSI APU against established architectures, including CPUs and\nGPUs, to quantify its energy efficiency and performance potential. We introduce\nan analytical framework for general-purpose compute-in-SRAM devices that\nreveals fundamental optimization principles by modeling performance trade-offs,\nthereby guiding program optimizations.\n  Exploiting the fine-grained parallelism of tightly integrated memory-compute\narchitectures requires careful data management. We address this by proposing\nthree optimizations: communication-aware reduction mapping, coalesced DMA, and\nbroadcast-friendly data layouts. When applied to retrieval-augmented generation\n(RAG) over large corpora (10GB--200GB), these optimizations enable our\ncompute-in-SRAM system to accelerate retrieval by 4.8$\\times$--6.6$\\times$ over\nan optimized CPU baseline, improving end-to-end RAG latency by\n1.1$\\times$--1.8$\\times$. The shared off-chip memory bandwidth is modeled using\na simulated HBM, while all other components are measured on the real\ncompute-in-SRAM device. Critically, this system matches the performance of an\nNVIDIA A6000 GPU for RAG while being significantly more energy-efficient\n(54.4$\\times$-117.9$\\times$ reduction). These findings validate the viability\nof compute-in-SRAM for complex, real-world applications and provide guidance\nfor advancing the technology.", "AI": {"tldr": "This paper analyzes the real-world performance and energy efficiency of a commercial compute-in-SRAM device and demonstrates its advantages over traditional CPUs and GPUs in specific applications.", "motivation": "The motivation is to understand the real-world potential and optimization strategies of compute-in-SRAM architectures using a commercial device under realistic workloads.", "method": "The study proposes three data management optimizations and uses an analytical framework to evaluate the GSI APU against CPUs and GPUs, measuring actual device performance and modeling shared off-chip memory bandwidth.", "result": "The compute-in-SRAM device achieves a 4.8\u20136.6x speedup in retrieval tasks and matches GPU performance in RAG workloads, while being significantly more energy-efficient (54.4\u2013117.9x).", "conclusion": "Compute-in-SRAM architectures are validated as highly energy-efficient systems capable of handling complex, real-world workloads, and the paper offers guidance for their further optimization and adoption."}}
{"id": "2509.05504", "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "categories": ["cs.PL", "cs.AR"], "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution.", "AI": {"tldr": "The paper presents two approaches, CrosSym and SEFOS, for symbolic execution of peripherals in Virtual Prototypes (VPs). Both surpass state-of-the-art tools by enabling cross-level verification and performing comparably in runtime.", "motivation": "Modern hardware designs are increasingly complex, requiring advanced verification techniques to ensure subsystem functionality. Virtual Prototypes are a key tool for this, but integration with techniques like symbolic execution is hindered by challenges in modifying the SystemC kernel.", "method": "The authors developed two tools: CrosSym, which modifies the SystemC kernel, and SEFOS, which modifies a symbolic execution engine. Both are evaluated on peripherals across different abstraction levels.", "result": "The tools demonstrated extensive features for various verification tasks, identifying over 300 mutants. SEFOS excels in compatibility with unmodified SystemC kernels and peripherals, while CrosSym achieves better runtime and memory efficiency.", "conclusion": "Both CrosSym and SEFOS outperform current state-of-the-art approaches, with comparable runtime performance and the added capability of cross-level verification."}}
{"id": "2509.05303", "pdf": "https://arxiv.org/pdf/2509.05303", "abs": "https://arxiv.org/abs/2509.05303", "authors": ["Sam Davidson", "Li Sun", "Bhavana Bhasker", "Laurent Callot", "Anoop Deoras"], "title": "Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Infrastructure as Code (IaC) is fundamental to modern cloud computing,\nenabling teams to define and manage infrastructure through machine-readable\nconfiguration files. However, different cloud service providers utilize diverse\nIaC formats. The lack of a standardized format requires cloud architects to be\nproficient in multiple IaC languages, adding complexity to cloud deployment.\nWhile Large Language Models (LLMs) show promise in automating IaC creation and\nmaintenance, progress has been limited by the lack of comprehensive benchmarks\nacross multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark\ndataset for evaluating LLM-based IaC generation and mutation across AWS\nCloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset\nconsists of triplets containing initial IaC templates, natural language\nmodification requests, and corresponding updated templates, created through a\nsynthetic data generation pipeline with rigorous validation. We evaluate\nseveral state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while\nmodern LLMs can achieve high success rates (>95%) in generating syntactically\nvalid IaC across formats, significant challenges remain in semantic alignment\nand handling complex infrastructure patterns. Our ablation studies highlight\nthe importance of prompt engineering and retry mechanisms in successful IaC\ngeneration. We release Multi-IaC-Bench to facilitate further research in\nAI-assisted infrastructure management and establish standardized evaluation\nmetrics for this crucial domain.", "AI": {"tldr": "The paper introduces Multi-IaC-Bench, a benchmark for evaluating large language models (LLMs) in generating and modifying Infrastructure as Code (IaC) across various formats like Terraform and AWS CloudFormation.", "motivation": "To address the problem of lacking standardized benchmarks for multi-format IaC generation and modification, which limits progress in automating IaC management using LLMs.", "method": "The authors created a benchmark dataset with triplets consisting of IaC templates, natural language modification requests, and respective updated templates. They validated this dataset rigorously and evaluated state-of-the-art LLMs using it.", "result": "State-of-the-art LLMs achieved over 95% success rates in generating syntactically correct IaC but struggled with semantic alignment and complex infrastructure patterns.", "conclusion": "The release of Multi-IaC-Bench establishes a standard for evaluating LLM-based IaC generation and highlights areas like better semantic understanding for further research."}}
{"id": "2509.05314", "pdf": "https://arxiv.org/pdf/2509.05314", "abs": "https://arxiv.org/abs/2509.05314", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8pages; 7figures; 4 tables", "summary": "Data scarcity continues to be a major challenge in the field of robotic\nmanipulation. Although diffusion models provide a promising solution for\ngenerating robotic manipulation videos, existing methods largely depend on 2D\ntrajectories, which inherently face issues with 3D spatial ambiguity. In this\nwork, we present a novel framework named ManipDreamer3D for generating\nplausible 3D-aware robotic manipulation videos from the input image and the\ntext instruction. Our method combines 3D trajectory planning with a\nreconstructed 3D occupancy map created from a third-person perspective, along\nwith a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D\nfirst reconstructs the 3D occupancy representation from the input image and\nthen computes an optimized 3D end-effector trajectory, minimizing path length\nwhile avoiding collisions. Next, we employ a latent editing technique to create\nvideo sequences from the initial image latent and the optimized 3D trajectory.\nThis process conditions our specially trained trajectory-to-video diffusion\nmodel to produce robotic pick-and-place videos. Our method generates robotic\nvideos with autonomously planned plausible 3D trajectories, significantly\nreducing human intervention requirements. Experimental results demonstrate\nsuperior visual quality compared to existing methods.", "AI": {"tldr": "ManipDreamer3D is a framework that creates realistic 3D-aware robotic manipulation videos from images and text input using a trajectory-to-video diffusion model, addressing issues with 3D spatial ambiguity.", "motivation": "To overcome the data scarcity problem in robotic manipulation and tackle the limitations of existing 2D trajectory-dependent video generation methods by providing a 3D-aware solution.", "method": "The method involves reconstructing a 3D occupancy map, calculating an optimized 3D trajectory for robotic action, and using a trajectory-to-video diffusion model for generating realistic manipulation videos.", "result": "The method delivers enhanced visual quality and plausible robotic manipulation videos with reduced need for human intervention compared to previous approaches.", "conclusion": "The novel ManipDreamer3D framework effectively addresses the challenges of robotic video generation by combining 3D trajectory planning and diffusion modeling, offering superior outcomes."}}
{"id": "2509.05359", "pdf": "https://arxiv.org/pdf/2509.05359", "abs": "https://arxiv.org/abs/2509.05359", "authors": ["Yanis Labrak", "Richard Dufour", "Micka\u00ebl Rouvier"], "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.", "AI": {"tldr": "The paper studies Speech Language Models (SLMs), focusing on improving speech modeling through pre-training optimization, including model architecture, data representation, and training robustness.", "motivation": "The authors aim to enhance the adaptation of pre-trained language models to the speech modality by optimizing the speech modeling process during continual pre-training.", "method": "They conducted experiments varying speech encoders, clustering granularity, and data selection to analyze strategies for improving discretization and robustness for various model scales.", "result": "The experiments revealed the importance of clustering strategies varying with model capacity, effective discrete vocabulary use, and the critical role of domain-matched data in model robustness.", "conclusion": "Optimal speech language model performance requires attention to architecture, data representation, and robust training strategies, emphasizing adaptations in model components for target applications."}}
{"id": "2509.05372", "pdf": "https://arxiv.org/pdf/2509.05372", "abs": "https://arxiv.org/abs/2509.05372", "authors": ["Piotr Przymus", "Andreas Happe", "J\u00fcrgen Cito"], "title": "Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Model (LLM) - based Automated Program Repair (APR) systems are\nincreasingly integrated into modern software development workflows, offering\nautomated patches in response to natural language bug reports. However, this\nreliance on untrusted user input introduces a novel and underexplored attack\nsurface. In this paper, we investigate the security risks posed by adversarial\nbug reports -- realistic-looking issue submissions crafted to mislead APR\nsystems into producing insecure or harmful code changes. We develop a\ncomprehensive threat model and conduct an empirical study to evaluate the\nvulnerability of state-of-the-art APR systems to such attacks. Our\ndemonstration comprises 51 adversarial bug reports generated across a spectrum\nof strategies, from manual curation to fully automated pipelines. We test these\nagainst leading APR model and assess both pre-repair defenses (e.g., LlamaGuard\nvariants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and\npost-repair detectors (GitHub Copilot, CodeQL). Our findings show that current\ndefenses are insufficient: 90\\% of crafted bug reports triggered\nattacker-aligned patches. The best pre-repair filter blocked only 47\\%, while\npost-repair analysis-often requiring human oversight-was effective in just 58\\%\nof cases. To support scalable security testing, we introduce a prototype\nframework for automating the generation of adversarial bug reports. Our\nanalysis exposes a structural asymmetry: generating adversarial inputs is\ninexpensive, while detecting or mitigating them remains costly and error-prone.\nWe conclude with practical recommendations for improving the robustness of APR\nsystems against adversarial misuse and highlight directions for future work on\ntrustworthy automated repair.", "AI": {"tldr": "The paper investigates the security vulnerabilities of Automated Program Repair (APR) systems utilizing Large Language Models (LLMs) by introducing adversarial bug reports that lead to insecure code patches. Existing defenses prove inadequate against such attacks.", "motivation": "As LLM-based APR systems become integral to software development workflows, the research aims to address the overlooked security risks posed by adversarial bug reports, which could mislead systems into generating harmful code changes.", "method": "The study involves the creation of 51 adversarial bug reports using varying strategies (manual and automated), testing them against state-of-the-art APR systems. It assesses pre-repair defenses (e.g., custom filters) and post-repair detectors while introducing an automated framework for adversarial report generation.", "result": "Findings indicate that current defenses are ineffective, as 90% of crafted bug reports resulted in insecure patches. The best pre-repair filter blocked only 47% of adversarial reports, and post-repair analysis was effective in just 58% of cases.", "conclusion": "APR systems are vulnerable to adversarial exploitation, with detection and mitigation being costly and error-prone. The paper outlines recommendations for enhancing system robustness and calls for further research into trustworthy APR solutions."}}
{"id": "2509.05305", "pdf": "https://arxiv.org/pdf/2509.05305", "abs": "https://arxiv.org/abs/2509.05305", "authors": ["Yingjie Zhao", "Yicheng Song", "Fan Xu", "Zhiping Xu"], "title": "Predicting Brain Morphogenesis via Physics-Transfer Learning", "categories": ["q-bio.NC", "cs.LG", "nlin.PS"], "comment": null, "summary": "Brain morphology is shaped by genetic and mechanical factors and is linked to\nbiological development and diseases. Its fractal-like features, regional\nanisotropy, and complex curvature distributions hinder quantitative insights in\nmedical inspections. Recognizing that the underlying elastic instability and\nbifurcation share the same physics as simple geometries such as spheres and\nellipses, we developed a physics-transfer learning framework to address the\ngeometrical complexity. To overcome the challenge of data scarcity, we\nconstructed a digital library of high-fidelity continuum mechanics modeling\nthat both describes and predicts the developmental processes of brain growth\nand disease. The physics of nonlinear elasticity from simple geometries is\nembedded into a neural network and applied to brain models. This\nphysics-transfer approach demonstrates remarkable performance in feature\ncharacterization and morphogenesis prediction, highlighting the pivotal role of\nlocalized deformation in dominating over the background geometry. The\ndata-driven framework also provides a library of reduced-dimensional\nevolutionary representations that capture the essential physics of the highly\nfolded cerebral cortex. Validation through medical images and domain expertise\nunderscores the deployment of digital-twin technology in comprehending the\nmorphological complexity of the brain.", "AI": {"tldr": "A physics-transfer learning framework uses principles from nonlinear elasticity to predict brain growth and diseases, addressing complex geometric challenges and limited data.", "motivation": "The paper aims to better understand brain morphology, which is shaped by genetics and mechanics and is critical in biological development and medical diagnostics.", "method": "The researchers developed a physics-transfer learning framework leveraging elastic instability and bifurcation physics to train a neural network and provided a library of continuum mechanics modeling.", "result": "The framework achieved notable performance in characterizing features, predicting morphogenesis, and offering reduced-dimensional evolutionary representations of the brain.", "conclusion": "Digital-twin technology embedded with physics-driven insights can shape future studies of brain morphology, aiding both biological and medical advancements."}}
{"id": "2509.05541", "pdf": "https://arxiv.org/pdf/2509.05541", "abs": "https://arxiv.org/abs/2509.05541", "authors": ["Diego Sanchez Espinosa", "Erik H Thiede", "Yunan Yang"], "title": "Cryo-EM as a Stochastic Inverse Problem", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.OC", "physics.data-an", "65M32, 49Q22, 65M75, 65K10"], "comment": "25 pages, 8 figures", "summary": "Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of\nbiomolecules, but structural heterogeneity remains a major challenge in 3D\nreconstruction. Traditional methods assume a discrete set of conformations,\nlimiting their ability to recover continuous structural variability. In this\nwork, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP)\nover probability measures, where the observed images are modeled as the\npush-forward of an unknown distribution over molecular structures via a random\nforward operator. We pose the reconstruction problem as the minimization of a\nvariational discrepancy between observed and simulated image distributions,\nusing statistical distances such as the KL divergence and the Maximum Mean\nDiscrepancy. The resulting optimization is performed over the space of\nprobability measures via a Wasserstein gradient flow, which we numerically\nsolve using particles to represent and evolve conformational ensembles. We\nvalidate our approach using synthetic examples, including a realistic protein\nmodel, which demonstrates its ability to recover continuous distributions over\nstructural states. We analyze the connection between our formulation and\nMaximum A Posteriori (MAP) approaches, which can be interpreted as instances of\nthe discretize-then-optimize (DTO) framework. We further provide a consistency\nanalysis, establishing conditions under which DTO methods, such as MAP\nestimation, converge to the solution of the underlying infinite-dimensional\ncontinuous problem. Beyond cryo-EM, the framework provides a general\nmethodology for solving SIPs involving random forward operators.", "AI": {"tldr": "This paper proposes a novel stochastic approach to resolve structural variability challenges in cryo-EM reconstruction by using a variational discrepancy minimization framework.", "motivation": "Structural heterogeneity in cryo-EM imaging limits conventional 3D reconstruction methods, necessitating a new approach to handle continuous structural variations.", "method": "The authors formulate cryo-EM reconstruction as a stochastic inverse problem, utilizing statistical distances like KL divergence and Maximum Mean Discrepancy, and solve it via Wasserstein gradient flow and particle-based methods.", "result": "The approach successfully recovers continuous distributions of molecular structures in synthetic scenarios, including a realistic protein model, and underscores links to Maximum A Posteriori (MAP) methods.", "conclusion": "This work introduces a powerful framework for addressing continuous variability in cryo-EM and offers a generalized method for stochastic inverse problems with random forward operators."}}
{"id": "2509.05323", "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "categories": ["cs.AI", "cs.MM"], "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.", "AI": {"tldr": "The paper explores attention mechanisms in video diffusion transformers by visualizing cross-attention maps for text-to-video models, offering insights for both analysis and artistic use.", "motivation": "The motivation is to bridge the gap between analytical and artistic applications of AI by investigating and visualizing attention maps, enabling better understanding and creative possibilities.", "method": "The method involves extracting and visualizing cross-attention maps from the open-source Wan model and analyzing their temporal and spatial behavior in generative video models.", "result": "The study demonstrates the interpretability of attention maps and explores their dual role as analytical tools and artistic resources through experiments and an artistic case study.", "conclusion": "The work advances Explainable AI in the arts, emphasizing the potential of AI components like attention maps as creative media for artists."}}
{"id": "2509.05307", "pdf": "https://arxiv.org/pdf/2509.05307", "abs": "https://arxiv.org/abs/2509.05307", "authors": ["Sachin Chhabra", "Hemanth Venkateswara", "Baoxin Li"], "title": "Label Smoothing++: Enhanced Label Regularization for Training Neural Networks", "categories": ["cs.CV"], "comment": "Published in British Machine Vision Conference (BMVC), 2024", "summary": "Training neural networks with one-hot target labels often results in\noverconfidence and overfitting. Label smoothing addresses this issue by\nperturbing the one-hot target labels by adding a uniform probability vector to\ncreate a regularized label. Although label smoothing improves the network's\ngeneralization ability, it assigns equal importance to all the non-target\nclasses, which destroys the inter-class relationships. In this paper, we\npropose a novel label regularization training strategy called Label\nSmoothing++, which assigns non-zero probabilities to non-target classes and\naccounts for their inter-class relationships. Our approach uses a fixed label\nfor the target class while enabling the network to learn the labels associated\nwith non-target classes. Through extensive experiments on multiple datasets, we\ndemonstrate how Label Smoothing++ mitigates overconfident predictions while\npromoting inter-class relationships and generalization capabilities.", "AI": {"tldr": "The paper introduces Label Smoothing++, a method improving label smoothing by considering inter-class relationships, offering better generalization and less overconfidence.", "motivation": "Training neural networks with one-hot labels often creates overfitting and overconfident predictions. Current label smoothing solutions fail to consider inter-class relationships.", "method": "Label Smoothing++ assigns non-zero probabilities to non-target classes, reflecting inter-class relationships, while keeping target class labels fixed. Non-target labels are learned during training.", "result": "Extensive experiments across multiple datasets show that Label Smoothing++ reduces overconfidence, accounts for inter-class relationships, and enhances generalization.", "conclusion": "Label Smoothing++ effectively mitigates overconfidence issues in neural networks training while preserving and enhancing inter-class relationships, leading to improved model performance."}}
{"id": "2509.05316", "pdf": "https://arxiv.org/pdf/2509.05316", "abs": "https://arxiv.org/abs/2509.05316", "authors": ["Praveen Bushipaka", "Lucia Passaro", "Tommaso Cucinotta"], "title": "Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A conventional LLM Unlearning setting consists of two subsets -\"forget\" and\n\"retain\", with the objectives of removing the undesired knowledge from the\nforget set while preserving the remaining knowledge from the retain. In\nprivacy-focused unlearning research, a retain set is often further divided into\nneighbor sets, containing either directly or indirectly connected to the forget\ntargets; and augmented by a general-knowledge set. A common practice in\nexisting benchmarks is to employ only a single neighbor set, with general\nknowledge which fails to reflect the real-world data complexities and\nrelationships. LLM Unlearning typically involves 1:1 sampling or cyclic\niteration sampling. However, the efficacy and stability of these de facto\nstandards have not been critically examined. In this study, we systematically\nevaluate these common practices. Our findings reveal that relying on a single\nneighbor set is suboptimal and that a standard sampling approach can obscure\nperformance trade-offs. Based on this analysis, we propose and validate an\ninitial set of best practices: (1) Incorporation of diverse neighbor sets to\nbalance forget efficacy and model utility, (2) Standard 1:1 sampling methods\nare inefficient and yield poor results, (3) Our proposed Modular Entity-Level\nUnlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate\nthat this modular approach, combined with robust algorithms, provides a clear\nand stable path towards effective unlearning.", "AI": {"tldr": "The paper analyzes practices in LLM unlearning, identifies shortcomings in current methods, and introduces a Modular Entity-Level Unlearning (MELU) strategy for improved efficacy.", "motivation": "To address shortcomings in existing LLM unlearning methods which rely on overly simplistic setups, such as single neighbor sets and conventional sampling strategies, that fail to capture real-world data complexities.", "method": "The study evaluates current LLM unlearning practices and introduces the Modular Entity-Level Unlearning (MELU) strategy, which involves incorporating diverse neighbor sets and moving away from 1:1 and cyclic sampling.", "result": "The analysis revealed that traditional methods are suboptimal, and MELU outperforms these by ensuring balanced forget efficacy and model utility through a modular approach.", "conclusion": "The proposed MELU strategy and diversified neighbor sets offer a stable and effective framework for LLM unlearning, addressing limitations in existing practices."}}
{"id": "2509.06577", "pdf": "https://arxiv.org/pdf/2509.06577", "abs": "https://arxiv.org/abs/2509.06577", "authors": ["Marcos Eduardo Valle", "Santiago Velasco-Forero", "Joao Batista Florindo", "Gustavo Jesus Angulo"], "title": "Approximating Condorcet Ordering for Vector-valued Mathematical Morphology", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "Submitted to the 4th International Conference on Discrete Geometry\n  and Mathematical Morphology (DGMM 2025)", "summary": "Mathematical morphology provides a nonlinear framework for image and spatial\ndata processing and analysis. Although there have been many successful\napplications of mathematical morphology to vector-valued images, such as color\nand hyperspectral images, there is still no consensus on the most suitable\nvector ordering for constructing morphological operators. This paper addresses\nthis issue by examining a reduced ordering approximating the Condorcet ranking\nderived from a set of vector orderings. Inspired by voting problems, the\nCondorcet ordering ranks elements from most to least voted, with voters\nrepresenting different orderings. In this paper, we develop a machine learning\napproach that learns a reduced ordering that approximates the Condorcet\nordering. Preliminary computational experiments confirm the effectiveness of\nlearning the reduced mapping to define vector-valued morphological operators\nfor color images.", "AI": {"tldr": "This paper explores the use of machine learning to develop a reduced ordering for vector-valued morphological operators, approximating the Condorcet ranking derived from multiple orderings.", "motivation": "Overcoming the lack of consensus on vector ordering methods in mathematical morphology for vector-valued images, essential for effective image analysis.", "method": "A machine learning approach is utilized to learn reduced orderings that approximate the Condorcet ranking, inspired by voting problems.", "result": "Preliminary computational experiments show the effectiveness of the proposed method in defining vector-valued morphological operators for color images.", "conclusion": "The machine learning-driven reduced ordering offers a promising framework to improve vector-valued morphological image processing."}}
{"id": "2509.05511", "pdf": "https://arxiv.org/pdf/2509.05511", "abs": "https://arxiv.org/abs/2509.05511", "authors": ["Dhanya R Mathews", "Mudit Verma", "Pooja Aggarwal", "J. Lakshmi"], "title": "Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology", "categories": ["cs.PF", "cs.DC"], "comment": null, "summary": "Cloud application services are distributed in nature and have components\nacross the stack working together to deliver the experience to end users. The\nwide adoption of microservice architecture exacerbates failure management due\nto increased service components. To be effective, the strategies to enhance the\napplication service resilience need to be autonomous and developed at the\nservice's granularity, considering its end-to-end components. However, the\nmassive amount of observability data generated by all these components across\nthe service stack poses a significant challenge in reacting to anomalies and\nrestoring the service quality in real time. Identifying the most informative\nobservability data from across the cloud service stack and timely localization\nof root causes of anomalies thus becomes crucial to ensure service resilience.\nThis article presents a novel approach that considers the application service\ntopology to select the most informative metrics across the cloud stack to\nsupport efficient, explainable, and accurate root cause identifications in case\nof performance anomalies. The usefulness of the selected metrics is then\nevaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for\nlocalizing the root cause of performance anomalies. As a step towards improving\nthe accuracy and efficiency of RCD, this article then proposes the\nTopology-Aware-RCD (TA-RCD) that incorporates the end-to-end application\nservice topology in RCD. The evaluation of the failure injection studies shows\nthat the proposed approach performs at least 2X times better on average than\nthe state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.", "AI": {"tldr": "The paper introduces a Topology-Aware-RCD approach to improve the identification of root causes in performance anomalies, demonstrating significant advancements over existing methods.", "motivation": "The adoption of microservice architecture increases the complexity in managing failures due to numerous interconnected service components. Maintaining service resilience is challenging with the vast observability data generated across cloud service stacks.", "method": "The study selects informative metrics based on application service topology and integrates this knowledge into an enhanced Root Cause Detection (RCD) algorithm, termed Topology-Aware-RCD (TA-RCD).", "result": "The evaluation proves that TA-RCD significantly outperforms state-of-the-art RCD methods, achieving at least 2X better performance on average for Top-3 and Top-5 recall during failure injection studies.", "conclusion": "Incorporating the application service topology approach enhances root cause detection accuracy and explainability, laying a foundation for improved cloud application service resilience and efficiency."}}
{"id": "2509.05688", "pdf": "https://arxiv.org/pdf/2509.05688", "abs": "https://arxiv.org/abs/2509.05688", "authors": ["Kuan-Ting Lin", "Ching-Te Chiu", "Jheng-Yi Chang", "Shi-Zong Huang", "Yu-Ting Li"], "title": "High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator", "categories": ["cs.AR"], "comment": null, "summary": "Deep convolution Neural Network (DCNN) has been widely used in computer\nvision tasks. However, for edge devices even inference has too large\ncomputational complexity and data access amount. The inference latency of\nstate-of-the-art models are impractical for real-world applications. In this\npaper, we propose a high utilization energy-aware real-time inference deep\nconvolutional neural network accelerator, which improves the performance of the\ncurrent accelerators. First, we use the 1x1 size convolution kernel as the\nsmallest unit of the computing unit. Then we design suitable computing unit\nbased on the requirements of each model. Secondly, we use Reuse Feature SRAM to\nstore the output of the current layer in the chip and use the value as the\ninput of the next layer. Moreover, we import Output Reuse Strategy and Ring\nStream Dataflow to reduce the amount of data exchange between chips and DRAM.\nFinally, we present On-fly Pooling Module to let the calculation of the Pooling\nlayer directly complete in the chip. With the aid of the proposed method, the\nimplemented acceleration chip has an extremely high hardware utilization rate.\nWe reduce a generous amount of data transfer on the specific module, ECNN.\nCompared to the methods without reuse strategy, we can reduce 533 times of data\naccess amount. At the same time, we have enough computing power to perform\nreal-time execution of the existing image classification model, VGG16 and\nMobileNet. Compared with the design in VWA, we can speed up 7.52 times and have\n1.92x energy efficiency", "AI": {"tldr": "This paper introduces a real-time DCNN accelerator for edge devices that improves performance and energy efficiency by optimizing computation, data reuse, and transfer processes.", "motivation": "To address the impracticality of inference latency and high computational complexity of state-of-the-art DCNN models on edge devices.", "method": "Introduced an accelerator featuring 1x1 convolution kernels, a Reuse Feature SRAM for minimizing data transfer, output reuse strategies, Ring Stream Dataflow, and an On-fly Pooling Module for efficient layer computation in hardware.", "result": "Achieved 533x reduction in data access amount, 7.52x speed improvement, and 1.92x energy efficiency compared to prior methods, ensuring real-time processing capabilities.", "conclusion": "The proposed hardware accelerator significantly enhances inference efficiency, making DCNNs feasible for real-time applications on edge devices while markedly reducing energy usage and data movement."}}
{"id": "2509.05586", "pdf": "https://arxiv.org/pdf/2509.05586", "abs": "https://arxiv.org/abs/2509.05586", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types", "categories": ["cs.PL", "cs.CC"], "comment": null, "summary": "Verifying linearizability of concurrent data structures is NP-hard, even for\nsimple types. We present fixed-parameter tractable algorithms for monitoring\nstacks, queues, and anagram-agnostic data types (AADTs), parameterized by the\nmaximum concurrency. Our approach leverages frontier graphs and partition\nstates to bound the search space. For AADTs, equivalence of linearizations\nenables monitoring in log-linear time. For stacks, we introduce a grammar-based\nmethod with a sub-cubic reduction to matrix multiplication, and for queues, a\nsplit-sequence transition system supporting efficient dynamic programming.\nThese results unify tractability guarantees for both order-sensitive and\nanagram-agnostic data types under bounded concurrency.", "AI": {"tldr": "The paper addresses the NP-hard problem of verifying linearizability in concurrent data structures and introduces fixed-parameter tractable algorithms for stacks, queues, and anagram-agnostic data types, leveraging innovative methods to reduce computational complexity.", "motivation": "The authors aim to tackle the computational complexity associated with verifying linearizability in concurrent data structures, an NP-hard problem even for basic types, and to provide practical methods under scenarios of bounded concurrency.", "method": "The approach involves leveraging frontier graphs, partition states, grammar-based methods (for stacks), a split-sequence transition system (for queues), and exploiting linearization equivalences (for AADTs) to optimize the verification process.", "result": "The proposed methods make linearizability verification for stacks, queues, and AADTs computationally tractable under bounded concurrency, achieving log-linear monitoring for AADTs and leveraging advanced methods like sub-cubic matrix multiplication for others.", "conclusion": "The paper unifies tractability guarantees for different types of concurrent data structures under bounded concurrency, offering efficient and scalable approaches to the long-standing problem of verifying linearizability."}}
{"id": "2509.05870", "pdf": "https://arxiv.org/pdf/2509.05870", "abs": "https://arxiv.org/abs/2509.05870", "authors": ["Edith Cohen", "Moshe Shechner", "Uri Stemmer"], "title": "A Simple and Robust Protocol for Distributed Counting", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "We revisit the distributed counting problem, where a server must continuously\napproximate the total number of events occurring across $k$ sites while\nminimizing communication. The communication complexity of this problem is known\nto be $\\Theta(\\frac{k}{\\epsilon}\\log N)$ for deterministic protocols. Huang,\nYi, and Zhang (2012) showed that randomization can reduce this to\n$\\Theta(\\frac{\\sqrt{k}}{\\epsilon}\\log N)$, but their analysis is restricted to\nthe {\\em oblivious setting}, where the stream of events is independent of the\nprotocol's outputs.\n  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed\ncounting that removes the oblivious assumption. However, their communication\ncomplexity is suboptimal by a $polylog(k)$ factor and their protocol is\nsubstantially more complex than the oblivious protocol of Huang et al. (2012).\nThis left open a natural question: could it be that the simple protocol of\nHuang et al. (2012) is already robust?\n  We resolve this question with two main contributions. First, we show that the\nprotocol of Huang et al. (2012) is itself not robust by constructing an\nexplicit adaptive attack that forces it to lose its accuracy. Second, we\npresent a new, surprisingly simple, robust protocol for distributed counting\nthat achieves the optimal communication complexity of\n$O(\\frac{\\sqrt{k}}{\\epsilon} \\log N)$. Our protocol is simpler than that of\nXiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and\nis the first to match the optimal oblivious complexity in the adaptive setting.", "AI": {"tldr": "This paper explores the distributed counting problem and addresses whether existing simple protocols are robust when faced with adaptive attacks. It introduces a new simple protocol that achieves optimal communication complexity for adaptive settings.", "motivation": "The authors aim to determine if the simple protocol for distributed counting proposed by Huang et al. (2012) is robust under adaptive attacks, as previous protocols were either suboptimal or overly complex.", "method": "The paper employs two approaches: constructing an explicit adaptive attack to demonstrate the non-robustness of Huang et al. (2012)'s protocol and designing a new, simple, robust protocol with optimal communication complexity.", "result": "The authors prove that Huang et al. (2012)'s protocol is not robust by presenting an adaptive attack and propose a new protocol that not only simplifies distributed counting but also achieves optimal communication complexity in adaptive settings.", "conclusion": "This paper resolves a critical question in distributed counting by presenting an effective robust protocol that is simpler and matches the optimal communication complexity, filling gaps left by prior research."}}
{"id": "2509.05315", "pdf": "https://arxiv.org/pdf/2509.05315", "abs": "https://arxiv.org/abs/2509.05315", "authors": ["Petros Loukas", "David Bassir", "Savvas Chatzichristofis", "Angelos Amanatiadis"], "title": "Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has pushed their\nboundaries to many applications in various domains. Recently, the research\ncommunity has started to evaluate their potential adoption in autonomous\nvehicles and especially as complementary modules in the perception and planning\nsoftware stacks. However, their evaluation is limited in synthetic datasets or\nmanually driving datasets without the ground truth knowledge and more\nprecisely, how the current perception and planning algorithms would perform in\nthe cases under evaluation. For this reason, this work evaluates LLMs on\nreal-world edge cases where current autonomous vehicles have been proven to\nfail. The proposed architecture consists of an open vocabulary object detector\ncoupled with prompt engineering and large language model contextual reasoning.\nWe evaluate several state-of-the-art models against real edge cases and provide\nqualitative comparison results along with a discussion on the findings for the\npotential application of LLMs as anomaly detectors in autonomous vehicles.", "AI": {"tldr": "This paper evaluates large language models (LLMs) on real-world edge cases to test their potential as complementary modules in autonomous vehicle perception and planning.", "motivation": "Current evaluations of LLMs in autonomous vehicles are limited to synthetic or manually-driven datasets without ground truth knowledge, lacking insight into how current algorithms perform in challenging cases.", "method": "The authors propose an architecture combining open vocabulary object detection, prompt engineering, and LLM-based contextual reasoning to evaluate state-of-the-art models against real-world edge cases.", "result": "Qualitative comparisons are conducted for several LLM-based models against real edge cases, showcasing their potential and limitations.", "conclusion": "LLMs show promise as anomaly detectors in autonomous vehicles, but further analysis is required for their practical integration."}}
{"id": "2509.05360", "pdf": "https://arxiv.org/pdf/2509.05360", "abs": "https://arxiv.org/abs/2509.05360", "authors": ["Jerry Li", "Evangelos Papalexakis"], "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges.", "AI": {"tldr": "The paper tackles hallucinations in large language models (LLMs) by analyzing N-Gram frequency tensors to detect factual versus fabricated content, achieving improved results over traditional and modern baselines.", "motivation": "The problem of hallucinations in LLMs (fabricating information) undermines their trustworthiness in generating accurate content. Current methods and metrics (e.g., ROUGE, Perplexity) lack sufficient semantic depth for effective hallucination detection.", "method": "The paper proposes using an N-Gram frequency tensor constructed from LLM outputs. Tensor decomposition is applied to extract features, which are then fed into an MLP binary classifier to distinguish between factual and hallucinated content.", "result": "The proposed method, evaluated on the HaluEval dataset, outperforms traditional baselines and demonstrates competitive performance against state-of-the-art LLM judges.", "conclusion": "The novel N-Gram tensor and MLP-based approach enrich semantic understanding and represent a significant step forward in addressing hallucination detection in LLMs."}}
{"id": "2509.05394", "pdf": "https://arxiv.org/pdf/2509.05394", "abs": "https://arxiv.org/abs/2509.05394", "authors": ["Zoltan Toth-Czifra"], "title": "Reverse Browser: Vector-Image-to-Code Generator", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to AIWare 2025 ArXiv Track", "summary": "Automating the conversion of user interface design into code (image-to-code\nor image-to-UI) is an active area of software engineering research. However,\nthe state-of-the-art solutions do not achieve high fidelity to the original\ndesign, as evidenced by benchmarks. In this work, I approach the problem\ndifferently: I use vector images instead of bitmaps as model input. I create\nseveral large datasets for training machine learning models. I evaluate the\navailable array of Image Quality Assessment (IQA) algorithms and introduce a\nnew, multi-scale metric. I then train a large open-weights model and discuss\nits limitations.", "AI": {"tldr": "The paper proposes using vector images instead of bitmaps to automate the conversion of user interface designs into code and introduces a new multi-scale metric for evaluation.", "motivation": "Image-to-code automation struggles with achieving high fidelity to original designs, prompting exploration of alternative approaches.", "method": "The paper introduces vector images as model inputs, creates large datasets, evaluates IQA algorithms, proposes a multi-scale metric, and trains a large open-weights model.", "result": "The trained open-weights model is evaluated, showing limitations but improvements in fidelity compared to prior methods.", "conclusion": "Using vector image inputs and a multi-scale metric offers potential improvements, but challenges remain in achieving high fidelity automation."}}
{"id": "2509.06426", "pdf": "https://arxiv.org/pdf/2509.06426", "abs": "https://arxiv.org/abs/2509.06426", "authors": ["Pembe Gizem \u00d6zdil", "Chuanfang Ning", "Jasper S. Phelps", "Sibo Wang-Chen", "Guy Elisha", "Alexander Blanke", "Auke Ijspeert", "Pavan Ramdya"], "title": "Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "comment": "23 pages, 11 figures", "summary": "Computational models are critical to advance our understanding of how neural,\nbiomechanical, and physical systems interact to orchestrate animal behaviors.\nDespite the availability of near-complete reconstructions of the Drosophila\nmelanogaster central nervous system, musculature, and exoskeleton, anatomically\nand physically grounded models of fly leg muscles are still missing. These\nmodels provide an indispensable bridge between motor neuron activity and joint\nmovements. Here, we introduce the first 3D, data-driven musculoskeletal model\nof Drosophila legs, implemented in both OpenSim and MuJoCo simulation\nenvironments. Our model incorporates a Hill-type muscle representation based on\nhigh-resolution X-ray scans from multiple fixed specimens. We present a\npipeline for constructing muscle models using morphological imaging data and\nfor optimizing unknown muscle parameters specific to the fly. We then combine\nour musculoskeletal models with detailed 3D pose estimation data from behaving\nflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of\nmuscle activity across diverse walking and grooming behaviors predict\ncoordinated muscle synergies that can be tested experimentally. Furthermore, by\ntraining imitation learning policies in MuJoCo, we test the effect of different\npassive joint properties on learning speed and find that damping and stiffness\nfacilitate learning. Overall, our model enables the investigation of motor\ncontrol in an experimentally tractable model organism, providing insights into\nhow biomechanics contribute to generation of complex limb movements. Moreover,\nour model can be used to control embodied artificial agents to generate\nnaturalistic and compliant locomotion in simulated environments.", "AI": {"tldr": "The paper introduces the first 3D musculoskeletal model of Drosophila legs, combining biomechanical accuracy with simulation environments to study motor control and biomechanics.", "motivation": "Understanding how neural, biomechanical, and physical systems interact to control animal behavior is crucial. Despite near-complete CNS and anatomical reconstructions of Drosophila, musculoskeletal models bridging motor neuron activity and joint movement have been lacking.", "method": "The study presents a 3D musculoskeletal model created using X-ray imaging and anatomical data, simulated in OpenSim and MuJoCo. They include Hill-type muscle representations and pipelines for parameter optimization, pose data generation, and imitation learning policy training.", "result": "The model predicts muscle synergies during various behaviors, shows how joint properties affect learning, and enables muscle-actuated behavioral replay. This provides a means to test experimental predictions on motor control.", "conclusion": "The proposed model bridges neuro-musculoskeletal gaps, enhancing understanding of motor control and biomechanics in Drosophila. It also facilitates naturalistic locomotion in artificial agents and offers tools for experimental testing."}}
{"id": "2509.05724", "pdf": "https://arxiv.org/pdf/2509.05724", "abs": "https://arxiv.org/abs/2509.05724", "authors": ["Matthew O'Callaghan", "Kaisey S. Mandel", "Gerry Gilmore"], "title": "Robust variational neural posterior estimation for simulation-based inference", "categories": ["stat.ML", "astro-ph.GA", "cs.LG"], "comment": "Main text: 16 pages, 6 figures", "summary": "Recent advances in neural density estimation have enabled powerful\nsimulation-based inference (SBI) methods that can flexibly approximate Bayesian\ninference for intractable stochastic models. Although these methods have\ndemonstrated reliable posterior estimation when the simulator accurately\nrepresents the underlying data generative process (GDP), recent work has shown\nthat they perform poorly in the presence of model misspecification. This poses\na significant problem for their use on real-world problems, due to simulators\nalways misrepresenting the true DGP to a certain degree. In this paper, we\nintroduce robust variational neural posterior estimation (RVNP), a method which\naddresses the problem of misspecification in amortised SBI by bridging the\nsimulation-to-reality gap using variational inference and error modelling. We\ntest RVNP on multiple benchmark tasks, including using real data from\nastronomy, and show that it can recover robust posterior inference in a\ndata-driven manner without adopting tunable hyperparameters or priors governing\nthe misspecification.", "AI": {"tldr": "The paper introduces RVNP to address model misspecification issues in simulation-based inference (SBI), ensuring robust posterior estimation.", "motivation": "To solve the challenge of model misspecification in SBI, which hinders accurate posterior inference in real-world problems.", "method": "Developed RVNP, which uses variational inference and error modeling to bridge the simulation-to-reality gap without reliance on hyperparameters or priors.", "result": "Tests on benchmark tasks, including astronomy data, showed RVNP could recover robust posterior inference in a data-driven way.", "conclusion": "RVNP provides a robust solution to SBI model misspecification, improving its real-world applicability."}}
{"id": "2509.05324", "pdf": "https://arxiv.org/pdf/2509.05324", "abs": "https://arxiv.org/abs/2509.05324", "authors": ["Rongqian Chen", "Shu Hong", "Rifatul Islam", "Mahdi Imani", "G. Gary Tan", "Tian Lan"], "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality", "categories": ["cs.AI"], "comment": "Accepted by ACM MobiHoc XR Security workshop 2025", "summary": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks.", "AI": {"tldr": "The paper introduces the Perception Graph, a model designed to analyze and detect cognitive attacks on AR systems by assessing perception distortion.", "motivation": "With AR systems increasingly deployed in tactical environments, they face vulnerabilities from cognitive attacks, which can manipulate user perception and compromise decision-making. There is a need for tools to counter these threats.", "method": "The Perception Graph mimics human perception by interpreting key information in AR environments and representing it in a structured, semantic form. It calculates a quantitative perception distortion score to identify and assess cognitive attacks.", "result": "The proposed model effectively quantifies the degree of perception distortion, offering a measurable approach to detect and analyze cognitive attacks in AR systems.", "conclusion": "Perception Graph provides an innovative and practical tool for enhancing the security of AR systems against cognitive attacks, ensuring more reliable user decision-making."}}
{"id": "2509.05317", "pdf": "https://arxiv.org/pdf/2509.05317", "abs": "https://arxiv.org/abs/2509.05317", "authors": ["Isac Holm"], "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective.", "AI": {"tldr": "The paper introduces VILOD, a visual interactive labeling tool for object detection that leverages Human-in-the-Loop and visual analytics approaches to make object labeling processes more interpretable, transparent, and efficient.", "motivation": "The motivation stems from the challenges associated with acquiring large, high-quality labeled datasets needed for object detection using deep learning, including high costs, time-consumption, and the limitations of existing Active Learning techniques.", "method": "The authors developed VILOD, a tool that integrates t-SNE projections, uncertainty heatmaps, and model state visualizations, enabling users to explore data, interpret model states and Active Learning suggestions, and implement iterative Human-in-the-Loop workflows.", "result": "The empirical study found that VILOD facilitates the adoption of different visually-guided labeling strategies and achieves competitive object detection performance compared to traditional Active Learning baselines.", "conclusion": "VILOD contributes to improving HITL-AL workflows by offering a more transparent and manageable approach to dataset annotation, enhancing the human-AI collaboration process."}}
{"id": "2509.05328", "pdf": "https://arxiv.org/pdf/2509.05328", "abs": "https://arxiv.org/abs/2509.05328", "authors": ["Xiang Yuan", "Jun Shu", "Deyu meng", "Zongben Xu"], "title": "Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. To remedy this,\nmost robust fine-tuning methods aim to preserve the pretrained weights,\nfeatures, or logits. However, we find that these methods cannot always improve\nOOD robustness for different model architectures. This is due to the OOD\nrobustness requiring the model function to produce stable prediction for input\ninformation of downstream tasks, while existing methods might serve as a poor\nproxy for the optimization in the function space. Based on this finding, we\npropose a novel regularization that constrains the distance of fine-tuning and\npre-trained model in the function space with the simulated OOD samples, aiming\nto preserve the OOD robustness of the pre-trained model. Besides, to further\nenhance the OOD robustness capability of the fine-tuning model, we introduce an\nadditional consistency regularization to promote stable predictions of\nperturbed samples. Extensive experiments demonstrate our approach could\nconsistently improve both downstream task ID fine-tuning performance and OOD\nrobustness across a variety of CLIP backbones, outperforming existing\nregularization-based robust fine-tuning methods.", "AI": {"tldr": "The paper introduces a regularization technique to enhance OOD robustness during model fine-tuning.", "motivation": "Existing robust fine-tuning methods fail to consistently enhance OOD robustness for different architectures during transfer learning.", "method": "Introduce regularization to constrain fine-tuning and pre-trained model distance in the function space, and add consistency regularization for stable predictions.", "result": "Shows consistent improvement in ID task performance and OOD robustness across diverse CLIP backbones, outperforming existing methods.", "conclusion": "The approach successfully enhances both ID fine-tuning performance and OOD robustness using innovative regularization strategies."}}
{"id": "2509.05790", "pdf": "https://arxiv.org/pdf/2509.05790", "abs": "https://arxiv.org/abs/2509.05790", "authors": ["Hai Dinh-Tuan", "Franz Florian Six"], "title": "Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach", "categories": ["cs.PF"], "comment": null, "summary": "Modern software architectures are characterized by their cloud-native,\nmodular, and microservice-based designs. While these systems are known for\ntheir efficiency, they also face complex challenges in service optimization,\nespecially in maintaining end-to-end quality of service across dynamically\ndistributed services. This paper introduces a novel approach using the concept\nof Service Affinity to address this challenge. The proposed method, termed\nService Affinity Graph-based Approach, employs a graph-based model to model the\ninteractions among microservices. It formulates the service placement as a\nminimum-weight k-cut problem and utilizes an approximation algorithm for\nservice clustering. This approach is realized through a conceptual framework\nthat takes into account a wide range of optimization objectives, ranging from\nenhancing application performance and enforcing data privacy to optimizing\noperational costs. In addition to presenting the SAGA framework in details,\nthis paper conducts an in-depth empirical evaluation using a prototype deployed\non a Kubernetes cluster. The results demonstrate a mean latency improvement of\n23.40%, validating the effectiveness of our approach. Finally, the paper\ncomprehensively discusses various aspects of the proposed methods, including\ntheir implications, challenges, and benefits, providing a thorough analysis of\nthe approach's impact.", "AI": {"tldr": "This paper proposes the Service Affinity Graph-based Approach (SAGA) to optimize service placement in microservice-based architectures, achieving notable efficiency improvements.", "motivation": "Modern cloud-native microservice-based systems face challenges in maintaining end-to-end quality of service due to their dynamic, distributed nature. This necessitates innovative optimization techniques.", "method": "The paper introduces the SAGA framework, which models microservice interactions using a graph-based approach and formulates service placement as a minimum-weight k-cut problem. It employs an approximation algorithm for effective service clustering.", "result": "The empirical evaluation of SAGA, deployed on a Kubernetes cluster, resulted in a 23.40% improvement in mean latency, showcasing its effectiveness.", "conclusion": "The proposed SAGA approach provides a systematic and impactful method for microservice placement optimization, addressing performance, privacy, and cost objectives while acknowledging related implications and challenges."}}
{"id": "2509.05937", "pdf": "https://arxiv.org/pdf/2509.05937", "abs": "https://arxiv.org/abs/2509.05937", "authors": ["Wei-Hsing Huang", "Jianwei Jia", "Yuyao Kong", "Faaiq Waqar", "Tai-Hao Wen", "Meng-Fan Chang", "Shimeng Yu"], "title": "Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems", "categories": ["cs.AR"], "comment": null, "summary": "Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an\ninnovative architectural paradigm capable of replicating conventional deep\nneural network (DNN) capabilities while utilizing significantly reduced\nparameter counts through the employment of parameterized B-spline functions\nwith trainable coefficients. Nevertheless, the B-spline functional components\ninherent to KAN architectures introduce distinct hardware acceleration\ncomplexities. While B-spline function evaluation can be accomplished through\nlook-up table (LUT) implementations that directly encode functional mappings,\nthus minimizing computational overhead, such approaches continue to demand\nconsiderable circuit infrastructure, including LUTs, multiplexers, decoders,\nand related components. This work presents an algorithm-hardware co-design\napproach for KAN acceleration. At the algorithmic level, techniques include\nAlignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity\naware mapping strategy, and circuit-level techniques include N:1 Time\nModulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)\ncircuits. This work conducts evaluations on large-scale KAN networks to\nvalidate the proposed methodologies. Non-ideality factors, including partial\nsum deviations from process variations, have been evaluated with statistics\nmeasured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally\ndetermined KAN hyperparameters in conjunction with circuit optimizations\nfabricated at the 22nm technology node, despite the parameter count for\nlarge-scale tasks in this work increasing by 500Kx to 807Kx compared to\ntiny-scale tasks in previous work, the area overhead increases by only 28Kx to\n41Kx, with power consumption rising by merely 51x to 94x, while accuracy\ndegradation remains minimal at 0.11% to 0.23%, demonstrating the scaling\npotential of our proposed architecture.", "AI": {"tldr": "The paper introduces Kolmogorov-Arnold Networks (KAN) with B-spline functions and proposes an algorithm-hardware co-design strategy for addressing acceleration complexities, achieving efficient scaling with minimal accuracy loss.", "motivation": "Motivated by the need to reduce parameter counts in neural networks while maintaining performance, the paper seeks to overcome hardware acceleration challenges associated with B-spline functional components in KAN architectures.", "method": "The paper employs techniques at both algorithmic and circuit levels, including KAN quantization and sparsity-aware strategies, as well as analog-compute-in-memory (ACIM) circuits fabricated using the TSMC 22nm technology node.", "result": "Despite a significant increase in parameters, area overhead and power consumption rose marginally, with accuracy degradation kept minimal, demonstrating the scalability of the proposed methodologies.", "conclusion": "The algorithm-hardware co-design approach successfully addresses hardware acceleration complexities in KAN architectures, enabling large-scale tasks with efficient scaling and negligible loss of accuracy."}}
{"id": "2509.06724", "pdf": "https://arxiv.org/pdf/2509.06724", "abs": "https://arxiv.org/abs/2509.06724", "authors": ["Florian Kohn", "Arthur Correnson", "Jan Baumeister", "Bernd Finkbeiner"], "title": "Pacing Types: Safe Monitoring of Asynchronous Streams", "categories": ["cs.PL"], "comment": null, "summary": "Stream-based monitoring is a real-time safety assurance mechanism for complex\ncyber-physical systems such as unmanned aerial vehicles. In this context, a\nmonitor aggregates streams of input data from sensors and other sources to give\nreal-time statistics and assessments of the system's health. Since monitors are\nsafety-critical components, it is crucial to ensure that they are free of\npotential runtime errors. One of the central challenges in designing reliable\nstream-based monitors is to deal with the asynchronous nature of data streams:\nin concrete applications, the different sensors being monitored produce values\nat different speeds, and it is the monitor's responsibility to correctly react\nto the asynchronous arrival of different streams of values. To ease this\nprocess, modern frameworks for stream-based monitoring such as RTLola feature\nan expressive specification language that allows to finely specify data\nsynchronization policies. While this feature dramatically simplifies the design\nof monitors, it can also lead to subtle runtime errors. To mitigate this issue,\nthis paper presents pacing types, a novel type system implemented in RTLola to\nensure that monitors for asynchronous streams are well-behaved at runtime. We\nformalize the essence of pacing types for a core fragment of RTLola, and\npresent a soundness proof of the pacing type system using a new logical\nrelation.", "AI": {"tldr": "Stream-based monitoring ensures real-time safety for systems like drones by aggregating sensor data. The paper introduces 'pacing types,' a new type system to avoid runtime errors in asynchronous data streams.", "motivation": "Ensuring runtime reliability in stream-based monitors, which are safety-critical for cyber-physical systems, especially under asynchronous data arrival from multiple sensors.", "method": "A type system called 'pacing types' was introduced and formalized in a core fragment of the RTLola framework. The authors provided a soundness proof using a novel logical relation.", "result": "The implementation of pacing types in RTLola demonstrates its effectiveness in preventing runtime errors in asynchronous stream monitoring.", "conclusion": "Pacing types significantly enhance the reliability of stream-based monitoring systems by managing asynchronous data stream challenges effectively."}}
{"id": "2509.06046", "pdf": "https://arxiv.org/pdf/2509.06046", "abs": "https://arxiv.org/abs/2509.06046", "authors": ["Philip Adams", "Menghao Li", "Shi Zhang", "Li Tan", "Qi Chen", "Mingqin Li", "Zengzhong Li", "Knut Risvik", "Harsha Vardhan Simhadri"], "title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers", "categories": ["cs.DC", "cs.DS", "cs.IR", "E.1; H.3.3"], "comment": null, "summary": "We present DISTRIBUTEDANN, a distributed vector search service that makes it\npossible to search over a single 50 billion vector graph index spread across\nover a thousand machines that offers 26ms median query latency and processes\nover 100,000 queries per second. This is 6x more efficient than existing\npartitioning and routing strategies that route the vector query to a subset of\npartitions in a scale out vector search system. DISTRIBUTEDANN is built using\ntwo well-understood components: a distributed key-value store and an in-memory\nANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for\nserving the Bing search engine, and we share our experience from making this\ntransition.", "AI": {"tldr": "DISTRIBUTEDANN is a scalable vector search system capable of querying 50 billion vectors across 1,000+ machines, achieving superior efficiency for Bing search engine.", "motivation": "To develop a scalable and efficient vector search service that improves query efficiency over massive datasets, addressing limitations in existing partitioning and routing strategies.", "method": "DISTRIBUTEDANN integrates a distributed key-value store with in-memory approximate nearest neighbor (ANN) indexing to create a highly efficient search mechanism.", "result": "The system achieves a 6x efficiency improvement over existing methods, processing 100,000+ queries per second with a 26ms median query latency while managing a graph index of 50 billion vectors.", "conclusion": "DISTRIBUTEDANN significantly outperforms conventional architectures and has been effectively adopted for powering search in the Bing search engine."}}
{"id": "2509.05338", "pdf": "https://arxiv.org/pdf/2509.05338", "abs": "https://arxiv.org/abs/2509.05338", "authors": ["Atsushi Masumori", "Norihiro Maruyama", "Itsuki Doi", "johnsmith", "Hiroki Sato", "Takashi Ikegami"], "title": "Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We introduce Plantbot, a hybrid lifeform that connects a living plant with a\nmobile robot through a network of large language model (LLM) modules. Each\nmodule - responsible for sensing, vision, dialogue, or action - operates\nasynchronously and communicates via natural language, enabling seamless\ninteraction across biological and artificial domains. This architecture\nleverages the capacity of LLMs to serve as hybrid interfaces, where natural\nlanguage functions as a universal protocol, translating multimodal data (soil\nmoisture, temperature, visual context) into linguistic messages that coordinate\nsystem behaviors. The integrated network transforms plant states into robotic\nactions, installing normativity essential for agency within the sensor-motor\nloop. By combining biological and robotic elements through LLM-mediated\ncommunication, Plantbot behaves as an embodied, adaptive agent capable of\nresponding autonomously to environmental conditions. This approach suggests\npossibilities for a new model of artificial life, where decentralized, LLM\nmodules coordination enable novel interactions between biological and\nartificial systems.", "AI": {"tldr": "The paper introduces Plantbot, a robot-plant hybrid connected through large language models (LLMs) to enable harmonious interaction between biological and artificial elements for autonomous adaptability.", "motivation": "The researchers aim to explore the integration of biological and artificial systems to enable autonomous, adaptive agents capable of novel interaction paradigms through language-mediated communication.", "method": "A decentralized network of LLM modules (handling sensing, vision, dialogue, and action) forms the core communication framework, translating multimodal data into natural language to coordinate adaptive responses.", "result": "The Plantbot acts as an embodied, adaptive agent capable of autonomously interpreting and responding to environmental conditions by transforming plant data into robotic actions.", "conclusion": "This work demonstrates the potential for using LLMs as hybrid interfaces to create new forms of artificial life that blend biological and robotic systems through decentralized coordination."}}
{"id": "2509.05385", "pdf": "https://arxiv.org/pdf/2509.05385", "abs": "https://arxiv.org/abs/2509.05385", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.", "AI": {"tldr": "SAGE is a framework designed to allow large language models to adapt and learn during inference by decomposing tasks into subtasks and dynamically fine-tuning with three main components.", "motivation": "Large language models struggle to continuously adapt and learn during reasoning at inference time, necessitating an approach for dynamic updates.", "method": "SAGE is a dynamic fine-tuning framework comprising the Trigger module for detecting failures, Trigger Buffer module for clustering anomalies, and Lora Store module for parameter updates.", "result": "SAGE demonstrated superior accuracy, robustness, and stability on atomic reasoning subtasks during test time, showing its effectiveness in dynamic knowledge updating.", "conclusion": "The paper concludes that SAGE resolves limitations in continuous learning for large models by enabling adaptive updates during inference for improved task-solving performance."}}
{"id": "2509.05540", "pdf": "https://arxiv.org/pdf/2509.05540", "abs": "https://arxiv.org/abs/2509.05540", "authors": ["Thiago Barradas", "Aline Paes", "V\u00e2nia de Oliveira Neves"], "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, article computer science, software engineering, software\n  testing, ia, llm", "summary": "The effective execution of tests for REST APIs remains a considerable\nchallenge for development teams, driven by the inherent complexity of\ndistributed systems, the multitude of possible scenarios, and the limited time\navailable for test design. Exhaustive testing of all input combinations is\nimpractical, often resulting in undetected failures, high manual effort, and\nlimited test coverage. To address these issues, we introduce RestTSLLM, an\napproach that uses Test Specification Language (TSL) in conjunction with Large\nLanguage Models (LLMs) to automate the generation of test cases for REST APIs.\nThe approach targets two core challenges: the creation of test scenarios and\nthe definition of appropriate input data. The proposed solution integrates\nprompt engineering techniques with an automated pipeline to evaluate various\nLLMs on their ability to generate tests from OpenAPI specifications. The\nevaluation focused on metrics such as success rate, test coverage, and mutation\nscore, enabling a systematic comparison of model performance. The results\nindicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),\nDeepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -\nconsistently produced robust and contextually coherent REST API tests. Among\nthem, Claude 3.5 Sonnet outperformed all other models across every metric,\nemerging in this study as the most suitable model for this task. These findings\nhighlight the potential of LLMs to automate the generation of tests based on\nAPI specifications.", "AI": {"tldr": "The paper introduces RestTSLLM, a novel method using Test Specification Language (TSL) combined with Large Language Models (LLMs) to automate REST API test case generation and evaluates various LLMs for their effectiveness.", "motivation": "Efficient testing of REST APIs is hampered by challenges like complexity, numerous scenarios, and limited testing time, necessitating automation to address undetected failures and improve test coverage.", "method": "The approach integrates Test Specification Language (TSL) with Large Language Models (LLMs) via prompt engineering and an automated pipeline to generate tests systematically using OpenAPI specifications.", "result": "The evaluation showed that Claude 3.5 Sonnet consistently outperformed other models such as Deepseek R1, Qwen 2.5 32b, and Sabia 3 in generating robust and coherent REST API tests based on metrics like success rate, test coverage, and mutation score.", "conclusion": "LLMs demonstrate strong potential in automating REST API test generation, with Claude 3.5 Sonnet leading as the most effective model among evaluated options."}}
{"id": "2509.06810", "pdf": "https://arxiv.org/pdf/2509.06810", "abs": "https://arxiv.org/abs/2509.06810", "authors": ["Gaia Molinaro", "Anne G. E. Collins"], "title": "Reward function compression facilitates goal-dependent reinforcement learning", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Reinforcement learning agents learn from rewards, but humans can uniquely\nassign value to novel, abstract outcomes in a goal-dependent manner. However,\nthis flexibility is cognitively costly, making learning less efficient. Here,\nwe propose that goal-dependent learning is initially supported by a\ncapacity-limited working memory system. With consistent experience, learners\ncreate a \"compressed\" reward function (a simplified rule defining the goal)\nwhich is then transferred to long-term memory and applied automatically upon\nreceiving feedback. This process frees up working memory resources, boosting\nlearning efficiency. We test this theory across six experiments. Consistent\nwith our predictions, our findings demonstrate that learning is parametrically\nimpaired by the size of the goal space, but improves when the goal space\nstructure allows for compression. We also find faster reward processing to\ncorrelate with better learning performance, supporting the idea that as goal\nvaluation becomes more automatic, more resources are available for learning. We\nleverage computational modeling to support this interpretation. Our work\nsuggests that efficient goal-directed learning relies on compressing complex\ngoal information into a stable reward function, shedding light on the cognitive\nmechanisms of human motivation. These findings generate new insights into the\nneuroscience of intrinsic motivation and could help improve behavioral\ntechniques that support people in achieving their goals.", "AI": {"tldr": "This paper explores how humans learn abstract goals more efficiently by compressing goal information into stable reward functions, freeing cognitive resources.", "motivation": "Understanding how humans assign value to abstract goals and why this process is cognitively taxing compared to traditional reinforcement learning mechanisms.", "method": "Six experiments were conducted, complemented by computational modeling, to explore goal space complexity and reward processing in relation to learning efficiency.", "result": "Findings show that learning efficiency is impaired by larger goal spaces but improves when goal structures allow for compression. Faster reward processing correlates with better learning outcomes.", "conclusion": "Human goal-directed learning is optimized by compressing goal-related information, activating intrinsic motivation mechanisms and providing insights to enhance behavioral support approaches."}}
{"id": "2509.05771", "pdf": "https://arxiv.org/pdf/2509.05771", "abs": "https://arxiv.org/abs/2509.05771", "authors": ["Darinka Dentcheva", "Xiangyu Tian"], "title": "Risk-averse Fair Multi-class Classification", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "We develop a new classification framework based on the theory of coherent\nrisk measures and systemic risk. The proposed approach is suitable for\nmulti-class problems when the data is noisy, scarce (relative to the dimension\nof the problem), and the labeling might be unreliable. In the first part of our\npaper, we provide the foundation of the use of systemic risk models and show\nhow to apply it in the context of linear and kernel-based multi-class problems.\nMore advanced formulation via a system-theoretic approach with non-linear\naggregation is proposed, which leads to a two-stage stochastic programming\nproblem. A risk-averse regularized decomposition method is designed to solve\nthe problem. We use a popular multi-class method as a benchmark in the\nperformance analysis of the proposed classification methods. We illustrate our\nideas by proposing several generalization of that method by the use of coherent\nmeasures of risk. The viability of the proposed risk-averse methods are\nsupported theoretically and numerically. Additionally, we demonstrate that the\napplication of systemic risk measures facilitates enforcing fairness in\nclassification. Analysis and experiments regarding the fairness of the proposed\nmodels are carefully conducted. For all methods, our numerical experiments\ndemonstrate that they are robust in the presence of unreliable training data\nand perform better on unknown data than the methods minimizing expected\nclassification errors. Furthermore, the performance improves when the number of\nclasses increases.", "AI": {"tldr": "The paper develops a new classification framework using systemic risk models for better handling noisy, scarce, and unreliable data in multi-class problems, emphasizing fairness and robust performance.", "motivation": "Existing classification frameworks struggle with unreliable labels, high-dimensional problems, and the need for fairness in multi-class scenarios.", "method": "The approach applies systemic risk measures in classification using linear and kernel-based methods, incorporates non-linear aggregation, and employs a risk-averse regularized decomposition to solve the problem.", "result": "Numerical experiments show robustness to unreliable data, better performance on unknown data, and improved outcomes as the number of classes increases.", "conclusion": "The proposed framework is theoretically and empirically viable, addresses fairness in classification, and outperforms traditional methods under challenging conditions."}}
{"id": "2509.05325", "pdf": "https://arxiv.org/pdf/2509.05325", "abs": "https://arxiv.org/abs/2509.05325", "authors": ["Liming Xu", "Yunbo Long", "Alexandra Brintrup"], "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction", "categories": ["cs.AI"], "comment": "This paper incldues 1 figure and 2 tables", "summary": "Artificial intelligence (AI) is transforming supply chain management, yet\nprogress in predictive tasks -- such as delivery delay prediction -- remains\nconstrained by the scarcity of high-quality, openly available datasets.\nExisting datasets are often proprietary, small, or inconsistently maintained,\nhindering reproducibility and benchmarking. We present SynDelay, a synthetic\ndataset designed for delivery delay prediction. Generated using an advanced\ngenerative model trained on real-world data, SynDelay preserves realistic\ndelivery patterns while ensuring privacy. Although not entirely free of noise\nor inconsistencies, it provides a challenging and practical testbed for\nadvancing predictive modelling. To support adoption, we provide baseline\nresults and evaluation metrics as initial benchmarks, serving as reference\npoints rather than state-of-the-art claims. SynDelay is publicly available\nthrough the Supply Chain Data Hub, an open initiative promoting dataset sharing\nand benchmarking in supply chain AI. We encourage the community to contribute\ndatasets, models, and evaluation practices to advance research in this area.\nAll code is openly accessible at https://supplychaindatahub.org.", "AI": {"tldr": "The paper introduces SynDelay, a synthetic dataset for delivery delay prediction in supply chain management, addressing the scarcity of high-quality, open datasets.", "motivation": "The paper is motivated by the lack of openly available, high-quality datasets that hinder progress and benchmarking in AI-driven predictive tasks for supply chain management.", "method": "SynDelay is created using an advanced generative model trained on real-world data to simulate realistic delivery patterns while preserving data privacy.", "result": "The SynDelay dataset provides a challenging testbed for predictive modeling and includes baseline benchmarks and metrics to support its adoption by the research community.", "conclusion": "SynDelay is publicly accessible and aims to stimulate collaboration in supply chain AI by encouraging contributions of datasets, models, and evaluations, enhancing progress in the field."}}
{"id": "2509.05319", "pdf": "https://arxiv.org/pdf/2509.05319", "abs": "https://arxiv.org/abs/2509.05319", "authors": ["Zhengda Li"], "title": "Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge distillation (KD) is a widely used technique to transfer knowledge\nfrom a large teacher network to a smaller student model. Traditional KD uses a\nfixed balancing factor alpha as a hyperparameter to combine the hard-label\ncross-entropy loss with the soft-label distillation loss. However, a static\nalpha is suboptimal because the optimal trade-off between hard and soft\nsupervision can vary during training.\n  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.\nFirst we try to make alpha as learnable parameter that can be automatically\nlearned and optimized during training. Then we introduce a formula to reflect\nthe gap between the student and the teacher to compute alpha dynamically,\nguided by student-teacher discrepancies, and further introduce a Context-Aware\nModule (CAM) using MLP + Attention to adaptively reweight class-wise teacher\noutputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as\nstudent demonstrate that our approach achieves superior accuracy compared to\nfixed-weight KD baselines, and yields more stable convergence.", "AI": {"tldr": "Knowledge Distillation traditionally uses a fixed alpha for balancing losses, but this paper introduces Adaptive Knowledge Distillation (AKD) with a learnable alpha and dynamic adjustments to improve training.", "motivation": "Static alpha in Knowledge Distillation is suboptimal because the balance between hard-label and soft-label supervision changes during training, necessitating adaptive adjustments.", "method": "The paper proposes AKD by making alpha a learnable parameter, introduces a dynamic formula based on student-teacher discrepancies, and incorporates a Context-Aware Module (MLP + Attention) to adaptively reweight teacher outputs.", "result": "Experiments with CIFAR-10, ResNet-50 as a teacher, and ResNet-18 as a student show that AKD outperforms fixed-weight KD baselines in terms of accuracy and training stability.", "conclusion": "Adaptive adjustments using AKD enhance Knowledge Distillation by optimizing the trade-off dynamically, improving performance and stability."}}
{"id": "2509.05429", "pdf": "https://arxiv.org/pdf/2509.05429", "abs": "https://arxiv.org/abs/2509.05429", "authors": ["Jie Fu", "Hong Yuan", "Zhili Chen", "Wendy Hui Wang"], "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks", "categories": ["cs.LG", "cs.CR"], "comment": "Acctepted by ACM CCS'25", "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is anonymously available at\nhttps://github.com/JeffffffFu/PGR.", "AI": {"tldr": "This study explores topology privacy issues in Graph Neural Networks (GNNs), proposing attacks and a defense framework to reduce structure leakage while maintaining model accuracy.", "motivation": "The paper is motivated by the urgent need to address privacy concerns in GNNs related to graph structure (topology), which has been neglected compared to edge-level privacy.", "method": "The researchers introduce Topology Inference Attacks (TIAs) for evaluating topology vulnerabilities and a defense framework called Private Graph Reconstruction (PGR), leveraging bi-level optimization.", "result": "Experiments show that PGR minimizes topology leakage effectively with negligible degradation in GNN model accuracy.", "conclusion": "The paper concludes GNNs are vulnerable to topology privacy risks, but the proposed PGR framework provides a promising solution to mitigate these issues without sacrificing model performance."}}
{"id": "2509.05794", "pdf": "https://arxiv.org/pdf/2509.05794", "abs": "https://arxiv.org/abs/2509.05794", "authors": ["Hai Dinh-Tuan", "Jialun Jiang"], "title": "Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing", "categories": ["cs.PF", "cs.CE"], "comment": null, "summary": "The widespread adoption of microservices architecture in modern software\nsystems has emphasized the need for efficient management of distributed\nservices. While stateless microservices enable straightforward migration,\nstateful microservices introduce added complexity due to the need to preserve\nin-memory state during migration. However, most container orchestrators,\nincluding Kubernetes, lack native support for live stateful service migration.\nThis paper proposes an optimized migration scheme for stateful services in\nKubernetes by integrating the Message-based Stateful Microservice Migration\n(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)\nfeature. Key enhancements include support for migrating StatefulSet-managed\nPods and the introduction of a Threshold-Based Cutoff Mechanism to handle high\nincoming message rates. Evaluation results demonstrate that MS2M for individual\nPods reduces downtime by 96.986% compared to cold migration methods, while the\nStatefulSet approach provides greater flexibility in managing stateful\nservices. These insights provide practical strategies for optimizing stateful\nmicroservice migration in cloud-native environments.", "AI": {"tldr": "The study proposes an optimized migration approach for stateful microservices in Kubernetes, significantly improving downtime and flexibility during migrations.", "motivation": "To address the lack of native support for live migration of stateful microservices in Kubernetes and improve management of distributed services.", "method": "The approach integrates the MS2M framework with Kubernetes' FCC feature, while introducing a Threshold-Based Cutoff Mechanism and extending support to StatefulSets.", "result": "The proposed solution achieves a 96.986% reduction in downtime for individual Pods during migration and enhances flexibility for StatefulSet-managed services.", "conclusion": "This method offers practical strategies to optimize stateful microservice migrations in cloud-native environments, addressing key limitations of existing systems."}}
{"id": "2509.06101", "pdf": "https://arxiv.org/pdf/2509.06101", "abs": "https://arxiv.org/abs/2509.06101", "authors": ["Fan Li", "Mimi Xie", "Yanan Guo", "Huize Li", "Xin Xin"], "title": "SCREME: A Scalable Framework for Resilient Memory Design", "categories": ["cs.AR"], "comment": null, "summary": "The continuing advancement of memory technology has not only fueled a surge\nin performance, but also substantially exacerbate reliability challenges.\nTraditional solutions have primarily focused on improving the efficiency of\nprotection schemes, i.e., Error Correction Codes (ECC), under the assumption\nthat allocating additional memory space for parity data is always expensive and\ntherefore not a scalable solution.\n  We break the stereotype by proposing an orthogonal approach that provides\nadditional, cost-effective memory space for resilient memory design. In\nparticular, we recognize that ECC chips (used for parity storage) do not\nnecessarily require the same performance level as regular data chips. This\noffers two-fold benefits: First, the bandwidth originally provisioned for a\nregular-performance ECC chip can instead be used to accommodate multiple\nlow-performance chips. Second, the cost of ECC chips can be effectively\nreduced, as lower performance often correlates with lower expense. In addition,\nwe observe that server-class memory chips are often provisioned with ample, yet\nunderutilized I/O resources. This further offers the opportunity to repurpose\nthese resources to enable flexible on-DIMM interconnections. Based on the above\ntwo insights, we finally propose SCREME, a scalable memory framework leverages\ncost-effective, albeit slower, chips -- naturally produced during rapid\ntechnology evolution -- to meet the growing reliability demands driven by this\nevolution.", "AI": {"tldr": "The paper introduces SCREME, a cost-efficient and scalable memory framework that enhances memory resilience using slower, cheaper ECC chips and underutilized I/O resources on memory chips.", "motivation": "Growing advancement in memory technology has increased performance but also amplified reliability challenges, with traditional approaches being inefficient in addressing the added costs of parity storage.", "method": "SCREME utilizes slower, cost-effective ECC chips for parity storage and repurposes underutilized I/O resources from server-class memory to create flexible on-DIMM interconnections.", "result": "SCREME achieves a balance of increased reliability and reduced costs by leveraging insights into memory performance and resource utilization.", "conclusion": "SCREME is a scalable and innovative approach to memory design, addressing reliability concerns while maintaining cost-efficiency by rethinking the design of parity storage and interconnection resources."}}
{"id": "2509.06752", "pdf": "https://arxiv.org/pdf/2509.06752", "abs": "https://arxiv.org/abs/2509.06752", "authors": ["Amir M. Ben-Amram", "Samir Genaim", "Jo\u00ebl Ouaknine", "James Worrell"], "title": "Termination Analysis of Linear-Constraint Programs", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems.", "AI": {"tldr": "This survey examines techniques for analyzing program termination with linear numerical constraints, focusing on ranking functions, transition invariants, and non-termination proof methods.", "motivation": "Understanding termination behavior of programs with linear constraints is crucial because these programs can exhibit undecidable problems, posing challenges for program correctness and reliability.", "method": "The paper systematically explores methods including ranking functions, disjunctive well-founded invariants, and non-termination witnesses while assessing their algorithmic complexity and trade-offs.", "result": "Identifies trade-offs between expressive power and computational complexity in various termination analysis techniques for linear constraint-based programs.", "conclusion": "Though effective for linear constraints, the discussed methods are limited in scope, excluding real-world programming languages and more abstract models with non-linear or probabilistic elements."}}
{"id": "2509.06064", "pdf": "https://arxiv.org/pdf/2509.06064", "abs": "https://arxiv.org/abs/2509.06064", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Non-Vertex-Transitive Graphs Under Round Robin", "categories": ["cs.DC", "math.CO"], "comment": "16 pages, 3 figures", "summary": "The Gathering problem for a swarm of robots asks for a distributed algorithm\nthat brings such entities to a common place, not known in advance. We consider\nthe well-known OBLOT model with robots constrained to move along the edges of a\ngraph, hence gathering in one vertex, eventually. Despite the classical setting\nunder which the problem has been usually approached, we consider the `hostile'\ncase where: i) the initial configuration may contain multiplicities, i.e. more\nthan one robot may occupy the same vertex; ii) robots cannot detect\nmultiplicities. As a scheduler for robot activation, we consider the\n\"favorable\" round-robin case, where robots are activated one at a time.\n  Our objective is to achieve a complete characterization of the problem in the\nbroad context of non-vertex-transitive graphs, i.e., graphs where the vertices\nare partitioned into at least two different classes of equivalence. We provide\na resolution algorithm for any configuration of robots moving on such graphs,\nalong with its correctness. Furthermore, we analyze its time complexity.", "AI": {"tldr": "The paper addresses the gathering problem for robots under certain constraints and provides a resolution algorithm for diverse graph structures.", "motivation": "The motivation is to tackle the challenging gathering problem for robots in hostile configurations on non-vertex-transitive graphs, with constraints like multiplicities and limited detection capabilities.", "method": "The study uses the OBLOT robot model where robots are constrained to graph edges and assumes a round-robin activation scheduler. It develops and analyzes a resolution algorithm applicable to the broad context of non-vertex-transitive graphs.", "result": "The authors deliver a resolution algorithm that tackles initial configurations with multiplicities and ensures correctness. They also analyze its time complexity.", "conclusion": "The algorithm succeeds in solving the gathering problem for robots on diverse, non-vertex-transitive graphs, addressing constraints that make the problem more difficult."}}
{"id": "2509.05345", "pdf": "https://arxiv.org/pdf/2509.05345", "abs": "https://arxiv.org/abs/2509.05345", "authors": ["Jiasheng Qu", "Zhuo Huang", "Dezhao Guo", "Hailin Sun", "Aoran Lyu", "Chengkai Dai", "Yeung Yam", "Guoxin Fang"], "title": "INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing", "categories": ["cs.RO", "cs.CG"], "comment": null, "summary": "We introduce a general, scalable computational framework for multi-axis 3D\nprinting based on implicit neural fields (INFs) that unifies all stages of\ntoolpath generation and global collision-free motion planning. In our pipeline,\ninput models are represented as signed distance fields, with fabrication\nobjectives such as support-free printing, surface finish quality, and extrusion\ncontrol being directly encoded in the optimization of an implicit guidance\nfield. This unified approach enables toolpath optimization across both surface\nand interior domains, allowing shell and infill paths to be generated via\nimplicit field interpolation. The printing sequence and multi-axis motion are\nthen jointly optimized over a continuous quaternion field. Our continuous\nformulation constructs the evolving printing object as a time-varying SDF,\nsupporting differentiable global collision handling throughout INF-based motion\nplanning. Compared to explicit-representation-based methods, INF-3DP achieves\nup to two orders of magnitude speedup and significantly reduces\nwaypoint-to-surface error. We validate our framework on diverse, complex models\nand demonstrate its efficiency with physical fabrication experiments using a\nrobot-assisted multi-axis system.", "AI": {"tldr": "This paper presents a computational framework for multi-axis 3D printing using implicit neural fields, optimizing toolpath generation and motion planning.", "motivation": "To address challenges in multi-axis 3D printing such as toolpath optimization, surface finish quality, support-free printing, and collision-free motion planning.", "method": "3D models are represented as signed distance fields to encode fabrication objectives. Toolpaths for shell and infill are interpolated via implicit neural fields, while printing sequences and motion are optimized through a continuous quaternion field.", "result": "INF-based methods achieve notable performance improvements, including up to two orders of magnitude speedup and reduced waypoint-to-surface error.", "conclusion": "The framework demonstrates efficacy through testing on complex models and physical experiments using robot-assisted multi-axis systems, proving its potential for 3D printing advancements."}}
{"id": "2509.05396", "pdf": "https://arxiv.org/pdf/2509.05396", "abs": "https://arxiv.org/abs/2509.05396", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.", "AI": {"tldr": "This paper explores the potential harm in multi-agent AI debate and reveals how diversity in model capabilities can impact debate dynamics and outcomes. It finds that debates can sometimes reduce accuracy, even among more capable agents.", "motivation": "The study aims to understand how diversity in AI model capabilities influences reasoning outcomes in multi-agent debates and seeks to identify challenges in such interactions.", "method": "Through experimental setups, the authors test debate scenarios involving models of varying capabilities, observing their reasoning dynamics and accuracy shifts over time.", "result": "The analysis finds that debates often lead to models shifting from correct to incorrect answers, and stronger models are also prone to agreeing with flawed reasoning from weaker ones, leading to accuracy drops.", "conclusion": "The study highlights shortcomings in multi-agent debate mechanisms, warning against naive implementations where agents may favor consensus over critical reasoning, causing performance degradation."}}
{"id": "2509.05585", "pdf": "https://arxiv.org/pdf/2509.05585", "abs": "https://arxiv.org/abs/2509.05585", "authors": ["Zhiyuan Zou", "Bangchao Wang", "Peng Liang", "Tingting Bi", "Huan Jin"], "title": "Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity", "categories": ["cs.SE", "cs.AI"], "comment": "45 pages, 5 images, 11 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "In the field of software traceability link recovery (TLR), textual similarity\nhas long been regarded as the core criterion. However, in tasks involving\nnatural language and programming language (NL-PL) artifacts, relying solely on\ntextual similarity is limited by their semantic gap. To this end, we conducted\na large-scale empirical evaluation across various types of TLR tasks, revealing\nthe limitations of textual similarity in NL-PL scenarios. To address these\nlimitations, we propose an approach that incorporates multiple domain-specific\nauxiliary strategies, identified through empirical analysis, into two models:\nthe Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based\nGemini 2.5 Pro via additional input information. We then evaluated our approach\nusing the widely studied requirements-to-code TLR task, a representative case\nof NL-PL TLR. Experimental results show that both the multi-strategy HGT and\nGemini 2.5 Pro models outperformed their original counterparts without strategy\nintegration. Furthermore, compared to the current state-of-the-art method\nHGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average\nF1-score improvements of 3.68% and 8.84%, respectively, across twelve\nopen-source projects, demonstrating the effectiveness of multi-strategy\nintegration in enhancing overall model performance for the requirements-code\nTLR task.", "AI": {"tldr": "Textual similarity alone is insufficient for software traceability link recovery (TLR) in NL-PL artifacts due to the semantic gap. Multi-strategy approaches outperform existing methods.", "motivation": "Address limitations of relying solely on textual similarity for TLR tasks in scenarios involving NL-PL artifacts.", "method": "Integrate domain-specific strategies into Heterogeneous Graph Transformer (HGT) and Gemini 2.5 Pro models, evaluated on requirements-to-code TLR.", "result": "Both modified models achieved notable F1-score improvements compared to their original versions and state-of-the-art methods.", "conclusion": "Multi-strategy integration effectively enhances model performance for NL-PL TLR tasks, showing promising results across diverse projects."}}
{"id": "2509.05775", "pdf": "https://arxiv.org/pdf/2509.05775", "abs": "https://arxiv.org/abs/2509.05775", "authors": ["Zilong Wang", "Turgay Ayer", "Shihao Yang"], "title": "Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery", "categories": ["stat.ML", "cs.LG"], "comment": "Pre-print for camera ready version for IEEE EMBS BHI 2025", "summary": "Estimating heterogeneous treatment effects is critical in domains such as\npersonalized medicine, resource allocation, and policy evaluation. A central\nchallenge lies in identifying subpopulations that respond differently to\ninterventions, thereby enabling more targeted and effective decision-making.\nWhile clustering methods are well-studied in unsupervised learning, their\nintegration with causal inference remains limited. We propose a novel framework\nthat clusters individuals based on estimated treatment effects using a learned\nkernel derived from causal forests, revealing latent subgroup structures. Our\napproach consists of two main steps. First, we estimate debiased Conditional\nAverage Treatment Effects (CATEs) using orthogonalized learners via the\nRobinson decomposition, yielding a kernel matrix that encodes sample-level\nsimilarities in treatment responsiveness. Second, we apply kernelized\nclustering to this matrix to uncover distinct, treatment-sensitive\nsubpopulations and compute cluster-level average CATEs. We present this\nkernelized clustering step as a form of regularization within the\nresidual-on-residual regression framework. Through extensive experiments on\nsemi-synthetic and real-world datasets, supported by ablation studies and\nexploratory analyses, we demonstrate the effectiveness of our method in\ncapturing meaningful treatment effect heterogeneity.", "AI": {"tldr": "The paper proposes a method for clustering individuals based on estimated treatment effects, enabling identification of treatment-sensitive subpopulations.", "motivation": "The paper aims to address the challenge of identifying subpopulations with differing responses to interventions to improve targeted decision-making.", "method": "The approach involves estimating Conditional Average Treatment Effects using debiased learners, deriving a kernel matrix, and applying kernelized clustering to uncover subpopulation structures.", "result": "The proposed method successfully captures treatment effect heterogeneity across various datasets, validated through experiments, ablation studies, and exploratory analyses.", "conclusion": "The framework reveals meaningful latent subgroup structures and promotes effective personalized intervention strategies."}}
{"id": "2509.05330", "pdf": "https://arxiv.org/pdf/2509.05330", "abs": "https://arxiv.org/abs/2509.05330", "authors": ["Seyed Muhammad Hossein Mousavi", "Atiye Ilanloo"], "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset", "categories": ["cs.AI"], "comment": null, "summary": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.", "AI": {"tldr": "The paper introduces the MVRS multimodal dataset, designed to advance emotion recognition in AI using synchronized VR-induced stimuli and various signal modalities.", "motivation": "Emotion recognition in AI suffers from a lack of multimodal datasets involving body motion and physiological signals, limiting advancements in healthcare and other applications.", "method": "The MVRS dataset collects synchronized VR-based emotional stimuli data from 13 participants through eye tracking, body motion, EMG, and GSR signals, extracting features and evaluating them via fusion techniques and classifiers.", "result": "The features from the dataset's modalities were successfully fused and classified, demonstrating strong emotion separability and validating the quality of the dataset.", "conclusion": "MVRS is a significant addition to multimodal affective computing and paves the way for improved AI emotion recognition research."}}
{"id": "2509.05321", "pdf": "https://arxiv.org/pdf/2509.05321", "abs": "https://arxiv.org/abs/2509.05321", "authors": ["Yunfei Guo", "Tao Zhang", "Wu Huang", "Yao Song"], "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,\nthat leverages the SEED-VD dataset to generate a multimodal dataset of EEG\nsignals conditioned on video stimuli. Additionally, we disclose an engineering\npipeline for aligning video and EEG data pairs, facilitating the training of\nmultimodal large models with EEG alignment capabilities. Personalized EEG\nsignals are generated using a self-play graph network (SPGN) integrated with a\ndiffusion model. As a major contribution, we release a new dataset comprising\nover 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG\nsignals at 200 Hz and emotion labels, enabling video-EEG alignment and\nadvancing multimodal research. This framework offers novel tools for emotion\nanalysis, data augmentation, and brain-computer interface applications, with\nsubstantial research and engineering significance.", "AI": {"tldr": "The paper presents Video2EEG-SPGN-Diffusion, an open-source framework that generates multimodal EEG data conditioned on video stimuli using SEED-VD. It incorporates tools for video-EEG alignment, personalized EEG generation, and releases a paired video-EEG dataset.", "motivation": "To address the lack of aligned multimodal datasets of EEG signals and video stimuli, thereby enabling advancements in emotion analysis and brain-computer interface research.", "method": "The framework uses the SEED-VD dataset and generates personalized EEG signals by combining a self-play graph network (SPGN) with a diffusion model. It also introduces an engineering pipeline for video-EEG data alignment.", "result": "The released dataset includes over 1000 samples of SEED-VD video stimuli paired with 62-channel EEG signals at 200 Hz and emotion labels, facilitating multimodal research and training.", "conclusion": "The framework and dataset provide significant tools for emotion analysis, data augmentation, and brain-computer interfaces, contributing to advancements in multimodal and aligned video-EEG research."}}
{"id": "2509.05449", "pdf": "https://arxiv.org/pdf/2509.05449", "abs": "https://arxiv.org/abs/2509.05449", "authors": ["Disha Makhija", "Manoj Ghuhan Arivazhagan", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Membership inference attacks (MIAs) reveal whether specific data was used to\ntrain machine learning models, serving as important tools for privacy auditing\nand compliance assessment. Recent studies have reported that MIAs perform only\nmarginally better than random guessing against large language models,\nsuggesting that modern pre-training approaches with massive datasets may be\nfree from privacy leakage risks. Our work offers a complementary perspective to\nthese findings by exploring how examining LLMs' internal representations,\nrather than just their outputs, may provide additional insights into potential\nmembership inference signals. Our framework, \\emph{memTrace}, follows what we\ncall \\enquote{neural breadcrumbs} extracting informative signals from\ntransformer hidden states and attention patterns as they process candidate\nsequences. By analyzing layer-wise representation dynamics, attention\ndistribution characteristics, and cross-layer transition patterns, we detect\npotential memorization fingerprints that traditional loss-based approaches may\nnot capture. This approach yields strong membership detection across several\nmodel families achieving average AUC scores of 0.85 on popular MIA benchmarks.\nOur findings suggest that internal model behaviors can reveal aspects of\ntraining data exposure even when output-based signals appear protected,\nhighlighting the need for further research into membership privacy and the\ndevelopment of more robust privacy-preserving training techniques for large\nlanguage models.", "AI": {"tldr": "The paper introduces a framework called memTrace, demonstrating that analyzing internal representations of LLMs, instead of just their outputs, can achieve high performance in detecting membership inference signals.", "motivation": "The motivation is to address the limitations of current membership inference attacks (MIAs) that focus only on the outputs of large language models, which show little privacy leakage.", "method": "The memTrace framework examines neural 'breadcrumbs' within large language models by analyzing their internal states, including layer-wise dynamics, attention distributions, and cross-layer transitions for potential memorization fingerprints.", "result": "The proposed method achieves strong membership detection capabilities, with an average AUC of 0.85 across common MIA benchmarks, outperforming traditional loss-based approaches.", "conclusion": "The internal behaviors of large language models can reveal training data exposure even when their outputs are secure, calling for improved privacy-preserving methods in their training."}}
{"id": "2509.06365", "pdf": "https://arxiv.org/pdf/2509.06365", "abs": "https://arxiv.org/abs/2509.06365", "authors": ["Omar Al Habsi", "Safa Mohammed Sali", "Anis Meribout", "Mahmoud Meribout", "Saif Almazrouei", "Mohamed Seghier"], "title": "Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects", "categories": ["cs.AR"], "comment": null, "summary": "There is a growing interest in portable MRI (pMRI) systems for point-of-care\nimaging, particularly in remote or resource-constrained environments. However,\nthe computational complexity of pMRI, especially in image reconstruction and\nmachine learning (ML) algorithms for enhanced imaging, presents significant\nchallenges. Such challenges can be potentially addressed by harnessing hardware\napplication solutions, though there is little focus in the current pMRI\nliterature on hardware acceleration. This paper bridges that gap by reviewing\nrecent developments in pMRI, emphasizing the role and impact of hardware\nacceleration to speed up image acquisition and reconstruction. Key technologies\nsuch as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays\n(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent\nperformance in terms of reconstruction speed and power consumption. This review\nalso highlights the promise of AI-powered reconstruction, open low-field pMRI\ndatasets, and innovative edge-based hardware solutions for the future of pMRI\ntechnology. Overall, hardware acceleration can enhance image quality, reduce\npower consumption, and increase portability for next-generation pMRI\ntechnology. To accelerate reproducible AI for portable MRI, we propose forming\na Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,\nretrospective multi-center testing, prospective reader and non-inferiority\ntrials) to provide standardized datasets, benchmarks, and regulator-ready\ntestbeds.", "AI": {"tldr": "The paper reviews advancements in portable MRI (pMRI), focusing on hardware acceleration and AI-driven solutions to improve imaging performance and portability.", "motivation": "To tackle computational challenges in portable MRI systems, particularly image reconstruction and machine learning, and explore hardware solutions to enhance pMRI usability for resource-constrained environments.", "method": "Reviewing key technologies like GPUs, FPGAs, and ASICs for hardware acceleration in pMRI, proposing AI reconstruction methods, and suggesting the creation of a Low-Field MRI Consortium.", "result": "Highlighted hardware technologies significantly improve speed, power efficiency, and materialize enhanced image reconstruction, along with a push toward standardized benchmarks.", "conclusion": "Hardware acceleration and AI have a transformative potential to uplift the capabilities and portability of pMRI systems, enabling efficient imaging even in remote settings."}}
{"id": "2509.06794", "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance.", "AI": {"tldr": "The paper introduces Dato, a Python-embedded programming model, to enhance dataflow accelerators for deep learning workloads by focusing on efficient data communication and hardware mapping.", "motivation": "Deep learning workloads are increasingly bottlenecked by data movement rather than computation. Current programming models either require excessive developer effort or oversimplify communication details, limiting optimization.", "method": "The authors propose Dato, a task-based programming model that introduces explicit streaming and sharding types. It maps tasks first virtually and then optimizes them physically for hardware constraints.", "result": "Dato achieved up to 84% hardware utilization and 2.81x speedup for certain kernels on AMD Ryzen AI NPU, and near-theoretical performance (98%) on custom systolic arrays with the Alveo FPGA.", "conclusion": "Dato balances high performance with ease of development, demonstrating its ability to optimize deep learning workloads on diverse dataflow accelerator hardware."}}
{"id": "2509.06229", "pdf": "https://arxiv.org/pdf/2509.06229", "abs": "https://arxiv.org/abs/2509.06229", "authors": ["Karolina Skrivankova", "Mark Handley", "Stephen Hailes"], "title": "20 Years in Life of a Smart Building: A retrospective", "categories": ["cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Operating an intelligent smart building automation system in 2025 is met with\nmany challenges: hardware failures, vendor obsolescence, evolving security\nthreats and more. None of these have been comprehensibly addressed by the\nindustrial building nor home automation industries, limiting feasibility of\noperating large, truly smart automation deployments. This paper introduces\nKaOS, a distributed control platform for constructing robust and evolvable\nsmart building automation systems using affordable, off-the-shelf IoT hardware.\nSupporting control applications and distributed system operations by leveraging\ncontainerisation and managed resource access, KaOS seeks to achieve\nflexibility, security, and fault tolerance without sacrificing\ncost-effectiveness. Initial evaluation confirms the practical feasibility of\nour approach, highlighting its potential to sustainably maintain and\nincrementally evolve building control functionalities over extended timeframes.", "AI": {"tldr": "The paper introduces KaOS, a distributed control platform designed to enhance robustness and adaptability in smart building automation systems using affordable IoT hardware.", "motivation": "To address the challenges facing intelligent smart building automation systems, such as hardware failures, security threats, and vendor obsolescence, which hinder the scalability and sustainability of automation systems.", "method": "KaOS uses affordable IoT hardware, containerization, and managed resource access to support control applications and distributed operations, focusing on flexibility, security, and fault tolerance.", "result": "Initial evaluation demonstrates that KaOS is practically feasible, showcasing its ability to maintain and evolve building control functionalities sustainably over time.", "conclusion": "KaOS provides a cost-effective approach to developing robust, secure, and adaptable smart building automation systems, with potential for long-term scalability and sustainability."}}
{"id": "2509.05355", "pdf": "https://arxiv.org/pdf/2509.05355", "abs": "https://arxiv.org/abs/2509.05355", "authors": ["Ahmed R. Sadik", "Muhammad Ashfaq", "Niko M\u00e4kitalo", "Tommi Mikkonen"], "title": "Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "The deployment of autonomous drone swarms in disaster response missions\nnecessitates the development of flexible, scalable, and robust coordination\nsystems. Traditional fixed architectures struggle to cope with dynamic and\nunpredictable environments, leading to inefficiencies in energy consumption and\nconnectivity. This paper addresses this gap by proposing an adaptive\narchitecture for drone swarms, leveraging a Large Language Model to dynamically\nselect the optimal architecture as centralized, hierarchical, or holonic based\non real time mission parameters such as task complexity, swarm size, and\ncommunication stability. Our system addresses the challenges of scalability,\nadaptability, and robustness,ensuring efficient energy consumption and\nmaintaining connectivity under varying conditions. Extensive simulations\ndemonstrate that our adaptive architecture outperforms traditional static\nmodels in terms of scalability, energy efficiency, and connectivity. These\nresults highlight the potential of our approach to provide a scalable,\nadaptable, and resilient solution for real world disaster response scenarios.", "AI": {"tldr": "The paper introduces an adaptive drone swarm architecture using a Large Language Model to optimize coordination for disaster response missions.", "motivation": "Improve drone swarm efficiency in dynamic disaster environments by addressing issues with traditional fixed architectures.", "method": "Utilize a Large Language Model to determine the optimal architecture (centralized, hierarchical, or holonic) based on real-time mission conditions.", "result": "Simulations show improved scalability, energy efficiency, and connectivity compared to static models.", "conclusion": "The adaptive architecture is a promising, robust, and scalable solution for real-world disaster response using autonomous drone swarms."}}
{"id": "2509.05425", "pdf": "https://arxiv.org/pdf/2509.05425", "abs": "https://arxiv.org/abs/2509.05425", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.", "AI": {"tldr": "The paper predicts translation quality without using the translation system.", "motivation": "To provide translation quality predictions without invoking the translation model, leveraging simpler features and linguistic insights.", "method": "The authors used gradient boosting models and simplicity features like token fertility ratios, token counts, and linguistic metadata to forecast performance.", "result": "The model achieved strong prediction accuracy with R\u00b2 = 0.66 for XX\u2192English and R\u00b2 = 0.72 for English\u2192XX on the FLORES-200 benchmark.", "conclusion": "Translation quality is influenced by token fertility and linguistic typology, offering a new perspective on multilingual evaluation and quality modeling."}}
{"id": "2509.05596", "pdf": "https://arxiv.org/pdf/2509.05596", "abs": "https://arxiv.org/abs/2509.05596", "authors": ["Soumyadip Bandyopadhyay", "Santonu Sarkar"], "title": "Verifying Correctness of PLC Software during System Evolution using Model Containment Approach", "categories": ["cs.SE", "cs.SC", "68", "D.3.1; D.2.4"], "comment": "31 pages with appendix", "summary": "Upgradation of Programmable Logic Controller (PLC) software is quite common\nto accommodate evolving industrial requirements. Verifying the correctness of\nsuch upgrades remains a significant challenge. In this paper, we propose a\nverification-based approach to ensure the correctness of the existing\nfunctionality in the upgraded version of a PLC software. The method converts\nthe older and the newer versions of the sequential function chart (SFC) into\ntwo Petri net models. We then verify whether one model is contained within\nanother, based on a novel containment checking algorithm grounded in symbolic\npath equivalence. For this purpose, we have developed a home-grown Petri\nnet-based containment checker. Experimental evaluation on 80 real-world\nbenchmarks from the OSCAT library highlights the scalability and effectiveness\nof the framework. We have compared our approach with verifAPS, a popular tool\nused for software upgradation, and observed nearly 4x performance improvement.", "AI": {"tldr": "The paper introduces a method to verify correctness in upgraded PLC software using Petri net models and a novel containment checking algorithm, resulting in improved performance compared to existing tools.", "motivation": "PLC software upgrades often face challenges in verifying the correctness of changes, which is critical for industrial requirements.", "method": "The authors propose a verification approach by converting sequential function charts into Petri net models and employing a symbolic path equivalence-based containment checking algorithm.", "result": "Experimental evaluation on 80 benchmarks reveals that the framework is scalable and effective, achieving a 4x performance improvement over verifAPS.", "conclusion": "The method provides a robust verification solution for PLC software upgradation, demonstrating significant performance and scalability advantages over current tools."}}
{"id": "2509.05852", "pdf": "https://arxiv.org/pdf/2509.05852", "abs": "https://arxiv.org/abs/2509.05852", "authors": ["Yichi Zhang", "Alexander Belloni", "Ethan X. Fang", "Junwei Lu", "Xiaoan Xu"], "title": "Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Motivated by the need for rigorous and scalable evaluation of large language\nmodels, we study contextual preference inference for pairwise comparison\nfunctionals of context-dependent preference score functions across domains.\nFocusing on the contextual Bradley-Terry-Luce model, we develop a\nsemiparametric efficient estimator that automates the debiased estimation\nthrough aggregating weighted residual balancing terms across the comparison\ngraph. We show that the efficiency is achieved when the weights are derived\nfrom a novel strategy called Fisher random walk. We also propose a\ncomputationally feasible method to compute the weights by a potential\nrepresentation of nuisance weight functions. We show our inference procedure is\nvalid for general score function estimators accommodating the practitioners'\nneed to implement flexible deep learning methods. We extend the procedure to\nmultiple hypothesis testing using a Gaussian multiplier bootstrap that controls\nfamilywise error and to distributional shift via a cross-fitted\nimportance-sampling adjustment for target-domain inference. Numerical studies,\nincluding language model evaluations under diverse contexts, corroborate the\naccuracy, efficiency, and practical utility of our method.", "AI": {"tldr": "The paper focuses on contextual preference inference for pairwise comparisons using a novel semiparametric estimator, optimizing accuracy and scalability in large language model evaluations through advanced computational strategies.", "motivation": "To develop rigorous and scalable approaches for evaluating large language models, addressing challenges in pairwise comparisons across diverse domains.", "method": "Introduces a semiparametric efficient estimator leveraging weighted residual balancing and Fisher random walk-derived weights, alongside computational strategies like nuisance weight functions and cross-fitted importance-sampling for adjustments.", "result": "The method is shown to achieve accurate and efficient inference, valid across general score functions and flexible modeling approaches, with utility confirmed via numerical studies.", "conclusion": "The proposed approach offers a robust, scalable, and adaptable framework for contextual preference inference, enhancing evaluation practices for language models and other applications."}}
{"id": "2509.05346", "pdf": "https://arxiv.org/pdf/2509.05346", "abs": "https://arxiv.org/abs/2509.05346", "authors": ["Bo Yuan", "Jiazi Hu"], "title": "Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning", "categories": ["cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.", "AI": {"tldr": "This paper compares three state-of-the-art LLMs in simulating tutoring tasks and finds GPT-4o most effective in generating feedback.", "motivation": "The study aims to evaluate LLMs for personalized learning roles and conduct head-to-head comparisons in realistic learning scenarios.", "method": "The researchers use a dataset of student quizzes and analyze LLM outputs along dimensions like accuracy, clarity, and appropriateness using the Gemini virtual judge.", "result": "GPT-4o outperformed DeepSeek-V3 and GLM-4.5 by producing more informative and structured tutoring feedback.", "conclusion": "The paper concludes that deploying LLMs is promising for personalized learning and suggests methodologies for future research."}}
{"id": "2509.05322", "pdf": "https://arxiv.org/pdf/2509.05322", "abs": "https://arxiv.org/abs/2509.05322", "authors": ["Pavithra Elumalai", "Sudharsan Vijayaraghavan", "Madhumita Mondal", "Areejit Samal"], "title": "Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19", "categories": ["cs.CV", "cs.LG", "cs.SI", "physics.comp-ph"], "comment": "21 pages, 4 figures, 9 tables", "summary": "Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for\ninvestigating the impact of network topology in deep learning by capturing how\ndifferent connectivity patterns impact both learning efficiency and model\nperformance. At the same time, they provide a natural framework for exploring\nedge-centric network measures as tools for pruning and optimization. In this\nstudy, we investigate three edge-centric network measures: Forman-Ricci\ncurvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness\ncentrality (EBC), to compress RWNNs by selectively retaining important synapses\n(or edges) while pruning the rest. As a baseline, RWNNs are trained for\nCOVID-19 chest x-ray image classification, aiming to reduce network complexity\nwhile preserving performance in terms of accuracy, specificity, and\nsensitivity. We extend prior work on pruning RWNN using ORC by incorporating\ntwo additional edge-centric measures, FRC and EBC, across three network\ngenerators: Erd\\\"{o}s-R\\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and\nBarab\\'{a}si-Albert (BA) model. We provide a comparative analysis of the\npruning performance of the three measures in terms of compression ratio and\ntheoretical speedup. A central focus of our study is to evaluate whether FRC,\nwhich is computationally more efficient than ORC, can achieve comparable\npruning effectiveness. Along with performance evaluation, we further\ninvestigate the structural properties of the pruned networks through modularity\nand global efficiency, offering insights into the trade-off between modular\nsegregation and network efficiency in compressed RWNNs. Our results provide\ninitial evidence that FRC-based pruning can effectively simplify RWNNs,\noffering significant computational advantages while maintaining performance\ncomparable to ORC.", "AI": {"tldr": "The paper explores edge-centric measures like Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC) to compress Randomly Wired Neural Networks (RWNNs) while retaining performance in COVID-19 chest x-ray image classification.", "motivation": "The study aims to understand how network topology influences learning efficiency and performance in deep learning, using edge-centric measures for better pruning and optimization in RWNNs.", "method": "Three edge-centric measures (FRC, ORC, EBC) are tested for pruning RWNNs generated by three models (ER, WS, BA). The pruning's impact on compression ratio, speedup, and performance metrics like accuracy is evaluated, with a focus on balancing network complexity, modularity, and efficiency.", "result": "FRC is shown to effectively prune RWNNs while maintaining performance comparable to ORC, offering computational advantages. Structural analysis details trade-offs in network modularity and efficiency post-pruning.", "conclusion": "FRC-based pruning provides an efficient means to simplify RWNNs without significant performance loss, presenting a practical edge-centric approach for network compression."}}
{"id": "2509.05460", "pdf": "https://arxiv.org/pdf/2509.05460", "abs": "https://arxiv.org/abs/2509.05460", "authors": ["Diego Feijer", "Himan Abdollahpouri", "Sanket Gupta", "Alexander Clare", "Yuxiao Wen", "Todd Wasson", "Maria Dimakopoulou", "Zahra Nazari", "Kyle Kretschman", "Mounia Lalmas"], "title": "Calibrated Recommendations with Contextual Bandits", "categories": ["cs.LG", "cs.IR", "stat.ML"], "comment": "Accepted at ACM RecSys '25, CONSEQUENCES workshop", "summary": "Spotify's Home page features a variety of content types, including music,\npodcasts, and audiobooks. However, historical data is heavily skewed toward\nmusic, making it challenging to deliver a balanced and personalized content\nmix. Moreover, users' preference towards different content types may vary\ndepending on the time of day, the day of week, or even the device they use. We\npropose a calibration method that leverages contextual bandits to dynamically\nlearn each user's optimal content type distribution based on their context and\npreferences. Unlike traditional calibration methods that rely on historical\naverages, our approach boosts engagement by adapting to how users interests in\ndifferent content types varies across contexts. Both offline and online results\ndemonstrate improved precision and user engagement with the Spotify Home page,\nin particular with under-represented content types such as podcasts.", "AI": {"tldr": "Spotify proposes a dynamic content calibration method using contextual bandits to optimize user engagement by adapting to varied user preferences across contexts.", "motivation": "The need to address imbalances in content types (music-heavy) and adapt to user preferences that vary by time, day, and device.", "method": "Using contextual bandits to dynamically learn and adapt content type distribution tailored to individual user contexts and preferences.", "result": "Improved precision and user engagement, especially for under-represented content types like podcasts, based on both offline and online tests.", "conclusion": "Personalized calibration using contextual bandits improves user engagement and addresses content distribution imbalances."}}
{"id": "2509.05584", "pdf": "https://arxiv.org/pdf/2509.05584", "abs": "https://arxiv.org/abs/2509.05584", "authors": ["Sadegh Jafari", "Aishwarya Sarkar", "Mohiuddin Bilwal", "Ali Jannesari"], "title": "ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization", "categories": ["cs.LG", "cs.CV", "cs.PF"], "comment": "13 pages, 3 figures, 5 tables, 1 algorithm", "summary": "Foundation models face growing compute and memory bottlenecks, hindering\ndeployment on resource-limited platforms. While compression techniques such as\npruning and quantization are widely used, most rely on uniform heuristics that\nignore architectural and runtime heterogeneity. Profiling tools expose\nper-layer latency, memory, and compute cost, yet are rarely integrated into\nautomated pipelines. We propose ProfilingAgent, a profiling-guided, agentic\napproach that uses large language models (LLMs) to automate compression via\nstructured pruning and post-training dynamic quantization. Our modular\nmulti-agent system reasons over static metrics (MACs, parameter counts) and\ndynamic signals (latency, memory) to design architecture-specific strategies.\nUnlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to\nbottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with\nResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive\nor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on\nsmaller datasets), while quantization achieves up to 74% memory savings with\n<0.5% accuracy loss. Our quantization also yields consistent inference speedups\nof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo\nhighlight the importance of LLM reasoning quality for iterative pruning. These\nresults establish agentic systems as scalable solutions for profiling-guided\nmodel optimization.", "AI": {"tldr": "ProfilingAgent is an agentic, profiling-guided approach to optimize foundation models through structured pruning and quantization, achieving high efficiency with minimal accuracy loss.", "motivation": "The paper aims to address compute and memory bottlenecks faced by foundation models, especially on resource-constrained platforms. Current compression methods often use heuristics that overlook model-specific architecture and runtime demands.", "method": "The authors developed ProfilingAgent, a modular multi-agent system leveraging large language models (LLMs) to automate compression. It combines profiling tools for static (e.g., MACs, parameter counts) and dynamic (e.g., latency, memory) metrics to devise customized pruning and quantization strategies.", "result": "Experiments demonstrated that pruning maintains or even improves accuracy (e.g., ~1% drop on ImageNet-1K, +2% gains on smaller datasets), while quantization saves up to 74% memory with <0.5% accuracy loss and speeds up inference up to 1.74x.", "conclusion": "ProfilingAgent proves that agentic systems guided by profiling tools are effective and scalable for optimizing foundation models, balancing efficiency improvements and minimal accuracy trade-offs."}}
{"id": "2509.06698", "pdf": "https://arxiv.org/pdf/2509.06698", "abs": "https://arxiv.org/abs/2509.06698", "authors": ["Leidy Mabel Alvero-Gonzalez", "Matias Miguez", "Eric Gutierrez", "Juan Sapriza", "Susana Pat\u00f3n", "David Atienza", "Jos\u00e9 Miranda"], "title": "VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing", "categories": ["cs.AR"], "comment": null, "summary": "Continuous monitoring of electrodermal activity (EDA) through wearable\ndevices has attracted much attention in recent times. However, the persistent\nchallenge demands analog front-end (AFE) systems with high sensitivity, low\npower consumption, and minimal calibration requirements to ensure practical\nusability in wearable technologies. In response to this challenge, this\nresearch introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog\nReadout tailored for continuous EDA sensing. The results show that our system\nachieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS\nrange and a negligible relative error of less than 0.0025% for\nfixed-resistance. Furthermore, the proposed system consumes only an average of\n2.3 uW based on post-layout validations and introduces a low noise\ncontribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.\nThis research aims to drive the evolution of wearable sensors characterized by\nseamless adaptability to diverse users, minimal power consumption, and\noutstanding noise resilience.", "AI": {"tldr": "This paper proposes VCO-CARE, a high-sensitivity, low-power Voltage-Controlled Oscillator-based Analog Readout designed for wearable electrodermal activity monitoring.", "motivation": "To address the challenge of high sensitivity, low power consumption, and minimal calibration in Analog Front-End systems for wearable electrodermal activity monitoring.", "method": "Introduction of VCO-CARE, which uses a Voltage-Controlled Oscillator-based Analog Readout coupled with post-layout validations to measure performance metrics like sensitivity, power consumption, and noise resilience.", "result": "The system achieves an average sensitivity of 40 pS, a negligible relative error below 0.0025%, consumes 2.3 uW of power, and shows low noise contribution of 0.8 uVrms in the 0-1.5 Hz band.", "conclusion": "VCO-CARE advances wearable sensor technology by delivering high adaptability, superior performance, and minimal resource consumption, addressing key challenges in EDA monitoring."}}
{"id": "2509.06845", "pdf": "https://arxiv.org/pdf/2509.06845", "abs": "https://arxiv.org/abs/2509.06845", "authors": ["Tom Lauwaerts", "Maarten Steevens", "Christophe Scholliers"], "title": "MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices", "categories": ["cs.PL", "cs.SE"], "comment": "This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025", "summary": "Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller.", "AI": {"tldr": "A novel multiverse debugging approach addresses challenges with debugging non-deterministic microcontroller programs involving input/output operations, preventing exploration of inaccessible states.", "motivation": "Debugging non-deterministic programs on microcontrollers is difficult, especially with unpredictable input-dependent execution paths where current methods cause inefficiencies and inaccuracies.", "method": "The paper introduces new semantics for debugging, ensuring exploration of only reachable states during regular execution, and develops a prototype debugger called MIO using WARDuino WebAssembly virtual machine.", "result": "The prototype, MIO, successfully demonstrates the feasibility and efficiency of the approach by enabling debugging on actual hardware, such as a Lego Mindstorms motor and STM32 microcontroller.", "conclusion": "This multiverse debugging approach improves the debugging experience for non-deterministic programs by correctly handling input/output operations and preventing the exploration of inaccessible states."}}
{"id": "2509.06261", "pdf": "https://arxiv.org/pdf/2509.06261", "abs": "https://arxiv.org/abs/2509.06261", "authors": ["Kyungmin Bin", "Seungbeom Choi", "Jimyoung Son", "Jieun Choi", "Daseul Bae", "Daehyeon Baek", "Kihyo Moon", "Minsung Jang", "Hyojung Lee"], "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.", "AI": {"tldr": "FineServe is a mixed-precision inference serving framework designed for quantized large language models, addressing challenges in memory fragmentation and resource scheduling.", "motivation": "To improve memory efficiency and GPU resource utilization for quantized LLMs, while addressing inefficiencies caused by smaller KV block sizes and diverse resource usage patterns.", "method": "FineServe introduces two techniques: KV Slab, a memory management system tailored for precision-aware model characteristics, and a two-level scheduling framework with global and local schedulers for efficient GPU resource allocation.", "result": "FineServe outperformed existing GPU sharing systems by achieving up to 2.2x higher SLO attainment and 1.8x greater token generation throughput during experiments.", "conclusion": "FineServe demonstrates a significant improvement in serving quantized LLMs, optimizing memory usage and scheduling to enhance throughput and accuracy for mixed-precision models."}}
{"id": "2509.05356", "pdf": "https://arxiv.org/pdf/2509.05356", "abs": "https://arxiv.org/abs/2509.05356", "authors": ["Justus Huebotter", "Pablo Lanillos", "Marcel van Gerven", "Serge Thill"], "title": "Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.", "AI": {"tldr": "The paper demonstrates that spiking neural networks (SNNs) can be effectively trained for robotic arm control in continuous environments, addressing challenges in high-dimensional motor tasks.", "motivation": "The paper addresses the challenge of extending the application of spiking neural networks beyond classification tasks to continuous motor control for robotics.", "method": "It proposes an end-to-end predictive-control framework that combines Leaky Integrate-and-Fire dynamics with surrogate gradients for joint optimization of a forward model and a policy network.", "result": "The approach successfully achieves stable training and precise torque control in both 2D and high-degree-of-freedom 6-DOF robotic tasks. An ablation study highlights the role of initialization, time constants, and regularization.", "conclusion": "SNNs are shown to be viable for motor control but require careful tuning of hyperparameters, emphasizing the importance of thoughtful design choices."}}
{"id": "2509.05440", "pdf": "https://arxiv.org/pdf/2509.05440", "abs": "https://arxiv.org/abs/2509.05440", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.", "AI": {"tldr": "The paper introduces a direct-scoring method for evaluating large language model-generated summaries by using synthetic summaries for pairwise rankings. It achieves comparable correlations with human judgment to state-of-the-art evaluators.", "motivation": "To address the limitation of pairwise comparison methods which cannot assign absolute scores to summaries, intended for applications requiring thresholding.", "method": "Proposing a direct-scoring method that uses synthetic summaries to provide pairwise rankings during evaluation.", "result": "The proposed method performs similarly to leading evaluators in sample-level correlation benchmarks, with slight performance variations (+0.03, -0.03, +0.05) across different datasets.", "conclusion": "The direct-scoring method is a viable alternative for evaluating machine-generated text, providing absolute scoring capabilities alongside comparable performance to existing methods. Synthetic summaries were also released to support future research."}}
{"id": "2509.05749", "pdf": "https://arxiv.org/pdf/2509.05749", "abs": "https://arxiv.org/abs/2509.05749", "authors": ["AmirHossein Naghshzan"], "title": "Automating API Documentation with LLMs: A BERTopic Approach", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Developers rely on API documentation, but official sources are often lengthy,\ncomplex, or incomplete. Many turn to community-driven forums like Stack\nOverflow for practical insights. We propose automating the summarization of\ninformal sources, focusing on Android APIs. Using BERTopic, we extracted\nprevalent topics from 3.6 million Stack Overflow posts and applied extractive\nsummarization techniques to generate concise summaries, including code\nsnippets. A user study with 30 Android developers assessed the summaries for\ncoherence, relevance, informativeness, and satisfaction, showing improved\nproductivity. Integrating formal API knowledge with community-generated content\nenhances documentation, making API resources more accessible and actionable\nwork.", "AI": {"tldr": "The study explores automating the summarization of informal sources like Stack Overflow to enhance Android API documentation using BERTopic and extractive summarization methods.", "motivation": "Official API documentation is often lengthy, complex, and incomplete, prompting the need for more accessible resources.", "method": "The authors extracted topics from 3.6 million Stack Overflow posts using BERTopic and applied extractive summarization to generate concise summaries with code snippets.", "result": "A user study involving 30 Android developers demonstrated that the generated summaries improved coherence, relevance, informativeness, satisfaction, and productivity.", "conclusion": "Integrating community-driven insights with formal API documentation enhances accessibility and practical usability of API resources."}}
{"id": "2509.05877", "pdf": "https://arxiv.org/pdf/2509.05877", "abs": "https://arxiv.org/abs/2509.05877", "authors": ["Marzieh Ajirak", "Anand Ravishankar", "Petar M. Djuric"], "title": "Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Accepted to EUSIPCO 2025", "summary": "Uncertainty Quantification (UQ) is essential in probabilistic machine\nlearning models, particularly for assessing the reliability of predictions. In\nthis paper, we present a systematic framework for estimating both epistemic and\naleatoric uncertainty in probabilistic models. We focus on Gaussian Process\nLatent Variable Models and employ scalable Random Fourier Features-based\nGaussian Processes to approximate predictive distributions efficiently. We\nderive a theoretical formulation for UQ, propose a Monte Carlo sampling-based\nestimation method, and conduct experiments to evaluate the impact of\nuncertainty estimation. Our results provide insights into the sources of\npredictive uncertainty and illustrate the effectiveness of our approach in\nquantifying the confidence in the predictions.", "AI": {"tldr": "The paper focuses on improving uncertainty quantification (UQ) in probabilistic machine learning using Gaussian Process Latent Variable Models and scalable methods.", "motivation": "To provide a framework to accurately assess both epistemic and aleatoric uncertainties in probabilistic models, enhancing prediction reliability.", "method": "The paper uses Gaussian Process Latent Variable Models, scalable Random Fourier Features-based Gaussian Processes, a theoretical formulation for UQ, and a Monte Carlo sampling-based estimation to quantify uncertainty.", "result": "Experiments showcase the sources of predictive uncertainty and validate the effectiveness of the proposed framework in confidence estimation.", "conclusion": "The approach offers a systematic and scalable method for UQ, providing insights and confidence in probabilistic model predictions."}}
{"id": "2509.05363", "pdf": "https://arxiv.org/pdf/2509.05363", "abs": "https://arxiv.org/abs/2509.05363", "authors": ["Lijie Ding", "Changwoo Do"], "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.MA"], "comment": "8 pages, 7 figures", "summary": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.", "AI": {"tldr": "SasAgent, a multi-agent AI system using LLMs, automates small-angle scattering (SAS) data analysis, featuring specialized tools derived from SasView Python library.", "motivation": "To simplify and automate complex SAS data analysis workflows using AI-driven systems.", "method": "SasAgent employs LLMs and a multi-agent structure, combining tools like SLD calculator, bump fitting tool, and retrieval-augmented generation documentation tool, managed via a coordinator agent.", "result": "Demonstrated ability to process complex prompts, perform SLD calculations, generate synthetic data, and fit experimental datasets with high accuracy in SAS research.", "conclusion": "LLM-driven AI systems like SasAgent can significantly enhance scientific workflows and automation in small-angle scattering data analysis."}}
{"id": "2509.05329", "pdf": "https://arxiv.org/pdf/2509.05329", "abs": "https://arxiv.org/abs/2509.05329", "authors": ["Juan Carlos Martinez-Sevilla", "Francesco Foscarin", "Patricia Garcia-Iasci", "David Rizo", "Jorge Calvo-Zaragoza", "Gerhard Widmer"], "title": "Optical Music Recognition of Jazz Lead Sheets", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "In this paper, we address the challenge of Optical Music Recognition (OMR)\nfor handwritten jazz lead sheets, a widely used musical score type that encodes\nmelody and chords. The task is challenging due to the presence of chords, a\nscore component not handled by existing OMR systems, and the high variability\nand quality issues associated with handwritten images. Our contribution is\ntwo-fold. We present a novel dataset consisting of 293 handwritten jazz lead\nsheets of 163 unique pieces, amounting to 2021 total staves aligned with\nHumdrum **kern and MusicXML ground truth scores. We also supply synthetic score\nimages generated from the ground truth. The second contribution is the\ndevelopment of an OMR model for jazz lead sheets. We discuss specific\ntokenisation choices related to our kind of data, and the advantages of using\nsynthetic scores and pretrained models. We publicly release all code, data, and\nmodels.", "AI": {"tldr": "The paper tackles Optical Music Recognition (OMR) for handwritten jazz lead sheets, providing a new dataset and developing a specialized OMR model.", "motivation": "Existing OMR systems struggle with handwritten jazz lead sheets due to the presence of chords and variability in handwriting quality.", "method": "The authors created a dataset of handwritten jazz lead sheets and synthetic score images, and designed a tailored OMR model utilizing tokenization techniques, pretrained models, and synthetic data.", "result": "They produced a dataset of 293 handwritten jazz lead sheets, aligned with ground truth scores, and demonstrated the ability to recognize handwritten jazz scores effectively.", "conclusion": "The paper advances OMR for jazz lead sheets, addressing previous limitations, while making all code, data, and models publicly accessible for further research."}}
{"id": "2509.05478", "pdf": "https://arxiv.org/pdf/2509.05478", "abs": "https://arxiv.org/abs/2509.05478", "authors": ["Jia Wang", "Xiao Wang", "Chi Zhang"], "title": "PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series (MTS) are ubiquitous in domains such as healthcare,\nclimate science, and industrial monitoring, but their high dimensionality,\nlimited labeled data, and non-stationary nature pose significant challenges for\nconventional machine learning methods. While recent self-supervised learning\n(SSL) approaches mitigate label scarcity by data augmentations or time\npoint-based contrastive strategy, they neglect the intrinsic periodic structure\nof MTS and fail to capture the dynamic evolution of latent states. We propose\nPLanTS, a periodicity-aware self-supervised learning framework that explicitly\nmodels irregular latent states and their transitions. We first designed a\nperiod-aware multi-granularity patching mechanism and a generalized contrastive\nloss to preserve both instance-level and state-level similarities across\nmultiple temporal resolutions. To further capture temporal dynamics, we design\na next-transition prediction pretext task that encourages representations to\nencode predictive information about future state evolution. We evaluate PLanTS\nacross a wide range of downstream tasks-including multi-class and multi-label\nclassification, forecasting, trajectory tracking and anomaly detection. PLanTS\nconsistently improves the representation quality over existing SSL methods and\ndemonstrates superior runtime efficiency compared to DTW-based methods.", "AI": {"tldr": "PLanTS is a self-supervised learning framework for multivariate time series (MTS) that incorporates periodicity to improve representation quality and runtime efficiency across diverse downstream tasks.", "motivation": "To address the challenges of high dimensionality, limited labeled data, and non-stationarity in multivariate time series (MTS) while overcoming the limitations of existing self-supervised learning approaches that fail to capture intrinsic periodic structures and latent state dynamics.", "method": "The paper introduces a periodicity-aware self-supervised learning framework (PLanTS) that uses a period-aware multi-granularity patching mechanism, a generalized contrastive loss, and a next-transition prediction task to model latent state dynamics and capture temporal structures.", "result": "PLanTS delivers enhanced representation quality and outperforms existing self-supervised learning methods in downstream tasks like classification, forecasting, and anomaly detection, while also improving runtime efficiency compared to methods relying on Dynamic Time Warping (DTW).", "conclusion": "PLanTS effectively captures the dynamic evolution and periodicity in MTS, offering a robust and efficient solution for self-supervised learning, particularly in scenarios with limited labels and complex temporal structures."}}
{"id": "2509.05750", "pdf": "https://arxiv.org/pdf/2509.05750", "abs": "https://arxiv.org/abs/2509.05750", "authors": ["Ilias Azizi", "Karima Echihab", "Themis Palpanas", "Vassilis Christophides"], "title": "Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.PF"], "comment": "Presented at ICML 2025 VecDB Workshop; an extended version appeared\n  in ACM SIGMOD 2025 ('Graph-Based Vector Search: An Experimental Evaluation of\n  the State-of-the-Art')", "summary": "Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. Although several paradigms (seed selection, incremental insertion,\nneighborhood propagation, neighborhood diversification, and divide-and-conquer)\nhave been employed to design in-memory graph-based vector search algorithms, a\nsystematic comparison of the key algorithmic advances is still missing. We\nconduct an exhaustive experimental evaluation of twelve state-of-the-art\nmethods on seven real data collections, with sizes up to 1 billion vectors. We\nshare key insights about the strengths and limitations of these methods; e.g.,\nthe best approaches are typically based on incremental insertion and\nneighborhood diversification, and the choice of the base graph can hurt\nscalability. Finally, we discuss open research directions, such as the\nimportance of devising more sophisticated data adaptive seed selection and\ndiversification strategies.", "AI": {"tldr": "The paper evaluates and compares 12 state-of-the-art graph-based vector search methods on large datasets and provides insights on their effectiveness.", "motivation": "To address the lack of a systematic comparison of graph-based vector search algorithms, which have become critical for analyzing large-scale, high-dimensional vector data.", "method": "The authors conduct an exhaustive experimental evaluation of twelve graph-based vector search algorithms on seven real-world datasets containing up to 1 billion vectors, evaluating their strengths and limitations under various conditions.", "result": "The analysis shows that incremental insertion and neighborhood diversification are the most effective strategies, while certain base graph choices can hinder scalability.", "conclusion": "This paper offers key insights into graph-based methods for vector search, highlighting gaps for improvement like better seed selection and diversification strategies."}}
{"id": "2509.06872", "pdf": "https://arxiv.org/pdf/2509.06872", "abs": "https://arxiv.org/abs/2509.06872", "authors": ["Zachary Kent", "Ugur Y. Yavuz", "Siddhartha Jayanti", "Stephanie Balzer", "Guy Blelloch"], "title": "Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs", "categories": ["cs.PL"], "comment": null, "summary": "In the past decade, many techniques have been developed to prove\nlinearizability, the gold standard of correctness for concurrent data\nstructures. Intuitively, linearizability requires that every operation on a\nconcurrent data structure appears to take place instantaneously, even when\ninterleaved with other operations. Most recently, Jayanti et al. presented the\nfirst sound and complete \"forward reasoning\" technique for proving\nlinearizability that relates the behavior of a concurrent data structure to a\nreference atomic data structure as time moves forward. This technique can be\nused to produce machine-checked proofs of linearizability in TLA+. However,\nwhile Jayanti et al.'s approach is shown to be sound and complete, a\nmechanization of this important metatheoretic result is still outstanding. As a\nresult, it is not possible to produce verified end-to-end proofs of\nlinearizability. To reduce the size of this trusted computing base, we\nformalize this forward reasoning technique and mechanize proofs of its\nsoundness and completeness in Rocq. As a case study, we use the approach to\nproduce a verified end-to-end proof of linearizability for a simple concurrent\nregister.", "AI": {"tldr": "The paper discusses the formalization and mechanization of a \"forward reasoning\" technique for proving linearizability in concurrent data structures, ensuring verified end-to-end proofs.", "motivation": "The paper aims to reduce the trusted computing base by mechanizing the soundness and completeness proofs of a forward reasoning technique for linearizability.", "method": "The authors formalize the forward reasoning approach and implement the proofs in Rocq, demonstrating its applicability through a case study on a simple concurrent register.", "result": "Verified mechanized proofs of soundness and completeness for the forward reasoning technique are produced, along with an end-to-end proof of linearizability for a concurrent register.", "conclusion": "Formalization and mechanization enhance reliability in proving linearizability, enabling more robust and verified end-to-end proofs of correctness for concurrent systems."}}
{"id": "2509.06362", "pdf": "https://arxiv.org/pdf/2509.06362", "abs": "https://arxiv.org/abs/2509.06362", "authors": ["Mo Xuan", "Zhang yue", "Wu Weigang"], "title": "MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS", "categories": ["cs.DC"], "comment": null, "summary": "Model-as-a-Service (MaaS) platforms face diverse Service Level Objective\n(SLO) requirements stemming from various large language model (LLM)\napplications, manifested in contextual complexity, first-token latency, and\nbetween-token latency. On the other hand, an LLM instance, when configured with\ndifferent parallelism strategies and inference batch sizes, exhibits distinct\nperformance characteristics and can thus be used to serve different SLO\nrequirements. However, current LLM inference systems typically deploy instances\nof the same model with identical configurations, lacking mechanisms to leverage\nsuch heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS\nOrchestrator, which comprises three modules: (1) a profiler characterizing\ninstance performance under diverse parallelism strategies and inference batch\nsizes; (2) a placer optimizing heterogeneous instance configurations; (3) a\ndistributor enabling SLO-aware request distribution and preventing cascaded\ntimeouts in continuous batching. Experiments show that MaaSO improves the SLO\nsatisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%\ncompared to existing approaches, and significantly lowers overall orchestration\noverhead.", "AI": {"tldr": "MaaSO is introduced as an orchestrator to optimize the Service Level Objectives (SLOs) for Model-as-a-Service platforms by employing heterogeneous configurations and dynamic request distributions.", "motivation": "Current MaaS platforms fail to address diverse SLO requirements adequately, relying on uniform configurations for LLM instances despite varied performance characteristics across strategies.", "method": "MaaSO incorporates a profiler for performance characterization, a placer for optimization of configurations, and a distributor for effective request handling and timeout mitigation.", "result": "Experiments reveal that MaaSO boosts SLO satisfaction ratios by 15-30%, reduces latency by 40-60%, and lowers orchestration overhead compared to conventional methods.", "conclusion": "MaaSO demonstrates significant advancements in optimizing SLOs for MaaS platforms with reduced latency and overhead, showcasing its utility in heterogeneous instance management."}}
{"id": "2509.05368", "pdf": "https://arxiv.org/pdf/2509.05368", "abs": "https://arxiv.org/abs/2509.05368", "authors": ["Quan Chen", "Chenrui Shi", "Qi Chen", "Yuwei Wu", "Zhi Gao", "Xintong Zhang", "Rui Gao", "Kun Wu", "Yunde Jia"], "title": "Long-Horizon Visual Imitation Learning via Plan and Code Reflection", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.9; I.2.10"], "comment": "9 pages, 4 figures. Submitted to AAAI 2026", "summary": "Learning from long-horizon demonstrations with complex action sequences\npresents significant challenges for visual imitation learning, particularly in\nunderstanding temporal relationships of actions and spatial relationships\nbetween objects. In this paper, we propose a new agent framework that\nincorporates two dedicated reflection modules to enhance both plan and code\ngeneration. The plan generation module produces an initial action sequence,\nwhich is then verified by the plan reflection module to ensure temporal\ncoherence and spatial alignment with the demonstration video. The code\ngeneration module translates the plan into executable code, while the code\nreflection module verifies and refines the generated code to ensure correctness\nand consistency with the generated plan. These two reflection modules jointly\nenable the agent to detect and correct errors in both the plan generation and\ncode generation, improving performance in tasks with intricate temporal and\nspatial dependencies. To support systematic evaluation, we introduce\nLongVILBench, a benchmark comprising 300 human demonstrations with action\nsequences of up to 18 steps. LongVILBench emphasizes temporal and spatial\ncomplexity across multiple task types. Experimental results demonstrate that\nexisting methods perform poorly on this benchmark, whereas our new framework\nestablishes a strong baseline for long-horizon visual imitation learning.", "AI": {"tldr": "The paper introduces a new framework with reflection modules to tackle challenges in long-horizon visual imitation learning, ensuring coherence and correctness in action plans and executable code.", "motivation": "To address the difficulties in learning from long-horizon demonstrations with complex temporal and spatial dependencies.", "method": "The method incorporates two reflection modules: one for plan verification and refinement, and another for code verification and refinement, paired with a new benchmark called LongVILBench.", "result": "The proposed framework significantly outperforms existing methods on the LongVILBench benchmark, demonstrating its effectiveness in handling temporal and spatial complexities.", "conclusion": "The framework provides a strong baseline for improving long-horizon visual imitation learning and advances systematic evaluation in this field."}}
{"id": "2509.05484", "pdf": "https://arxiv.org/pdf/2509.05484", "abs": "https://arxiv.org/abs/2509.05484", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "categories": ["cs.CL"], "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality.", "AI": {"tldr": "The paper discusses a multi-stage Large Language Model (LLM)-based framework for analyzing hospital call center messages to classify topics and reasons effectively. LLMs like o3 and gpt-5 were tested, achieving high accuracy, and their outputs were transformed into actionable insights through a visualization tool.", "motivation": "To leverage the extensive text data generated by hospital call centers to gain actionable insights without the extensive manual effort required by traditional supervised learning methods.", "method": "A multi-stage framework using different Large Language Models (LLMs), such as reasoning, general-purpose, and lightweight models, for message topic identification and classification. Models were evaluated for performance, and the best outputs were integrated into a decision-support visualization tool.", "result": "The model 'o3' achieved the best performance with a 78.4% weighted F1-score and 79.2% accuracy, followed by gpt-5 with 75.3% weighted F1-score and 76.2% accuracy. Outputs satisfy data security and HIPAA compliance requirements.", "conclusion": "The proposed LLM framework enhances the utilization of hospital call center data by simplifying and improving analysis processes. This approach supports better patient care quality and identifies staff training needs."}}
{"id": "2509.05769", "pdf": "https://arxiv.org/pdf/2509.05769", "abs": "https://arxiv.org/abs/2509.05769", "authors": ["Edyta Brzychczy", "Urszula Jessen", "Krzysztof Kluza", "Sridhar Sriram", "Manuel Vargas Nettelnstroth"], "title": "IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining", "categories": ["cs.SE"], "comment": "17 pages, conference draft", "summary": "This paper presents IoT Miner, a novel framework for automatically creating\nhigh-level event logs from raw industrial sensor data to support process\nmining. In many real-world settings, such as mining or manufacturing, standard\nevent logs are unavailable, and sensor data lacks the structure and semantics\nneeded for analysis. IoT Miner addresses this gap using a four-stage pipeline:\ndata preprocessing, unsupervised clustering, large language model (LLM)-based\nlabeling, and event log construction. A key innovation is the use of LLMs to\ngenerate meaningful activity labels from cluster statistics, guided by\ndomain-specific prompts. We evaluate the approach on sensor data from a\nLoad-Haul-Dump (LHD) mining machine and introduce a new metric,\nSimilarity-Weighted Accuracy, to assess labeling quality. Results show that\nricher prompts lead to more accurate and consistent labels. By combining AI\nwith domain-aware data processing, IoT Miner offers a scalable and\ninterpretable method for generating event logs from IoT data, enabling process\nmining in settings where traditional logs are missing.", "AI": {"tldr": "IoT Miner is a framework designed to create event logs from raw sensor data using a four-stage pipeline, incorporating large language models for labeling.", "motivation": "Traditional event logs for process mining are often unavailable in industrial settings. IoT Miner tackles the challenge of deriving structured event logs from unstructured sensor data.", "method": "The framework includes data preprocessing, unsupervised clustering, LLM-based labeling with domain prompts, and event log construction. A new metric, Similarity-Weighted Accuracy, is introduced for evaluation.", "result": "Evaluations on Load-Haul-Dump mining machine data showed that domain-specific, rich prompts significantly enhance the accuracy and consistency of activity labels generated by LLMs.", "conclusion": "IoT Miner merges AI and domain-specific data processing to provide a scalable solution for extracting event logs from IoT data, addressing gaps in process mining for environments lacking traditional logs."}}
{"id": "2509.06147", "pdf": "https://arxiv.org/pdf/2509.06147", "abs": "https://arxiv.org/abs/2509.06147", "authors": ["Zaile Li", "Yuchen Wan", "L. Jeff Hong"], "title": "Additive Distributionally Robust Ranking and Selection", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Due to the 1,920-character limit imposed on the abstract field, the\n  abstract presented here is a truncated version of the full abstract provided\n  in the PDF. The only omitted sentence is: We also prove the additivity and\n  consistency for GAA procedures", "summary": "Ranking and selection (R&S) aims to identify the alternative with the best\nmean performance among $k$ simulated alternatives. The practical value of R&S\ndepends on accurate simulation input modeling, which often suffers from the\ncurse of input uncertainty due to limited data. Distributionally robust ranking\nand selection (DRR&S) addresses this challenge by modeling input uncertainty\nvia an ambiguity set of $m > 1$ plausible input distributions, resulting in\n$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:\nadditivity in budget allocation is essential for efficiency. However, existing\njustifications are heuristic, and fundamental properties such as consistency\nand the precise allocation pattern induced by additivity remain poorly\nunderstood. In this paper, we propose a simple additive allocation (AA)\nprocedure that aims to exclusively sample the $k + m - 1$ previously\nhypothesized critical scenarios. Leveraging boundary-crossing arguments, we\nestablish a lower bound on the probability of correct selection and\ncharacterize the procedure's budget allocation behavior. We then prove that AA\nis consistent and, surprisingly, achieves additivity in the strongest sense: as\nthe total budget increases, only $k + m - 1$ scenarios are sampled infinitely\noften. Notably, the worst-case scenarios of non-best alternatives may not be\namong them, challenging prior beliefs about their criticality. These results\noffer new and counterintuitive insights into the additive structure of DRR&S.\nTo improve practical performance while preserving this structure, we introduce\na general additive allocation (GAA) framework that flexibly incorporates\nsampling rules from traditional R&S procedures in a modular fashion. Numerical\nexperiments support our theoretical findings and demonstrate the competitive\nperformance of the proposed GAA procedures.", "AI": {"tldr": "This paper addresses the challenge of ranking and selection (R&S) under input uncertainty with a proposed additive allocation (AA) method, demonstrating its consistency and theoretically grounded efficiency.", "motivation": "The motivation is to improve the efficiency and accuracy of Distributionally Robust Ranking and Selection (DRR&S) methods in the presence of input uncertainty caused by limited data, a common practical issue.", "method": "The authors propose an additive allocation (AA) procedure that focuses sampling on critical scenarios, using boundary-crossing arguments to establish correctness and behavior. They also introduce a General Additive Allocation (GAA) framework for practical adaptability.", "result": "The AA procedure proves to be consistent and efficiently allocates sampling to only $k + m - 1$ critical scenarios as the total budget increases. This challenges previously held assumptions about scenario criticality. The GAA framework performs well in numerical experiments.", "conclusion": "The paper provides new insights into additive structures in DRR&S, showcasing the theoretical and practical advantage of sampling only critical scenarios for efficient R&S in uncertain contexts."}}
{"id": "2509.05375", "pdf": "https://arxiv.org/pdf/2509.05375", "abs": "https://arxiv.org/abs/2509.05375", "authors": ["Arend Hintze"], "title": "Characterizing Fitness Landscape Structures in Prompt Engineering", "categories": ["cs.AI"], "comment": null, "summary": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.", "AI": {"tldr": "The paper analyzes the optimization landscape of prompt engineering for language models using autocorrelation of semantic embeddings. It identifies contrasting topologies for systematic and diversified prompt generation.", "motivation": "There is limited understanding of the underlying optimization landscapes governing prompt engineering for language models, making it necessary to characterize these landscapes for better optimization strategies.", "method": "The study uses autocorrelation analysis of semantic embedding spaces to characterize landscapes. It experiments with systematic enumeration (1,024 prompts) and diversity-driven generation (1,000 prompts) across error detection tasks.", "result": "Systematic prompt generation reveals smooth, decaying landscape patterns, while diversified generation shows rugged and hierarchically structured landscapes. Analysis across 10 error detection categories highlights variability in ruggedness.", "conclusion": "The findings provide insights into prompt engineering optimization, suggesting the need for task-specific tailoring due to varying landscape topologies."}}
{"id": "2509.05333", "pdf": "https://arxiv.org/pdf/2509.05333", "abs": "https://arxiv.org/abs/2509.05333", "authors": ["Junghyun Park", "Tuan Anh Nguyen", "Dugki Min"], "title": "RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real world deployments often expose modern object recognition models to\ndomain shifts that precipitate a severe drop in accuracy. Such shifts encompass\n(i) variations in low level image statistics, (ii) changes in object pose and\nviewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent\nclasses. To mitigate this degradation, we introduce the Re-Thinking Vision\nLanguage Model (RT-VLM) framework. The foundation of this framework is a unique\nsynthetic dataset generation pipeline that produces images annotated with\n\"4-Clues\": precise bounding boxes, class names, detailed object-level captions,\nand a comprehensive context-level caption for the entire scene. We then perform\nparameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this\nresource. At inference time, a two stage Re-Thinking scheme is executed: the\nmodel first emits its own four clues, then re examines these responses as\nevidence and iteratively corrects them. Across robustness benchmarks that\nisolate individual domain shifts, RT-VLM consistently surpasses strong\nbaselines. These findings indicate that the integration of structured\nmultimodal evidence with an explicit self critique loop constitutes a promising\nroute toward reliable and transferable visual understanding.", "AI": {"tldr": "This paper introduces the RT-VLM framework to tackle domain shift challenges in object recognition.", "motivation": "Current object recognition models face a steep accuracy drop under domain shifts such as changes in image statistics, object poses, and visual confusion among classes.", "method": "The RT-VLM framework builds upon a synthetic dataset generation pipeline annotated with '4-Clues' and tunes a vision-language model (Llama 3.2 11B) for parameter efficient supervised learning. At inference, it employs a two-stage process: generate clues and iteratively refine results.", "result": "RT-VLM demonstrates superior performance over strong baselines on robustness benchmarks isolating domain shift factors.", "conclusion": "The integration of multimodal evidence with a self-critical iterative loop offers a robust approach for reliable and transferable visual understanding."}}
{"id": "2509.05481", "pdf": "https://arxiv.org/pdf/2509.05481", "abs": "https://arxiv.org/abs/2509.05481", "authors": ["Eric Palanques-Tost", "Hanna Krasowski", "Murat Arcak", "Ron Weiss", "Calin Belta"], "title": "STL-based Optimization of Biomolecular Neural Networks for Regression and Control", "categories": ["cs.LG", "q-bio.MN", "q-bio.QM"], "comment": null, "summary": "Biomolecular Neural Networks (BNNs), artificial neural networks with\nbiologically synthesizable architectures, achieve universal function\napproximation capabilities beyond simple biological circuits. However, training\nBNNs remains challenging due to the lack of target data. To address this, we\npropose leveraging Signal Temporal Logic (STL) specifications to define\ntraining objectives for BNNs. We build on the quantitative semantics of STL,\nenabling gradient-based optimization of the BNN weights, and introduce a\nlearning algorithm that enables BNNs to perform regression and control tasks in\nbiological systems. Specifically, we investigate two regression problems in\nwhich we train BNNs to act as reporters of dysregulated states, and a feedback\ncontrol problem in which we train the BNN in closed-loop with a chronic disease\nmodel, learning to reduce inflammation while avoiding adverse responses to\nexternal infections. Our numerical experiments demonstrate that STL-based\nlearning can solve the investigated regression and control tasks efficiently.", "AI": {"tldr": "Biomolecular Neural Networks (BNNs) can approximate universal functions and are trained effectively using Signal Temporal Logic (STL) specifications. Numerical experiments showcase success in regression and control tasks.", "motivation": "Training BNNs is challenging due to the lack of appropriate target data for their biological architecture. The aim is to explore alternative training approaches.", "method": "The authors utilize the quantitative semantics of Signal Temporal Logic (STL) for gradient-based optimization and propose a training algorithm for BNNs that handles regression and control tasks.", "result": "STL-based learning was successfully applied to train BNNs in regression tasks (as reporters of dysregulated states) and in a control task (mitigating inflammation in chronic disease while avoiding issues with infections).", "conclusion": "The proposed STL-based training enables BNNs to learn effectively for various tasks, proving their versatility and efficiency in solving biologically relevant problems."}}
{"id": "2509.06716", "pdf": "https://arxiv.org/pdf/2509.06716", "abs": "https://arxiv.org/abs/2509.06716", "authors": ["Th\u00e9o Matricon", "Mathieu Acher", "Helge Spieker", "Arnaud Gotlieb"], "title": "Efficiently Ranking Software Variants with Minimal Benchmarks", "categories": ["cs.SE", "cs.PF"], "comment": null, "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability.", "AI": {"tldr": "The paper introduces BISection Sampling (BISS), a novel method for reducing computational costs in software benchmarking by optimizing test suite instances while preserving ranking stability of software variants.", "motivation": "Benchmarking is computationally expensive and time-consuming, yet critical for evaluating software variants in terms of performance, regression detection, and system design. The authors aim to reduce this cost while preserving the clarity of results.", "method": "The BISection Sampling (BISS) strategy combines test suite optimization with a divide-and-conquer approach to selectively retain critical tests while removing others, ensuring that the rankings of software variants remain stable.", "result": "On datasets from various domains such as LLM leaderboards and SAT competitions, BISS reduced benchmark computational costs to an average of 44%, with up to 99% reduction in more than half of the benchmarks, without sacrificing ranking stability.", "conclusion": "The BISS method is an effective optimization technique for benchmarking, enabling significant computational cost savings while maintaining reliable variant rankings, and performing better than baseline methods."}}
{"id": "2509.06162", "pdf": "https://arxiv.org/pdf/2509.06162", "abs": "https://arxiv.org/abs/2509.06162", "authors": ["M. Rezaalipour", "F. Costa", "M. Biasion", "R. Otoni", "G. A. Constantinides", "L. Pozzi"], "title": "An Improved Template for Approximate Computing", "categories": ["cs.LG", "cs.AR", "B.2.1; B.6.3"], "comment": "4 pages, 5 figures", "summary": "Deploying neural networks on edge devices entails a careful balance between\nthe energy required for inference and the accuracy of the resulting\nclassification. One technique for navigating this tradeoff is approximate\ncomputing: the process of reducing energy consumption by slightly reducing the\naccuracy of arithmetic operators. In this context, we propose a methodology to\nreduce the area of the small arithmetic operators used in neural networks -\ni.e., adders and multipliers - via a small loss in accuracy, and show that we\nimprove area savings for the same accuracy loss w.r.t. the state of the art. To\nachieve our goal, we improve on a boolean rewriting technique recently\nproposed, called XPAT, where the use of a parametrisable template to rewrite\ncircuits has proved to be highly beneficial. In particular, XPAT was able to\nproduce smaller circuits than comparable approaches while utilising a naive sum\nof products template structure. In this work, we show that template parameters\ncan act as proxies for chosen metrics and we propose a novel template based on\nparametrisable product sharing that acts as a close proxy to synthesised area.\nWe demonstrate experimentally that our methodology converges better to low-area\nsolutions and that it can find better approximations than both the original\nXPAT and two other state-of-the-art approaches.", "AI": {"tldr": "The paper introduces a methodology enhancing small arithmetic operators for neural networks, optimizing for energy efficiency and requiring only a minimal loss in computation accuracy.", "motivation": "There is a pressing need to balance energy consumption and inference accuracy when deploying neural networks on edge devices. Approximate computing offers potential solutions but current approaches need refinement.", "method": "The authors leverage improvements on XPAT, a recent method, which uses parametrizable templates for Boolean rewriting. They propose a novel template emphasizing parametrizable product sharing as a proxy for synthesized area optimizations.", "result": "Experimental results show superior convergence to low-area solutions and better approximation performance compared to XPAT and other state-of-the-art methods.", "conclusion": "Using parametrisable templates is effective for optimizing small arithmetic operators in neural networks, offering improved area savings and comparable accuracy losses."}}
{"id": "2509.05462", "pdf": "https://arxiv.org/pdf/2509.05462", "abs": "https://arxiv.org/abs/2509.05462", "authors": ["Grigory Kondyrev", "David I. Spivak"], "title": "The compact double category $\\mathbf{Int}(\\mathbf{Poly}_*)$ models control flow and data transformations", "categories": ["math.CT", "cs.PL", "18M30, 18M10, 18M35, 18B20, 18M60", "D.1.7; D.3.3"], "comment": "28 pages including many diagrams", "summary": "Hasegawa showed that control flow in programming languages -- while loops and\nif-then-else statements -- can be modeled using traced cocartesian categories,\nsuch as the category $\\mathbf{Set}_*$ of pointed sets. In this paper we define\nan operad $\\mathscr{W}$ of wiring diagrams that provides syntax for categories\nwhose control flow moreover includes data transformations, including deleting,\nduplicating, permuting, and applying pre-specified functions to variables. In\nthe most basic version, the operad underlies $\\mathbf{Int}(\\mathbf{Poly}_*)$,\nwhere $\\mathbf{Int}(\\mathscr{T})$ denotes the free compact category on a traced\ncategory $\\mathscr{T}$, as defined by Joyal, Street, and Verity; to do so, we\nshow that $\\mathbf{Poly}_*$, as well as any multivariate version of it, is\ntraced. We show moreover that whenever $\\mathscr{T}$ is uniform -- a condition\nalso defined by Hasegawa and satisfied by $\\mathbf{Int}(\\mathscr{T})$ -- the\nresulting $\\mathbf{Int}$-construction extends to a double category\n$\\mathbb{I}\\mathbf{nt}(\\mathscr{T})$, which is compact in the sense of\nPatterson. Finally, we define a universal property of the double category\n$\\mathbb{I}\\mathbf{nt}(\\mathbf{Poly}_*)$ and\n$\\mathbb{I}\\mathbf{nt}(\\mathbf{Set}_*)$ by which one can track trajectories as\nthey move through the control flow associated to a wiring diagram.", "AI": {"tldr": "The paper constructs an operad for syntax accommodating control flow and data transformations in programming languages.", "motivation": "To enhance the modeling of control flow in programming by including data transformations in the framework defined by traced cocartesian categories.", "method": "The authors define an operad of wiring diagrams, demonstrate the use of traced properties in categories, and extend constructions to a compact double category.", "result": "Introduced the operad underlying categories like \\( \\mathbf{Int}(\\mathbf{Poly}_*) \\) and extended compact double categories to model control flow and data transformations.", "conclusion": "Compact double categories enable tracking trajectories through control flow, providing a universal property for modeling transformations and wiring diagrams."}}
{"id": "2509.06514", "pdf": "https://arxiv.org/pdf/2509.06514", "abs": "https://arxiv.org/abs/2509.06514", "authors": ["Mpoki Mwaisela", "Peterson Yuhala", "Pascal Felber", "Valerio Schiavoni"], "title": "IM-PIR: In-Memory Private Information Retrieval", "categories": ["cs.DC"], "comment": null, "summary": "Private information retrieval (PIR) is a cryptographic primitive that allows\na client to securely query one or multiple servers without revealing their\nspecific interests. In spite of their strong security guarantees, current PIR\nconstructions are computationally costly. Specifically, most PIR\nimplementations are memory-bound due to the need to scan extensive databases\n(in the order of GB), making them inherently constrained by the limited memory\nbandwidth in traditional processor-centric computing\narchitectures.Processing-in-memory (PIM) is an emerging computing paradigm that\naugments memory with compute capabilities, addressing the memory bandwidth\nbottleneck while simultaneously providing extensive parallelism.Recent research\nhas demonstrated PIM's potential to significantly improve performance across a\nrange of data-intensive workloads, including graph processing, genome analysis,\nand machine learning.\n  In this work, we propose the first PIM-based architecture for multi-server\nPIR. We discuss the algorithmic foundations of the latter and show how its\noperations align with the core strengths of PIM architectures: extensive\nparallelism and high memory bandwidth. Based on this observation, we design and\nimplement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,\nthe first openly commercialized PIM architecture. Our evaluation demonstrates\nthat a PIM-based multi-server PIR implementation significantly improves query\nthroughput by more than 3.7x when compared to a standard CPU-based PIR\napproach.", "AI": {"tldr": "This paper introduces the IM-PIR architecture, a PIM-based approach to multi-server PIR, achieving a 3.7x query throughput improvement over conventional CPU-based PIR.", "motivation": "Current PIR systems suffer from high computational costs and memory bandwidth limitations, which restrict their efficiency in managing large databases.", "method": "The authors leverage UPMEM, a commercial PIM architecture, to design and implement IM-PIR by exploiting PIM's high parallelism and memory bandwidth.", "result": "IM-PIR demonstrated query throughput improvements of over 3.7 times compared to traditional CPU-based PIR implementations.", "conclusion": "The integration of PIM into PIR systems significantly enhances performance and showcases the potential of PIM for cryptographic applications."}}
{"id": "2509.05391", "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Sch\u00fcppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "categories": ["cs.RO", "cs.HC", "cs.MM"], "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks.", "AI": {"tldr": "The paper evaluates tracking performance of Magic Leap 2 controllers using a robotic arm and optical systems to establish benchmarks.", "motivation": "Public benchmarks for tool tracking with AR on modern Head-Mounted Displays are inadequate, necessitating rigorous evaluation.", "method": "A robotic arm executes repeatable motion paired with optical tracking to measure static and dynamic controller performance under diverse conditions.", "result": "Quantitative baselines for the Magic Leap 2 controller's accuracy and repeatability are established.", "conclusion": "The evaluation method is robust and transferable, providing insights into the controllers' suitability for industrial AR-guidance tasks."}}
{"id": "2509.05486", "pdf": "https://arxiv.org/pdf/2509.05486", "abs": "https://arxiv.org/abs/2509.05486", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).", "AI": {"tldr": "The study explores tokenization challenges in African languages, highlighting that higher token inflation leads to reduced model accuracy and increased computational costs, while recommending morphologically aware tokenization for fairness.", "motivation": "The paper seeks to address the structural disadvantages faced by morphologically complex, low-resource languages due to tokenization inefficiency, which impacts computational resource demands and model performance.", "method": "The authors evaluated 10 large language models on the AfriMMLU dataset, which includes 9,000 multiple-choice questions spanning five subjects in 16 African languages.", "result": "The study found that token fertility (tokens/word) negatively correlates with model accuracy and increases training costs, while reasoning models performed better across languages compared to non-reasoning ones.", "conclusion": "The findings advocate for morphologically aware tokenization, fair pricing strategies, and multilingual benchmarks to ensure equitable advances in NLP for diverse languages."}}
{"id": "2509.05881", "pdf": "https://arxiv.org/pdf/2509.05881", "abs": "https://arxiv.org/abs/2509.05881", "authors": ["Qianheng Zhang", "Song Gao", "Chen Wei", "Yibo Zhao", "Ying Nie", "Ziru Chen", "Shijie Chen", "Yu Su", "Huan Sun"], "title": "GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation", "categories": ["cs.SE", "cs.AI", "I.2"], "comment": "34 pages, 8 figures", "summary": "Recent advances in large language models (LLMs) have fueled growing interest\nin automating geospatial analysis and GIS workflows, yet their actual\ncapabilities remain uncertain. In this work, we call for rigorous evaluation of\nLLMs on well-defined geoprocessing tasks before making claims about full GIS\nautomation. To this end, we present GeoAnalystBench, a benchmark of 50\nPython-based tasks derived from real-world geospatial problems and carefully\nvalidated by GIS experts. Each task is paired with a minimum deliverable\nproduct, and evaluation covers workflow validity, structural alignment,\nsemantic similarity, and code quality (CodeBLEU). Using this benchmark, we\nassess both proprietary and open source models. Results reveal a clear gap:\nproprietary models such as ChatGPT-4o-mini achieve high validity 95% and\nstronger code alignment (CodeBLEU 0.39), while smaller open source models like\nDeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%\nvalidity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as\nspatial relationship detection or optimal site selection, remain the most\nchallenging across all models. These findings demonstrate both the promise and\nlimitations of current LLMs in GIS automation and provide a reproducible\nframework to advance GeoAI research with human-in-the-loop support.", "AI": {"tldr": "The paper introduces GeoAnalystBench to rigorously evaluate large language models (LLMs) for geospatial tasks, finding proprietary models outperform open-source ones while highlighting challenges in complex spatial reasoning.", "motivation": "The paper aims to address uncertainty in the capabilities of LLMs for automating GIS workflows by providing rigorous evaluation before assuming they can fully automate tasks.", "method": "The authors created GeoAnalystBench, a benchmark consisting of 50 Python geospatial tasks validated by GIS experts. Tasks are evaluated based on workflow validity, structural alignment, semantic similarity, and code quality using metrics like CodeBLEU.", "result": "Proprietary models like ChatGPT-4o-mini showed stronger performance with 95% validity and CodeBLEU of 0.39, whereas open source models, such as DeepSeek-R1-7B, often produced incomplete workflows, achieving only 48.5% validity and 0.272 CodeBLEU.", "conclusion": "While current LLMs exhibit promise for GIS automation, they have significant limitations in tasks involving advanced spatial reasoning. The GeoAnalystBench offers a reproducible evaluation framework to guide future GeoAI improvements."}}
{"id": "2509.06303", "pdf": "https://arxiv.org/pdf/2509.06303", "abs": "https://arxiv.org/abs/2509.06303", "authors": ["Yingying Fan", "Jingyuan Liu", "Jinchi Lv", "Ao Sun"], "title": "MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "110 pages, 4 figures", "summary": "We propose a new inference framework, named MOSAIC, for change-point\ndetection in dynamic networks with the simultaneous low-rank and sparse-change\nstructure. We establish the minimax rate of detection boundary, which relies on\nthe sparsity of changes. We then develop an eigen-decomposition-based test with\nscreened signals that approaches the minimax rate in theory, with only a minor\nlogarithmic loss. For practical implementation of MOSAIC, we adjust the\ntheoretical test by a novel residual-based technique, resulting in a pivotal\nstatistic that converges to a standard normal distribution via the martingale\ncentral limit theorem under the null hypothesis and achieves full power under\nthe alternative hypothesis. We also analyze the minimax rate of testing\nboundary for dynamic networks without the low-rank structure, which almost\naligns with the results in high-dimensional mean-vector change-point inference.\nWe showcase the effectiveness of MOSAIC and verify our theoretical results with\nseveral simulation examples and a real data application.", "AI": {"tldr": "MOSAIC is a new inference framework designed for change-point detection in dynamic networks, incorporating low-rank and sparse change structures. It achieves theoretical optimality in detection rates and practical robustness via residual-based techniques.", "motivation": "The paper aims to address challenges in detecting change-points in dynamic networks, specifically with low-rank and sparse-change structures, which are crucial in understanding transitions or anomalies in network dynamics.", "method": "MOSAIC employs eigen-decomposition-based testing combined with screened signals to approach theoretical minimax detection rates. It further uses a residual-based adjustment to make the test statistic conform to standard normal distribution for practical application.", "result": "MOSAIC achieves full theoretical power under the alternative hypothesis and validates its framework and theoretical results with simulations and real data applications.", "conclusion": "The proposed MOSAIC framework effectively balances theoretical rigor and practical utility, making it a robust tool for change-point inference in dynamic networks with structured sparsity."}}
{"id": "2509.05378", "pdf": "https://arxiv.org/pdf/2509.05378", "abs": "https://arxiv.org/abs/2509.05378", "authors": ["Andreas Motzfeldt", "Joakim Edin", "Casper L. Christensen", "Christian Hardmeier", "Lars Maal\u00f8e", "Anna Rogers"], "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding", "categories": ["cs.AI", "cs.MA"], "comment": "EMNLP Findings 2025", "summary": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).", "AI": {"tldr": "The study introduces a framework called \"Code Like Humans\" using large language models for medical coding, supporting all ICD-10 codes and improving performance on rare diagnoses.", "motivation": "Inaccuracies and limitations in existing medical coding methods, particularly with handling a high number of labels and rare diagnoses.", "method": "The framework uses large language models and aligns with official coding guidelines to map clinical notes to the full ICD-10 coding system (over 70K labels).", "result": "Improved performance on rare diagnosis codes, while fine-tuned classifiers maintain an advantage on high-frequency codes.", "conclusion": "The framework enhances medical coding capabilities and highlights system limitations, providing directions for addressing blind spots in coding performance."}}
{"id": "2509.05334", "pdf": "https://arxiv.org/pdf/2509.05334", "abs": "https://arxiv.org/abs/2509.05334", "authors": ["Diwen Huang"], "title": "A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices", "categories": ["cs.CV", "cs.MM", "H.5.1; I.2.10"], "comment": "6 pages, 3 figures, 1 table. Independent research preprint", "summary": "Performance metrics in sports, such as shot speed and angle, provide crucial\nfeedback for athlete development. However, the technology to capture these\nmetrics has historically been expensive, complex, and largely inaccessible to\namateur and recreational players. This paper addresses this gap in the context\nof badminton, one of the world's most popular sports, by introducing a novel,\ncost-effective, and user-friendly system for measuring smash speed using\nubiquitous smartphone technology. Our approach leverages a custom-trained\nYOLOv5 model for shuttlecock detection, combined with a Kalman filter for\nrobust trajectory tracking. By implementing a video-based kinematic speed\nestimation method with spatiotemporal scaling, the system automatically\ncalculates the shuttlecock's velocity from a standard video recording. The\nentire process is packaged into an intuitive mobile application, democratizing\naccess to high-level performance analytics and empowering players at all levels\nto analyze and improve their game.", "AI": {"tldr": "The paper introduces a cost-effective badminton smash speed measurement system using smartphone technology, employing YOLOv5 for detection and a Kalman filter for tracking.", "motivation": "Current methods for measuring sports metrics like shot speed and angle are expensive and inaccessible to amateur players.", "method": "The system uses a YOLOv5 model for shuttlecock detection, a Kalman filter for trajectory tracking, and a video-based kinematic speed estimation method packaged into a mobile app.", "result": "The system allows players to measure smash speed by processing video recordings with spatiotemporal scaling.", "conclusion": "This innovative solution democratizes access to performance analysis tools, enabling players to enhance their badminton gameplay."}}
{"id": "2509.05485", "pdf": "https://arxiv.org/pdf/2509.05485", "abs": "https://arxiv.org/abs/2509.05485", "authors": ["Maksim Kazanskii", "Artem Kasianov"], "title": "Prior Distribution and Model Confidence", "categories": ["cs.LG"], "comment": "10 pages,4 tables, 5 images", "summary": "This paper investigates the impact of training data distribution on the\nperformance of image classification models. By analyzing the embeddings of the\ntraining set, we propose a framework to understand the confidence of model\npredictions on unseen data without the need for retraining. Our approach\nfilters out low-confidence predictions based on their distance from the\ntraining distribution in the embedding space, significantly improving\nclassification accuracy. We demonstrate this on the example of several\nclassification models, showing consistent performance gains across\narchitectures. Furthermore, we show that using multiple embedding models to\nrepresent the training data enables a more robust estimation of confidence, as\ndifferent embeddings capture complementary aspects of the data. Combining these\nembeddings allows for better detection and exclusion of out-of-distribution\nsamples, resulting in further accuracy improvements. The proposed method is\nmodel-agnostic and generalizable, with potential applications beyond computer\nvision, including domains such as Natural Language Processing where prediction\nreliability is critical.", "AI": {"tldr": "This paper proposes a framework to improve image classification accuracy by analyzing training data embeddings to filter out low-confidence predictions, demonstrating consistent gains and potential cross-domain utility.", "motivation": "To investigate how training data distribution affects model performance, and to improve the reliability of predictions by assessing confidence without retraining.", "method": "Analyzing training set embeddings to estimate prediction confidence, filtering low-confidence predictions using their distances from the training distribution, and leveraging multiple embedding models for robust confidence estimation.", "result": "The method significantly improves classification accuracy, consistently across different model architectures, by filtering low-confidence or out-of-distribution samples.", "conclusion": "The framework is model-agnostic, generalizable, and useful across domains like Natural Language Processing, where ensuring prediction reliability is crucial."}}
{"id": "2509.06289", "pdf": "https://arxiv.org/pdf/2509.06289", "abs": "https://arxiv.org/abs/2509.06289", "authors": ["Shaoqi Wei", "Senling Wang", "Hiroshi Kai", "Yoshinobu Higami", "Ruijun Ma", "Tianming Ni", "Xiaoqing Wen", "Hiroshi Takahashi"], "title": "A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults", "categories": ["cs.LG", "cs.AR", "cs.ET", "B.7.3"], "comment": "21 pages, 9 figures, plan to submit to ACM TODAES", "summary": "Silent Data Errors (SDEs) from time-zero defects and aging degrade\nsafety-critical systems. Functional testing detects SDE-related faults but is\nexpensive to simulate. We present a unified spatio-temporal graph convolutional\nnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impact\nprobabilities (FIPs) in large sequential circuits, supporting quantitative risk\nassessment. Gate-level netlists are modeled as spatio-temporal graphs to\ncapture topology and signal timing; dedicated spatial and temporal encoders\npredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method\nreduces simulation time by more than 10x while maintaining high accuracy (mean\nabsolute error 0.024 for 5-cycle predictions). The framework accepts features\nfrom testability metrics or fault simulation, allowing efficiency-accuracy\ntrade-offs. A test-point selection study shows that choosing observation points\nby predicted FIPs improves detection of long-cycle, hard-to-detect faults. The\napproach scales to SoC-level test strategy optimization and fits downstream\nelectronic design automation flows.", "AI": {"tldr": "This paper introduces a spatio-temporal graph convolutional network (ST-GCN) to quickly and accurately predict long-cycle fault impact probabilities (FIPs) in sequential circuits, reducing simulation time by over 10x while maintaining high accuracy.", "motivation": "To address the challenges of detecting long-cycle Silent Data Errors (SDEs) in safety-critical systems, which are expensive to simulate with traditional functional testing.", "method": "Gate-level netlists are represented as spatio-temporal graphs. The proposed ST-GCN uses dedicated spatial and temporal encoders to predict multi-cycle FIPs efficiently and allows flexibility in using different input features, enabling trade-offs between efficiency and accuracy.", "result": "The method demonstrates a mean absolute error of 0.024 for 5-cycle predictions and significantly reduces simulation time by over 10x. It also enhances the detection of hard-to-detect faults when observation points are selected based on predicted FIPs.", "conclusion": "The ST-GCN framework is efficient and scalable, fitting seamlessly into electronic design automation workflows and promising utility for SoC-level test strategy optimization."}}
{"id": "2509.06616", "pdf": "https://arxiv.org/pdf/2509.06616", "abs": "https://arxiv.org/abs/2509.06616", "authors": ["Anton Paramonov", "Yann Vonlanthen", "Quentin Kniep", "Jakub Sliwinski", "Roger Wattenhofer"], "title": "Mangrove: Fast and Parallelizable State Replication for Blockchains", "categories": ["cs.DC"], "comment": null, "summary": "Mangrove is a novel scaling approach to building blockchains with parallel\nsmart contract support. Unlike in monolithic blockchains, where a single\nconsensus mechanism determines a strict total order over all transactions,\nMangrove uses separate consensus instances per smart contract, without a global\norder. To allow multiple instances to run in parallel while ensuring that no\nconflicting transactions are committed, we propose a mechanism called Parallel\nOptimistic Agreement. Additionally, for simple transactions, we leverage a\nlightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove\nis optimized for performance under optimistic conditions, where there is no\nmisbehavior and the network is synchronous. Under these conditions, our\nprotocol can achieve a latency of 2 communication steps between creating and\nexecuting a transaction.", "AI": {"tldr": "Mangrove introduces a new way of scaling blockchains using parallel smart contract support by deploying separate consensus instances per contract without a global transaction order.", "motivation": "Scaling blockchains to support parallel execution of smart contracts efficiently while addressing challenges of performance under optimistic conditions such as network synchrony.", "method": "Mangrove employs a mechanism called Parallel Optimistic Agreement for parallel execution and uses Byzantine Reliable Broadcast for simpler transactions to enhance efficiency and reduce latency.", "result": "Achieves a latency of two communication steps between transaction creation and execution in optimal conditions.", "conclusion": "Mangrove enables blockchain scalability by optimizing for performance under favorable conditions, without relying on a strict total order of transactions across contracts."}}
{"id": "2509.05397", "pdf": "https://arxiv.org/pdf/2509.05397", "abs": "https://arxiv.org/abs/2509.05397", "authors": ["Matthew Lai", "Keegan Go", "Zhibin Li", "Torsten Kroger", "Stefan Schaal", "Kelsey Allen", "Jonathan Scholz"], "title": "RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "Published in Science Robotics", "summary": "Modern robotic manufacturing requires collision-free coordination of multiple\nrobots to complete numerous tasks in shared, obstacle-rich workspaces. Although\nindividual tasks may be simple in isolation, automated joint task allocation,\nscheduling, and motion planning under spatio-temporal constraints remain\ncomputationally intractable for classical methods at real-world scales.\nExisting multi-arm systems deployed in the industry rely on human intuition and\nexperience to design feasible trajectories manually in a labor-intensive\nprocess. To address this challenge, we propose a reinforcement learning (RL)\nframework to achieve automated task and motion planning, tested in an\nobstacle-rich environment with eight robots performing 40 reaching tasks in a\nshared workspace, where any robot can perform any task in any order. Our\napproach builds on a graph neural network (GNN) policy trained via RL on\nprocedurally-generated environments with diverse obstacle layouts, robot\nconfigurations, and task distributions. It employs a graph representation of\nscenes and a graph policy neural network trained through reinforcement learning\nto generate trajectories of multiple robots, jointly solving the sub-problems\nof task allocation, scheduling, and motion planning. Trained on large randomly\ngenerated task sets in simulation, our policy generalizes zero-shot to unseen\nsettings with varying robot placements, obstacle geometries, and task poses. We\nfurther demonstrate that the high-speed capability of our solution enables its\nuse in workcell layout optimization, improving solution times. The speed and\nscalability of our planner also open the door to new capabilities such as\nfault-tolerant planning and online perception-based re-planning, where rapid\nadaptation to dynamic task sets is required.", "AI": {"tldr": "The paper proposes a graph neural network-based reinforcement learning framework for multi-robot task allocation, scheduling, and motion planning in obstacle-rich environments.", "motivation": "To address the inefficiency and labor-intensive nature of manually planning multi-robot trajectories in industrial applications.", "method": "The approach utilizes graph representations of environments and a graph policy neural network trained via reinforcement learning on procedurally generated environments to automate task allocation, scheduling, and motion planning.", "result": "The framework demonstrates high-speed, zero-shot generalization to unseen scenarios with varying parameters and showcases its application in workcell layout optimization, fault-tolerant planning, and online re-planning.", "conclusion": "The speed and scalability of the proposed method enable efficient and adaptable multi-robot planning, paving the way for automated solutions in dynamic industrial environments."}}
{"id": "2509.05505", "pdf": "https://arxiv.org/pdf/2509.05505", "abs": "https://arxiv.org/abs/2509.05505", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.", "AI": {"tldr": "The paper presents a Q&A system using Retrieval-Augmented Generation (RAG) for biomedical literature, improving access to evidence-based and accurate medical information.", "motivation": "To address the limitations of existing health search engines and the delay in public access to biomedical research by providing concise, accurate, and context-aware medical information.", "method": "The system employs MiniLM-based semantic embeddings and FAISS vector search for information retrieval and uses a fine-tuned Mistral-7B-v0.3 (optimized with QLoRA) for answer generation, validating its effectiveness on general medical queries and domain-specific tasks like breast cancer literature.", "result": "Experiments using BERTScore (F1) indicate significant improvements in factual and semantic relevance compared to baseline models.", "conclusion": "The RAG-based system effectively bridges the gap between complex biomedical literature and public understanding, with potential for multilingual, private, and personalized AI adaptations in the future."}}
{"id": "2509.05941", "pdf": "https://arxiv.org/pdf/2509.05941", "abs": "https://arxiv.org/abs/2509.05941", "authors": ["Chaoqian Ouyang", "Ling Yue", "Shimin Di", "Libin Zheng", "Shaowu Pan", "Min-Ling Zhang"], "title": "Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has created a significant\nintegration challenge in the AI agent ecosystem, often called the \"$N \\times M$\nproblem,\" where N models require custom integrations for M tools. This\nfragmentation stifles innovation and creates substantial development overhead.\nWhile the Model Context Protocol (MCP) has emerged as a standard to resolve\nthis, its adoption is hindered by the manual effort required to convert the\nvast universe of existing software into MCP-compliant services. This is\nespecially true for the millions of open-source repositories on GitHub, the\nworld's largest collection of functional code. This paper introduces Code2MCP,\na highly automated, agentic framework designed to transform any GitHub\nrepository into a functional MCP service with minimal human intervention. Our\nsystem employs a multi-stage workflow that automates the entire process, from\ncode analysis and environment configuration to service generation and\ndeployment. A key innovation of our framework is an LLM-driven, closed-loop\n\"Run--Review--Fix\" cycle, which enables the system to autonomously debug and\nrepair the code it generates. Code2MCP produces not only deployable services\nbut also comprehensive technical documentation, acting as a catalyst to\naccelerate the MCP ecosystem by systematically unlocking the world's largest\nopen-source code repository and automating the critical last mile of tool\nintegration. The code is open-sourced at\nhttps://github.com/DEFENSE-SEU/MCP-Github-Agent.", "AI": {"tldr": "Code2MCP automates transforming GitHub repositories into MCP-compliant services, addressing the integration bottleneck in the AI agent ecosystem.", "motivation": "The paper addresses the integration complexity in the AI ecosystem, where many AI models must be manually integrated with existing tools, leading to significant development overhead.", "method": "The proposed solution, Code2MCP, uses an automated framework driven by a multi-stage process and a closed-loop LLM-based cycle to analyze, debug, and deploy services from GitHub repositories.", "result": "Code2MCP successfully automates the integration process, producing functional MCP services, debugged code, and comprehensive documentation with minimal human intervention.", "conclusion": "This framework systematically unlocks the potential of open-source code repositories, advancing the MCP ecosystem and easing the tool integration process for large language models."}}
{"id": "2509.06308", "pdf": "https://arxiv.org/pdf/2509.06308", "abs": "https://arxiv.org/abs/2509.06308", "authors": ["Seung Hyun Moon"], "title": "Minimax optimal transfer learning for high-dimensional additive regression", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "This is a draft version of the paper. All responsibilities are\n  assigned to the first author", "summary": "This paper studies high-dimensional additive regression under the transfer\nlearning framework, where one observes samples from a target population\ntogether with auxiliary samples from different but potentially related\nregression models. We first introduce a target-only estimation procedure based\non the smooth backfitting estimator with local linear smoothing. In contrast to\nprevious work, we establish general error bounds under sub-Weibull($\\alpha$)\nnoise, thereby accommodating heavy-tailed error distributions. In the\nsub-exponential case ($\\alpha=1$), we show that the estimator attains the\nminimax lower bound under regularity conditions, which requires a substantial\ndeparture from existing proof strategies. We then develop a novel two-stage\nestimation method within a transfer learning framework, and provide theoretical\nguarantees at both the population and empirical levels. Error bounds are\nderived for each stage under general tail conditions, and we further\ndemonstrate that the minimax optimal rate is achieved when the auxiliary and\ntarget distributions are sufficiently close. All theoretical results are\nsupported by simulation studies and real data analysis.", "AI": {"tldr": "This paper develops methods for high-dimensional additive regression under a transfer learning framework, introducing novel estimators and deriving error bounds for heavy-tailed noise and transfer learning cases.", "motivation": "To address challenges in high-dimensional additive regression, especially under transfer learning scenarios with heavy-tailed errors and related populations.", "method": "The paper proposes a target-only estimation procedure using smooth backfitting with local linear smoothing and a two-stage estimation method in the transfer learning framework, accompanied by theoretical guarantees and error analyses.", "result": "Error bounds are derived for both methods under sub-Weibull noise conditions, and minimax optimal rates are achieved when auxiliary and target distributions are sufficiently related.", "conclusion": "The results advance the robustness and theoretical understanding of additive regression, offering practical insights through simulations and real data analyses."}}
{"id": "2509.05381", "pdf": "https://arxiv.org/pdf/2509.05381", "abs": "https://arxiv.org/abs/2509.05381", "authors": ["Madhava Gaikwad"], "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins", "categories": ["cs.AI", "cs.LG", "68T01, 68T20, 68Q87"], "comment": "21 pages", "summary": "Large language models are increasingly aligned to human preferences through\nreinforcement learning from human feedback (RLHF) and related methods such as\nDirect Preference Optimization (DPO), Constitutional AI, and RLAIF. While\neffective, these methods exhibit recurring failure patterns i.e., reward\nhacking, sycophancy, annotator drift, and misgeneralization. We introduce the\nconcept of the Alignment Gap, a unifying lens for understanding recurring\nfailures in feedback-based alignment. Using a KL-tilting formalism, we\nillustrate why optimization pressure tends to amplify divergence between proxy\nrewards and true human intent. We organize these failures into a catalogue of\nMurphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to\nframe trade-offs among optimization strength, value capture, and\ngeneralization. Small-scale empirical studies serve as illustrative support.\nFinally, we propose the MAPS framework (Misspecification, Annotation, Pressure,\nShift) as practical design levers. Our contribution is not a definitive\nimpossibility theorem but a perspective that reframes alignment debates around\nstructural limits and trade-offs, offering clearer guidance for future design.", "AI": {"tldr": "This paper explores why alignment failures occur in language models trained with human feedback, proposing new frameworks and categorizations to address them.", "motivation": "There are recurring failure modes in language models aligned using RLHF and similar methods, such as reward hacking and misgeneralization, which necessitate a deeper understanding and refined approach.", "method": "The paper introduces the concept of an 'Alignment Gap,' uses KL-tilting formalism to analyze failure dynamics, categorizes failures as Murphy's Laws of AI Alignment, frames trade-offs with the 'Alignment Trilemma,' and proposes the MAPS framework for addressing them.", "result": "Empirical studies support the ideas, with the MAPS framework and alignment concepts providing new perspectives for addressing alignment challenges.", "conclusion": "The study reframes alignment debates by focusing on structural limits and trade-offs, offering practical frameworks and insights for improving language model alignment."}}
{"id": "2509.05335", "pdf": "https://arxiv.org/pdf/2509.05335", "abs": "https://arxiv.org/abs/2509.05335", "authors": ["Zebo Xu", "Shaoyun Yu", "Mark Torrance", "Guido Nottbusch", "Nan Zhao", "Zhenguang Cai"], "title": "A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research", "categories": ["cs.CV"], "comment": null, "summary": "Understanding what linguistic components (e.g., phonological, semantic, and\northographic systems) modulate Chinese handwriting at the character, radical,\nand stroke levels remains an important yet understudied topic. Additionally,\nthere is a lack of comprehensive tools for capturing and batch-processing\nfine-grained handwriting data. To address these issues, we constructed a\nlarge-scale handwriting database in which 42 Chinese speakers for each\nhandwriting 1200 characters in a handwriting-to-dictation task. Additionally,\nwe enhanced the existing handwriting package and provided comprehensive\ndocumentation for the upgraded OpenHandWrite_Toolbox, which can easily modify\nthe experimental design, capture the stroke-level handwriting trajectory, and\nbatch-process handwriting measurements (e.g., latency, duration, and\npen-pressure). In analysing our large-scale database, multiple regression\nresults show that orthographic predictors impact handwriting preparation and\nexecution across character, radical, and stroke levels. Phonological factors\nalso influence execution at all three levels. Importantly, these lexical\neffects demonstrate hierarchical attenuation - they were most pronounced at the\ncharacter level, followed by the radical, and were weakest at the stroke\nlevels. These findings demonstrate that handwriting preparation and execution\nat the radical and stroke levels are closely intertwined with linguistic\ncomponents. This database and toolbox offer valuable resources for future\npsycholinguistic and neurolinguistic research on the handwriting of characters\nand sub-characters across different languages.", "AI": {"tldr": "This paper explores how linguistic components like phonological, semantic, and orthographic systems influence Chinese handwriting across character, radical, and stroke levels. It introduces a large-scale handwriting database and upgraded tools for detailed handwriting analysis.", "motivation": "To understand the interaction between linguistic components and Chinese handwriting at distinct hierarchical levels, and to address the absence of tools for processing detailed handwriting data.", "method": "The authors created a handwriting database with 42 speakers writing 1200 characters each, used a handwriting-to-dictation task, upgraded the OpenHandWrite_Toolbox for batch processing, and analyzed data using multiple regression methods.", "result": "The study found that orthographic predictors affected handwriting preparation and execution across all levels, while phonological factors influenced execution across all levels. Lexical effects showed hierarchical attenuation, decreasing from character to radical to stroke levels.", "conclusion": "Handwriting at radical and stroke levels is strongly linked to linguistic components, and both the database and toolbox provide significant resources for advancing psycholinguistic and neurolinguistic handwriting research."}}
{"id": "2509.05488", "pdf": "https://arxiv.org/pdf/2509.05488", "abs": "https://arxiv.org/abs/2509.05488", "authors": ["Hongjun Xu", "Junxi Xia", "Weisi Yang", "Yueyuan Sui", "Stephen Xia"], "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs", "categories": ["cs.LG", "cs.AI", "cs.OS", "C.3; I.2.6; D.2.13; D.4.7"], "comment": "4 pages, 1 figures", "summary": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to\nlimited memory, the lack of native operator support, and the absence of\nembedded-friendly toolchains. We present, to our knowledge, the first\ndeployment of a Mamba-based neural architecture on a resource-constrained MCU,\na fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline\nmaps a trained PyTorch Mamba model to on-device execution by (1) exporting\nmodel weights into a lightweight format, and (2) implementing a handcrafted\nMamba layer and supporting operators in C with operator fusion and memory\nlayout optimization. MambaLite-Micro eliminates large intermediate tensors,\nreducing 83.0% peak memory, while maintaining an average numerical error of\nonly 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on\nkeyword spotting(KWS) and human activity recognition (HAR) tasks,\nMambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully\npreserving classification accuracy. We further validated portability by\ndeploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating\nconsistent operation across heterogeneous embedded platforms and paving the way\nfor bringing advanced sequence models like Mamba to real-world\nresource-constrained applications.", "AI": {"tldr": "MambaLite-Micro is a runtime-free inference engine designed to deploy Mamba models on microcontrollers (MCUs) by optimizing memory usage and maintaining high accuracy.", "motivation": "The paper aims to address challenges in deploying Mamba models on MCUs, such as limited memory, lack of native operator support, and embedded-friendly toolchains, to enable resource-constrained applications.", "method": "The authors developed a pipeline that maps trained PyTorch Mamba models to MCUs by exporting model weights into a lightweight format and implementing optimized handcrafted Mamba layers and operators in C, with operator fusion and memory layout optimization.", "result": "MambaLite-Micro reduced peak memory usage by 83.0%, maintained a minimal numerical error of 1.7x10-5, and preserved classification accuracy for KWS and HAR tasks while achieving consistent operation on both ESP32S3 and STM32H7 microcontrollers.", "conclusion": "The study successfully demonstrated the deployment of Mamba-based neural architectures on MCUs, ensuring portability across heterogeneous platforms and enabling advanced sequence models in real-world, resource-limited environments."}}
{"id": "2509.06690", "pdf": "https://arxiv.org/pdf/2509.06690", "abs": "https://arxiv.org/abs/2509.06690", "authors": ["Usman Haider", "Lukasz Szemet", "Daniel Kelly", "Vasileios Sergis", "Andrew C. Daly", "Karl Mason"], "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring", "categories": ["cs.CV", "cs.AI", "cs.AR", "N/A", "I.2.9; I.2.10; I.4.6"], "comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes\n  dataset description, BioLite U-Net architecture, benchmark results on edge\n  device (Raspberry Pi 4B)", "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.", "AI": {"tldr": "This paper introduces BioLite U-Net, a lightweight semantic segmentation framework optimized for real-time bioprinting, achieving high accuracy and fast processing on resource-constrained hardware.", "motivation": "To address challenges in ensuring real-time fidelity and consistency during the bioprinting process, under constraints of limited imaging data and resource-constrained embedded hardware.", "method": "The researchers developed a manually annotated dataset of 787 RGB images and proposed the BioLite U-Net architecture, employing depthwise separable convolutions for efficient, low-computation semantic segmentation. Comparative benchmarks were performed against MobileNet-based models.", "result": "BioLite U-Net achieved an mIoU of 92.85%, Dice score of 96.17%, and on-device inference time of 335 ms per frame on a Raspberry Pi 4B, making it 1300x smaller than MobileNetV2-DeepLabV3+.", "conclusion": "The BioLite U-Net framework successfully balances segmentation accuracy, computational efficiency, and deployability, making it highly viable for integration into real-time, intelligent bioprinting systems."}}
{"id": "2509.05433", "pdf": "https://arxiv.org/pdf/2509.05433", "abs": "https://arxiv.org/abs/2509.05433", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering", "categories": ["cs.RO"], "comment": "20 pages, 5 figures", "summary": "Haptic interfaces that can simultaneously modulate multiple physical\nproperties remain a fundamental challenge in human-robot interaction. Existing\nsystems typically allow the rendering of either geometric features or\nmechanical properties, but rarely both, within wearable form factors. Here, we\nintroduce HapMorph, a pneumatic framework that enables continuous, simultaneous\nmodulation of object size and stiffness through antagonistic fabric-based\npneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for\nhands interaction achieving size variation from 50 to 104 mm, stiffness\nmodulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through\nsystematic characterization, we demonstrate decoupled control of size and\nstiffness properties via dual-chamber pressure regulation. Human perception\nstudies with 10 participants reveal that users can distinguish nine discrete\nstates across three size categories and three stiffness levels with 89.4%\naccuracy and 6.7 s average response time. We further demonstrate extended\narchitectures that combine AFPAs with complementary pneumatic structures to\nenable shape or geometry morphing with concurrent stiffness control. Our\nresults establish antagonistic pneumatic principle as a pathway toward\nnext-generation haptic interfaces, capable of multi-dimensiona rendering\nproperties within practical wearable constraints.", "AI": {"tldr": "HapMorph introduces a pneumatic framework enabling wearable haptic interfaces to simultaneously modify size and stiffness.", "motivation": "Existing wearable haptic systems struggle to concurrently render geometric and mechanical properties.", "method": "Antagonistic fabric-based pneumatic actuators (AFPAs) with dual-chamber pressure regulation offer decoupled control of size and stiffness.", "result": "HapMorph prototype achieves size variation (50-104 mm), stiffness modulation (up to 4.7 N/mm), and lightweight design (21 g). Tests show users can effectively distinguish nine states with high accuracy (89.4%) and quick response (6.7 s).", "conclusion": "The study establishes AFPAs as a route toward advanced wearable haptic interfaces with multidimensional rendering capabilities."}}
{"id": "2509.05553", "pdf": "https://arxiv.org/pdf/2509.05553", "abs": "https://arxiv.org/abs/2509.05553", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline B\u00e9n\u00e9dicte Moumoula", "Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "AI": {"tldr": "This paper explores whether large language models genuinely understand concepts or merely recognize patterns, introducing bidirectional reasoning as a test for true comprehension.", "motivation": "The study seeks to determine if large language models can achieve genuine understanding by introducing bidirectional reasoning as a criterion, especially focusing on reversibility of learned transformations.", "method": "The researchers propose Contrastive Fine-Tuning (CFT), which uses three types of training examples to instill bidirectional reasoning: positive semantic examples, negative semantic examples, and examples designed to test reverse obfuscation.", "result": "Contrastive Fine-Tuning (CFT) enabled models to achieve bidirectional reasoning, maintaining strong performance on forward tasks while also excelling at reverse reasoning tasks.", "conclusion": "Bidirectional reasoning provides both a framework for evaluating genuine understanding and a practical training method to develop more capable AI systems."}}
{"id": "2509.05980", "pdf": "https://arxiv.org/pdf/2509.05980", "abs": "https://arxiv.org/abs/2509.05980", "authors": ["Xingliang Wang", "Baoyi Wang", "Chen Zhi", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion", "categories": ["cs.SE"], "comment": null, "summary": "LLMs excel in localized code completion but struggle with repository-level\ntasks due to limited context windows and complex semantic and structural\ndependencies across codebases. While Retrieval-Augmented Generation (RAG)\nmitigates context scarcity by retrieving relevant code snippets, current\napproaches face significant limitations. They overly rely on textual similarity\nfor retrieval, neglecting structural relationships such as call chains and\ninheritance hierarchies, and lose critical structural information by naively\nconcatenating retrieved snippets into text sequences for LLM input. To address\nthese shortcomings, GRACE constructs a multi-level, multi-semantic code graph\nthat unifies file structures, abstract syntax trees, function call graphs,\nclass hierarchies, and data flow graphs to capture both static and dynamic code\nsemantics. For retrieval, GRACE employs a Hybrid Graph Retriever that\nintegrates graph neural network-based structural similarity with textual\nretrieval, refined by a graph attention network-based re-ranker to prioritize\ntopologically relevant subgraphs. To enhance context, GRACE introduces a\nstructural fusion mechanism that merges retrieved subgraphs with the local code\ncontext and preserves essential dependencies like function calls and\ninheritance. Extensive experiments on public repository-level benchmarks\ndemonstrate that GRACE significantly outperforms state-of-the-art methods\nacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the\nstrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every\ndataset. The code is available at\nhttps://anonymous.4open.science/r/grace_icse-C3D5.", "AI": {"tldr": "GRACE is a novel framework for repository-level code generation that uses a hybrid graph-based retrieval method and structural fusion to enhance LLM performance on code tasks.", "motivation": "Existing language models struggle with repository-level code generation tasks due to limited context windows and insufficient handling of complex semantic and structural dependencies. Existing retrieval-augmented methods overly focus on textual similarities and fail to preserve structural relationships in code.", "method": "GRACE creates a multi-level, multi-semantic code graph that integrates static and dynamic code elements (e.g., file structure, call graphs). For retrieval, it uses a hybrid graph retrieval system combining graph neural network-based structural similarity with textual retrieval. It re-ranks results using graph attention networks and introduces a structural fusion mechanism to preserve critical dependencies in the context.", "result": "GRACE shows substantial performance improvements, achieving 8.19% EM and 7.51% ES point gains over previous state-of-the-art methods on public repository-level benchmarks using DeepSeek-V3 as the language model backbone.", "conclusion": "GRACE effectively addresses limitations of current code-generation approaches by leveraging graph-based retrieval and structural fusion, enabling superior handling of repository-level code tasks. This advance demonstrates its potential in improving LLM capabilities for real-world software development tasks."}}
{"id": "2509.06575", "pdf": "https://arxiv.org/pdf/2509.06575", "abs": "https://arxiv.org/abs/2509.06575", "authors": ["Yian Huang", "Yang Feng", "Zhiliang Ying"], "title": "Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination.", "AI": {"tldr": "This paper presents RAS (Robust and Adaptive Spectral method) for robust multi-task learning, excelling under high task contamination up to 80%.", "motivation": "Address the challenges in representation-based MTL posed by contamination, outliers, and adversarial tasks, which current methods fail to handle effectively.", "method": "Developed RAS, capable of distilling shared inlier representations without prior knowledge about contamination levels or the representation dimension, supported by theoretical error bounds.", "result": "RAS performs robust learning under significant contamination, adapts to inlier-outlier structures, provides guarantees to avoid negative transfer, and supports transfer learning with proven efficiency.", "conclusion": "RAS extends MTL applicability with its resilience to high contamination, demonstrating theoretical and empirical superiority over existing methods, ensuring robust and adaptable learning."}}
{"id": "2509.05469", "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.", "AI": {"tldr": "The paper introduces a multi-agent system for quickly and realistically editing bicycle facility designs on real-world street-view images, aimed at enhancing public engagement in transportation planning.", "motivation": "Current methods for creating realistic street-design scenarios are labor-intensive, limiting their utility for public engagement and collaborative decision-making. Generative AI approaches face challenges in handling complex street views without large amounts of training data.", "method": "The framework uses a multi-agent system that combines lane localization, prompt optimization, design generation, and automated evaluation to create realistic street-view designs that adapt to varying conditions.", "result": "The system was tested across diverse urban conditions and demonstrated that it could consistently produce visually coherent and context-compliant design results.", "conclusion": "The work lays a foundation for using multi-agent systems in infrastructure planning, making street design more accessible and collaborative."}}
{"id": "2509.05337", "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "categories": ["cs.CV", "cs.RO"], "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "The paper proposes a fall prediction method using a hybrid model combining Dynamic Graph Neural Networks (DGNN) and Long Short-Term Memory (LSTM) networks to anticipate falls with high accuracy by analyzing human gait.", "motivation": "Falls pose a critical risk in assistive robotic systems, and while detection has seen progress, there is a gap in predicting falls before they occur and analyzing the transient state of instability.", "method": "The method utilizes a hybrid DGNN-LSTM model, with DGNN classifying gait states (stable, transient, fall) and LSTM predicting human movement over time. Skeletal features from video sequences are used as input.", "result": "Using OUMVLP-Pose and URFD datasets, the proposed model outperformed existing DGNN-centric models and those from the literature, showing lower prediction error and higher accuracy due to separating prediction and classification tasks.", "conclusion": "Decoupling motion prediction and gait classification improves the prediction performance. The method also provides valuable insights into the transient state, enhancing assistive robotic systems."}}
{"id": "2509.05489", "pdf": "https://arxiv.org/pdf/2509.05489", "abs": "https://arxiv.org/abs/2509.05489", "authors": ["Peixuan Han", "Adit Krishnan", "Gerald Friedland", "Jiaxuan You", "Chris Kong"], "title": "Self-Aligned Reward: Towards Effective and Efficient Reasoners", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards has significantly advanced\nreasoning in large language models (LLMs), but such signals remain coarse,\noffering only binary correctness feedback. This limitation often results in\ninefficiencies, including overly verbose reasoning and high computational cost,\nwhile existing solutions often compromise accuracy. To address this, we\nintroduce self-aligned reward (SAR), a self-guided signal that complements\nverifiable rewards to encourage both reasoning accuracy and efficiency. SAR is\ndefined as the relative perplexity difference between an answer conditioned on\nthe query and the standalone answer, thereby favoring responses that are\nconcise and query-specific. Quantitative analysis reveals that SAR reliably\ndistinguishes answer quality: concise, correct answers score higher than\nredundant ones, and partially correct answers score higher than entirely\nincorrect ones. Evaluation on 4 models across 7 benchmarks shows that\nintegrating SAR with prevalent RL algorithms like PPO and GRPO improves\naccuracy by 4%, while reducing inference cost by 30%. Further analysis\ndemonstrates that SAR achieves a Pareto-optimal trade-off between correctness\nand efficiency compared to reward signals based on length or self-confidence.\nWe also show that SAR shortens responses while preserving advanced reasoning\nbehaviors, demonstrating its ability to suppress unnecessary elaboration\nwithout losing critical reasoning. These results highlight the promise of\nself-aligned reward as a fine-grained complement to verifiable rewards, paving\nthe way for more efficient and effective LLM training.", "AI": {"tldr": "The paper introduces Self-Aligned Reward (SAR), a signal designed to enhance reasoning in large language models by balancing response accuracy and efficiency. SAR improves accuracy while significantly reducing inference costs.", "motivation": "Current reinforcement learning techniques with verifiable rewards provide only binary correctness feedback, leading to inefficiencies like verbose reasoning and high computational costs, often compromising accuracy.", "method": "The authors propose SAR, quantified by the relative perplexity difference between a query-conditioned answer and a standalone answer. This metric promotes concise, query-specific responses. SAR is integrated with RL algorithms like PPO and GRPO to evaluate its effectiveness.", "result": "SAR improved reasoning accuracy by 4% and reduced inference costs by 30% across 4 models and 7 benchmarks. SAR demonstrated its ability to distinguish answer quality and achieve a Pareto-optimal trade-off between correctness and efficiency.", "conclusion": "Self-Aligned Reward enhances reasoning accuracy and efficiency without sacrificing advanced reasoning capabilities, offering a compelling, fine-grained complement to verifiable rewards for training large language models."}}
{"id": "2509.05675", "pdf": "https://arxiv.org/pdf/2509.05675", "abs": "https://arxiv.org/abs/2509.05675", "authors": ["Amin Pakzad", "Pedro Arduino", "Wenyang Zhang", "Ertugrul Tacirouglu"], "title": "Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation", "categories": ["math.NA", "cs.DC", "cs.NA", "74S05", "G.1.8"], "comment": "8 pages, 20 figures, conference paper, Proceedings of the XVII PCSMGE", "summary": "The demand for high-fidelity numerical simulations in soil-structure\ninteraction analysis is on the rise, yet a standardized workflow to guide the\ncreation of such simulations remains elusive. This paper aims to bridge this\ngap by presenting a step-by-step guideline proposing a workflow for dynamic\nanalysis of structures with pile foundations. The proposed workflow encompasses\ninstructions on how to use Domain Reduction Method for loading, Perfectly\nMatched Layer elements for wave absorption, soil-structure interaction modeling\nusing Embedded interface elements, and domain decomposition for efficient use\nof processing units. Through a series of numerical simulations, we showcase the\npractical application of this workflow. Our results reveal the efficacy of the\nDomain Reduction Method in reducing simulation size without compromising model\nfidelity, show the precision of Perfectly Matched Layer elements in modeling\ninfinite domains, highlight the efficiency of Embedded Interface elements in\nestablishing connections between structures and the soil domain, and\ndemonstrate the overall effectiveness of the proposed workflow in conducting\nhigh-fidelity simulations. While our study focuses on simplified geometries and\nloading scenarios, it serves as a foundational framework for future research\nendeavors aimed at exploring more intricate structural configurations and\ndynamic loading conditions", "AI": {"tldr": "This paper presents a standardized workflow for dynamic soil-structure interaction analyses involving pile foundations, including methods for load application, wave absorption, efficient computation, and interface modeling.", "motivation": "The motivation is to address the lack of a standardized workflow for high-fidelity simulations in soil-structure interaction analysis.", "method": "The study introduces a workflow using methods like Domain Reduction for loading, Perfectly Matched Layer elements for wave absorption, Embedded interface elements for soil-structure interaction, and domain decomposition for computational efficiency.", "result": "The results show the Domain Reduction Method reduced simulation size effectively, Perfectly Matched Layer elements modeled infinite domains accurately, Embedded Interface elements efficiently simulated connections, and the overall workflow demonstrated high reliability.", "conclusion": "The study provides a foundational guide for soil-structure interaction simulations, paving the way for research into more complex scenarios."}}
{"id": "2509.05475", "pdf": "https://arxiv.org/pdf/2509.05475", "abs": "https://arxiv.org/abs/2509.05475", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Autonomous regolith excavation is a cornerstone of in-situ resource\nutilization for a sustained human presence beyond Earth. However, this task is\nfundamentally hindered by the complex interaction dynamics of granular media\nand the operational need for robots to use diverse tools. To address these\nchallenges, this work introduces a framework where a model-based reinforcement\nlearning agent learns within a parallelized simulation. This environment\nleverages high-fidelity particle physics and procedural generation to create a\nvast distribution of both lunar terrains and excavation tool geometries. To\nmaster this diversity, the agent learns an adaptive interaction strategy by\ndynamically modulating its own stiffness and damping at each control step\nthrough operational space control. Our experiments demonstrate that training\nwith a procedural distribution of tools is critical for generalization and\nenables the development of sophisticated tool-aware behavior. Furthermore, we\nshow that augmenting the agent with visual feedback significantly improves task\nsuccess. These results represent a validated methodology for developing the\nrobust and versatile autonomous systems required for the foundational tasks of\nfuture space missions.", "AI": {"tldr": "The paper presents a model-based reinforcement learning framework that enables robots to adaptively excavate lunar regolith with diverse tools and terrains.", "motivation": "The study aims to solve the complexity of granular media interactions and the need for robots to use diverse tools for autonomous excavation in space environments.", "method": "The framework employs a parallelized simulation with high-fidelity physics and procedural generation of terrains and tools. The agent learns adaptive strategies by modulating its stiffness and damping during control using operational space control.", "result": "The experiments showed that training with a diverse distribution of tools improves generalization and that visual feedback significantly enhances task success.", "conclusion": "The methodology offers a robust way to develop adaptable autonomous systems for critical space exploration tasks, like in-situ resource utilization."}}
{"id": "2509.05566", "pdf": "https://arxiv.org/pdf/2509.05566", "abs": "https://arxiv.org/abs/2509.05566", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "title": "Ad hoc conventions generalize to new referents", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "AI": {"tldr": "The paper investigates how people develop shared naming systems for new objects, exploring whether these are arbitrary or conceptually broader. Evidence shows that these systems lead to generalization, suggesting conceptual coordination.", "motivation": "To understand whether and how people generalize naming conventions developed through communication, shedding light on whether these conventions are arbitrary or conceptually broad.", "method": "A dyadic communication study with 302 participants using the KiloGram dataset involving abstract tangram images. Participants coordinated on referential conventions for some images and were tested for generalization on new, undiscussed images.", "result": "Participants showed increased alignment on undiscussed images after forming naming conventions, with generalization capabilities decaying based on visual similarity and robustness across different image nameability levels.", "conclusion": "Ad hoc naming conventions are not arbitrary but reflect deeper conceptual coordination, offering insights for theories of reference and adaptive language agent design."}}
{"id": "2509.05995", "pdf": "https://arxiv.org/pdf/2509.05995", "abs": "https://arxiv.org/abs/2509.05995", "authors": ["Sharon Guardado", "Risha Parveen", "Zheying Zhang", "Maruf Rayhan", "Nirnaya Tripathi"], "title": "Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities", "categories": ["cs.SE"], "comment": "Accepted by the 33rd IEEE International Requirements Engineering 2025\n  (RE'25), Valencia, Spain, September 1-5, 2025", "summary": "The integration of Large Language Models (LLMs) in Requirements Engineering\n(RE) education is reshaping pedagogical approaches, seeking to enhance student\nengagement and motivation while providing practical tools to support their\nprofessional future. This study empirically evaluates the impact of integrating\nLLMs in RE coursework. We examined how the guided use of LLMs influenced\nstudents' learning experiences, and what benefits and challenges they perceived\nin using LLMs in RE practices. The study collected survey data from 179\nstudents across two RE courses in two universities. LLMs were integrated into\ncoursework through different instructional formats, i.e., individual\nassignments versus a team-based Agile project. Our findings indicate that LLMs\nimproved students' comprehension of RE concepts, particularly in tasks like\nrequirements elicitation and documentation. However, students raised concerns\nabout LLMs in education, including academic integrity, overreliance on AI, and\nchallenges in integrating AI-generated content into assignments. Students who\nworked on individual assignments perceived that they benefited more than those\nwho worked on team-based assignments, highlighting the importance of contextual\nAI integration. This study offers recommendations for the effective integration\nof LLMs in RE education. It proposes future research directions for balancing\nAI-assisted learning with critical thinking and collaborative practices in RE\ncourses.", "AI": {"tldr": "This study assesses how integrating Large Language Models (LLMs) in Requirements Engineering (RE) courses impacts student learning, showing improvements but also concerns about overreliance and integration challenges.", "motivation": "The paper aims to explore how LLMs can enhance educational methods in RE, improving student engagement and preparing them for industry practices.", "method": "The researchers collected survey data from 179 students across two universities, comparing individual assignments and team-based Agile projects to evaluate the integration of LLMs.", "result": "LLMs improved students' understanding of RE concepts but raised concerns regarding dependence on AI, academic integrity, and its integration into coursework.", "conclusion": "Contextual and balanced integration of LLMs in education is essential to leverage their benefits while fostering critical thinking and collaborative skills."}}
{"id": "2509.06576", "pdf": "https://arxiv.org/pdf/2509.06576", "abs": "https://arxiv.org/abs/2509.06576", "authors": ["Yinjie Wang", "Doudou Zhou", "Yue Liu", "Junwei Lu", "Tianxi Cai"], "title": "Automated Hierarchical Graph Construction for Multi-source Electronic Health Records", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Electronic Health Records (EHRs), comprising diverse clinical data such as\ndiagnoses, medications, and laboratory results, hold great promise for\ntranslational research. EHR-derived data have advanced disease prevention,\nimproved clinical trial recruitment, and generated real-world evidence.\nSynthesizing EHRs across institutions enables large-scale, generalizable\nstudies that capture rare diseases and population diversity, but remains\nhindered by the heterogeneity of medical codes, institution-specific\nterminologies, and the absence of standardized data structures. These barriers\nlimit the interpretability, comparability, and scalability of EHR-based\nanalyses, underscoring the need for robust methods to harmonize and extract\nmeaningful insights from distributed, heterogeneous data. To address this, we\npropose MASH (Multi-source Automated Structured Hierarchy), a fully automated\nframework that aligns medical codes across institutions using neural optimal\ntransport and constructs hierarchical graphs with learned hyperbolic\nembeddings. During training, MASH integrates information from pre-trained\nlanguage models, co-occurrence patterns, textual descriptions, and supervised\nlabels to capture semantic and hierarchical relationships among medical\nconcepts more effectively. Applied to real-world EHR data, including diagnosis,\nmedication, and laboratory codes, MASH produces interpretable hierarchical\ngraphs that facilitate the navigation and understanding of heterogeneous\nclinical data. Notably, it generates the first automated hierarchies for\nunstructured local laboratory codes, establishing foundational references for\ndownstream applications.", "AI": {"tldr": "MASH is an automated framework for harmonizing and structuring diverse EHR data across institutions using advanced neural and hierarchical modeling methods.", "motivation": "EHR data remain underutilized due to complexity and heterogeneity in medical codes and terminologies, hindering large-scale and generalizable studies.", "method": "MASH aligns medical codes using neural optimal transport and constructs hierarchical graphs through hyperbolic embeddings, integrating multi-modal information during training.", "result": "MASH successfully generates hierarchical graphs for diagnosis, medication, and laboratory codes, improving interpretability and creating foundational references for local laboratory codes.", "conclusion": "MASH enables better navigation and comprehension of heterogeneous EHR data, paving the way for enhanced data harmonization and downstream clinical research applications."}}
{"id": "2509.05550", "pdf": "https://arxiv.org/pdf/2509.05550", "abs": "https://arxiv.org/abs/2509.05550", "authors": ["Zixi Li"], "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation", "categories": ["cs.AI"], "comment": "Code available at: https://github.com/lizixi-0x2F/TreeGPT", "summary": "We introduce TreeGPT, a novel neural architecture that combines\ntransformer-based attention mechanisms with global parent-child aggregation for\nprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.\nUnlike traditional approaches that rely solely on sequential processing or\ngraph neural networks, TreeGPT employs a hybrid design that leverages both\nself-attention for capturing local dependencies and a specialized Tree\nFeed-Forward Network (TreeFFN) for modeling hierarchical tree structures\nthrough iterative message passing.\n  The core innovation lies in our Global Parent-Child Aggregation mechanism,\nformalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in\nE_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the\nhidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges\ninvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This\nformulation enables each node to progressively aggregate information from the\nentire tree structure through $T$ iterations.\n  Our architecture integrates optional enhancements including gated aggregation\nwith learnable edge weights, residual connections for gradient stability, and\nbidirectional propagation for capturing both bottom-up and top-down\ndependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging\nvisual reasoning benchmark requiring abstract pattern recognition and rule\ninference. Experimental results demonstrate that TreeGPT achieves 96\\%\naccuracy, significantly outperforming transformer baselines (1.3\\%),\nlarge-scale models like Grok-4 (15.9\\%), and specialized program synthesis\nmethods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive\nablation study reveals that edge projection is the most critical component,\nwith the combination of edge projection and gating achieving optimal\nperformance.", "AI": {"tldr": "TreeGPT is a hybrid neural network combining attention mechanisms and a hierarchical tree model for program synthesis tasks, achieving 96% accuracy on a difficult dataset.", "motivation": "Improve neural program synthesis tasks by effectively processing Abstract Syntax Trees rather than relying on sequential or graph-based methods.", "method": "TreeGPT employs a hybrid architecture with self-attention and a novel Tree Feed-Forward Network using a Global Parent-Child Aggregation mechanism for hierarchical tree modeling.", "result": "TreeGPT outperforms existing methods on the ARC Prize 2025 dataset with 96% accuracy using only 1.5M parameters.", "conclusion": "TreeGPT\u2019s innovative use of tree aggregation and message-passing enables superior performance in program synthesis, highlighting its efficiency and effectiveness."}}
{"id": "2509.05340", "pdf": "https://arxiv.org/pdf/2509.05340", "abs": "https://arxiv.org/abs/2509.05340", "authors": ["Dibya Jyoti Bora", "Mrinal Kanti Mishra"], "title": "Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a\npivotal challenge in medical image analysis due to the heterogeneous nature of\ntumor morphology and intensity distributions. Accurate delineation of tumor\nboundaries is critical for clinical decision-making, radiotherapy planning, and\nlongitudinal disease monitoring. In this study, we perform a comprehensive\ncomparative analysis of two major clustering paradigms applied in MRI tumor\nsegmentation: hard clustering, exemplified by the K-Means algorithm, and soft\nclustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each\npixel strictly to a single cluster, FCM introduces partial memberships, meaning\neach pixel can belong to multiple clusters with varying degrees of association.\nExperimental validation was performed using the BraTS2020 dataset,\nincorporating pre-processing through Gaussian filtering and Contrast Limited\nAdaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice\nSimilarity Coefficient (DSC) and processing time, which collectively\ndemonstrated that K-Means achieved superior speed with an average runtime of\n0.3s per image, whereas FCM attained higher segmentation accuracy with an\naverage DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher\ncomputational cost (1.3s per image). These results highlight the inherent\ntrade-off between computational efficiency and boundary precision.", "AI": {"tldr": "This paper compares two clustering methods, K-Means and Fuzzy C-Means (FCM), for brain tumor MRI segmentation, highlighting FCM's higher accuracy but slower processing speed.", "motivation": "To address the challenge of accurately segmenting brain tumors from MRI scans due to their varying shape, size, and intensity patterns, which is crucial for clinical applications like treatment planning.", "method": "The authors evaluated hard clustering (K-Means) and soft clustering (FCM) methods using the BraTS2020 dataset, employing Gaussian filtering and CLAHE for pre-processing, and validating results with metrics such as Dice Similarity Coefficient and runtime.", "result": "K-Means achieved fast processing speeds (0.3s per image) but had lower segmentation accuracy (DSC of 0.43), whereas FCM delivered higher accuracy (DSC of 0.67) at a slower speed (1.3s per image).", "conclusion": "There is a trade-off between computational speed and segmentation precision, with K-Means being faster and FCM providing better accuracy for brain tumor MRI segmentation."}}
{"id": "2509.05542", "pdf": "https://arxiv.org/pdf/2509.05542", "abs": "https://arxiv.org/abs/2509.05542", "authors": ["Qi Cao", "Pengtao Xie"], "title": "DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training", "categories": ["cs.LG"], "comment": null, "summary": "Training multimodal process reward models (PRMs) is challenged by\ndistribution shifts and noisy data. We introduce DreamPRM-1.5, an\ninstance-reweighted framework that adaptively adjusts the importance of each\ntraining example via bi-level optimization. We design two complementary\nstrategies: Instance Table, effective for smaller datasets, and Instance Net,\nscalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5\nachieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.", "AI": {"tldr": "DreamPRM-1.5 introduces an adaptive reweighting approach to tackle challenges in training multimodal process reward models, achieving state-of-the-art accuracy on the MMMU benchmark.", "motivation": "The paper aims to address challenges such as distribution shifts and noisy datasets when training multimodal process reward models (PRMs).", "method": "DreamPRM-1.5 adopts bi-level optimization to adaptively reweight training examples, using two strategies\u2014Instance Table for smaller datasets and Instance Net for scalability to larger datasets.", "result": "DreamPRM-1.5 surpasses previous benchmarks, achieving 84.6% accuracy on the MMMU dataset, outperforming GPT-5.", "conclusion": "DreamPRM-1.5 demonstrates the effectiveness of adaptive reweighting combined with test-time scaling, setting a new performance standard for multimodal process reward models."}}
{"id": "2509.05679", "pdf": "https://arxiv.org/pdf/2509.05679", "abs": "https://arxiv.org/abs/2509.05679", "authors": ["Viet Hoang Pham", "Hyo-Sung Ahn"], "title": "Distributed Deep Learning using Stochastic Gradient Staleness", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Despite the notable success of deep neural networks (DNNs) in solving complex\ntasks, the training process still remains considerable challenges. A primary\nobstacle is the substantial time required for training, particularly as high\nperforming DNNs tend to become increasingly deep (characterized by a larger\nnumber of hidden layers) and require extensive training datasets. To address\nthese challenges, this paper introduces a distributed training method that\nintegrates two prominent strategies for accelerating deep learning: data\nparallelism and fully decoupled parallel backpropagation algorithm. By\nutilizing multiple computational units operating in parallel, the proposed\napproach enhances the amount of training data processed in each iteration while\nmitigating locking issues commonly associated with the backpropagation\nalgorithm. These features collectively contribute to significant improvements\nin training efficiency. The proposed distributed training method is rigorously\nproven to converge to critical points under certain conditions. Its\neffectiveness is further demonstrated through empirical evaluations, wherein an\nDNN is trained to perform classification tasks on the CIFAR-10 dataset.", "AI": {"tldr": "The paper proposes a distributed training method for deep neural networks combining data parallelism and fully decoupled parallel backpropagation to address challenges in training time and efficiency.", "motivation": "To overcome lengthy training times and scalability issues with increasingly deeper networks and large datasets in deep learning.", "method": "The approach integrates data parallelism and a decoupled parallel backpropagation algorithm, utilizing multiple computational units to improve processing per iteration and mitigate backpropagation locking issues.", "result": "The method is proven to converge under certain conditions and is empirically validated by successfully training a DNN on CIFAR-10 classification tasks.", "conclusion": "The distributed training method significantly enhances training efficiency and demonstrates practical feasibility for deep learning tasks."}}
{"id": "2509.05500", "pdf": "https://arxiv.org/pdf/2509.05500", "abs": "https://arxiv.org/abs/2509.05500", "authors": ["Yanda Yang", "Max Sokolich", "Fatma Ceren Kirmizitas", "Sambeeta Das", "Andreas A. Malikopoulos"], "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance", "categories": ["cs.RO", "cs.AI"], "comment": "56 pages, 19 figures including Supplementary Materials. Supplementary\n  videos available at\n  https://robotyyd.github.io/yanda-yang.github.io/vascular-parkour.html.\n  Preprint. This version has not been peer reviewed", "summary": "Autonomous microrobots in blood vessels could enable minimally invasive\ntherapies, but navigation is challenged by dense, moving obstacles. We propose\na real-time path planning framework that couples an analytic geometry global\nplanner (AGP) with two reactive local escape controllers, one based on rules\nand one based on reinforcement learning, to handle sudden moving obstacles.\nUsing real-time imaging, the system estimates the positions of the microrobot,\nobstacles, and targets and computes collision-free motions. In simulation, AGP\nyields shorter paths and faster planning than weighted A* (WA*), particle swarm\noptimization (PSO), and rapidly exploring random trees (RRT), while maintaining\nfeasibility and determinism. We extend AGP from 2D to 3D without loss of speed.\nIn both simulations and experiments, the combined global planner and local\ncontrollers reliably avoid moving obstacles and reach targets. The average\nplanning time is 40 ms per frame, compatible with 25 fps image acquisition and\nreal-time closed-loop control. These results advance autonomous microrobot\nnavigation and targeted drug delivery in vascular environments.", "AI": {"tldr": "The paper introduces a framework for navigating autonomous microrobots in blood vessels using a real-time path planner combined with local controllers to avoid obstacles in dynamic environments.", "motivation": "To address the challenge of navigating autonomous microrobots through dense and moving obstacles in blood vessels for minimally invasive therapies.", "method": "The framework employs a global path planner using analytic geometry and two local escape controllers (rules-based and reinforcement learning) that process real-time imaging to compute collision-free paths.", "result": "In simulations, the proposed planner (AGP) demonstrated shorter paths and faster planning compared to alternatives like WA*, PSO, and RRT. The system successfully avoided moving obstacles and achieved target navigation both in 2D and 3D environments.", "conclusion": "The study advances real-time control capabilities for microrobots in vascular environments, paving the way for enhanced targeted drug delivery systems."}}
{"id": "2509.05602", "pdf": "https://arxiv.org/pdf/2509.05602", "abs": "https://arxiv.org/abs/2509.05602", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "categories": ["cs.CL"], "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "AI": {"tldr": "The paper introduces CoPeD, a method targeting better reasoning in SLMs by focusing on rationale correctness and data weighting to mitigate noise from LLM-generated CoT data.", "motivation": "To address the problem of noisy rationales in CoT data fine-tuned into small language models, which reduce reasoning quality.", "method": "Proposes two strategies: correctness-aware task settings to improve rationale reliability and a Correctness-Aware Weighted loss to prioritize meaningful training samples.", "result": "The experiments showed CoPeD significantly enhanced reasoning quality in both in-distribution and out-of-distribution benchmark tasks.", "conclusion": "CoPeD helps SLMs learn more accurate reasoning while mitigating spurious correlations by improving the CoT data and the way it's utilized."}}
{"id": "2509.06012", "pdf": "https://arxiv.org/pdf/2509.06012", "abs": "https://arxiv.org/abs/2509.06012", "authors": ["Jukka Ruohonen"], "title": "A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering", "categories": ["cs.SE"], "comment": "Submitted to REFSQ 2026", "summary": "Out of a personal puzzlement, recent peer review comments, and demonstrable\nconfusion in the existing literature, the paper presents a rapid review of the\nconcept of legal requirements (LRs) in requirements engineering (RE) research.\nAccording to reviewing results, a normative understanding of LRs has often been\npresent, although proper definitions and conceptual operationalizations are\nlacking. Some papers also see LRs as functional and others as non-functional\nrequirements. Legal requirements are often characterized as being vague and\ncomplex, requiring a lot of effort to elicit, implement, and validate. These\ncharacterizations supposedly correlate with knowledge gaps among requirements\nengineers. LRs are also seen to often change and overlap. They may be also\nprioritized. According to the literature, they seem to be also reluctantly\nimplemented, often providing only a minimal baseline for other requirements.\nWith these and other observations, the review raises critical arguments about\napparent knowledge gaps, including a lack of empirical evidence backing the\nobservations and enduring conceptual confusion.", "AI": {"tldr": "The paper reviews the concept of legal requirements in requirements engineering, noting a lack of clear definitions, empirical evidence, and enduring conceptual confusion.", "motivation": "There is confusion in existing literature regarding the concept of legal requirements, highlighted by peer review comments and personal puzzlement.", "method": "The paper conducts a rapid review of literature to analyze perspectives on legal requirements in requirements engineering.", "result": "Legal requirements are often vague, complex, overlapping, and reluctantly implemented, lacking clear definitions or empirical backing in literature.", "conclusion": "The study identifies significant knowledge gaps and conceptual confusion in handling legal requirements within requirements engineering, suggesting a need for clearer definitions and evidence-based approaches."}}
{"id": "2509.06856", "pdf": "https://arxiv.org/pdf/2509.06856", "abs": "https://arxiv.org/abs/2509.06856", "authors": ["Guan-Yu Chen", "Xi Yang"], "title": "Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We propose a novel randomized framework for the estimation problem of\nlarge-scale linear statistical models, namely Sequential Least-Squares\nEstimators with Fast Randomized Sketching (SLSE-FRS), which integrates\nSketch-and-Solve and Iterative-Sketching methods for the first time. By\niteratively constructing and solving sketched least-squares (LS) subproblems\nwith increasing sketch sizes to achieve better precisions, SLSE-FRS gradually\nrefines the estimators of the true parameter vector, ultimately producing\nhigh-precision estimators. We analyze the convergence properties of SLSE-FRS,\nand provide its efficient implementation. Numerical experiments show that\nSLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned\nConjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS)\nmethod.", "AI": {"tldr": "The paper introduces SLSE-FRS, a method for solving large-scale linear statistical models using iterative randomized sketching for improved estimation precision.", "motivation": "To address inefficiencies in estimating parameters for large-scale linear statistical models by combining and improving existing Sketch-and-Solve and Iterative-Sketching methods.", "method": "The proposed method iteratively constructs sketched LS subproblems with progressively increasing sketch sizes to refine estimations, providing efficient implementation and convergence analysis.", "result": "Experimental results demonstrate that SLSE-FRS outperforms existing methods, such as PCG and IDS, in terms of accuracy and computational efficiency.", "conclusion": "SLSE-FRS successfully integrates two sketching approaches to provide an accurate and efficient estimator for large-scale linear statistical models, setting a new benchmark in the field."}}
{"id": "2509.05578", "pdf": "https://arxiv.org/pdf/2509.05578", "abs": "https://arxiv.org/abs/2509.05578", "authors": ["Ruixun Liu", "Lingyu Kong", "Derun Li", "Hang Zhao"], "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown strong vision-language\nreasoning abilities but still lack robust 3D spatial understanding, which is\ncritical for autonomous driving. This limitation stems from two key challenges:\n(1) the difficulty of constructing accessible yet effective 3D representations\nwithout expensive manual annotations, and (2) the loss of fine-grained spatial\ndetails in VLMs due to the absence of large-scale 3D vision-language\npretraining. To address these challenges, we propose OccVLA, a novel framework\nthat integrates 3D occupancy representations into a unified multimodal\nreasoning process. Unlike prior approaches that rely on explicit 3D inputs,\nOccVLA treats dense 3D occupancy as both a predictive output and a supervisory\nsignal, enabling the model to learn fine-grained spatial structures directly\nfrom 2D visual inputs. The occupancy predictions are regarded as implicit\nreasoning processes and can be skipped during inference without performance\ndegradation, thereby adding no extra computational overhead. OccVLA achieves\nstate-of-the-art results on the nuScenes benchmark for trajectory planning and\ndemonstrates superior performance on 3D visual question-answering tasks,\noffering a scalable, interpretable, and fully vision-based solution for\nautonomous driving.", "AI": {"tldr": "The paper introduces OccVLA, a framework that enhances multimodal large language models (MLLMs) with 3D spatial reasoning by integrating 3D occupancy representations without extra inference costs. It achieves state-of-the-art results in autonomous driving benchmarks.", "motivation": "MLLMs show vision-language reasoning capabilities but struggle with 3D spatial understanding, crucial in autonomous driving. Challenges include constructing accessible 3D representations and lack of large-scale 3D vision-language pretraining.", "method": "OccVLA incorporates 3D occupancy representations into multimodal reasoning, treating dense 3D occupancy as predictive output and supervisory signal learned from 2D inputs. It eliminates reasoning overhead during inference.", "result": "OccVLA achieves state-of-the-art performance in trajectory planning (nuScenes benchmark) and excels in 3D visual question-answering tasks, highlighting its scalability and interpretability.", "conclusion": "OccVLA offers a scalable, interpretable, and vision-based solution for enhancing 3D spatial understanding in autonomous driving, addressing key limitations in multimodal large language models."}}
{"id": "2509.05341", "pdf": "https://arxiv.org/pdf/2509.05341", "abs": "https://arxiv.org/abs/2509.05341", "authors": ["Abhijeet Manoj Pal", "Rajbabu Velmurugan"], "title": "Handling imbalance and few-sample size in ML based Onion disease classification", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages, 8 figures", "summary": "Accurate classification of pests and diseases plays a vital role in precision\nagriculture, enabling efficient identification, targeted interventions, and\npreventing their further spread. However, current methods primarily focus on\nbinary classification, which limits their practical applications, especially in\nscenarios where accurately identifying the specific type of disease or pest is\nessential. We propose a robust deep learning based model for multi-class\nclassification of onion crop diseases and pests. We enhance a pre-trained\nConvolutional Neural Network (CNN) model by integrating attention based modules\nand employing comprehensive data augmentation pipeline to mitigate class\nimbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1\nscore on real-world field image dataset. This model gives better results than\nother approaches using the same datasets.", "AI": {"tldr": "A new deep learning model is developed for classifying onion diseases and pests with 96.90% accuracy, surpassing existing approaches.", "motivation": "To address the limitations of binary classification in agriculture by enabling more precise multi-class identification of pests and diseases.", "method": "Enhanced pre-trained CNN with attention-based modules and data augmentation to improve performance and address class imbalance.", "result": "The proposed model achieved 96.90% accuracy and a 0.96 F1 score on real-world datasets, outperforming competing methods.", "conclusion": "The study demonstrates the efficacy of an enhanced CNN model for practical, multi-class pest and disease classification in precision agriculture."}}
{"id": "2509.05545", "pdf": "https://arxiv.org/pdf/2509.05545", "abs": "https://arxiv.org/abs/2509.05545", "authors": ["Yang Yu"], "title": "Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks", "categories": ["cs.LG"], "comment": null, "summary": "Solving long-horizon goal-conditioned tasks remains a significant challenge\nin reinforcement learning (RL). Hierarchical reinforcement learning (HRL)\naddresses this by decomposing tasks into more manageable sub-tasks, but the\nautomatic discovery of the hierarchy and the joint training of multi-level\npolicies often suffer from instability and can lack theoretical guarantees. In\nthis paper, we introduce Reinforcement Learning with Anticipation (RLA), a\nprincipled and potentially scalable framework designed to address these\nlimitations. The RLA agent learns two synergistic models: a low-level,\ngoal-conditioned policy that learns to reach specified subgoals, and a\nhigh-level anticipation model that functions as a planner, proposing\nintermediate subgoals on the optimal path to a final goal. The key feature of\nRLA is the training of the anticipation model, which is guided by a principle\nof value geometric consistency, regularized to prevent degenerate solutions. We\npresent proofs that RLA approaches the globally optimal policy under various\nconditions, establishing a principled and convergent method for hierarchical\nplanning and execution in long-horizon goal-conditioned tasks.", "AI": {"tldr": "The paper introduces a scalable framework, Reinforcement Learning with Anticipation (RLA), to tackle long-horizon, goal-conditioned tasks using hierarchical reinforcement learning.", "motivation": "To address long-horizon goal-conditioned tasks, which are a major challenge in reinforcement learning, and to overcome instability and theoretical limitations in hierarchical RL methods.", "method": "RLA employs two models: a low-level goal-conditioned policy for subgoal achievement and a high-level anticipation model that proposes intermediate subgoals, guided by value geometric consistency principles.", "result": "RLA demonstrates convergence towards globally optimal policies under certain conditions, offering a consistent and principled approach to hierarchical planning.", "conclusion": "RLA provides a scalable and theoretically grounded framework for hierarchical reinforcement learning that can be used effectively for long-horizon goal-conditioned tasks."}}
{"id": "2509.05759", "pdf": "https://arxiv.org/pdf/2509.05759", "abs": "https://arxiv.org/abs/2509.05759", "authors": ["Jinkun Geng", "Shuai Mu", "Anirudh Sivaraman", "Balaji Prabhakar"], "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]", "categories": ["cs.NI", "cs.DB", "cs.DC", "68M10, 68M15"], "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)", "summary": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga.", "AI": {"tldr": "Tiga, a new geo-replicated, scalable transactional database design, achieves 1 WRTT transaction commits for most scenarios, with improved throughput and lower latency.", "motivation": "To improve the efficiency of geo-replicated transactional databases like Google Spanner by committing transactions in 1 WRTT while minimizing computational overhead.", "method": "The approach combines concurrency control and consensus into a single process using synchronized clocks to proactively order transactions by future timestamps. For rare delays, a fallback mechanism completes commits in 1.5-2 WRTTs.", "result": "Tiga demonstrates 1.3\u20137.2x higher throughput and 1.4\u20134.6x lower latency compared to existing solutions, especially excelling in 1-WRTT latency transactions.", "conclusion": "Tiga effectively combines strict serializability, consistent replication, and high performance into a scalable, open-source solution for transactional databases."}}
{"id": "2509.05547", "pdf": "https://arxiv.org/pdf/2509.05547", "abs": "https://arxiv.org/abs/2509.05547", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning.", "AI": {"tldr": "TeleopLab is a cost-effective remote learning system allowing students to operate a robotic arm and lab equipment via a smartphone-based interface, significantly improving usability and task performance.", "motivation": "To provide a scalable and effective solution for hands-on learning in remote STEM education, addressing cost and usability challenges.", "method": "Developed TeleopLab, a mobile teleoperation system utilizing a robotic arm, adaptive gripper, cameras, lab equipment, and a smartphone interface. User studies were performed to assess task performance, system usability, and workload.", "result": "Achieved a 46.1% reduction in task completion time, positive usability score (SUS: 73.8), and a manageable workload score (NASA TLX: 38.2). Students' perspectives toward the system improved after use.", "conclusion": "TeleopLab effectively bridges physical labs and remote learning, offering a scalable solution for remote STEM education with significant usability and performance improvements."}}
{"id": "2509.05605", "pdf": "https://arxiv.org/pdf/2509.05605", "abs": "https://arxiv.org/abs/2509.05605", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "AI": {"tldr": "This paper proposes Icon2, a novel method for constructing high-quality preference datasets for large language models, leveraging their internal regulation mechanisms for efficiency and accuracy.", "motivation": "The motivation is to address challenges in preference dataset construction, such as distribution mismatches and high computational costs, which hinder the alignment of LLMs with human preferences.", "method": "Icon2 extracts layer-wise direction vectors in LLMs to represent human preferences, uses these vectors to filter self-synthesized instructions, and applies bidirectional control during decoding for generating high-quality response pairs.", "result": "Experimental results show improvements in both alignment and efficiency, including a 13.89% and 13.45% average win rate increase on AlpacaEval 2.0 and Arena-Hard respectively, and up to 48.1% reduction in computational costs.", "conclusion": "The Icon2 method enables more efficient and tailored construction of preference datasets, leading to improved model alignment and reduced computational expenses."}}
{"id": "2509.06052", "pdf": "https://arxiv.org/pdf/2509.06052", "abs": "https://arxiv.org/abs/2509.06052", "authors": ["Qingyuan Li", "Binchang Li", "Cuiyun Gao", "Shuzheng Gao", "Zongjie Li"], "title": "Empirical Study of Code Large Language Models for Binary Security Patch Detection", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "Security patch detection (SPD) is crucial for maintaining software security,\nas unpatched vulnerabilities can lead to severe security risks. In recent\nyears, numerous learning-based SPD approaches have demonstrated promising\nresults on source code. However, these approaches typically cannot be applied\nto closed-source applications and proprietary systems that constitute a\nsignificant portion of real-world software, as they release patches only with\nbinary files, and the source code is inaccessible. Given the impressive\nperformance of code large language models (LLMs) in code intelligence and\nbinary analysis tasks such as decompilation and compilation optimization, their\npotential for detecting binary security patches remains unexplored, exposing a\nsignificant research gap between their demonstrated low-level code\nunderstanding capabilities and this critical security task. To address this\ngap, we construct a large-scale binary patch dataset containing \\textbf{19,448}\nsamples, with two levels of representation: assembly code and pseudo-code, and\nsystematically evaluate \\textbf{19} code LLMs of varying scales to investigate\ntheir capability in binary SPD tasks. Our initial exploration demonstrates that\ndirectly prompting vanilla code LLMs struggles to accurately identify security\npatches from binary patches, and even state-of-the-art prompting techniques\nfail to mitigate the lack of domain knowledge in binary SPD within vanilla\nmodels. Drawing on the initial findings, we further investigate the fine-tuning\nstrategy for injecting binary SPD domain knowledge into code LLMs through two\nlevels of representation. Experimental results demonstrate that fine-tuned LLMs\nachieve outstanding performance, with the best results obtained on the\npseudo-code representation.", "AI": {"tldr": "The paper explores security patch detection in binary files using fine-tuned code large language models (LLMs), highlighting their effectiveness over vanilla LLMs.", "motivation": "To address the gap in detecting security patches in closed-source software systems where source code is not accessible, leveraging the potential of code large language models.", "method": "Constructed a large-scale binary patch dataset with 19,448 samples in assembly code and pseudo-code representations, tested 19 LLMs, and implemented fine-tuning strategies to inject domain knowledge for binary SPD.", "result": "Fine-tuned LLMs demonstrated exceptional performance in detecting security patches, especially using pseudo-code representation.", "conclusion": "Fine-tuning code LLMs with relevant domain knowledge significantly enhances their effectiveness in binary security patch detection compared to vanilla LLMs."}}
{"id": "2509.06894", "pdf": "https://arxiv.org/pdf/2509.06894", "abs": "https://arxiv.org/abs/2509.06894", "authors": ["Nils Detering", "Luca Galimberti", "Anastasis Kratsios", "Giulia Livieri", "A. Martina Neuman"], "title": "Learning from one graph: transductive learning guarantees via the geometry of small random worlds", "categories": ["stat.ML", "cs.LG", "math.MG", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "Since their introduction by Kipf and Welling in $2017$, a primary use of\ngraph convolutional networks is transductive node classification, where missing\nlabels are inferred within a single observed graph and its feature matrix.\nDespite the widespread use of the network model, the statistical foundations of\ntransductive learning remain limited, as standard inference frameworks\ntypically rely on multiple independent samples rather than a single graph. In\nthis work, we address these gaps by developing new concentration-of-measure\ntools that leverage the geometric regularities of large graphs via\nlow-dimensional metric embeddings. The emergent regularities are captured using\na random graph model; however, the methods remain applicable to deterministic\ngraphs once observed. We establish two principal learning results. The first\nconcerns arbitrary deterministic $k$-vertex graphs, and the second addresses\nrandom graphs that share key geometric properties with an Erd\\H{o}s-R\\'{e}nyi\ngraph $\\mathbf{G}=\\mathbf{G}(k,p)$ in the regime $p \\in \\mathcal{O}((\\log\n(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the\nsecond. We then extend these results to the graph convolutional network\nsetting, where additional challenges arise. Lastly, our learning guarantees\nremain informative even with a few labelled nodes $N$ and achieve the optimal\nnonparametric rate $\\mathcal{O}(N^{-1/2})$ as $N$ grows.", "AI": {"tldr": "This paper develops new statistical tools for transductive node classification in graph convolutional networks, leveraging geometric regularities via low-dimensional embeddings.", "motivation": "There is a gap in statistical foundations for transductive learning in graph networks, as prior inference frameworks typically depend on multiple samples instead of a single graph.", "method": "The authors introduce concentration-of-measure tools using random and deterministic graph models, focusing on geometric regularities in graph structures.", "result": "Two main results are highlighted: statistical learning guarantees for arbitrary deterministic graphs and random graphs with properties similar to an Erd\u0151s\u2013R\u00e9nyi graph in specific regimes. The framework is extended to graph convolutional networks with optimal learning rates achieved even for few labeled nodes.", "conclusion": "This work advances the understanding of transductive learning in graph networks and provides guarantees for effective learning under realistic conditions."}}
{"id": "2509.05685", "pdf": "https://arxiv.org/pdf/2509.05685", "abs": "https://arxiv.org/abs/2509.05685", "authors": ["Jian Yang", "Jiahui Wu", "Li Fang", "Hongchao Fan", "Bianying Zhang", "Huijie Zhao", "Guangyi Yang", "Rui Xin", "Xiong You"], "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Transforming road network data into vector representations using deep\nlearning has proven effective for road network analysis. However, urban road\nnetworks' heterogeneous and hierarchical nature poses challenges for accurate\nrepresentation learning. Graph neural networks, which aggregate features from\nneighboring nodes, often struggle due to their homogeneity assumption and focus\non a single structural scale. To address these issues, this paper presents\nMSRFormer, a novel road network representation learning framework that\nintegrates multi-scale spatial interactions by addressing their flow\nheterogeneity and long-distance dependencies. It uses spatial flow convolution\nto extract small-scale features from large trajectory datasets, and identifies\nscale-dependent spatial interaction regions to capture the spatial structure of\nroad networks and flow heterogeneity. By employing a graph transformer,\nMSRFormer effectively captures complex spatial dependencies across multiple\nscales. The spatial interaction features are fused using residual connections,\nwhich are fed to a contrastive learning algorithm to derive the final road\nnetwork representation. Validation on two real-world datasets demonstrates that\nMSRFormer outperforms baseline methods in two road network analysis tasks. The\nperformance gains of MSRFormer suggest the traffic-related task benefits more\nfrom incorporating trajectory data, also resulting in greater improvements in\ncomplex road network structures with up to 16% improvements compared to the\nmost competitive baseline method. This research provides a practical framework\nfor developing task-agnostic road network representation models and highlights\ndistinct association patterns of the interplay between scale effects and flow\nheterogeneity of spatial interactions.", "AI": {"tldr": "The paper introduces MSRFormer, a framework for road network representation learning using multi-scale spatial interactions and graph transformers, resulting in up to 16% performance improvement over competitive baselines.", "motivation": "Urban road networks are heterogeneous and hierarchical, posing challenges for accurate representation learning using traditional graph neural networks.", "method": "The MSRFormer framework integrates spatial flow convolution, scale-dependent regions, graph transformers, and residual connections to capture multi-scale spatial dependencies, followed by contrastive learning for representation.", "result": "MSRFormer outperformed baseline methods on two real-world datasets, achieving up to 16% improvement in traffic-related tasks and analysis of complex road network structures.", "conclusion": "This research offers a practical, task-agnostic framework for road network representation and highlights the interplay between scale effects and flow heterogeneity of spatial interactions."}}
{"id": "2509.05342", "pdf": "https://arxiv.org/pdf/2509.05342", "abs": "https://arxiv.org/abs/2509.05342", "authors": ["Gaspard Beaudouin", "Minghan Li", "Jaeyeon Kim", "Sunghoon Yoon", "Mengyu Wang"], "title": "Delta Velocity Rectified Flow for Text-to-Image Editing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,\npath-aware editing framework within rectified flow models for text-to-image\nediting. DVRF is a distillation-based method that explicitly models the\ndiscrepancy between the source and target velocity fields in order to mitigate\nover-smoothing artifacts rampant in prior distillation sampling approaches. We\nfurther introduce a time-dependent shift term to push noisy latents closer to\nthe target trajectory, enhancing the alignment with the target distribution. We\ntheoretically demonstrate that when this shift is disabled, DVRF reduces to\nDelta Denoising Score, thereby bridging score-based diffusion optimization and\nvelocity-based rectified-flow optimization. Moreover, when the shift term\nfollows a linear schedule under rectified-flow dynamics, DVRF generalizes the\nInversion-free method FlowEdit and provides a principled theoretical\ninterpretation for it. Experimental results indicate that DVRF achieves\nsuperior editing quality, fidelity, and controllability while requiring no\narchitectural modifications, making it efficient and broadly applicable to\ntext-to-image editing tasks. Code is available at\nhttps://github.com/gaspardbd/DeltaVelocityRectifiedFlow.", "AI": {"tldr": "This paper introduces Delta Velocity Rectified Flow (DVRF), a text-to-image editing method that enhances editing quality by addressing oversmoothing artifacts and aligning trajectories without the need for inversion.", "motivation": "The authors aim to address oversmoothing artifacts and lack of controllability encountered in prior distillation-based text-to-image editing methods, while maintaining efficiency and model compatibility.", "method": "The method uses a distillation-based approach, modeling source-target velocity field discrepancies and applying a time-dependent shift term to improve alignment with target distributions. It theoretically connects to prior methods like Delta Denoising Score and FlowEdit under rectified-flow dynamics.", "result": "DVRF demonstrates superior performance in editing quality, fidelity, and controllability compared to existing methods, without requiring architectural changes.", "conclusion": "DVRF establishes a robust and efficient framework for text-to-image editing, linking previous generation methods theoretically and offering a broadly applicable tool for practical implementations."}}
{"id": "2509.05884", "pdf": "https://arxiv.org/pdf/2509.05884", "abs": "https://arxiv.org/abs/2509.05884", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "title": "Introduction to Number Theoretic Transform", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "The Number Theoretic Transform (NTT) can be regarded as a variant of the\nDiscrete Fourier Transform. NTT has been quite a powerful mathematical tool in\ndeveloping Post-Quantum Cryptography and Homomorphic Encryption. The Fourier\nTransform essentially decomposes a signal into its frequencies. They are\ntraditionally sine or cosine waves. NTT works more over groups or finite fields\nrather than on a continuous signal and polynomials work as the analog of sine\nwaves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has\nbeen proven to be useful in lattice-based cryptography due to its ability to\nreduce the complexity of polynomial multiplication from quadratic to\nquasilinear. We have introduced the concepts of cyclic, negacyclic convolutions\nalong with NTT and its inverse and their fast versions.", "AI": {"tldr": "The paper explores the use of the Number Theoretic Transform (NTT), a discrete variant of the Fourier Transform, in lattice-based cryptography and homomorphic encryption, emphasizing its efficiency in polynomial multiplication.", "motivation": "To leverage the mathematical framework of NTT for optimizing computational processes in post-quantum cryptography and homomorphic encryption.", "method": "Introduced concepts such as cyclic and negacyclic convolutions alongside NTT and its inverse. Fast versions of NTT were presented to improve polynomial multiplication.", "result": "NTT reduces polynomial multiplication complexity from quadratic to quasilinear, demonstrating its utility in cryptographic applications.", "conclusion": "NTT emerges as an efficient computational tool, particularly in lattice-based cryptography, offering significant performance improvements and mathematical elegance."}}
{"id": "2509.05581", "pdf": "https://arxiv.org/pdf/2509.05581", "abs": "https://arxiv.org/abs/2509.05581", "authors": ["Arturo Flores Alvarez", "Fatemeh Zargarbashi", "Havel Liu", "Shiqi Wang", "Liam Edwards", "Jessica Anz", "Alex Xu", "Fan Shi", "Stelian Coros", "Dennis W. Hong"], "title": "Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "68T40", "I.2.9; I.2.6"], "comment": "8 pages, 11 figures, accepted at IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2025", "summary": "We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a\ncustom-built humanoid robot designed for entertainment applications. Unlike\ntraditional humanoids, entertainment robots present unique challenges due to\naesthetic-driven design choices. Cosmo embodies these with a disproportionately\nlarge head (16% of total mass), limited sensing, and protective shells that\nconsiderably restrict movement. To address these challenges, we apply\nAdversarial Motion Priors (AMP) to enable the robot to learn natural-looking\nmovements while maintaining physical stability. We develop tailored domain\nrandomization techniques and specialized reward structures to ensure safe\nsim-to-real, protecting valuable hardware components during deployment. Our\nexperiments demonstrate that AMP generates stable standing and walking\nbehaviors despite Cosmo's extreme mass distribution and movement constraints.\nThese results establish a promising direction for robots that balance aesthetic\nappeal with functional performance, suggesting that learning-based methods can\neffectively adapt to aesthetic-driven design constraints.", "AI": {"tldr": "The paper introduces a Reinforcement Learning system for Cosmo, a humanoid robot designed for entertainment, using adversarial motion priors to achieve natural and physically stable movements despite design challenges.", "motivation": "To address the challenges of creating humanoid robots with aesthetically-driven designs, such as Cosmo's large head and limited mobility, while ensuring both functionality and safety.", "method": "The paper integrates Adversarial Motion Priors (AMP) with domain randomization techniques and specialized reward systems to train Cosmo for natural and stable locomotion under physical constraints.", "result": "The AMP approach demonstrated success in generating stable standing and walking behaviors for Cosmo despite its extreme mass distribution and movement limitations.", "conclusion": "Learning-based approaches like AMP can effectively overcome aesthetic-driven design constraints, paving the way for functional entertainment robots."}}
{"id": "2509.05607", "pdf": "https://arxiv.org/pdf/2509.05607", "abs": "https://arxiv.org/abs/2509.05607", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "AI": {"tldr": "This paper presents a framework for Generative Search Engine Optimization (GSEO) with a new benchmark (CC-GSEO-Bench) and a multi-agent system to measure and improve how content influences generative search engines.", "motivation": "Traditional SEO metrics are inadequate for generative search engines, which prioritize synthesized answers. The paper aims to redefine content optimization in this new search paradigm.", "method": "The authors introduce CC-GSEO-Bench, a benchmark with a multi-dimensional evaluation system, and utilize a multi-agent system to automate and refine content optimization through an analyze-revise-evaluate workflow.", "result": "The framework offers insights into how content influences generative search output and provides actionable strategies for creators.", "conclusion": "The study establishes foundational tools and methodologies for advancing research in GSEO, enabling better content optimization for generative search engines."}}
{"id": "2509.06085", "pdf": "https://arxiv.org/pdf/2509.06085", "abs": "https://arxiv.org/abs/2509.06085", "authors": ["Jerin Yasmin", "Wenxin Jiang", "James C. Davis", "Yuan Tian"], "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to Empirical Software Engineering (EMSE) Journal", "summary": "Pre-trained models (PTMs) are machine learning models that have been trained\nin advance, often on large-scale data, and can be reused for new tasks, thereby\nreducing the need for costly training from scratch. Their widespread adoption\nintroduces a new class of software dependency, which we term Software\nDependencies 2.0, extending beyond conventional libraries to learned behaviors\nembodied in trained models and their associated artifacts. The integration of\nPTMs as software dependencies in real projects remains unclear, potentially\nthreatening maintainability and reliability of modern software systems that\nincreasingly rely on them. Objective: In this study, we investigate Software\nDependencies 2.0 in open-source software (OSS) projects by examining the reuse\nof PTMs, with a focus on how developers manage and integrate these models.\nSpecifically, we seek to understand: (1) how OSS projects structure and\ndocument their PTM dependencies; (2) what stages and organizational patterns\nemerge in the reuse pipelines of PTMs within these projects; and (3) the\ninteractions among PTMs and other learned components across pipeline stages. We\nconduct a mixed-methods analysis of a statistically significant random sample\nof 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories\nreusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM\nreuse by identifying patterns and qualitatively investigate how developers\nintegrate and manage these models in practice.", "AI": {"tldr": "Pre-trained models (PTMs) are increasingly adopted as software dependencies, termed Software Dependencies 2.0, raising concerns about their integration, maintainability, and reliability. This study explores how open-source projects manage PTM reuse.", "motivation": "Open-source software increasingly relies on pre-trained models for various tasks, introducing new challenges in managing these learned dependencies, which may affect the reliability and maintainability of software systems.", "method": "The study analyzes a random sample of 401 GitHub repositories from the PeaTMOSS dataset, performing quantitative pattern identification and qualitative analysis of developers' practices in reusing PTMs from Hugging Face and PyTorch Hub.", "result": "The researchers identified patterns in how projects structure and document PTM dependencies and revealed organizational and pipeline stages for integrating these models. Additionally, they observed interactions among PTMs and other components.", "conclusion": "This study highlights the complexity in integrating PTMs as dependencies in open-source projects and provides insights into structuring reuse pipelines for better management and reliability of software systems."}}
{"id": "2509.03652", "pdf": "https://arxiv.org/pdf/2509.03652", "abs": "https://arxiv.org/abs/2509.03652", "authors": ["E. Khalafyan", "A. E. Allahverdyan", "A. Hovhannisyan"], "title": "Nonnegative matrix factorization and the principle of the common cause", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": null, "summary": "Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction\nmethod. The principle of the common cause (PCC) is a basic methodological\napproach in probabilistic causality, which seeks an independent mixture model\nfor the joint probability of two dependent random variables. It turns out that\nthese two concepts are closely related. This relationship is explored\nreciprocally for several datasets of gray-scale images, which are conveniently\nmapped into probability models. On one hand, PCC provides a predictability tool\nthat leads to a robust estimation of the effective rank of NMF. Unlike other\nestimates (e.g., those based on the Bayesian Information Criteria), our\nestimate of the rank is stable against weak noise. We show that NMF implemented\naround this rank produces features (basis images) that are also stable against\nnoise and against seeds of local optimization, thereby effectively resolving\nthe NMF nonidentifiability problem. On the other hand, NMF provides an\ninteresting possibility of implementing PCC in an approximate way, where larger\nand positively correlated joint probabilities tend to be explained better via\nthe independent mixture model. We work out a clustering method, where data\npoints with the same common cause are grouped into the same cluster. We also\nshow how NMF can be employed for data denoising.", "AI": {"tldr": "The paper explores the relationship between Nonnegative Matrix Factorization (NMF) and the Principle of the Common Cause (PCC), demonstrating mutual benefits for feature stability, rank estimation, clustering, and data denoising in gray-scale image datasets.", "motivation": "The motivation is to investigate the close relationship between NMF and PCC and leverage this connection for more robust feature discovery, rank estimation, and clustering, especially in scenarios involving noise.", "method": "The study uses datasets of gray-scale images and applies NMF methods combined with principles drawn from PCC. PCC is employed as a tool to estimate the effective rank of NMF, improving robustness and stability. Additionally, NMF is used for clustering and data denoising.", "result": "The proposed method leads to rank estimation that is robust to weak noise and produces stable NMF features. The approach resolves the nonidentifiability problem in NMF and enables clustering data points sharing a common cause. The method also demonstrates effectiveness in data denoising.", "conclusion": "This reciprocal exploration shows that NMF and PCC can significantly enhance one another. PCC informs a stable NMF rank, while NMF offers a practical way to approximate PCC for solving tasks like clustering and denoising. The proposed techniques achieve noise-resilient results with practical implications for image analysis and probabilistic modeling."}}
{"id": "2509.05714", "pdf": "https://arxiv.org/pdf/2509.05714", "abs": "https://arxiv.org/abs/2509.05714", "authors": ["Zhaoyu Fan", "Kaihang Pan", "Mingze Zhou", "Bosheng Qin", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Fei Wu", "Yueting Zhuang"], "title": "Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs", "categories": ["cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "Knowledge editing enables multimodal large language models (MLLMs) to\nefficiently update outdated or incorrect information. However, existing\nbenchmarks primarily emphasize cognitive-level modifications while lacking a\nfocus on deeper meta-cognitive processes. To bridge this gap, we introduce\nCogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge\nediting abilities across three levels: (1) Counterfactual-Driven Editing,\nassessing self-awareness of knowledge correctness changes; (2) Boundary\nConstraint Editing, ensuring appropriate generalization without unintended\ninterference; and (3) Noise-Robust Editing, promoting reflective evaluation of\nuncertain information. To advance meta-cognitive editing, we propose MIND\n(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that\nconstructs a meta-knowledge memory for self-awareness, employs game-theoretic\ninteractions to monitor knowledge activation, and incorporates label refinement\nfor noise-robust updates. Extensive experiments show that MIND significantly\noutperforms existing cognitive editing approaches, achieving strong performance\non both traditional and meta-cognitive knowledge editing benchmarks.", "AI": {"tldr": "This paper introduces CogEdit, a benchmark for evaluating meta-cognitive knowledge editing in multimodal large language models (MLLMs), and proposes MIND, a framework to enhance such editing. The approach significantly outperforms existing methods.", "motivation": "Existing knowledge editing benchmarks for MLLMs focus on cognitive-level changes but lack evaluation of deeper meta-cognitive processes.", "method": "The paper introduces CogEdit, a benchmark assessing meta-cognitive editing across three challenges: Counterfactual-Driven Editing, Boundary Constraint Editing, and Noise-Robust Editing. Additionally, they propose MIND, a framework with meta-knowledge memory, game-theoretic monitoring, and label refinement features.", "result": "MIND demonstrates notable improvements over existing cognitive editing methods in both traditional and meta-cognitive benchmarks.", "conclusion": "The study highlights the importance of meta-cognitive knowledge editing in MLLMs and establishes MIND as an effective solution for addressing existing challenges."}}
{"id": "2509.05343", "pdf": "https://arxiv.org/pdf/2509.05343", "abs": "https://arxiv.org/abs/2509.05343", "authors": ["Zahid Ullah", "Minki Hong", "Tahir Mahmood", "Jihie Kim"], "title": "Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has become a powerful tool for medical image analysis; however,\nconventional Convolutional Neural Networks (CNNs) often fail to capture the\nfine-grained and complex features critical for accurate diagnosis. To address\nthis limitation, we systematically integrate attention mechanisms into five\nwidely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,\nDenseNet121, and EfficientNetB5, to enhance their ability to focus on salient\nregions and improve discriminative performance. Specifically, each baseline\nmodel is augmented with either a Squeeze and Excitation block or a hybrid\nConvolutional Block Attention Module, allowing adaptive recalibration of\nchannel and spatial feature representations. The proposed models are evaluated\non two distinct medical imaging datasets, a brain tumor MRI dataset comprising\nmultiple tumor subtypes, and a Products of Conception histopathological dataset\ncontaining four tissue categories. Experimental results demonstrate that\nattention augmented CNNs consistently outperform baseline architectures across\nall metrics. In particular, EfficientNetB5 with hybrid attention achieves the\nhighest overall performance, delivering substantial gains on both datasets.\nBeyond improved classification accuracy, attention mechanisms enhance feature\nlocalization, leading to better generalization across heterogeneous imaging\nmodalities. This work contributes a systematic comparative framework for\nembedding attention modules in diverse CNN architectures and rigorously\nassesses their impact across multiple medical imaging tasks. The findings\nprovide practical insights for the development of robust, interpretable, and\nclinically applicable deep learning based decision support systems.", "AI": {"tldr": "The paper enhances five existing CNN architectures with attention mechanisms, resulting in improved performance and interpretability for medical image analysis.", "motivation": "Conventional CNNs struggle to capture fine-grained features necessary for accurate medical image diagnosis.", "method": "The authors integrated attention modules (Squeeze and Excitation block or hybrid Convolutional Block Attention Module) into five widely used CNNs and tested their performance on two medical imaging datasets.", "result": "Attention-augmented CNNs outperformed baseline architectures, with EfficientNetB5 using hybrid attention achieving the best results on both datasets.", "conclusion": "The study offers a framework for enhancing CNNs with attention modules to improve accuracy, feature localization, and generalization in medical imaging tasks."}}
{"id": "2509.05615", "pdf": "https://arxiv.org/pdf/2509.05615", "abs": "https://arxiv.org/abs/2509.05615", "authors": ["Xiaoguang Zhu", "Lianlong Sun", "Yang Liu", "Pengyi Jiang", "Uma Srivatsa", "Nipavan Chiamvimonvat", "Vladimir Filkov"], "title": "Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE TKDE", "summary": "Medical multimodal representation learning aims to integrate heterogeneous\nclinical data into unified patient representations to support predictive\nmodeling, which remains an essential yet challenging task in the medical data\nmining community. However, real-world medical datasets often suffer from\nmissing modalities due to cost, protocol, or patient-specific constraints.\nExisting methods primarily address this issue by learning from the available\nobservations in either the raw data space or feature space, but typically\nneglect the underlying bias introduced by the data acquisition process itself.\nIn this work, we identify two types of biases that hinder model generalization:\nmissingness bias, which results from non-random patterns in modality\navailability, and distribution bias, which arises from latent confounders that\ninfluence both observed features and outcomes. To address these challenges, we\nperform a structural causal analysis of the data-generating process and propose\na unified framework that is compatible with existing direct prediction-based\nmultimodal learning methods. Our method consists of two key components: (1) a\nmissingness deconfounding module that approximates causal intervention based on\nbackdoor adjustment and (2) a dual-branch neural network that explicitly\ndisentangles causal features from spurious correlations. We evaluated our\nmethod in real-world public and in-hospital datasets, demonstrating its\neffectiveness and causal insights.", "AI": {"tldr": "The paper addresses multimodal representation learning for medical data while dealing with biases due to missing modalities and latent confounders by introducing a causal framework.", "motivation": "Existing methods in medical multimodal representation learning fail to account for biases in the data acquisition process, specifically missingness bias and distribution bias.", "method": "The authors propose a framework with two main components: a missingness deconfounding module using causal adjustment and a dual-branch neural network for disentangling causal features.", "result": "Through evaluations on public and in-hospital datasets, the framework showed effectiveness in predictive modeling and highlighted causal insights.", "conclusion": "The proposed approach successfully handles biases in medical multimodal data, improving generalization and providing clearer causal interpretations."}}
{"id": "2509.06133", "pdf": "https://arxiv.org/pdf/2509.06133", "abs": "https://arxiv.org/abs/2509.06133", "authors": ["Pradyumna Kaushal"], "title": "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles", "categories": ["cs.CR", "cs.DC", "cs.SE", "cs.SY", "eess.SY", "C.2.4; K.6.5; D.4.6"], "comment": "13 pages, 5 figures. Whitepaper submission; LaTeX source with\n  compiled .bbl. Includes architecture diagrams, tables, and code listings\n  (TypeScript & Solidity)", "summary": "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets.", "AI": {"tldr": "This paper introduces VehiclePassport, a blockchain and privacy-forward digital passport for securely managing vehicle lifecycle records.", "motivation": "Current vehicle lifecycle records are fragmented, difficult to verify, and susceptible to fraud, creating inefficiencies and risks.", "method": "VehiclePassport leverages blockchain for immutable event anchoring and zero-knowledge proofs (ZKPs) for privacy-preserving verification, supported by an open-source tech stack.", "result": "The system provides a cost-efficient (<$0.02/event), scalable, and fast (<10ms validation) solution anchored on Polygon zkEVM, compatible with millions of vehicles.", "conclusion": "VehiclePassport offers a GDPR-compliant, trustless system for global mobility data markets, eliminating paper processes and enabling applications in insurance, resale, and regulation."}}
{"id": "2509.05599", "pdf": "https://arxiv.org/pdf/2509.05599", "abs": "https://arxiv.org/abs/2509.05599", "authors": ["Kai Zhang", "Guoyang Zhao", "Jianxing Shi", "Bonan Liu", "Weiqing Qi", "Jun Ma"], "title": "MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion", "categories": ["cs.RO"], "comment": null, "summary": "Detecting and localizing glass in 3D environments poses significant\nchallenges for visual perception systems, as the optical properties of glass\noften hinder conventional sensors from accurately distinguishing glass\nsurfaces. The lack of real-world datasets focused on glass objects further\nimpedes progress in this field. To address this issue, we introduce a new\ndataset featuring a wide range of glass configurations with precise 3D\nannotations, collected from distinct real-world scenarios. On the basis of this\ndataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D\nglass detection across diverse environments. To overcome the challenges posed\nby the ambiguous appearance and context diversity of glass, we propose an\nadaptive feature fusion module that empowers the network to effectively capture\ncontextual information in varying conditions. Additionally, to exploit the\ndistinct planar geometry of glass surfaces, we present a plane regression\npipeline, which enables seamless integration of geometric properties within our\nframework. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches in both glass segmentation and monocular glass\ndepth estimation. Our results highlight the advantages of combining geometric\nand contextual cues for transparent surface understanding.", "AI": {"tldr": "This paper introduces a new dataset for 3D glass detection and presents a novel approach, MonoGlass3D, which incorporates adaptive feature fusion and a plane regression technique, outperforming state-of-the-art methods.", "motivation": "Challenges in detecting glass in 3D environments due to its optical properties and lack of real-world datasets focused on glass objects.", "method": "Development of MonoGlass3D, which uses an adaptive feature fusion module for varying contexts and a plane regression pipeline to integrate geometric properties of glass surfaces.", "result": "MonoGlass3D surpasses state-of-the-art methods for glass segmentation and monocular glass depth estimation through a combination of geometric and contextual cues.", "conclusion": "The study emphasizes the effectiveness of combining geometric and contextual information for better understanding and detecting transparent surfaces like glass."}}
{"id": "2509.05609", "pdf": "https://arxiv.org/pdf/2509.05609", "abs": "https://arxiv.org/abs/2509.05609", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "AI": {"tldr": "Addressing the challenge of aligning acoustic and linguistic representations in knowledge transfer for automatic speech recognition (ASR) through unbalanced optimal transport-based alignment.", "motivation": "The paper aims to improve ASR by tackling the structured and asymmetric alignment issue between acoustic and linguistic representations, which is central to knowledge transfer.", "method": "Introduces an unbalanced optimal transport-based alignment model to handle mismatches, asymmetries, and redundancy in acoustic and linguistic mappings flexibly.", "result": "The proposed model, tested on a CTC-based ASR system, effectively enhances the matching flexibility and ASR performance.", "conclusion": "Unbalanced optimal transport-based alignment is a promising solution for structured mismatches in acoustic-linguistic representation, leading to improved ASR outcomes."}}
{"id": "2509.06216", "pdf": "https://arxiv.org/pdf/2509.06216", "abs": "https://arxiv.org/abs/2509.06216", "authors": ["Ahmed E. Hassan", "Hao Li", "Dayi Lin", "Bram Adams", "Tse-Hsun Chen", "Yutaro Kashiwa", "Dong Qiu"], "title": "Agentic Software Engineering: Foundational Pillars and a Research Roadmap", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future.", "AI": {"tldr": "The paper introduces Structured Agentic Software Engineering (SASE), a vision for SE 3.0 where intelligent agents collaborate with humans for complex, goal-oriented software engineering tasks.", "motivation": "As software engineering tasks grow increasingly complex with advancements in AI, there is a need to redefine its foundational practices to accommodate intelligent agents and foster enhanced human-AI collaboration.", "method": "The authors propose two new environments: the Agent Command Environment (ACE) for human oversight and mentoring of agents, and the Agent Execution Environment (AEE), where agents execute tasks and seek human guidance when required.", "result": "The work establishes a conceptual framework, including new processes and modalities, to enable structured interactions between human engineers and AI agents.", "conclusion": "The paper advocates for a paradigm shift toward agentic SE practices, emphasizing a collaborative and trustworthy future. It also provides a roadmap for addressing challenges and adapting SE education to this future."}}
{"id": "2509.05757", "pdf": "https://arxiv.org/pdf/2509.05757", "abs": "https://arxiv.org/abs/2509.05757", "authors": ["Sarang Patil", "Zeyong Zhang", "Yiran Huang", "Tengfei Ma", "Mengjia Xu"], "title": "Hyperbolic Large Language Models", "categories": ["cs.AI"], "comment": "32 pages, 6 figures", "summary": "Large language models (LLMs) have achieved remarkable success and\ndemonstrated superior performance across various tasks, including natural\nlanguage processing (NLP), weather forecasting, biological protein folding,\ntext generation, and solving mathematical problems. However, many real-world\ndata exhibit highly non-Euclidean latent hierarchical anatomy, such as protein\nnetworks, transportation networks, financial networks, brain networks, and\nlinguistic structures or syntactic trees in natural languages. Effectively\nlearning intrinsic semantic entailment and hierarchical relationships from\nthese raw, unstructured input data using LLMs remains an underexplored area.\nDue to its effectiveness in modeling tree-like hierarchical structures,\nhyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity\nas an expressive latent representation space for complex data modeling across\ndomains such as graphs, images, languages, and multi-modal data. Here, we\nprovide a comprehensive and contextual exposition of recent advancements in\nLLMs that leverage hyperbolic geometry as a representation space to enhance\nsemantic representation learning and multi-scale reasoning. Specifically, the\npaper presents a taxonomy of the principal techniques of Hyperbolic LLMs\n(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log\nmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)\nhyperbolic state-space models. We also explore crucial potential applications\nand outline future research directions. A repository of key papers, models,\ndatasets, and code implementations is available at\nhttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.", "AI": {"tldr": "This paper explores how hyperbolic geometry, a non-Euclidean space, can enhance the representation learning in large language models (LLMs), focusing on complex hierarchical data.", "motivation": "The motivation stems from the need to improve LLMs' capabilities to model non-Euclidean hierarchical relationships inherent in real-world data.", "method": "Four categories of Hyperbolic LLMs (HypLLMs) are discussed, including models using exp/log maps, fine-tuned models, fully hyperbolic models, and hyperbolic state-space models.", "result": "The paper offers a taxonomy of techniques, introduces potential applications, and provides a repository of relevant resources on Hyperbolic LLMs.", "conclusion": "Leveraging hyperbolic geometry in LLMs offers promise for enhanced hierarchical data modeling and lays the groundwork for future research directions in this area."}}
{"id": "2509.05348", "pdf": "https://arxiv.org/pdf/2509.05348", "abs": "https://arxiv.org/abs/2509.05348", "authors": ["Ashen Rodrigo", "Isuru Munasinghe", "Asanka Perera"], "title": "Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Timely and accurate detection of defects and contaminants in solar panels is\ncritical for maintaining the efficiency and reliability of photovoltaic\nsystems. This study presents a comprehensive evaluation of five\nstate-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,\nEfficientDet, and Swin Transformer, for identifying physical and electrical\ndefects as well as surface contaminants such as dust, dirt, and bird droppings\non solar panels. A custom dataset, annotated in the COCO format and\nspecifically designed for solar panel defect and contamination detection, was\ndeveloped alongside a user interface to train and evaluate the models. The\nperformance of each model is assessed and compared based on mean Average\nPrecision (mAP), precision, recall, and inference speed. The results\ndemonstrate the trade-offs between detection accuracy and computational\nefficiency, highlighting the relative strengths and limitations of each model.\nThese findings provide valuable guidance for selecting appropriate detection\napproaches in practical solar panel monitoring and maintenance scenarios.\n  The dataset will be publicly available at\nhttps://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.", "AI": {"tldr": "This study evaluates five object detection models (YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer) for detecting defects and contaminants on solar panels. A custom dataset, annotated in COCO format, was developed to train and evaluate the models.", "motivation": "Ensuring the timely and accurate detection of defects and contaminants is critical to maintaining the efficiency and reliability of photovoltaic systems.", "method": "Five state-of-the-art object detection models were evaluated using a custom, publicly-available dataset designed for solar panel inspection. Performance metrics like mAP, precision, recall, and inference speed were measured.", "result": "The study highlights the trade-offs between detection accuracy and computational efficiency, comparing the strengths and limitations of each model.", "conclusion": "The research provides practical guidance on selecting suitable object detection models for solar panel monitoring and contributes a public dataset to aid further work in this area."}}
{"id": "2509.05656", "pdf": "https://arxiv.org/pdf/2509.05656", "abs": "https://arxiv.org/abs/2509.05656", "authors": ["Bo Lyu", "Yu Cui", "Tuo Shi", "Ke Li"], "title": "OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural architecture search (NAS) is a hard computationally expensive\noptimization problem with a discrete, vast, and spiky search space. One of the\nkey research efforts dedicated to this space focuses on accelerating NAS via\ncertain proxy evaluations of neural architectures. Different from the prevalent\npredictor-based methods using surrogate models and differentiable architecture\nsearch via supernetworks, we propose an optimization proxy to streamline the\nNAS as an end-to-end optimization framework, named OptiProxy-NAS. In\nparticular, using a proxy representation, the NAS space is reformulated to be\ncontinuous, differentiable, and smooth. Thereby, any differentiable\noptimization method can be applied to the gradient-based search of the relaxed\narchitecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$\nsearch spaces across three different domains including computer vision, natural\nlanguage processing, and resource-constrained NAS fully demonstrate the\nsuperior search results and efficiency. Further experiments on low-fidelity\nscenarios verify the flexibility.", "AI": {"tldr": "This paper presents OptiProxy-NAS, a method for neural architecture search (NAS) that reformulates the search space to make it continuous, differentiable, and smooth, improving efficiency and search outcomes.", "motivation": "The need to address the challenges of computational expense and inefficiency in NAS due to its discrete, vast, and complex search space.", "method": "The proposed OptiProxy-NAS leverages a proxy-based representation to reframe the NAS problem, enabling the use of gradient-based optimization techniques for efficient and precise architecture search.", "result": "Experiments across 12 NAS tasks in diverse domains (computer vision, NLP, and resource-constrained NAS) demonstrate superior search results and efficiency. It also adapts well to low-fidelity scenarios.", "conclusion": "OptiProxy-NAS not only enhances the effectiveness and efficiency of NAS but also exhibits flexibility across various fidelity levels and domains, marking it as a promising framework for NAS optimization."}}
{"id": "2509.06163", "pdf": "https://arxiv.org/pdf/2509.06163", "abs": "https://arxiv.org/abs/2509.06163", "authors": ["Victoria Kozlova", "Ben Biedermann"], "title": "Social Dynamics of DAOs: Power, Onboarding, and Inclusivity", "categories": ["cs.CY", "cs.DC", "C.2.4; C.2.1; D.4.6; K.4.3"], "comment": null, "summary": "This report explores the often-overlooked cultural and social dynamics\nshaping participation and power in DAOs. Drawing on qualitative interviews and\nethnographic observations, it shows how factors such as financial privilege,\ninformal gatekeeping, visibility bias, and onboarding structures create\nbarriers to meaningful inclusion. While DAOs are frequently framed as\npermissionless and egalitarian, the lived experiences of contributors reveal a\nmore complex reality, one in which soft power and implicit norms determine\npeople's position within DAOs. Instead of offering solutionist prescriptions,\nthis report argues for a deeper cultural reflection within the DAO ecosystem.\nIt highlights that decentralisation is not solely a protocol-level feature, but\nan ongoing social process that requires intentional cultivation of trust,\nbelonging, and epistemic plurality. With this report, we want to sharpen the\ncollective awareness of structural blind spots and call for building more\ninclusive and culturally conscious decentralised systems.", "AI": {"tldr": "The paper examines cultural and social factors influencing inclusion and power dynamics in DAOs, revealing barriers and complexities despite their egalitarian claims.", "motivation": "To address the overlooked cultural and social barriers affecting inclusivity and power structures in decentralised autonomous organisations (DAOs).", "method": "Qualitative interviews and ethnographic observations conducted to explore contributors' lived experiences in DAOs.", "result": "Findings reveal barriers such as financial privilege, informal gatekeeping, and onboarding structures, which challenge DAOs' promises of permissionlessness and egalitarianism.", "conclusion": "Decentralisation in DAOs requires intentional cultivation of trust, belonging, and epistemic diversity; cultural awareness is pivotal for inclusivity."}}
{"id": "2509.05672", "pdf": "https://arxiv.org/pdf/2509.05672", "abs": "https://arxiv.org/abs/2509.05672", "authors": ["Juho Kalliokoski", "Evan G. Center", "Steven M. LaValle", "Timo Ojala", "Basak Sakcak"], "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation", "categories": ["cs.RO"], "comment": "Immersive telepresence, shared control", "summary": "Telepresence robots enable users to interact with remote environments, but\nefficient and intuitive navigation remains a challenge. In this work, we\ndeveloped and evaluated a shared control method, in which the robot navigates\nautonomously while allowing users to affect the path generation to better suit\ntheir needs. We compared this with control switching, where users toggle\nbetween direct and automated control. We hypothesized that shared control would\nmaintain efficiency comparable to control switching while potentially reducing\nuser workload. The results of two consecutive user studies (each with final\nsample of n=20) showed that shared control does not degrade navigation\nefficiency, but did not show a significant reduction in task load compared to\ncontrol switching. Further research is needed to explore the underlying factors\nthat influence user preference and performance in these control systems.", "AI": {"tldr": "Shared control for telepresence robots was tested against control switching, showing no efficiency loss but inconclusive impact on workload.", "motivation": "Enhance navigation efficiency and intuitive user experience for telepresence robots.", "method": "Developed shared control approach and compared with control switching via user studies (n=20 per study).", "result": "Shared control maintained navigation efficiency but did not significantly lower task load compared to control switching.", "conclusion": "Shared control shows promise; further research needed to examine user preferences and performance factors."}}
{"id": "2509.05617", "pdf": "https://arxiv.org/pdf/2509.05617", "abs": "https://arxiv.org/abs/2509.05617", "authors": ["Shay Dahary", "Avi Edana", "Alexander Apartsin", "Yehudit Aperstein"], "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 2 figures", "summary": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.", "AI": {"tldr": "This paper focuses on predicting emotional intensity in song lyrics using both zero-shot Large Language Models (LLMs) and fine-tuned methods, resulting in insights on their performance.", "motivation": "Understanding emotional content in song lyrics can enhance listener experiences and improve music information retrieval systems.", "method": "The authors created a manually labeled dataset using a Mean Opinion Score (MOS) approach and evaluated LLMs in zero-shot scenarios, alongside fine-tuning a BERT-based model for multi-label emotion prediction.", "result": "Zero-shot LLMs demonstrated notable strengths but were outperformed by the fine-tuned BERT model in capturing nuanced emotional content.", "conclusion": "The study emphasizes the potential of LLMs for emotion recognition and provides a key dataset for advancing music information retrieval applications."}}
{"id": "2509.06301", "pdf": "https://arxiv.org/pdf/2509.06301", "abs": "https://arxiv.org/abs/2509.06301", "authors": ["Dharun Anandayuvaraj", "Zain Hammadeh", "Andreas Lund", "Alexandra Holloway", "James C. Davis"], "title": "Learning From Software Failures: A Case Study at a National Space Research Center", "categories": ["cs.SE"], "comment": null, "summary": "Software failures can have significant consequences, making learning from\nfailures a critical aspect of software engineering. While software\norganizations are recommended to conduct postmortems, the effectiveness and\nadoption of these practices vary widely. Understanding how engineers gather,\ndocument, share, and apply lessons from failures is essential for improving\nreliability and preventing recurrence. High-reliability organizations (HROs)\noften develop software systems where failures carry catastrophic risks,\nrequiring continuous learning to ensure reliability. These organizations\nprovide a valuable setting to examine practices and challenges for learning\nfrom software failures. Such insight could help develop processes and tools to\nimprove reliability and prevent recurrence. However, we lack in-depth industry\nperspectives on the practices and challenges of learning from failures.\n  To address this gap, we conducted a case study through 10 in-depth interviews\nwith research software engineers at a national space research center. We\nexamine how they learn from failures: how they gather, document, share, and\napply lessons. To assess transferability, we include data from 5 additional\ninterviews at other HROs. Our findings provide insight into how engineers learn\nfrom failures in practice. To summarize: (1) failure learning is informal, ad\nhoc, and inconsistently integrated into SDLC; (2) recurring failures persist\ndue to absence of structured processes; and (3) key challenges, including time\nconstraints, knowledge loss from turnover and fragmented documentation, and\nweak process enforcement, undermine systematic learning. Our findings deepen\nunderstanding of how software engineers learn from failures and offer guidance\nfor improving failure management practices.", "AI": {"tldr": "This paper examines the practices and challenges of how software engineers at high-reliability organizations (HROs) learn from failures, identifying informal and inconsistent processes, recurring failures, and key challenges such as time constraints and fragmented documentation.", "motivation": "The motivation is to enhance our understanding of how software engineers gather, document, share, and apply lessons learned from software failures, especially in high-reliability organizations where failures carry catastrophic risks.", "method": "The study involved 10 in-depth interviews with research software engineers at a national space research center, supplemented by 5 additional interviews at other HROs to assess transferability.", "result": "Findings reveal that failure learning is informal, ad hoc, and inconsistently integrated into the software development life cycle, with recurring failures persisting due to a lack of structured processes. Key challenges include time constraints, knowledge loss, fragmented documentation, and weak enforcement of processes.", "conclusion": "The study highlights the need for structured failure management practices and tools to improve systematic learning, reliability, and the prevention of recurring failures in software engineering."}}
{"id": "2509.05563", "pdf": "https://arxiv.org/pdf/2509.05563", "abs": "https://arxiv.org/abs/2509.05563", "authors": ["Junyoung Park", "Cheolwoo Park", "Jeongyoun Ahn"], "title": "Interpretable dimension reduction for compositional data", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML", "stat.TH"], "comment": "62 pages, 5 figures", "summary": "High-dimensional compositional data, such as those from human microbiome\nstudies, pose unique statistical challenges due to the simplex constraint and\nexcess zeros. While dimension reduction is indispensable for analyzing such\ndata, conventional approaches often rely on log-ratio transformations that\ncompromise interpretability and distort the data through ad hoc zero\nreplacements. We introduce a novel framework for interpretable dimension\nreduction of compositional data that avoids extra transformations and zero\nimputations. Our approach generalizes the concept of amalgamation by softening\nits operation, mapping high-dimensional compositions directly to a\nlower-dimensional simplex, which can be visualized in ternary plots. The\nframework further provides joint visualization of the reduction matrix,\nenabling intuitive, at-a-glance interpretation. To achieve optimal reduction\nwithin our framework, we incorporate sufficient dimension reduction, which\ndefines a new identifiable objective: the central compositional subspace. For\nestimation, we propose a compositional kernel dimension reduction (CKDR)\nmethod. The estimator is provably consistent, exhibits sparsity that reveals\nunderlying amalgamation structures, and comes with an intrinsic predictive\nmodel for downstream analyses. Applications to real microbiome datasets\ndemonstrate that our approach provides a powerful graphical exploration tool\nfor uncovering meaningful biological patterns, opening a new pathway for\nanalyzing high-dimensional compositional data.", "AI": {"tldr": "The paper presents a new framework for dimension reduction in high-dimensional compositional data, avoiding traditional transformation issues and enabling direct visualization and interpretation.", "motivation": "Conventional methods for analyzing high-dimensional compositional data often compromise interpretability and require ad hoc zero imputations, creating the need for a more robust and interpretable approach.", "method": "The authors introduce a framework based on softening amalgamation and propose a compositional kernel dimension reduction (CKDR) method, which avoids extra transformations and provides visualization tools for intuitive interpretation.", "result": "The CKDR method is proven to be consistent, sparse, and provides predictive capabilities. Applications to microbiome datasets show its ability to uncover meaningful biological patterns visually.", "conclusion": "The proposed framework and CKDR open new possibilities for analyzing and interpreting high-dimensional compositional data, providing a more reliable and intuitive alternative to existing methods."}}
{"id": "2509.05764", "pdf": "https://arxiv.org/pdf/2509.05764", "abs": "https://arxiv.org/abs/2509.05764", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "categories": ["cs.AI"], "comment": "This paper has been accepted by ICONIP 2025 but not published", "summary": "With the evolution of generative AI, multi - agent systems leveraging large -\nlanguage models(LLMs) have emerged as a powerful tool for complex tasks.\nHowever, these systems face challenges in quantifying agent performance and\nlack mechanisms to assess agent credibility. To address these issues, we\nintroduce DRF, a dynamic reputation filtering framework. DRF constructs an\ninteractive rating network to quantify agent performance, designs a reputation\nscoring mechanism to measure agent honesty and capability, and integrates an\nUpper Confidence Bound - based strategy to enhance agent selection efficiency.\nExperiments show that DRF significantly improves task completion quality and\ncollaboration efficiency in logical reasoning and code - generation tasks,\noffering a new approach for multi - agent systems to handle large - scale\ntasks.", "AI": {"tldr": "The paper introduces DRF, a dynamic framework to improve credibility and efficiency in multi-agent systems using LLMs.", "motivation": "Multi-agent systems with LLMs face challenges in assessing performance and agent credibility.", "method": "Developed DRF, comprising an interactive rating network, reputation scoring, and UCB-based agent selection.", "result": "Experimental results show improved task completion quality and efficiency in logical reasoning and code-generation.", "conclusion": "DRF enhances collaboration and efficiency, providing a novel solution for multi-agent systems in complex large-scale tasks."}}
{"id": "2509.05352", "pdf": "https://arxiv.org/pdf/2509.05352", "abs": "https://arxiv.org/abs/2509.05352", "authors": ["Cuong Manh Hoang"], "title": "Unsupervised Instance Segmentation with Superpixels", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation is essential for numerous computer vision applications,\nincluding robotics, human-computer interaction, and autonomous driving.\nCurrently, popular models bring impressive performance in instance segmentation\nby training with a large number of human annotations, which are costly to\ncollect. For this reason, we present a new framework that efficiently and\neffectively segments objects without the need for human annotations. Firstly, a\nMultiCut algorithm is applied to self-supervised features for coarse mask\nsegmentation. Then, a mask filter is employed to obtain high-quality coarse\nmasks. To train the segmentation network, we compute a novel superpixel-guided\nmask loss, comprising hard loss and soft loss, with high-quality coarse masks\nand superpixels segmented from low-level image features. Lastly, a\nself-training process with a new adaptive loss is proposed to improve the\nquality of predicted masks. We conduct experiments on public datasets in\ninstance segmentation and object detection to demonstrate the effectiveness of\nthe proposed framework. The results show that the proposed framework\noutperforms previous state-of-the-art methods.", "AI": {"tldr": "The paper introduces a novel framework for instance segmentation that does not require human annotations, utilizing a MultiCut algorithm, superpixel-guided loss, and a self-training process, achieving state-of-the-art results.", "motivation": "Large-scale human annotations for instance segmentation are expensive and time-consuming to collect, necessitating an efficient approach to achieve accurate segmentation without manual effort.", "method": "The proposed framework uses a MultiCut algorithm for coarse mask generation, a mask filter for refining high-quality masks, a novel superpixel-guided mask loss for network training, and a self-training process with an adaptive loss function.", "result": "The framework was tested on public datasets for instance segmentation and object detection, outperforming previous state-of-the-art methods in accuracy and effectiveness.", "conclusion": "The study demonstrates that instance segmentation can be effectively achieved without human annotations, introducing a cost-efficient and powerful methodology for computer vision tasks."}}
{"id": "2509.05663", "pdf": "https://arxiv.org/pdf/2509.05663", "abs": "https://arxiv.org/abs/2509.05663", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Thomas B\u00e4ck", "Anna V. Kononova"], "title": "DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches", "categories": ["cs.LG"], "comment": "Submitted to the Reliability Engineering & System Safety journal", "summary": "Truly unsupervised approaches for time series anomaly detection are rare in\nthe literature. Those that exist suffer from a poorly set threshold, which\nhampers detection performance, while others, despite claiming to be\nunsupervised, need to be calibrated using a labelled data subset, which is\noften not available in the real world. This work integrates active learning\nwith an existing unsupervised anomaly detection method by selectively querying\nthe labels of multivariate time series, which are then used to refine the\nthreshold selection process. To achieve this, we introduce a novel query\nstrategy called the dissimilarity-based query strategy (DQS). DQS aims to\nmaximise the diversity of queried samples by evaluating the similarity between\nanomaly scores using dynamic time warping. We assess the detection performance\nof DQS in comparison to other query strategies and explore the impact of\nmislabelling, a topic that is underexplored in the literature. Our findings\nindicate that DQS performs best in small-budget scenarios, though the others\nappear to be more robust when faced with mislabelling. Therefore, in the real\nworld, the choice of query strategy depends on the expertise of the oracle and\nthe number of samples they are willing to label. Regardless, all query\nstrategies outperform the unsupervised threshold even in the presence of\nmislabelling. Thus, whenever it is feasible to query an oracle, employing an\nactive learning-based threshold is recommended.", "AI": {"tldr": "This paper integrates active learning into unsupervised time series anomaly detection by introducing a dissimilarity-based query strategy (DQS) to refine threshold selection for better performance. All active learning strategies outperform unsupervised thresholds.", "motivation": "The motivation is to address issues in unsupervised time series anomaly detection, particularly poorly set thresholds and reliance on labeled data subsets, which may not exist in real-world scenarios.", "method": "The method integrates active learning with an unsupervised anomaly detection approach, introducing the DQS query strategy that maximizes diversity of queried samples by using dynamic time warping to evaluate anomaly score similarity.", "result": "DQS performs best in low-budget scenarios for anomaly detection compared to other query strategies, but it is more sensitive to mislabeling.", "conclusion": "Active learning-based threshold selection outperforms purely unsupervised methods, even when mislabeled data exists. The choice of query strategy should depend on oracle expertise and labeling budget."}}
{"id": "2509.06466", "pdf": "https://arxiv.org/pdf/2509.06466", "abs": "https://arxiv.org/abs/2509.06466", "authors": ["Erwan Meunier", "Julien M. Hendrickx"], "title": "Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading", "categories": ["math.OC", "cs.AI", "cs.DC", "cs.MA"], "comment": "7 pages, 5 figures. Paper accepted for the 64th IEEE Conference on\n  Decision and Control (2025)", "summary": "We analyze Decentralized Online Optimization algorithms using the Performance\nEstimation Problem approach which allows, to automatically compute exact\nworst-case performance of optimization algorithms. Our analysis shows that\nseveral available performance guarantees are very conservative, sometimes by\nmultiple orders of magnitude, and can lead to misguided choices of algorithm.\nMoreover, at least in terms of worst-case performance, some algorithms appear\nnot to benefit from inter-agent communications for a significant period of\ntime. We show how to improve classical methods by tuning their step-sizes, and\nfind that we can save up to 20% on their actual worst-case performance regret.", "AI": {"tldr": "The study analyzes decentralized online optimization algorithms to determine their exact worst-case performance and finds that current performance guarantees are overly conservative.", "motivation": "The motivation is to evaluate the accuracy of current performance guarantees for decentralized optimization algorithms and to analyze the interplay of inter-agent communication, performance, and tuning.", "method": "The authors utilized the Performance Estimation Problem approach for a data-driven evaluation of worst-case performance for these algorithms.", "result": "The authors found existing performance guarantees overly conservative, observed limited benefit from inter-agent communication in some cases, and demonstrated that tuning step-sizes could enhance performance by up to 20%.", "conclusion": "Current performance guarantees for decentralized optimization often overestimate limitations and can be improved significantly through refined tuning, enhancing algorithm choice and efficiency."}}
{"id": "2509.05701", "pdf": "https://arxiv.org/pdf/2509.05701", "abs": "https://arxiv.org/abs/2509.05701", "authors": ["Siyuan Wang", "Shuyi Zhang", "Zhen Tian", "Yuheng Yao", "Gongsen Wang", "Yu Zhao"], "title": "A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm", "categories": ["cs.RO"], "comment": null, "summary": "Robot path planning is a fundamental challenge in enhancing the environmental\nadaptability of autonomous navigation systems. This paper presents a hybrid\npath planning algorithm, A-star PRM, which incorporates dynamic weights. By\nembedding the Manhattan distance heuristic of the A-star algorithm into the\nrandom sampling process of PRM, the algorithm achieves a balanced optimization\nof path quality and computational efficiency. The approach uses a hierarchical\nsampling strategy and a dynamic connection mechanism, greatly improving\nadaptability to complex obstacle distributions. Experiments show that under a\nbaseline configuration with one thousand sampled vertices, the path length of\nA-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter\nthan that of PRM with p value less than 0.01. With high-density sampling using\nthree thousand vertices, the path length is reduced by 0.94 percent, 1036.61\nmeters compared with 1046.42 meters, while the increase in computational time\nis cut to about one tenth of the PRM increase, 71 percent compared with 785\npercent. These results confirm the comprehensive advantages of A-star PRM in\npath quality, stability, and computational efficiency. Compared with existing\nhybrid algorithms, the proposed method shows clear benefits, especially in\nnarrow channels and scenarios with dynamic obstacles.", "AI": {"tldr": "The paper introduces a hybrid path planning algorithm, A-star PRM, with dynamic weights to optimize path quality and computational efficiency, demonstrating significant improvements in complex environments.", "motivation": "Address the challenge of robot path planning to enhance adaptability of autonomous navigation in environments with complex obstacles.", "method": "Develop A-star PRM by integrating Manhattan distance heuristic into PRM\u2019s random sampling process, employing hierarchical sampling and dynamic connections.", "result": "Experiments show improved path quality (42.3% shorter path) and computational efficiency (only 71% time increase compared to PRM\u2019s 785%) in high-density sampling scenarios.", "conclusion": "A-star PRM outperforms existing algorithms in path quality, computational stability, and efficiency, particularly in narrow channels and dynamic obstacle scenarios."}}
{"id": "2509.05635", "pdf": "https://arxiv.org/pdf/2509.05635", "abs": "https://arxiv.org/abs/2509.05635", "authors": ["Liang Zhang", "Yuan Li", "Shijie Zhang", "Zheng Zhang", "Xitong Li"], "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "The paper addresses intent detection in conversational systems using a novel framework called SAID, which incorporates both textual and relational structure information.", "motivation": "Existing few-shot intent detection methods focus primarily on textual data, overlooking the structural relationships in conversational systems, such as query-query and query-answer relations.", "method": "The authors propose SAID, a pretraining framework that integrates textual and relational information. They introduce a novel query-adaptive attention network (QueryAdapt) to explicitly model these relations for better knowledge transfer.", "result": "SAID demonstrated superior performance compared to state-of-the-art intent detection methods in experiments on two real-world datasets.", "conclusion": "Integrating textual and relational structure information enhances few-shot intent detection, and SAID provides a promising approach for advancing conversational systems through this integration."}}
{"id": "2509.06324", "pdf": "https://arxiv.org/pdf/2509.06324", "abs": "https://arxiv.org/abs/2509.06324", "authors": ["Zhuohang Shen", "Mohammed Yaseen", "Denini Silva", "Kevin Guan", "Junho Lee", "Marcelo d'Amorim", "Owolabi Legunsen"], "title": "A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation", "categories": ["cs.SE"], "comment": "23 pages, 7 figures", "summary": "Runtime verification (RV) now scales for testing thousands of open-source\nJava projects, helping find hundreds of bugs. The popular Python ecosystem\ncould use such benefits. But, today's Python RV systems are limited to a domain\nor specification logic, or slow. We propose PyMOP, a generic, extensible, and\nefficient RV system for Python. PyMOP supports five logics, implements five\nexisting monitoring algorithms, ships with 73 API specs of Python and\nwidely-used libraries, supports three instrumentation strategies, and users can\neasily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we\nfind mainly that (i) the default monitoring algorithm for Java is often not the\nfastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic\nanalysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were\nfixed by developers. PyMOP's generality and efficiency position it well as an\nexcellent platform for the next advances on RV for Python.", "AI": {"tldr": "The paper introduces PyMOP, an efficient runtime verification (RV) system for Python supporting multiple logics, algorithms, and instrumentation strategies, and demonstrates its efficiency and effectiveness in large-scale testing.", "motivation": "The motivation is to bring scalable runtime verification (RV), which has been successful in the Java ecosystem for finding bugs, to the Python ecosystem, as existing Python RV systems are limited in scope, logic support, or performance.", "method": "The authors propose PyMOP, a generic and extensible RV system for Python that supports five logics, five monitoring algorithms, 73 API specs of Python libraries, and three instrumentation strategies. The system is evaluated on over 290,000 unit tests spanning 1,463 GitHub projects.", "result": "PyMOP proves to be highly efficient, being up to 1,168.3x faster than other dynamic analysis tools. It also successfully helped find 121 bugs, of which 44 were fixed by developers.", "conclusion": "PyMOP demonstrates generality and efficiency, making it a promising platform for future advances in runtime verification for Python applications."}}
{"id": "2509.05627", "pdf": "https://arxiv.org/pdf/2509.05627", "abs": "https://arxiv.org/abs/2509.05627", "authors": ["Sarah H. Cen", "Salil Goyal", "Zaynah Javed", "Ananya Karthik", "Percy Liang", "Daniel E. Ho"], "title": "Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives", "categories": ["cs.CY", "cs.LG", "stat.ML"], "comment": "34 pages, 13 figures", "summary": "AI audits play a critical role in AI accountability and safety. One branch of\nthe law for which AI audits are particularly salient is anti-discrimination\nlaw. Several areas of anti-discrimination law implicate the \"less\ndiscriminatory alternative\" (LDA) requirement, in which a protocol (e.g.,\nmodel) is defensible if no less discriminatory protocol that achieves\ncomparable performance can be found with a reasonable amount of effort.\nNotably, the burden of proving an LDA exists typically falls on the claimant\n(the party alleging discrimination). This creates a significant hurdle in AI\ncases, as the claimant would seemingly need to train a less discriminatory yet\nhigh-performing model, a task requiring resources and expertise beyond most\nlitigants. Moreover, developers often shield information about and access to\ntheir model and training data as trade secrets, making it difficult to\nreproduce a similar model from scratch.\n  In this work, we present a procedure enabling claimants to determine if an\nLDA exists, even when they have limited compute, data, information, and model\naccess. We focus on the setting in which fairness is given by demographic\nparity and performance by binary cross-entropy loss. As our main result, we\nprovide a novel closed-form upper bound for the loss-fairness Pareto frontier\n(PF). We show how the claimant can use it to fit a PF in the \"low-resource\nregime,\" then extrapolate the PF that applies to the (large) model being\ncontested, all without training a single large model. The expression thus\nserves as a scaling law for loss-fairness PFs. To use this scaling law, the\nclaimant would require a small subsample of the train/test data. Then, the\nclaimant can fit the context-specific PF by training as few as 7 (small)\nmodels. We stress test our main result in simulations, finding that our scaling\nlaw holds even when the exact conditions of our theory do not.", "AI": {"tldr": "This paper proposes a novel approach allowing claimants in anti-discrimination cases to assess the availability of less discriminatory alternatives to AI models, even with limited resources and access.", "motivation": "The paper addresses the challenge claimants face in proving the existence of a less discriminatory alternative model under anti-discrimination law, given the constraints of limited access to data, models, and computational resources.", "method": "The authors present a closed-form upper bound for the loss-fairness Pareto frontier, enabling claimants to scale this result to large models using limited data and computational resources. This approach requires training as few as seven small models to extrapolate results to contested larger models.", "result": "A scaling law for loss-fairness Pareto frontiers is derived, showing that claimants can estimate fairness-performance trade-offs in AI models under low-resource conditions. Simulations confirm the validity of the scaling law.", "conclusion": "The proposed procedure effectively empowers claimants to assess less discriminatory alternatives for AI models under resource-constrained scenarios, enhancing accessibility to AI audits in legal cases."}}
{"id": "2509.05772", "pdf": "https://arxiv.org/pdf/2509.05772", "abs": "https://arxiv.org/abs/2509.05772", "authors": ["Nasser Alkhulaifi", "Ismail Gokay Dogan", "Timothy R. Cargan", "Alexander L. Bowler", "Direnc Pekaslan", "Nicholas J. Watson", "Isaac Triguero"], "title": "Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation", "categories": ["cs.AI"], "comment": "22 pages, 10 figures, journal-based paper", "summary": "Decision-making under uncertainty in energy management is complicated by\nunknown parameters hindering optimal strategies, particularly in Battery Energy\nStorage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat\nforecasting and optimisation as separate processes, allowing prediction errors\nto cascade into suboptimal decisions as models minimise forecasting errors\nrather than optimising downstream tasks. The emerging Decision-Focused Learning\n(DFL) methods overcome this limitation by integrating prediction and\noptimisation; however, they are relatively new and have been tested primarily\non synthetic datasets or small-scale problems, with limited evidence of their\npractical viability. Real-world BESS applications present additional\nchallenges, including greater variability and data scarcity due to collection\nconstraints and operational limitations. Because of these challenges, this work\nleverages Automated Feature Engineering (AFE) to extract richer representations\nand improve the nascent approach of DFL. We propose an AFE-DFL framework\nsuitable for small datasets that forecasts electricity prices and demand while\noptimising BESS operations to minimise costs. We validate its effectiveness on\na novel real-world UK property dataset. The evaluation compares DFL methods\nagainst PTO, with and without AFE. The results show that, on average, DFL\nyields lower operating costs than PTO and adding AFE further improves the\nperformance of DFL methods by 22.9-56.5% compared to the same models without\nAFE. These findings provide empirical evidence for DFL's practical viability in\nreal-world settings, indicating that domain-specific AFE enhances DFL and\nreduces reliance on domain expertise for BESS optimisation, yielding economic\nbenefits with broader implications for energy management systems facing similar\nchallenges.", "AI": {"tldr": "A novel AFE-DFL framework integrates automated feature engineering with decision-focused learning to optimize battery storage operations and outperforms traditional methods.", "motivation": "The paper aims to address the inefficiencies in energy storage decision-making caused by the separation of forecasting and optimization, and to explore the real-world applicability of decision-focused learning (DFL) methods to overcome these issues, particularly in scenarios with high variability and scarce data.", "method": "The authors propose an AFE-DFL framework which combines automated feature engineering (AFE) with decision-focused learning (DFL). It is applied to forecast electricity prices and demand while optimizing battery energy storage operations. Performance was assessed using a real-world dataset from UK properties.", "result": "The AFE-DFL framework significantly outperformed traditional Predict-Then-Optimise (PTO) methods, reducing operating costs by 22.9-56.5% through the integration of AFE. DFL also demonstrated consistently lower operating costs than PTO in the evaluation.", "conclusion": "Integrating AFE into DFL enhances its practical viability for real-world applications, reduces dependency on domain knowledge, and provides cost-saving benefits, offering broader implications for energy management systems dealing with similar challenges."}}
{"id": "2509.05388", "pdf": "https://arxiv.org/pdf/2509.05388", "abs": "https://arxiv.org/abs/2509.05388", "authors": ["Juan Olalla-Pombo", "Alberto Bad\u00edas", "Miguel \u00c1ngel Sanz-G\u00f3mez", "Jos\u00e9 Mar\u00eda Ben\u00edtez", "Francisco Javier Mont\u00e1ns"], "title": "Augmented Structure Preserving Neural Networks for cell biomechanics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cell biomechanics involve a great number of complex phenomena that are\nfundamental to the evolution of life itself and other associated processes,\nranging from the very early stages of embryo-genesis to the maintenance of\ndamaged structures or the growth of tumors. Given the importance of such\nphenomena, increasing research has been dedicated to their understanding, but\nthe many interactions between them and their influence on the decisions of\ncells as a collective network or cluster remain unclear. We present a new\napproach that combines Structure Preserving Neural Networks, which study cell\nmovements as a purely mechanical system, with other Machine Learning tools\n(Artificial Neural Networks), which allow taking into consideration\nenvironmental factors that can be directly deduced from an experiment with\nComputer Vision techniques. This new model, tested on simulated and real cell\nmigration cases, predicts complete cell trajectories following a roll-out\npolicy with a high level of accuracy. This work also includes a mitosis event\nprediction model based on Neural Networks architectures which makes use of the\nsame observed features.", "AI": {"tldr": "The paper develops a model combining Structure Preserving Neural Networks and other Machine Learning tools to study cell biomechanics, particularly cell migrations and mitosis events.", "motivation": "To understand complex biomechanical phenomena in cellular processes, including embryo-genesis, damage repair, and tumor growth, and address unclear interactions influencing cells as collective networks.", "method": "The approach integrates Structure Preserving Neural Networks for mechanical aspects of cell movements with Artificial Neural Networks to incorporate environmental factors derived from Computer Vision techniques. The model is tested on simulated and real cell migration data.", "result": "The proposed model achieves high accuracy in predicting cell trajectories and includes a robust mitosis event prediction using Neural Network architectures.", "conclusion": "This new approach effectively combines computational techniques to enhance predictive capabilities in the study of cell biomechanics, providing insights into both mechanical and environmental factors affecting cell behaviors."}}
{"id": "2509.05671", "pdf": "https://arxiv.org/pdf/2509.05671", "abs": "https://arxiv.org/abs/2509.05671", "authors": ["Labani Halder", "Tanmay Sen", "Sarbani Palit"], "title": "GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "comment": null, "summary": "Human Activity Recognition (HAR) using multimodal sensor data remains\nchallenging due to noisy or incomplete measurements, scarcity of labeled\nexamples, and privacy concerns. Traditional centralized deep learning\napproaches are often constrained by infrastructure availability, network\nlatency, and data sharing restrictions. While federated learning (FL) addresses\nprivacy by training models locally and sharing only model parameters, it still\nhas to tackle issues arising from the use of heterogeneous multimodal data and\ndifferential privacy requirements. In this article, a Graph-based Multimodal\nFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse\nsensor streams such as a pressure mat, depth camera, and multiple\naccelerometers are modeled as modality-specific graphs, processed through\nresidual Graph Convolutional Neural Networks (GCNs), and fused via\nattention-based weighting rather than simple concatenation. The fused\nembeddings enable robust activity classification, while differential privacy\nsafeguards data during federated aggregation. Experimental results show that\nthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with\nup to 2 percent higher accuracy in non-DP settings in both centralized and\nfederated paradigms. More importantly, significant improvements are observed\nunder differential privacy constraints: MultiModalGCN consistently surpasses\nMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on\nthe privacy budget and setting. These results highlight the robustness of\ngraph-based modeling in multimodal learning, where GNNs prove more resilient to\nthe performance degradation introduced by DP noise.", "AI": {"tldr": "This paper presents GraMFedDHAR, a graph-based multimodal federated learning framework for human activity recognition (HAR), focusing on privacy-preserving methods and robustness in handling heterogeneous sensor data.", "motivation": "The paper aims to address challenges in HAR such as noisy/incomplete sensor data, limited labeled data, and privacy concerns, while finding alternatives to conventional centralized deep learning systems affected by infrastructure, latency, and data-sharing limitations.", "method": "GraMFedDHAR uses modality-specific sensor graphs processed through residual Graph Convolutional Networks (GCNs), followed by attention-based fusion for classification. Differential privacy is applied during federated aggregation to enhance security.", "result": "GraMFedDHAR's MultiModalGCN model shows higher accuracy (up to 2% improvement) compared to MultiModalFFN in centralized and federated setups without DP. Under differential privacy, MultiModalGCN outperforms by 7-13%, showcasing robustness against DP noise.", "conclusion": "Graph-based modeling with GCNs and attention-based fusion enhances multimodal learning performance, especially under differential privacy constraints, making GraMFedDHAR a robust solution for privacy-preserving HAR tasks."}}
{"id": "2509.06552", "pdf": "https://arxiv.org/pdf/2509.06552", "abs": "https://arxiv.org/abs/2509.06552", "authors": ["Zheqi Lv", "Wenqiao Zhang", "Kairui Fu", "Qi Tian", "Shengyu Zhang", "Jiajie Su", "Jingyuan Chen", "Kun Kuang", "Fei Wu"], "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing", "categories": ["cs.LG", "cs.CV", "cs.DC", "cs.IR"], "comment": "Published on MM'25: Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.", "AI": {"tldr": "The paper proposes Persona, a backpropagation-free, prototype-based personalization method that addresses real-time data distribution shifts on devices without post-deployment retraining.", "motivation": "The study is motivated by the challenge of on-device real-time data distribution shifts that hinder lightweight models\u2019 generalization, a problem overlooked due to reliance on fine-tuning approaches.", "method": "Persona uses a neural adapter in the cloud to generate a parameter editing matrix based on device data, adapting on-device models to data distributions and dynamically refining through prototypes and cross-layer knowledge transfer.", "result": "Experiments on vision and recommendation tasks across multiple datasets show Persona effectively enhances generalization and adapts efficiently.", "conclusion": "Persona presents a lightweight, personalized, and generalizable solution, offering a practical approach to tackle on-device data shifts without retraining."}}
{"id": "2509.05723", "pdf": "https://arxiv.org/pdf/2509.05723", "abs": "https://arxiv.org/abs/2509.05723", "authors": ["Liansheng Wang", "Xinke Zhang", "Chenhui Li", "Dongjiao He", "Yihan Pan", "Jianjun Yi"], "title": "Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous\nsystems, yet its deployment on resource-constrained platforms remains\nchallenging due to computational and memory limitations. We propose Super-LIO,\na robust LIO system that demands both high performance and accuracy, ideal for\napplications such as aerial robots and mobile autonomous systems. At the core\nof Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that\nlimits each voxel to eight fused subvoxels, enabling strict point density\ncontrol and incremental denoising during map updates. This design enables a\nsimple yet efficient and accurate map structure, which can be easily integrated\ninto existing LIO frameworks. Additionally, Super-LIO designs a\nheuristic-guided KNN strategy (HKNN) that accelerates the correspondence search\nby leveraging spatial locality, further reducing runtime overhead. We evaluated\nthe proposed system using four publicly available datasets and several\nself-collected datasets, totaling more than 30 sequences. Extensive testing on\nboth X86 and ARM platforms confirms that Super-LIO offers superior efficiency\nand robustness, while maintaining competitive accuracy. Super-LIO processes\neach frame approximately 73% faster than SOTA, while consuming less CPU\nresources. The system is fully open-source and plug-and-play compatible with a\nwide range of LiDAR sensors and platforms. The implementation is available at:\nhttps://github.com/Liansheng-Wang/Super-LIO.git", "AI": {"tldr": "Super-LIO is a fast and robust LiDAR-Inertial Odometry (LIO) system featuring a compact map structure and an efficient search strategy, offering significant performance improvements over state-of-the-art methods.", "motivation": "Address the challenges of deploying LIO on resource-constrained platforms like aerial robots and mobile autonomous systems, due to computational and memory limitations.", "method": "Introduces a compact octo-voxel-based map structure (OctVox) for strict point density control and denoising, along with a heuristic-guided KNN strategy (HKNN) to enhance correspondence search efficiency.", "result": "Super-LIO processes frames 73% faster than state-of-the-art methods, with reduced CPU usage, while maintaining competitive accuracy, as verified across over 30 datasets on various platforms.", "conclusion": "Super-LIO is an efficient, robust, and accurate LIO system suitable for resource-constrained autonomous systems, provided as open-source and compatible across multiple sensors and platforms."}}
{"id": "2509.05657", "pdf": "https://arxiv.org/pdf/2509.05657", "abs": "https://arxiv.org/abs/2509.05657", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP2025", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "AI": {"tldr": "This paper introduces LM-Searcher, a flexible and cross-domain LLM-driven Neural Architecture Search (NAS) framework using a universal encoding method and reformulated ranking task.", "motivation": "Current LLM-driven NAS methods rely heavily on domain-specific tuning, limiting their applicability and scalability across diverse optimization tasks.", "method": "Developed LM-Searcher, featuring NCode (a universal architecture representation), a reformulated NAS as a ranking task, and a curated dataset for training LLMs via instruction-tuning and pruning-based sampling strategies.", "result": "LM-Searcher demonstrated strong performance in both in-domain (e.g., CNNs for classification) and out-of-domain (e.g., LoRA for segmentation) tasks, outperforming or competing with state-of-the-art methods.", "conclusion": "LM-Searcher provides a scalable, generalizable solution for NAS, enabling efficient cross-domain optimization without extensive domain-specific adaptations."}}
{"id": "2509.06429", "pdf": "https://arxiv.org/pdf/2509.06429", "abs": "https://arxiv.org/abs/2509.06429", "authors": ["Mehmet Bilal Er", "Nagehan \u0130lhan", "Umut Kuran"], "title": "Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction", "categories": ["cs.SE"], "comment": null, "summary": "The use of Large Language Models (LLMs) in software engineering tasks is\ngrowing, especially in the areas of bug fixing and code generation.\nNevertheless, these models often yield unstable results; when executed at\ndifferent times with the same input, they can generate radically different\ncode. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly\nassessed, despite the fact that this instability has typically been discussed\nin the literature in relation to code generation. The purpose of this study is\nto look into how unstable an LLM like ChatGPT is when it comes to fixing code\nbugs. We examine the structural, syntactic, and functional variations among\nseveral fix recommendations made in response to the same prompt using code\nsamples with various error types. Additionally, we assess how instability is\naffected by the temperature settings (0, 0.5, and 1) used for the model's\ndeterministic operation. For a total of 20 problems in the experimental\nanalysis, the model produced three fix suggestions at each temperature value,\ncomparing nine distinct outputs for each problem. The Syntax Similarity and\nOutput Equivalence Rate (OER) metrics were used to assess the outputs'\nstructural and functional consistency. The results demonstrate that the model's\noutputs become much more unstable and variable as the temperature rises, with\nhigh temperatures showing especially high rates of functional failure.\nAccording to syntax similarity analyses, the suggested fixes show notable\nstructural differences at high temperatures but are fairly similar at low\ntemperatures. The purpose of this study is to provide important methodological\ninsights into how LLM-based error correction systems can be applied more\nconsistently in software development processes while also casting doubt on\ntheir dependability.", "AI": {"tldr": "This study investigates the instability of Large Language Models (LLMs), like ChatGPT, in fixing software bugs, revealing that model outputs become notably inconsistent, especially at higher temperature settings.", "motivation": "To assess the stability and reliability of LLMs, particularly in their bug-fixing tasks, which have not been thoroughly studied despite their growing adoption in software engineering.", "method": "The study evaluates the variability in fix suggestions from LLMs under different temperature settings (0, 0.5, 1) using 20 problems. Outputs were compared using structural similarity and functional equivalence metrics.", "result": "Increased temperature settings result in significant variability and functional failures in outputs. Low temperatures yield more structurally and functionally consistent results.", "conclusion": "LLM-based bug-fixing systems are currently unreliable at high variability settings, and this work provides insights into improving their consistency in software development."}}
{"id": "2509.05818", "pdf": "https://arxiv.org/pdf/2509.05818", "abs": "https://arxiv.org/abs/2509.05818", "authors": ["Won Seok Jang", "Hieu Tran", "Manav Mistry", "SaiKiran Gandluri", "Yifan Zhang", "Sharmin Sultana", "Sunjae Kown", "Yuan Zhang", "Zonghai Yao", "Hong Yu"], "title": "Chatbot To Help Patients Understand Their Health", "categories": ["cs.AI"], "comment": "Accepted in EMNLP 2025 Findings", "summary": "Patients must possess the knowledge necessary to actively participate in\ntheir care. We present NoteAid-Chatbot, a conversational AI that promotes\npatient understanding via a novel 'learning as conversation' framework, built\non a multi-agent large language model (LLM) and reinforcement learning (RL)\nsetup without human-labeled data. NoteAid-Chatbot was built on a lightweight\nLLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on\nconversational data synthetically generated using medical conversation\nstrategies, followed by RL with rewards derived from patient understanding\nassessments in simulated hospital discharge scenarios. Our evaluation, which\nincludes comprehensive human-aligned assessments and case studies, demonstrates\nthat NoteAid-Chatbot exhibits key emergent behaviors critical for patient\neducation, such as clarity, relevance, and structured dialogue, even though it\nreceived no explicit supervision for these attributes. Our results show that\neven simple Proximal Policy Optimization (PPO)-based reward modeling can\nsuccessfully train lightweight, domain-specific chatbots to handle multi-turn\ninteractions, incorporate diverse educational strategies, and meet nuanced\ncommunication objectives. Our Turing test demonstrates that NoteAid-Chatbot\nsurpasses non-expert human. Although our current focus is on healthcare, the\nframework we present illustrates the feasibility and promise of applying\nlow-cost, PPO-based RL to realistic, open-ended conversational domains,\nbroadening the applicability of RL-based alignment methods.", "AI": {"tldr": "The paper presents NoteAid-Chatbot, a conversational AI for enhancing patient understanding of medical information, trained using lightweight language models and reinforcement learning (RL).", "motivation": "The motivation is to enable patients to actively participate in their healthcare by improving their understanding through conversational AI.", "method": "A multi-agent lightweight LLaMA model was trained via two steps: supervised fine-tuning with synthetically generated data and reinforcement learning using Proximal Policy Optimization (PPO).", "result": "The chatbot achieves clarity, relevance, and structured dialogue, outperforming non-expert humans in evaluations, despite no explicit supervision for these attributes.", "conclusion": "The study demonstrates that lightweight, domain-specific chatbots can effectively utilize cost-efficient RL techniques like PPO for realistic, open-ended multi-turn conversational domains beyond healthcare."}}
{"id": "2509.05431", "pdf": "https://arxiv.org/pdf/2509.05431", "abs": "https://arxiv.org/abs/2509.05431", "authors": ["GodsGift Uzor", "Tania-Amanda Nkoyo Fredrick Eneye", "Chukwuebuka Ijezue"], "title": "Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Brain tumor segmentation is a critical pre-processing step in the medical\nimage analysis pipeline that involves precise delineation of tumor regions from\nhealthy brain tissue in medical imaging data, particularly MRI scans. An\nefficient and effective decoding mechanism is crucial in brain tumor\nsegmentation especially in scenarios with limited computational resources.\nHowever these decoding mechanisms usually come with high computational costs.\nTo address this concern EMCAD a new efficient multi-scale convolutional\nattention decoder designed was utilized to optimize both performance and\ncomputational efficiency for brain tumor segmentation on the BraTs2020 dataset\nconsisting of MRI scans from 369 brain tumor patients. The preliminary result\nobtained by the model achieved a best Dice score of 0.31 and maintained a\nstable mean Dice score of 0.285 plus/minus 0.015 throughout the training\nprocess which is moderate. The initial model maintained consistent performance\nacross the validation set without showing signs of over-fitting.", "AI": {"tldr": "This paper introduces EMCAD, an optimized decoder for brain tumor segmentation in MRI scans, achieving moderate Dice scores while maintaining computational efficiency.", "motivation": "Existing decoding mechanisms for brain tumor segmentation are computationally costly, creating a need for efficient solutions.", "method": "The authors developed EMCAD, a multi-scale convolutional attention decoder, and tested its performance on the BraTs2020 dataset with MRI scans.", "result": "EMCAD achieved a best Dice score of 0.31 and a mean Dice score of 0.285\u00b10.015 during training, demonstrating stable performance without over-fitting.", "conclusion": "EMCAD provides a balance between segmentation accuracy and computational efficiency, showcasing its potential for resource-constrained medical imaging applications."}}
{"id": "2509.06588", "pdf": "https://arxiv.org/pdf/2509.06588", "abs": "https://arxiv.org/abs/2509.06588", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity", "categories": ["eess.SY", "cs.DC", "cs.SY", "eess.SP", "math.OC"], "comment": "Digital Signal Processing journal", "summary": "This paper considers automatic generation control over an information-sharing\nnetwork of communicating generators as a multi-agent system. The optimization\nsolution is distributed among the agents based on information consensus\nalgorithms, while addressing the generators' ramp-rate-limits (RRL). This is\ntypically ignored in the existing linear/nonlinear optimization solutions but\nthey exist in real-time power generation scenarios. Without addressing the RRL,\nthe generators cannot follow the assigned rate of generating power by the\noptimization algorithm; therefore, the existing solutions may not necessarily\nconverge to the exact optimal cost or may lose feasibility in practice. The\nproposed solution in this work addresses the ramp-rate-limit constraint along\nwith the box constraint (limits on the generated powers) and the\ncoupling-constraint (generation-demand balance) at all iteration times of the\nalgorithm. The latter is referred to as the anytime feasibility and implies\nthat at every termination point of the algorithm, the balance between the\ndemand and generated power holds. To improve the convergence rate of the\nalgorithm we further consider internal signum-based nonlinearity. We also show\nthat our solution can tolerate communication link removal. This follows from\nthe uniform-connectivity assumption on the communication network.", "AI": {"tldr": "The paper presents a distributed solution for automatic generation control over a network of generators, addressing ramp-rate-limits (RRL), typically neglected in existing solutions.", "motivation": "Existing optimization solutions for automatic generation control often ignore ramp-rate limits (RRL), which are crucial for real-time power generation scenarios.", "method": "A distributed optimization approach using information consensus algorithms, incorporating constraints such as RRL, box limits, and generation-demand balance (anytime feasibility).", "result": "The method ensures anytime feasibility of constraints, improves convergence rate with signum-based nonlinearity, and is tolerant to communication link removal under uniform-connectivity.", "conclusion": "The proposed approach offers a more practical and feasible solution for real-time power generation while enhancing algorithm robustness and convergence."}}
{"id": "2509.05777", "pdf": "https://arxiv.org/pdf/2509.05777", "abs": "https://arxiv.org/abs/2509.05777", "authors": ["Zhihao Lin", "Zhen Tian"], "title": "Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey", "categories": ["cs.RO"], "comment": "This paper provides a comprehensive review for scenario-based\n  game-theoretic methods", "summary": "Game-based interactive driving simulations have emerged as versatile\nplatforms for advancing decision-making algorithms in road transport mobility.\nWhile these environments offer safe, scalable, and engaging settings for\ntesting driving strategies, ensuring both realism and robust performance amid\ndynamic and diverse scenarios remains a significant challenge. Recently, the\nintegration of game-based techniques with advanced learning frameworks has\nenabled the development of adaptive decision-making models that effectively\nmanage the complexities inherent in varied driving conditions. These models\noutperform traditional simulation methods, especially when addressing\nscenario-specific challenges, ranging from obstacle avoidance on highways and\nprecise maneuvering during on-ramp merging to navigation in roundabouts,\nunsignalized intersections, and even the high-speed demands of autonomous\nracing. Despite numerous innovations in game-based interactive driving, a\nsystematic review comparing these approaches across different scenarios is\nstill missing. This survey provides a comprehensive evaluation of game-based\ninteractive driving methods by summarizing recent advancements and inherent\nroadway features in each scenario. Furthermore, the reviewed algorithms are\ncritically assessed based on their adaptation of the standard game model and an\nanalysis of their specific mechanisms to understand their impact on\ndecision-making performance. Finally, the survey discusses the limitations of\ncurrent approaches and outlines promising directions for future research.", "AI": {"tldr": "The paper surveys advancements in game-based driving simulations for decision-making, evaluates these methods across different scenarios, and discusses limitations and future directions.", "motivation": "The motivation is to address the lack of a systematic review comparing game-based interactive driving approaches across diverse driving scenarios.", "method": "The authors summarize advancements in game-based driving methods, assess the algorithms' mechanism specific to scenarios, and critically evaluate their decision-making performance.", "result": "The survey highlights advancements, identifies challenges, and uncovers the gaps and limitations in current game-based driving simulations for decision-making.", "conclusion": "The paper concludes with a discussion of the limitations of existing methods and suggests promising directions for future research in game-based driving simulations."}}
{"id": "2509.05660", "pdf": "https://arxiv.org/pdf/2509.05660", "abs": "https://arxiv.org/abs/2509.05660", "authors": ["Hong Su"], "title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse.", "AI": {"tldr": "The paper introduces a method to enable large language models (LLMs) to reuse solutions for questions with low or hidden similarities, overcoming traditional similarity constraints.", "motivation": "To enable LLMs to reuse methods for diverse questions, especially when questions exhibit low or hidden similarities, going beyond conventional constraints.", "method": "The method involves separating question and solution pairs, guiding the LLM to adapt solutions to related questions, even for those with partial or hidden similarities.", "result": "The approach improves the likelihood of filtering reusable solutions, enhancing the practicality of cross-question solution reuse.", "conclusion": "This method extends the applicability of LLMs in method reuse by addressing limitations of similarity-based constraints, improving their effectiveness in diverse scenarios."}}
{"id": "2509.06530", "pdf": "https://arxiv.org/pdf/2509.06530", "abs": "https://arxiv.org/abs/2509.06530", "authors": ["Sylvain Gu\u00e9rin", "Salvador Martinez", "Ciprian Teodorov"], "title": "Modeling in the Design Multiverse", "categories": ["cs.SE", "D.2.10"], "comment": null, "summary": "Real-world design processes often involve the evolution and divergence of\ndesign paths (by branching, revising, merging, etc.), especially when multiple\nstakeholders or teams operate concurrently and/or explore different\nalternatives for complex and heterogeneous systems. Unfortunately, this\nvariability in time and space can not be directly managed in current modeling\nspaces but requires resorting to external tools and methodologies.\n  In order to tackle this problem, we introduce the Design Multiverse. The\nDesign Multiverse aims to integrate in the modeling space a selection of\nrevisions and variants, representing snapshots of a design state composed of\nmultiple artifacts. This enables stakeholders to seamlessly trace, analyze, and\nmanage design decisions, system variants, and their interdependencies.\nConcretely, in this paper we present a conceptual definition of the Design\nMultiverse, discuss usage scenarios such as model product lines and\nmodel/metamodel co-evolution, and propose an implementation leveraging the\nmodel federation paradigm.", "AI": {"tldr": "The paper introduces the Design Multiverse, a methodology to manage evolving and divergent design paths within the modeling space without relying on external tools.", "motivation": "Address the challenge of managing variability in design paths across time and space directly within the modeling space, eliminating the need for external tools.", "method": "The authors propose the Design Multiverse conceptual framework and implement it using the model federation paradigm for managing revisions, variants, and interdependencies in design processes.", "result": "The Design Multiverse integrates revisions and variants into the modeling space for stakeholders to trace, analyze, and manage design decisions and system evolution seamlessly.", "conclusion": "The Design Multiverse enhances modeling capabilities, facilitating better management of design variability, especially in complex systems."}}
{"id": "2509.05766", "pdf": "https://arxiv.org/pdf/2509.05766", "abs": "https://arxiv.org/abs/2509.05766", "authors": ["Jiaju Miao", "Wei Zhu"], "title": "Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Anomaly detection underpins critical applications from network security and\nintrusion detection to fraud prevention, where recognizing aberrant patterns\nrapidly is indispensable. Progress in this area is routinely impeded by two\nobstacles: extreme class imbalance and the curse of dimensionality. To combat\nthe former, we previously introduced Precision-Recall Curve (PRC)\nclassification trees and their ensemble extension, the PRC Random Forest\n(PRC-RF). Building on that foundation, we now propose a hybrid framework that\nintegrates PRC-RF with autoencoders, unsupervised machine learning methods that\nlearn compact latent representations, to confront both challenges\nsimultaneously. Extensive experiments across diverse benchmark datasets\ndemonstrate that the resulting Autoencoder-PRC-RF model achieves superior\naccuracy, scalability, and interpretability relative to prior methods,\naffirming its potential for high-stakes anomaly-detection tasks.", "AI": {"tldr": "The paper introduces Autoencoder-PRC-RF, a hybrid framework for anomaly detection that addresses class imbalance and high dimensionality using PRC Random Forests and autoencoders.", "motivation": "The authors aim to improve anomaly detection by addressing challenges like extreme class imbalance and high dimensionality.", "method": "The proposed hybrid framework combines PRC Random Forests with autoencoders to simultaneously handle class imbalance and dimensionality reduction.", "result": "The Autoencoder-PRC-RF model outperforms existing methods in terms of accuracy, scalability, and interpretability on benchmark datasets.", "conclusion": "The Autoencoder-PRC-RF framework is a promising solution for critical anomaly detection tasks, offering significant improvements over traditional methods."}}
{"id": "2509.05933", "pdf": "https://arxiv.org/pdf/2509.05933", "abs": "https://arxiv.org/abs/2509.05933", "authors": ["Md Hasebul Hasan", "Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration", "categories": ["cs.AI", "I.2.7"], "comment": "27 Pages", "summary": "Agentic AI has significantly extended the capabilities of large language\nmodels (LLMs) by enabling complex reasoning and tool use. However, most\nexisting frameworks are tailored to domains such as mathematics, coding, or web\nautomation, and fall short on geospatial tasks that require spatial reasoning,\nmulti-hop planning, and real-time map interaction. To address these challenges,\nwe introduce MapAgent, a hierarchical multi-agent plug-and-play framework with\ncustomized toolsets and agentic scaffolds for map-integrated geospatial\nreasoning. Unlike existing flat agent-based approaches that treat tools\nuniformly-often overwhelming the LLM when handling similar but subtly different\ngeospatial APIs-MapAgent decouples planning from execution. A high-level\nplanner decomposes complex queries into subgoals, which are routed to\nspecialized modules. For tool-heavy modules-such as map-based services-we then\ndesign a dedicated map-tool agent that efficiently orchestrates related APIs\nadaptively in parallel to effectively fetch geospatial data relevant for the\nquery, while simpler modules (e.g., solution generation or answer extraction)\noperate without additional agent overhead. This hierarchical design reduces\ncognitive load, improves tool selection accuracy, and enables precise\ncoordination across similar APIs. We evaluate MapAgent on four diverse\ngeospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and\nMapQA-and demonstrate substantial gains over state-of-the-art tool-augmented\nand agentic baselines. We open-source our framwork at\nhttps://github.com/Hasebul/MapAgent.", "AI": {"tldr": "MapAgent is a hierarchical multi-agent framework designed for geospatial reasoning tasks, enhancing large language models by providing specialized tools for efficient map interaction and planning.", "motivation": "To address the gap in existing agentic AI frameworks which excel in general tasks but struggle with geospatial reasoning, planning, and real-time map-related interactions.", "method": "The authors propose a hierarchical multi-agent framework with a high-level planner and specialized modules, including a dedicated map-tool agent to handle geospatial APIs efficiently. It decouples planning from execution and uses custom toolsets for diverse geospatial benchmarks.", "result": "MapAgent demonstrated significant performance improvements over existing baselines in four geospatial benchmarks: MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA.", "conclusion": "MapAgent effectively bridges the gap in geospatial reasoning, demonstrating superior efficiency and accuracy in handling complex queries while reducing cognitive load. The framework is open-sourced for broader applications."}}
{"id": "2509.05441", "pdf": "https://arxiv.org/pdf/2509.05441", "abs": "https://arxiv.org/abs/2509.05441", "authors": ["Tejaswini Medi", "Hsien-Yi Wang", "Arianna Rampini", "Margret Keuper"], "title": "FAVAE-Effective Frequency Aware Latent Tokenizer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, the reconstructed images\noften lack realism, particularly in textured regions with sharp transitions,\ndue to loss of fine details governed by high frequencies. We conduct a detailed\nfrequency decomposition of existing state-of-the-art (SOTA) latent tokenizers\nand show that conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency\ninformation, when jointly optimized, leading to over-smoothed outputs and\nvisual artifacts that diminish perceptual quality. To address this, we propose\na wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework\nthat explicitly decouples the optimization of low- and high-frequency\ncomponents. This decoupling enables improved reconstruction of fine textures\nwhile preserving global structure. Our approach bridges the fidelity gap in\ncurrent latent tokenizers and emphasizes the importance of frequency-aware\noptimization for realistic image representation, with broader implications for\napplications in content creation, neural rendering, and medical imaging.", "AI": {"tldr": "This paper introduces a frequency-aware variational autoencoder (FA-VAE) to improve image generation quality by addressing high-frequency detail loss in latent embeddings.", "motivation": "Latent generative models often lose fine details in high-frequency regions of images, leading to unrealistic textures and over-smoothed outputs.", "method": "The authors propose a wavelet-based FA-VAE framework, which decouples the optimization of low- and high-frequency components to improve image reconstruction quality.", "result": "The proposed FA-VAE bridges the fidelity gap in latent tokenizers, reconstructs fine textures more effectively, and enhances overall image realism.", "conclusion": "Frequency-aware optimization is crucial for realistic image representation, with broad applications in content creation, neural rendering, and medical imaging."}}
{"id": "2509.05697", "pdf": "https://arxiv.org/pdf/2509.05697", "abs": "https://arxiv.org/abs/2509.05697", "authors": ["Iara Cunha", "Marcos Eduardo Valle"], "title": "Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure", "categories": ["cs.LG", "math.OC"], "comment": "Submitted to the 4th International Conference on Discrete Geometry\n  and Mathematical Morphology (DGMM 2025)", "summary": "A morphological perceptron is a multilayer feedforward neural network in\nwhich neurons perform elementary operations from mathematical morphology. For\nmulticlass classification tasks, a morphological perceptron with a competitive\nlayer (MPCL) is obtained by integrating a winner-take-all output layer into the\nstandard morphological architecture. The non-differentiability of morphological\noperators renders gradient-based optimization methods unsuitable for training\nsuch networks. Consequently, alternative strategies that do not depend on\ngradient information are commonly adopted. This paper proposes the use of the\nconvex-concave procedure (CCP) for training MPCL networks. The training problem\nis formulated as a difference of convex (DC) functions and solved iteratively\nusing CCP, resulting in a sequence of linear programming subproblems.\nComputational experiments demonstrate the effectiveness of the proposed\ntraining method in addressing classification tasks with MPCL networks.", "AI": {"tldr": "The paper proposes using the convex-concave procedure (CCP) to train morphological perceptron networks (MPCL), solving non-differentiable optimization with iterative linear programming.", "motivation": "Training morphological perceptron networks is challenging because their non-differentiable nature makes gradient-based optimization methods unsuitable.", "method": "The training problem for MPCL networks is formulated as a difference of convex (DC) functions, and the convex-concave procedure (CCP) is applied iteratively to solve it through linear programming subproblems.", "result": "The experimentation validates the proposed method's effectiveness for classification tasks with MPCL networks.", "conclusion": "CCP provides a viable solution for training morphological perceptrons, paving the way for improved classification models using this approach."}}
{"id": "2509.05923", "pdf": "https://arxiv.org/pdf/2509.05923", "abs": "https://arxiv.org/abs/2509.05923", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "title": "eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the visual-inertial setup is commonly\nadopted due to complementary characteristics between sensors (e.g., scale\nperception and low drift). For optimal event-based visual-inertial fusion,\naccurate spatiotemporal (extrinsic and temporal) calibration is required. In\nthis work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator\nfor event-based visual-inertial systems, utilizing the widely used circle grid\nboard. Building upon the grid pattern recognition and tracking methods in\neKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and\nefficient initialization, where all parameters in the estimator would be\naccurately recovered. Subsequently, a continuous-time-based batch optimization\nis conducted to refine the initialized parameters toward better states. The\nresults of extensive real-world experiments show that eKalibr-Inertial can\nachieve accurate event-based visual-inertial spatiotemporal calibration. The\nimplementation of eKalibr-Inertial is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "AI": {"tldr": "The paper introduces eKalibr-Inertial, a spatiotemporal calibration method for event-based visual-inertial systems, demonstrating its accuracy through real-world experiments.", "motivation": "To improve the accuracy of spatiotemporal calibration in event-based visual-inertial systems, crucial for applications like motion estimation and robotic perception.", "method": "The method involves an initialization phase for recovering parameters followed by a continuous-time batch optimization, utilizing a circle grid board for calibration.", "result": "Real-world experiments confirm that the proposed method achieves precise spatiotemporal calibration for event-based systems.", "conclusion": "eKalibr-Inertial provides an effective calibration solution, and its open-source implementation is expected to aid further research in the community."}}
{"id": "2509.05668", "pdf": "https://arxiv.org/pdf/2509.05668", "abs": "https://arxiv.org/abs/2509.05668", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "categories": ["cs.CL", "cs.AI"], "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "AI": {"tldr": "The paper introduces Llama-GENBA-10B, a 10-billion parameter trilingual foundation model focusing on English, German, and low-resource Bavarian languages.", "motivation": "To address the English-centric bias in large language models and promote Bavarian as a low-resource language within multilingual NLP.", "method": "The method involves curated multilingual training data, a unified tokenizer, optimized model architecture, cross-lingual hyperparameters, and the creation of a trilingual evaluation suite.", "result": "Llama-GENBA-10B achieves competitive cross-lingual performance, outperforming state-of-the-art models like Apertus-8B-2509 and gemma-2-9b in Bavarian and equaling EuroLLM in German. It is also energy-efficient.", "conclusion": "The model serves as a scalable blueprint for developing inclusive multilingual foundation models, particularly for low-resource languages."}}
{"id": "2509.06688", "pdf": "https://arxiv.org/pdf/2509.06688", "abs": "https://arxiv.org/abs/2509.06688", "authors": ["Heerok Banerjee"], "title": "Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool", "categories": ["cs.SE"], "comment": null, "summary": "Domain-specific languages (DSLs) play a crucial role in resolving internal\ndependencies across enterprises and boosts their upfront business management\nprocesses. Yet, a lot of development is needed to build modelling frameworks\nwhich support graphical interfaces (canvas, pallettes etc.), hierarchical\nstructures and easy implementation to shorten the gap for novice users. In this\npaper, a DSL namely, Bmod is introduced, which can be used to model evacuation\nscenarios. The language is built using Eclipse Modelling Framework (EMF) and\nEclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also\nshown between Eclipse EMF/GMF and other modelling tools such as AToMPM,\nmetaDepth, Sirius etc with respect to expressiveness, learning curve and\nperformance.", "AI": {"tldr": "This paper introduces a DSL called Bmod, designed for modeling evacuation scenarios, created using Eclipse EMF and GMF, and compares it to similar tools.", "motivation": "The paper seeks to address the lack of user-friendly graphical modeling tools with hierarchical structures to support novice users in building domain-specific languages.", "method": "The authors introduce Bmod, which is developed using Eclipse Modelling Framework (EMF) and Eclipse Graphical Modelling Framework (GMF), and perform a comparative analysis with other modeling tools.", "result": "Bmod is shown to be an effective tool for modeling evacuation scenarios, with its expressiveness, learning curve, and performance compared favorably to alternatives.", "conclusion": "Bmod addresses the identified limitations in existing tools and demonstrates potential in improving business management processes for novice users through its user-friendly framework."}}
{"id": "2509.05778", "pdf": "https://arxiv.org/pdf/2509.05778", "abs": "https://arxiv.org/abs/2509.05778", "authors": ["Arantxa Urrea-Casta\u00f1o", "Nicol\u00e1s Segura-Kunsagi", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2"], "comment": "20 pages and appendix", "summary": "Out-of-distribution (OOD) detection plays a key role in enhancing the\nrobustness of artificial intelligence systems by identifying inputs that differ\nsignificantly from the training distribution, thereby preventing unreliable\npredictions and enabling appropriate fallback mechanisms. Developing reliable\nOOD detection methods is a significant challenge, and rigorous evaluation of\nthese techniques is essential for ensuring their effectiveness, as it allows\nresearchers to assess their performance under diverse conditions and to\nidentify potential limitations or failure modes. Cross-validation (CV) has\nproven to be a highly effective tool for providing a reasonable estimate of the\nperformance of a learning algorithm. Although OOD scenarios exhibit particular\ncharacteristics, an appropriate adaptation of CV can lead to a suitable\nevaluation framework for this setting. This work proposes a dual CV framework\nfor robust evaluation of OOD detection models, aimed at improving the\nreliability of their assessment. The proposed evaluation framework aims to\neffectively integrate in-distribution (ID) and OOD data while accounting for\ntheir differing characteristics. To achieve this, ID data are partitioned using\na conventional approach, whereas OOD data are divided by grouping samples based\non their classes. Furthermore, we analyze the context of data with class\nhierarchy to propose a data splitting that considers the entire class hierarchy\nto obtain fair ID-OOD partitions to apply the proposed evaluation framework.\nThis framework is called Dual Cross-Validation for Robust Out-of-Distribution\nDetection (DCV-ROOD). To test the validity of the evaluation framework, we\nselected a set of state-of-the-art OOD detection methods, both with and without\noutlier exposure. The results show that the method achieves very fast\nconvergence to the true performance.", "AI": {"tldr": "This paper introduces a new evaluation framework called DCV-ROOD for improving the reliability of Out-of-Distribution (OOD) detection models, using dual cross-validation methods.", "motivation": "The robustness of AI systems depends on effectively detecting OOD inputs, which ensures reliable predictions and prevents failures. Proper evaluation methods are needed to assess OOD detection performance under varied conditions.", "method": "The paper proposes a dual cross-validation (DCV) framework, where in-distribution (ID) data is split using standard methods, and OOD data is divided considering class hierarchy and class grouping for fair evaluation.", "result": "The experimental results demonstrate the fast convergence of DCV-ROOD to the true performance metrics of selected state-of-the-art OOD detection methods.", "conclusion": "DCV-ROOD offers a robust evaluation framework that integrates ID and OOD data effectively, enhancing the reliability of assessments for existing OOD detection techniques."}}
{"id": "2509.06024", "pdf": "https://arxiv.org/pdf/2509.06024", "abs": "https://arxiv.org/abs/2509.06024", "authors": ["Haoyang He", "Zihua Rong", "Kun Ji", "Chenyang Li", "Qing Huang", "Chong Xia", "Lan Yang", "Honggang Zhang"], "title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has recently become the dominant paradigm for\nstrengthening the reasoning abilities of large language models (LLMs). Yet the\nrule-based reward functions commonly used on mathematical or programming\nbenchmarks assess only answer format and correctness, providing no signal as to\nwhether the induced Chain-of-Thought (CoT) actually improves the answer.\nFurthermore, such task-specific training offers limited control over logical\ndepth and therefore may fail to reveal a model's genuine reasoning capacity. We\npropose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward\nframework that reshapes both reward and advantage signals. (i) A Reasoning\nQuality Reward assigns fine-grained credit to those reasoning chains that\ndemonstrably raise the likelihood of the correct answer, directly incentivising\nthe trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage\ndecays the advantage of responses whose length deviates from a\nvalidation-derived threshold, stabilising training. To facilitate rigorous\nassessment, we also release Logictree, a dynamically constructed deductive\nreasoning dataset that functions both as RL training data and as a\ncomprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B\nmodel attains GPT-o3-mini level performance on Logictree with 400 trianing\nsteps, while the average confidence of CoT-augmented answers rises by 30%. The\nmodel further exhibits generalisation across diverse logical-reasoning\ndatasets, and the mathematical benchmark AIME24. These results illuminate how\nRL shapes CoT behaviour and chart a practical path toward enhancing\nformal-reasoning skills in large language models. All code and data are\navailable in repository https://github.com/Henryhe09/DRER.", "AI": {"tldr": "The paper introduces Dynamic Reasoning Efficiency Reward (DRER), a novel reinforcement learning framework designed to enhance reasoning abilities in large language models (LLMs) by improving chain-of-thought (CoT) quality and stabilizing training lengths. It includes the Logictree dataset for comprehensive benchmarking.", "motivation": "Existing rule-based reward systems for LLMs focus on output correctness but neglect the quality of the reasoning process, limiting control over logical depth and underutilizing the model's reasoning capacity.", "method": "DRER implements two components: (i) Reasoning Quality Reward focuses on rewarding reasoning paths that increase correct-answer likelihood, incentivizing productive CoT; (ii) Dynamic Length Advantage penalizes responses deviating from a length threshold to stabilize training. Logictree dataset complements the framework as training data and benchmark.", "result": "When tested on the Logictree benchmark, DRER improved CoT-based reasoning confidence by 30%, and a 7B model achieved GPT-o3-mini-level performance within just 400 training steps. It also generalized well across other logical reasoning datasets and AIME24.", "conclusion": "DRER effectively shapes reasoning behavior in LLMs, enhancing their formal-reasoning skills, and provides a practical approach to improving logic-based performance in tasks. It\u2019s paired with open-source code and data for accessibility."}}
{"id": "2509.05446", "pdf": "https://arxiv.org/pdf/2509.05446", "abs": "https://arxiv.org/abs/2509.05446", "authors": ["Iftekhar Haider Chowdhury", "Zaed Ikbal Syed", "Ahmed Faizul Haque Dhrubo", "Mohammad Abdul Qayum"], "title": "Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's", "categories": ["cs.CV"], "comment": "This paper includes figures and two tables, and our work outperforms\n  the existing research that has been published in a journal", "summary": "Deep Convolutional Neural Networks have achieved state of the art performance\nacross various computer vision tasks, however their practical deployment is\nlimited by computational and memory overhead. This paper introduces\nDifferential Sensitivity Fusion Pruning, a novel single shot filter pruning\nframework that focuses on evaluating the stability and redundancy of filter\nimportance scores across multiple criteria. Differential Sensitivity Fusion\nPruning computes a differential sensitivity score for each filter by fusing the\ndiscrepancies among gradient based sensitivity, first order Taylor expansion,\nand KL divergence of activation distributions. An exponential scaling mechanism\nis applied to emphasize filters with inconsistent importance across metrics,\nidentifying candidates that are structurally unstable or less critical to the\nmodel performance. Unlike iterative or reinforcement learning based pruning\nstrategies, Differential Sensitivity Fusion Pruning is efficient and\ndeterministic, requiring only a single forward-backward pass for scoring and\npruning. Extensive experiments across varying pruning rates between 50 to 70\npercent demonstrate that Differential Sensitivity Fusion Pruning significantly\nreduces model complexity, achieving over 80 percent Floating point Operations\nPer Seconds reduction while maintaining high accuracy. For instance, at 70\npercent pruning, our approach retains up to 98.23 percent of baseline accuracy,\nsurpassing traditional heuristics in both compression and generalization. The\nproposed method presents an effective solution for scalable and adaptive Deep\nConvolutional Neural Networks compression, paving the way for efficient\ndeployment on edge and mobile platforms.", "AI": {"tldr": "Differential Sensitivity Fusion Pruning is a novel filter pruning method that reduces computational complexity in neural networks while maintaining high accuracy, suitable for mobile and edge devices.", "motivation": "While Deep Convolutional Neural Networks perform well, they are computationally heavy, limiting their use in practical scenarios like on edge or mobile platforms.", "method": "This method evaluates filter importance stability using multiple criteria (gradient sensitivity, Taylor expansion, KL divergence), computes a combined differential sensitivity score, applies exponential scaling to emphasize inconsistencies, and prunes filters in a single forward-backward pass.", "result": "The approach achieves 50-70% pruning rates, reduces floating-point operations by over 80%, and retains up to 98.23% of model accuracy, outperforming traditional methods.", "conclusion": "Differential Sensitivity Fusion Pruning offers an efficient, scalable solution for compressing and deploying neural networks, improving both performance and adaptability."}}
{"id": "2509.05732", "pdf": "https://arxiv.org/pdf/2509.05732", "abs": "https://arxiv.org/abs/2509.05732", "authors": ["Lenart Treven", "Bhavya Sukhija", "Jonas Rothfuss", "Stelian Coros", "Florian D\u00f6rfler", "Andreas Krause"], "title": "Simulation Priors for Data-Efficient Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "How do we enable AI systems to efficiently learn in the real-world?\nFirst-principles models are widely used to simulate natural systems, but often\nfail to capture real-world complexity due to simplifying assumptions. In\ncontrast, deep learning approaches can estimate complex dynamics with minimal\nassumptions but require large, representative datasets. We propose SimPEL, a\nmethod that efficiently combines first-principles models with data-driven\nlearning by using low-fidelity simulators as priors in Bayesian deep learning.\nThis enables SimPEL to benefit from simulator knowledge in low-data regimes and\nleverage deep learning's flexibility when more data is available, all the while\ncarefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse\nsystems, including biological, agricultural, and robotic domains, showing\nsuperior performance in learning complex dynamics. For decision-making, we\ndemonstrate that SimPEL bridges the sim-to-real gap in model-based\nreinforcement learning. On a high-speed RC car task, SimPEL learns a highly\ndynamic parking maneuver involving drifting with substantially less data than\nstate-of-the-art baselines. These results highlight the potential of SimPEL for\ndata-efficient learning and control in complex real-world environments.", "AI": {"tldr": "SimPEL integrates first-principles models with data-driven learning using Bayesian deep learning to achieve data-efficient and uncertainty-aware learning in complex real-world systems.", "motivation": "To address the limitations of both first-principles models, which cannot capture real-world complexity, and deep learning, which requires large datasets, by combining their strengths.", "method": "SimPEL employs low-fidelity simulators as priors in Bayesian deep learning, allowing the system to utilize simulator knowledge in low-data scenarios while leveraging deep learning's capabilities with increasing data.", "result": "SimPEL outperformed state-of-the-art methods in diverse fields such as biology, agriculture, and robotics, and effectively reduced the sim-to-real gap for model-based reinforcement learning, exhibiting superior performance in tasks like high-speed RC car maneuvers.", "conclusion": "SimPEL demonstrates strong potential for efficient learning in complex environments, offering a robust balance between simulator-driven and data-driven approaches while carefully quantifying uncertainty."}}
{"id": "2509.06031", "pdf": "https://arxiv.org/pdf/2509.06031", "abs": "https://arxiv.org/abs/2509.06031", "authors": ["Junhui Huang", "Yuhe Gong", "Changsheng Li", "Xingguang Duan", "Luis Figueredo"], "title": "ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction", "categories": ["cs.RO"], "comment": null, "summary": "We present ZLATTE, a geometry-aware, learning-free framework for\nlanguage-driven trajectory reshaping in human-robot interaction. Unlike prior\nlearning-based methods, ZLATTE leverages Vision-Language Models to register\nobjects as geometric primitives and employs a Large Language Model to translate\nnatural language instructions into explicit geometric and kinematic\nconstraints. These constraints are integrated into a potential field\noptimization to adapt initial trajectories while preserving feasibility and\nsafety. A multi-agent strategy further enhances robustness under complex or\nconflicting commands. Simulation and real-world experiments demonstrate that\nZLATTE achieves smoother, safer, and more interpretable trajectory\nmodifications compared to state-of-the-art baselines.", "AI": {"tldr": "ZLATTE enables language-driven trajectory reshaping without requiring training, using vision and language models for geometric constraints, and demonstrating improved performance over existing methods.", "motivation": "To create a robust approach for adjusting robot trajectories using natural language inputs while addressing safety and feasibility without learning-based methods.", "method": "The framework uses Vision-Language Models for object registration, Large Language Models for translating instructions into constraints, and a potential field optimization for trajectory adjustments. A multi-agent approach handles complex commands.", "result": "Simulation and real-world tests show ZLATTE produces safer, smoother, and more interpretable trajectory modifications compared to other methods.", "conclusion": "ZLATTE proves successful in reshaping robot trajectories via natural language, offering advantages in safety, interpretability, and performance over existing solutions."}}
{"id": "2509.05691", "pdf": "https://arxiv.org/pdf/2509.05691", "abs": "https://arxiv.org/abs/2509.05691", "authors": ["Ningyuan Deng", "Hanyu Duan", "Yixuan Tang", "Yi Yang"], "title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content.", "AI": {"tldr": "The study evaluates the ability of text embedding models to handle numerical information accurately and finds limitations in their performance.", "motivation": "With increasing applications of embedding models in fields like finance and healthcare, numerical understanding is critical, yet rarely evaluated systematically.", "method": "The authors test 13 popular text embedding models on synthetic financial data to analyze their performance in capturing numerical nuances.", "result": "The models generally fail to accurately represent numerical information in text.", "conclusion": "Text embedding models need improvement to handle numerical content effectively, and the study provides insights to guide future research in this direction."}}
{"id": "2509.05864", "pdf": "https://arxiv.org/pdf/2509.05864", "abs": "https://arxiv.org/abs/2509.05864", "authors": ["Jiachun Li", "Kaining Shi", "David Simchi-Levi"], "title": "Beyond ATE: Multi-Criteria Design for A/B Testing", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "A/B testing is a widely adopted methodology for estimating conditional\naverage treatment effects (CATEs) in both clinical trials and online platforms.\nWhile most existing research has focused primarily on maximizing estimation\naccuracy, practical applications must also account for additional\nobjectives-most notably welfare or revenue loss. In many settings, it is\ncritical to administer treatments that improve patient outcomes or to implement\nplans that generate greater revenue from customers. Within a machine learning\nframework, such objectives are naturally captured through the notion of\ncumulative regret. In this paper, we investigate the fundamental trade-off\nbetween social welfare loss and statistical accuracy in (adaptive) experiments\nwith heterogeneous treatment effects. We establish matching upper and lower\nbounds for the resulting multi-objective optimization problem and employ the\nconcept of Pareto optimality to characterize the necessary and sufficient\nconditions for optimal experimental designs. Beyond estimating CATEs,\npractitioners often aim to deploy treatment policies that maximize welfare\nacross the entire population. We demonstrate that our Pareto-optimal adaptive\ndesign achieves optimal post-experiment welfare, irrespective of the\nin-experiment trade-off between accuracy and welfare. Furthermore, since\nclinical and commercial data are often highly sensitive, it is essential to\nincorporate robust privacy guarantees into any treatment-allocation mechanism.\nTo this end, we develop differentially private algorithms that continue to\nachieve our established lower bounds, showing that privacy can be attained at\nnegligible cost.", "AI": {"tldr": "The paper explores adaptive experimentation to balance statistical accuracy and social welfare loss, presenting Pareto-optimal designs for optimal treatment allocation and integrating privacy through differential algorithms.", "motivation": "The motivation is to address the challenges in balancing estimation accuracy with welfare/revenue considerations in adaptive experiments, particularly for treatments that improve patient outcomes or maximize revenue.", "method": "Using a machine learning approach, the paper introduces Pareto-optimal designs to tackle the trade-offs in adaptive experiments, and develops differentially private algorithms to ensure privacy with minimal cost.", "result": "The study provides upper and lower bounds for multi-objective optimization, ensuring optimal post-experiment welfare and demonstrating negligible cost for incorporating privacy into treatment allocation mechanisms.", "conclusion": "The approach achieves optimal welfare outcomes with robust privacy guarantees, advancing the design of adaptive experiments that account for practical objectives like social welfare and accuracy."}}
{"id": "2509.06160", "pdf": "https://arxiv.org/pdf/2509.06160", "abs": "https://arxiv.org/abs/2509.06160", "authors": ["Haozhe Wang", "Haoran Que", "Qixin Xu", "Minghao Liu", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Wei Ye", "Tong Yang", "Wenhao Huang", "Ge Zhang", "Fangzhen Lin"], "title": "Reverse-Engineered Reasoning for Open-Ended Generation", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.", "AI": {"tldr": "The paper introduces a new paradigm called REverse-Engineered Reasoning (REER) to address limitations in current methods for deep reasoning in open-ended tasks, achieving competitive results with leading AI models.", "motivation": "Current methods for deep reasoning, such as reinforcement learning and instruction distillation, have limitations when applied to open-ended and creative generation tasks.", "method": "The REER paradigm works by reverse-engineering known-good solutions to computationally uncover the latent reasoning steps, avoiding the need for trial-and-error or expensive instruction distillation.", "result": "The authors curated a dataset, DeepWriting-20K, containing 20,000 reasoning examples. They trained DeepWriter-8B on this data, achieving performance comparable to or better than models like GPT-4o and Claude 3.5.", "conclusion": "REER offers a scalable, gradient-free approach to deep reasoning, demonstrating that reverse-engineering reasoning processes can outperform traditional forward-building methods in creative and open-ended tasks."}}
{"id": "2509.05483", "pdf": "https://arxiv.org/pdf/2509.05483", "abs": "https://arxiv.org/abs/2509.05483", "authors": ["Jinhao Wang", "Florian Vogl", "Pascal Sch\u00fctz", "Sa\u0161a \u0106ukovi\u0107", "William R. Taylor"], "title": "Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging", "categories": ["cs.CV"], "comment": "This work has been accepted at MICCAI 2025", "summary": "Veriserum is an open-source dataset designed to support the training of deep\nlearning registration for dual-plane fluoroscopic analysis. It comprises\napproximately 110,000 X-ray images of 10 knee implant pair combinations (2\nfemur and 5 tibia implants) captured during 1,600 trials, incorporating poses\nassociated with daily activities such as level gait and ramp descent. Each\nimage is annotated with an automatically registered ground-truth pose, while\n200 images include manually registered poses for benchmarking.\n  Key features of Veriserum include dual-plane images and calibration tools.\nThe dataset aims to support the development of applications such as 2D/3D image\nregistration, image segmentation, X-ray distortion correction, and 3D\nreconstruction. Freely accessible, Veriserum aims to advance computer vision\nand medical imaging research by providing a reproducible benchmark for\nalgorithm development and evaluation. The Veriserum dataset used in this study\nis publicly available via\nhttps://movement.ethz.ch/data-repository/veriserum.html, with the data stored\nat ETH Z\\\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.", "AI": {"tldr": "Veriserum provides 110,000 X-ray images of knee implants for deep learning applications, offering automatic and manual annotations for pose analysis.", "motivation": "To aid in improving medical imaging analysis and algorithm development through a robust, open-source dataset for computer vision tasks.", "method": "The dataset features dual-plane X-ray images of knee implants with annotated poses collected in diverse trials and activities.", "result": "Offers annotated X-ray images, along with calibration tools, enabling benchmarking and development for 2D/3D registration and other medical imaging applications.", "conclusion": "Veriserum supports reproducible research by being a publicly available benchmark resource, promoting advancements in medical imaging and computer vision."}}
{"id": "2509.05735", "pdf": "https://arxiv.org/pdf/2509.05735", "abs": "https://arxiv.org/abs/2509.05735", "authors": ["Jiaqi Chen", "Ji Shi", "Cansu Sancaktar", "Jonas Frey", "Georg Martius"], "title": "Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at Reinforcement Learning Conference (RLC 2025); Code\n  available at: https://github.com/swsychen/Offline_vs_Online_in_MBRL", "summary": "Data collection is crucial for learning robust world models in model-based\nreinforcement learning. The most prevalent strategies are to actively collect\ntrajectories by interacting with the environment during online training or\ntraining on offline datasets. At first glance, the nature of learning\ntask-agnostic environment dynamics makes world models a good candidate for\neffective offline training. However, the effects of online vs. offline data on\nworld models and thus on the resulting task performance have not been\nthoroughly studied in the literature. In this work, we investigate both\nparadigms in model-based settings, conducting experiments on 31 different\nenvironments. First, we showcase that online agents outperform their offline\ncounterparts. We identify a key challenge behind performance degradation of\noffline agents: encountering Out-Of-Distribution states at test time. This\nissue arises because, without the self-correction mechanism in online agents,\noffline datasets with limited state space coverage induce a mismatch between\nthe agent's imagination and real rollouts, compromising policy training. We\ndemonstrate that this issue can be mitigated by allowing for additional online\ninteractions in a fixed or adaptive schedule, restoring the performance of\nonline training with limited interaction data. We also showcase that\nincorporating exploration data helps mitigate the performance degradation of\noffline agents. Based on our insights, we recommend adding exploration data\nwhen collecting large datasets, as current efforts predominantly focus on\nexpert data alone.", "AI": {"tldr": "The paper discusses the differences between offline and online data collection methods in model-based reinforcement learning, finding that online methods typically perform better due to their ability to self-correct Out-Of-Distribution errors.", "motivation": "To understand the impact of online vs. offline data collection methods on the performance of world models in model-based reinforcement learning tasks.", "method": "Conducted experiments across 31 environments, analyzed performance degradation factors for offline agents, and proposed solutions like adaptive scheduling of online interaction and using exploration data.", "result": "Online strategies outperform offline ones, as offline models suffer from Out-Of-Distribution mismatches. Incorporating additional online interactions and exploration data ameliorates this gap.", "conclusion": "Hybrid approaches combining online interaction with exploration data collection can improve the efficiency and robustness of world models in reinforcement learning."}}
{"id": "2509.06048", "pdf": "https://arxiv.org/pdf/2509.06048", "abs": "https://arxiv.org/abs/2509.06048", "authors": ["Yi Dong", "Yangjun Liu", "Jinjun Duan", "Yang Li", "Zhendong Dai"], "title": "Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness", "categories": ["cs.RO"], "comment": "Yi Dong and Yangjun Liu contributed equally to the work. Accepted by\n  Robotics and Autonomous Systems.\n  https://authors.elsevier.com/c/1lgjX3HdG3supQ", "summary": "With the rapid development of the warehousing and logistics industries, the\npacking of goods has gradually attracted the attention of academia and\nindustry. The packing of footwear products is a typical representative\npaired-item packing task involving irregular shapes and deformable objects.\nAlthough studies on shoe packing have been conducted, different initial states\ndue to the irregular shapes of shoes and standard packing placement poses have\nnot been considered. This study proposes a robotic manipulation framework,\nincluding a perception module, reorientation planners, and a packing planner,\nthat can complete the packing of pairs of shoes in any initial state. First, to\nadapt to the large intraclass variations due to the state, shape, and\ndeformation of the shoe, we propose a vision module based on semantic\nkeypoints, which can also infer more information such as size, state, pose, and\nmanipulation points by combining geometric features. Subsequently, we not only\nproposed primitive-based reorientation methods for different states of a single\ndeformable shoe but also proposed a fast reorientation method for the top state\nusing box edge contact and gravity, which further improved the efficiency of\nreorientation. Finally, based on the perception module and reorientation\nmethods, we propose a task planner for shoe pair packing in any initial state\nto provide an optimal packing strategy. Real-world experiments were conducted\nto verify the robustness of the reorientation methods and the effectiveness of\nthe packing strategy for various types of shoes. In this study, we highlight\nthe potential of semantic keypoint representation methods, introduce new\nperspectives on the reorientation of 3D deformable objects and multi-object\nmanipulation, and provide a reference for paired object packing.", "AI": {"tldr": "This study proposes a robotic framework for packing shoe pairs under varied initial conditions, integrating vision, reorientation, and planning modules, validated through experiments.", "motivation": "To address the challenges in packing irregular and deformable footwear items by developing strategies to handle intraclass variations, irregular shapes, and diverse initial states.", "method": "The paper introduces a framework comprising a vision module for semantic keypoint-based perception, state-specific reorientation planners, and a task planner to optimize shoe pair packing from any initial condition.", "result": "Experimental validations showed the proposed framework's robustness in reorienting and efficiently packing footwear of various types.", "conclusion": "The study demonstrates the effectiveness of semantic keypoints for 3D deformable object manipulation, offering new insights and strategies for paired-object packing tasks."}}
{"id": "2509.05716", "pdf": "https://arxiv.org/pdf/2509.05716", "abs": "https://arxiv.org/abs/2509.05716", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Fahmida Islam", "Maryam Tahermazandarani", "Quan Z. Sheng"], "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages, 12 figures, 4 tables", "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.", "AI": {"tldr": "This survey paper reviews the state-of-the-art in Conversational Question Answering (ConvQA), covering system components, machine learning techniques, language models, and datasets, and detailing future research areas.", "motivation": "ConvQA systems are increasingly important for enabling coherent, context-aware conversations across domains like customer support, education, and healthcare.", "method": "The paper examines core ConvQA system components, advanced machine learning techniques (e.g., reinforcement learning, transfer learning), large language models, and key datasets, offering analysis and insights.", "result": "It highlights the interplay of core ConvQA components, impact of large language models on scalability and architecture, and suggests advancements in addressing technical challenges.", "conclusion": "The survey provides a foundational understanding of ConvQA, identifies gaps, and outlines open research directions for improving accuracy and efficiency in multi-turn conversations."}}
{"id": "2509.06774", "pdf": "https://arxiv.org/pdf/2509.06774", "abs": "https://arxiv.org/abs/2509.06774", "authors": ["Hridoy Sankar Dutta", "Sana Ansari", "Swati Kumari", "Shounak Ravi Bhalerao"], "title": "OpenCoderRank: AI-Driven Technical Assessments Made Easy", "categories": ["cs.SE"], "comment": null, "summary": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments.", "AI": {"tldr": "The paper introduces 'OpenCoderRank,' a platform for simulating technical assessments, catering to the needs of both problem setters and solvers.", "motivation": "To address the dual challenge where Large Language Models assist in creating challenging questions but also compromise assessment integrity by offering solutions.", "method": "Developed OpenCoderRank, a customizable, free platform to simulate technical assessments that supports both problem preparation and solving.", "result": "The platform allows problem solvers to practice under time constraints and unfamiliar scenarios while enabling setters to host their assessments effectively.", "conclusion": "OpenCoderRank serves as an accessible tool that benefits both sides of the assessment process, particularly in environments with limited resources."}}
{"id": "2509.05865", "pdf": "https://arxiv.org/pdf/2509.05865", "abs": "https://arxiv.org/abs/2509.05865", "authors": ["Rishabh Dixit", "Yuan Hui", "Rayan Saab"], "title": "The Measure of Deception: An Analysis of Data Forging in Machine Unlearning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Motivated by privacy regulations and the need to mitigate the effects of\nharmful data, machine unlearning seeks to modify trained models so that they\neffectively ``forget'' designated data. A key challenge in verifying unlearning\nis forging -- adversarially crafting data that mimics the gradient of a target\npoint, thereby creating the appearance of unlearning without actually removing\ninformation. To capture this phenomenon, we consider the collection of data\npoints whose gradients approximate a target gradient within tolerance\n$\\epsilon$ -- which we call an $\\epsilon$-forging set -- and develop a\nframework for its analysis. For linear regression and one-layer neural\nnetworks, we show that the Lebesgue measure of this set is small. It scales on\nthe order of $\\epsilon$, and when $\\epsilon$ is small enough, $\\epsilon^d$.\nMore generally, under mild regularity assumptions, we prove that the forging\nset measure decays as $\\epsilon^{(d-r)/2}$, where $d$ is the data dimension and\n$r<d$ is the nullity of a variation matrix defined by the model gradients.\nExtensions to batch SGD and almost-everywhere smooth loss functions yield the\nsame asymptotic scaling. In addition, we establish probability bounds showing\nthat, under non-degenerate data distributions, the likelihood of randomly\nsampling a forging point is vanishingly small. These results provide evidence\nthat adversarial forging is fundamentally limited and that false unlearning\nclaims can, in principle, be detected.", "AI": {"tldr": "This paper tackles machine unlearning, introducing a framework to analyze 'forging,' the adversarial mimicking of gradient data to falsely simulate unlearning.", "motivation": "The paper aims to address privacy needs and harmful data issues by ensuring models can effectively \"forget\" data while combating adversarial misuse.", "method": "It analyzes the forging set (points mimicking target gradients) for linear regression and neural networks, deriving scaling laws and probability bounds under various assumptions.", "result": "The measure of forging sets is shown to decay asymptotically as $\\epsilon^{(d-r)/2}$, and random sampling likelihood is negligible for non-degenerate data.", "conclusion": "Adversarial forging is inherently constrained, enabling the detection of false unlearning claims in practical scenarios."}}
{"id": "2509.06174", "pdf": "https://arxiv.org/pdf/2509.06174", "abs": "https://arxiv.org/abs/2509.06174", "authors": ["Wei Han", "Geng Zhan", "Sicheng Yu", "Chenyu Wang", "Bryan Hooi"], "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains", "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 5 figures, 7 tables", "summary": "O1/R1 style large reasoning models (LRMs) signal a substantial leap forward\nover conventional instruction-following LLMs. By applying test-time scaling to\ngenerate extended reasoning paths, they establish many SOTAs across a wide\nrange of complex reasoning tasks. However, recent studies show that LRMs are\nprone to suffer from overthinking -- the tendency to overcomplicate simple\nproblems, leading to excessive strategy switching and long, convoluted\nreasoning traces that hinder their interpretability. To mitigate this issue, we\nconduct a systematic investigation into the reasoning efficiency of a broad set\nof LRMs and uncover a common dilemma: the difficulty in balancing multiple\ngeneration objectives such as correctness and brevity. Based on this discovery,\nwe propose a test-time scaling method, EDIT (Efficient Dynamic Inference\nTrimming), which efficiently guides LRMs to identify the shortest correct\nreasoning paths at test time. EDIT employs constraint-guided generation while\njointly tracking length and answer distributions under varying constraints,\nallowing it to select responses that strike an optimal balance between\nconciseness and correctness. Extensive experiments across diverse models and\ndatasets show that EDIT substantially enhance the reasoning efficiency,\nproducing compact yet informative outputs that improve readability and user\nexperience.", "AI": {"tldr": "The paper addresses inefficiency in reasoning due to overthinking by LRMs and introduces EDIT, a method to guide these models toward concise, correct reasoning paths.", "motivation": "Large Reasoning Models (LRMs) hold promise in complex tasks but suffer from overthinking, leading to interpretability challenges and compromised reasoning efficiency.", "method": "EDIT is introduced as a test-time scaling approach that employs constraint-guided generation, tracking both length and answer distributions to find the shortest correct reasoning paths.", "result": "EDIT enhances reasoning efficiency in LRMs across diverse models and datasets, producing concise yet informative outputs.", "conclusion": "EDIT successfully mitigates overthinking in LRMs, balancing correctness and brevity to improve interpretability and user experience."}}
{"id": "2509.05490", "pdf": "https://arxiv.org/pdf/2509.05490", "abs": "https://arxiv.org/abs/2509.05490", "authors": ["Andrzej D. Dobrzycki", "Ana M. Bernardos", "Jos\u00e9 R. Casar"], "title": "An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.4.8; I.2.6"], "comment": "31 pages, 14 figures, 9 tables", "summary": "The You Only Look Once (YOLO) architecture is crucial for real-time object\ndetection. However, deploying it in resource-constrained environments such as\nunmanned aerial vehicles (UAVs) requires efficient transfer learning. Although\nlayer freezing is a common technique, the specific impact of various freezing\nconfigurations on contemporary YOLOv8 and YOLOv10 architectures remains\nunexplored, particularly with regard to the interplay between freezing depth,\ndataset characteristics, and training dynamics. This research addresses this\ngap by presenting a detailed analysis of layer-freezing strategies. We\nsystematically investigate multiple freezing configurations across YOLOv8 and\nYOLOv10 variants using four challenging datasets that represent critical\ninfrastructure monitoring. Our methodology integrates a gradient behavior\nanalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper\ninsights into training dynamics under different freezing strategies. Our\nresults reveal that there is no universal optimal freezing strategy but,\nrather, one that depends on the properties of the data. For example, freezing\nthe backbone is effective for preserving general-purpose features, while a\nshallower freeze is better suited to handling extreme class imbalance. These\nconfigurations reduce graphics processing unit (GPU) memory consumption by up\nto 28% compared to full fine-tuning and, in some cases, achieve mean average\nprecision (mAP@50) scores that surpass those of full fine-tuning. Gradient\nanalysis corroborates these findings, showing distinct convergence patterns for\nmoderately frozen models. Ultimately, this work provides empirical findings and\npractical guidelines for selecting freezing strategies. It offers a practical,\nevidence-based approach to balanced transfer learning for object detection in\nscenarios with limited resources.", "AI": {"tldr": "The paper investigates layer-freezing strategies for YOLOv8 and YOLOv10 architectures in resource-restricted environments, emphasizing their effectiveness on different datasets and training dynamics.", "motivation": "To identify the specific impact of layer-freezing configurations on YOLOv8 and YOLOv10 architectures and their suitability in resource-constrained environments like UAVs.", "method": "The study analyzes multiple layer-freezing strategies across critical datasets, using methods like L2 norm for gradient behavior and Grad-CAM for visual explanations to understand training dynamics.", "result": "Findings indicate no universally optimal freezing strategy; the best approach depends on dataset characteristics. Strategies can reduce GPU memory by 28% and improve performance, outperforming full fine-tuning in certain cases.", "conclusion": "The research delivers empirical findings and practical guidance for using layer-freezing in transfer learning, optimizing object detection for limited-resource scenarios."}}
{"id": "2509.06061", "pdf": "https://arxiv.org/pdf/2509.06061", "abs": "https://arxiv.org/abs/2509.06061", "authors": ["Faiza Babakano", "Ahmed Fahmin", "Bojie Shen", "Muhammad Aamir Cheema", "Isma Farah Siddiqui"], "title": "Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain", "categories": ["cs.RO", "cs.DB"], "comment": null, "summary": "Autonomous Mobile Robots (AMRs) operate on battery power, making energy\nefficiency a critical consideration, particularly in outdoor environments where\nterrain variations affect energy consumption. While prior research has\nprimarily focused on computing energy-efficient paths from a source to a\ndestination, these approaches often overlook practical scenarios where a robot\nneeds to pick up an object en route - an action that can significantly impact\nenergy consumption due to changes in payload. This paper introduces the\nObject-Pickup Minimum Energy Path Problem (OMEPP), which addresses\nenergy-efficient route planning for AMRs required to pick up an object from one\nof many possible locations and deliver it to a destination. To address OMEPP,\nwe first introduce a baseline algorithm that employs the Z star algorithm, a\nvariant of A star tailored for energy-efficient routing, to iteratively visit\neach pickup point. While this approach guarantees optimality, it suffers from\nhigh computational cost due to repeated searches at each pickup location. To\nmitigate this inefficiency, we propose a concurrent PCPD search that manages\nmultiple Z star searches simultaneously across all pickup points. Central to\nour solution is the Payload-Constrained Path Database (PCPD), an extension of\nthe Compressed Path Database (CPD) that incorporates payload constraints. We\ndemonstrate that PCPD significantly reduces branching factors during search,\nimproving overall performance. Although the concurrent PCPD search may produce\nslightly suboptimal solutions, extensive experiments on real-world datasets\nshow it achieves near-optimal performance while being one to two orders of\nmagnitude faster than the baseline algorithm.", "AI": {"tldr": "This paper introduces algorithms for energy-efficient path planning in Autonomous Mobile Robots (AMRs) that pick up objects en route, focusing on the Object-Pickup Minimum Energy Path Problem (OMEPP).", "motivation": "The paper aims to address energy-efficient path planning for AMRs operating in outdoor environments, specifically focusing on scenarios where robots pick up objects mid-route, a challenge not sufficiently addressed in prior studies.", "method": "The authors proposed a baseline algorithm using the Z star algorithm and a concurrent PCPD search, incorporating a Payload-Constrained Path Database (PCPD) to optimize path planning.", "result": "The concurrent PCPD search achieves near-optimal performance and is one to two orders of magnitude faster than the baseline algorithm, as proven through experiments on real-world datasets.", "conclusion": "While the concurrent PCPD search may slightly compromise optimality, it substantially improves computational efficiency for AMRs with object-pickup tasks, making it a practical solution for real-world applications."}}
{"id": "2509.05719", "pdf": "https://arxiv.org/pdf/2509.05719", "abs": "https://arxiv.org/abs/2509.05719", "authors": ["Donya Rooein", "Flor Miriam Plaza-del-Arco", "Debora Nozza", "Dirk Hovy"], "title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP.", "AI": {"tldr": "The study explores challenges in Farsi NLP, particularly for sentiment, emotion, and toxicity tasks, uncovering issues like limited datasets and unstable predictive model results.", "motivation": "Investigate and highlight the challenges specific to Farsi, a middle-resource language, in advancing NLP capabilities for subjective tasks such as sentiment and emotion analysis.", "method": "Reviewing 110 publications, examining dataset availability and demographic inclusivity, and evaluating model performance for subjective NLP tasks in Farsi.", "result": "Findings highlight insufficient data volume, poor dataset quality, lack of demographic factors in datasets, and instability in model predictions.", "conclusion": "Improving Farsi's NLP performance requires more comprehensive, diverse, and high-quality datasets along with better representation of demographic factors."}}
{"id": "2509.06911", "pdf": "https://arxiv.org/pdf/2509.06911", "abs": "https://arxiv.org/abs/2509.06911", "authors": ["Margarida Ferreira", "Victor Nicolet", "Luan Pham", "Joey Dodds", "Daniel Kroening", "Ines Lynce", "Ruben Martins"], "title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline.", "AI": {"tldr": "HyGLAD is a new algorithm that creates patterns for anomaly detection in event data, emphasizing interpretability and efficiency.", "motivation": "The need to model event data for anomaly detection in stationary systems where deviations could indicate malicious activities.", "method": "The method involves inferring equivalence classes of entities with similar behavior to create interpretable regular expressions for anomaly detection.", "result": "HyGLAD outperformed seven deep-learning methods on five real-world datasets in both accuracy (1.2x precision, 1.3x recall) and efficiency (single CPU vs GPU).", "conclusion": "HyGLAD offers a more interpretable and computationally efficient alternative to deep-learning approaches for anomaly detection in event data."}}
{"id": "2509.05922", "pdf": "https://arxiv.org/pdf/2509.05922", "abs": "https://arxiv.org/abs/2509.05922", "authors": ["Peilin Rao", "Randall R. Rojas"], "title": "Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation", "categories": ["q-fin.ST", "econ.EM", "stat.ML", "91G80, 62P05", "J.4"], "comment": "Working Paper. 68 pages, 10 figures, 16 tables. Keywords: Causal\n  Inference, Machine Learning, Financial Econometrics, Market Microstructure,\n  Asset Pricing", "summary": "This paper provides robust, new evidence on the causal drivers of market\ntroughs. We demonstrate that conclusions about these triggers are critically\nsensitive to model specification, moving beyond restrictive linear models with\na flexible DML average partial effect causal machine learning framework. Our\nrobust estimates identify the volatility of options-implied risk appetite and\nmarket liquidity as key causal drivers, relationships misrepresented or\nobscured by simpler models. These findings provide high-frequency empirical\nsupport for intermediary asset pricing theories. This causal analysis is\nenabled by a high-performance nowcasting model that accurately identifies\ncapitulation events in real-time.", "AI": {"tldr": "The paper uses a causal machine learning framework to identify key drivers of market troughs, emphasizing options-implied risk appetite and market liquidity.", "motivation": "The motivation is to identify and better understand the causal factors behind market troughs, addressing limitations in linear models and incorporating advanced techniques.", "method": "The paper uses a flexible machine learning framework (DML average partial effect) to provide robust causal estimates and employs a high-performance nowcasting model to analyze capitulation events.", "result": "Key causal drivers of market troughs identified in the study are options-implied risk appetite volatility and market liquidity.", "conclusion": "The findings support intermediary asset pricing theories and highlight the value of advanced models for analyzing financial markets in high-frequency settings."}}
{"id": "2509.06235", "pdf": "https://arxiv.org/pdf/2509.06235", "abs": "https://arxiv.org/abs/2509.06235", "authors": ["Olivier Schipper", "Yudi Zhang", "Yali Du", "Mykola Pechenizkiy", "Meng Fang"], "title": "PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.6; I.2.8"], "comment": "for the source code, see https://github.com/aialt/PillagerBench", "summary": "LLM-based agents have shown promise in various cooperative and strategic\nreasoning tasks, but their effectiveness in competitive multi-agent\nenvironments remains underexplored. To address this gap, we introduce\nPillagerBench, a novel framework for evaluating multi-agent systems in\nreal-time competitive team-vs-team scenarios in Minecraft. It provides an\nextensible API, multi-round testing, and rule-based built-in opponents for\nfair, reproducible comparisons. We also propose TactiCrafter, an LLM-based\nmulti-agent system that facilitates teamwork through human-readable tactics,\nlearns causal dependencies, and adapts to opponent strategies. Our evaluation\ndemonstrates that TactiCrafter outperforms baseline approaches and showcases\nadaptive learning through self-play. Additionally, we analyze its learning\nprocess and strategic evolution over multiple game episodes. To encourage\nfurther research, we have open-sourced PillagerBench, fostering advancements in\nmulti-agent AI for competitive environments.", "AI": {"tldr": "PillagerBench evaluates competitive multi-agent systems in Minecraft, introducing TactiCrafter, an LLM-based system excelling in teamwork and adaptive strategies.", "motivation": "Effective evaluation of competitive multi-agent systems, particularly team-vs-team scenarios, remains largely unexplored.", "method": "Developed PillagerBench, a Minecraft-based framework for evaluation, and introduced TactiCrafter, an LLM-driven system focusing on teamwork, adaptation, and learning causal dependencies.", "result": "TactiCrafter surpassed baseline solutions, demonstrating adaptive learning and strategic evolution via self-play.", "conclusion": "PillagerBench and TactiCrafter enable advancements in competitive multi-agent systems, fostering innovation through open-source availability."}}
{"id": "2509.05512", "pdf": "https://arxiv.org/pdf/2509.05512", "abs": "https://arxiv.org/abs/2509.05512", "authors": ["Bryce Grant", "Peng Wang"], "title": "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IROS 2025", "summary": "This paper introduces Quaternion Approximate Networks (QUAN), a novel deep\nlearning framework that leverages quaternion algebra for rotation equivariant\nimage classification and object detection. Unlike conventional quaternion\nneural networks attempting to operate entirely in the quaternion domain, QUAN\napproximates quaternion convolution through Hamilton product decomposition\nusing real-valued operations. This approach preserves geometric properties\nwhile enabling efficient implementation with custom CUDA kernels. We introduce\nIndependent Quaternion Batch Normalization (IQBN) for training stability and\nextend quaternion operations to spatial attention mechanisms. QUAN is evaluated\non image classification (CIFAR-10/100, ImageNet), object detection (COCO,\nDOTA), and robotic perception tasks. In classification tasks, QUAN achieves\nhigher accuracy with fewer parameters and faster convergence compared to\nexisting convolution and quaternion-based models. For objection detection, QUAN\ndemonstrates improved parameter efficiency and rotation handling over standard\nConvolutional Neural Networks (CNNs) while establishing the SOTA for quaternion\nCNNs in this downstream task. These results highlight its potential for\ndeployment in resource-constrained robotic systems requiring rotation-aware\nperception and application in other domains.", "AI": {"tldr": "QUAN is a novel framework using quaternion algebra and real-valued operations for rotation-equivariant image classification and object detection.", "motivation": "To address the limitations of conventional quaternion neural networks and Convolutional Neural Networks in rotation-equivariant tasks.", "method": "Using Hamilton product decomposition for quaternion convolution and introducing IQBN, spatial attention mechanisms, evaluated on classification and detection tasks.", "result": "Achieved higher accuracy, faster convergence, improved rotation handling, and set SOTA for quaternion CNNs in object detection.", "conclusion": "QUAN is efficient and suitable for rotation-aware applications, particularly in resource-constrained environments like robotics."}}
{"id": "2509.05768", "pdf": "https://arxiv.org/pdf/2509.05768", "abs": "https://arxiv.org/abs/2509.05768", "authors": ["Chen Shao", "Yue Wang", "Zhenyi Zhu", "Zhanbo Huang", "Sebastian P\u00fctz", "Benjamin Sch\u00e4fer", "Tobais K\u00e4fer", "Michael F\u00e4rber"], "title": "Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "4 pages, CIKM 2025", "summary": "Energy forecasting is vital for grid reliability and operational efficiency.\nAlthough recent advances in time series forecasting have led to progress,\nexisting benchmarks remain limited in spatial and temporal scope and lack\nmulti-energy features. This raises concerns about their reliability and\napplicability in real-world deployment. To address this, we present the Real-E\ndataset, covering over 74 power stations across 30+ European countries over a\n10-year span with rich metadata. Using Real- E, we conduct an extensive data\nanalysis and benchmark over 20 baselines across various model types. We\nintroduce a new metric to quantify shifts in correlation structures and show\nthat existing methods struggle on our dataset, which exhibits more complex and\nnon-stationary correlation dynamics. Our findings highlight key limitations of\ncurrent methods and offer a strong empirical basis for building more robust\nforecasting models", "AI": {"tldr": "The paper presents the Real-E dataset, addressing gaps in energy forecasting benchmarks with wider spatial and temporal coverage, and highlights limitations in current forecasting methods.", "motivation": "To address limitations in existing energy forecasting benchmarks which lack extensive spatial and temporal scope and multi-energy features, reducing their real-world applicability.", "method": "The authors introduce the Real-E dataset, spanning 74 power stations in 30+ countries over 10 years, perform comprehensive data analysis, evaluate over 20 baseline models, and develop a metric to assess correlation structure shifts.", "result": "Their analysis shows that existing forecasting methods struggle with the Real-E dataset due to its complex, non-stationary correlation dynamics, providing key insights into these methods' limitations.", "conclusion": "The study underscores the need for robust forecasting models by showcasing the insufficiencies of current methods and providing a rich dataset (Real-E) for addressing these challenges."}}
{"id": "2509.06115", "pdf": "https://arxiv.org/pdf/2509.06115", "abs": "https://arxiv.org/abs/2509.06115", "authors": ["Runjiao Bao", "Lin Zhang", "Tianwei Niu", "Haoyu Yuan", "Shoukun Wang"], "title": "Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Four-wheel independent steering (4WIS) systems provide mobile robots with a\nrich set of motion modes, such as Ackermann steering, lateral steering, and\nparallel movement, offering superior maneuverability in constrained\nenvironments. However, existing path planning methods generally assume a single\nkinematic model and thus fail to fully exploit the multi-modal capabilities of\n4WIS platforms. To address this limitation, we propose an extended Hybrid A*\nframework that operates in a four-dimensional state space incorporating both\nspatial states and motion modes. Within this framework, we design multi-modal\nReeds-Shepp curves tailored to the distinct kinematic constraints of each\nmotion mode, develop an enhanced heuristic function that accounts for\nmode-switching costs, and introduce a terminal connection strategy with\nintelligent mode selection to ensure smooth transitions between different\nsteering patterns. The proposed planner enables seamless integration of\nmultiple motion modalities within a single path, significantly improving\nflexibility and adaptability in complex environments. Results demonstrate\nsignificantly improved planning performance for 4WIS robots in complex\nenvironments.", "AI": {"tldr": "The paper presents an extended Hybrid A* path planning method for 4WIS mobile robots, enabling enhanced motion modes and flexibility in complex environments.", "motivation": "Current path planning methods for 4WIS robots fail to utilize their multi-modal motion capabilities due to reliance on single kinematic models.", "method": "The authors propose a four-dimensional Hybrid A* framework, with multi-modal Reeds-Shepp curves, an improved heuristic, and a mode-switching terminal connection strategy.", "result": "The proposed planner enhances flexibility and adaptability, demonstrating significant improvements in planning performance for 4WIS robots in constrained spaces.", "conclusion": "Extending the Hybrid A* approach successfully exploits the diverse motion modes of 4WIS systems, offering practical solutions for complex navigation challenges."}}
{"id": "2509.05729", "pdf": "https://arxiv.org/pdf/2509.05729", "abs": "https://arxiv.org/abs/2509.05729", "authors": ["Charles M. Varmantchaonala", "Niclas G\u00d6tting", "Nils-Erik Sch\u00dctte", "Jean Louis E. K. Fendji", "Christopher Gies"], "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "categories": ["cs.CL"], "comment": null, "summary": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.", "AI": {"tldr": "This paper introduces QCSE, a quantum context-sensitive embedding model leveraging quantum computation for handling complex linguistic context in NLP.", "motivation": "To address the complexity of natural language understanding and mitigate data scarcity issues for low-resource languages, by leveraging quantum computation's expressibility.", "method": "QCSE employs quantum-native context learning with five innovative context matrix computation methods, including exponential decay and sinusoidal modulation.", "result": "The model demonstrates context sensitivity in both Fulani (low-resource) and English corpora, showing quantum systems' potential in linguistic tasks.", "conclusion": "Quantum computation has promising applications in improving NLP tasks, especially in low-resource language contexts, opening pathways for further exploration in QNLP."}}
{"id": "2509.05643", "pdf": "https://arxiv.org/pdf/2509.05643", "abs": "https://arxiv.org/abs/2509.05643", "authors": ["Carmine Cesarano", "Roberto Natella"], "title": "FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Coverage-guided fuzzing has been widely applied to address zero-day\nvulnerabilities in general-purpose software and operating systems. This\napproach relies on instrumenting the target code at compile time. However,\napplying it to industrial systems remains challenging, due to proprietary and\nclosed-source compiler toolchains and lack of access to source code. FuzzBox\naddresses these limitations by integrating emulation with fuzzing: it\ndynamically instruments code during execution in a virtualized environment, for\nthe injection of fuzz inputs, failure detection, and coverage analysis, without\nrequiring source code recompilation and hardware-specific dependencies. We show\nthe effectiveness of FuzzBox through experiments in the context of a\nproprietary MILS (Multiple Independent Levels of Security) hypervisor for\nindustrial applications. Additionally, we analyze the applicability of FuzzBox\nacross commercial IoT firmware, showcasing its broad portability.", "AI": {"tldr": "FuzzBox enables coverage-guided fuzzing without source code or proprietary compiler requirements by dynamically instrumenting code in a virtualized environment.", "motivation": "Address the challenges posed by closed-source, proprietary toolchains, and lack of source code in applying coverage-guided fuzzing to industrial systems.", "method": "Integrates emulation and fuzzing by dynamically instrumenting code execution in a virtualized environment, eliminating the need for recompilation or hardware-specific dependencies.", "result": "Demonstrated effectiveness on a proprietary MILS hypervisor and extended applicability to commercial IoT firmware.", "conclusion": "FuzzBox is a versatile, source-independent fuzzing framework suitable for industrial systems and IoT environments."}}
{"id": "2509.05930", "pdf": "https://arxiv.org/pdf/2509.05930", "abs": "https://arxiv.org/abs/2509.05930", "authors": ["Ali Zeynali", "Mahsa Sahebdel", "Qingsong Liu", "Mohammad Hajiesmaili", "Ramesh K. Sitaraman"], "title": "Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "comment": "10 pages, 14 pages appendix", "summary": "We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)\nproblem, a new framework that integrates three key objectives in online\ndecision-making under uncertainty: (1) tracking cost for following a\ndynamically moving target, (2) adversarial perturbation cost for withstanding\nunpredictable disturbances, and (3) switching cost for penalizing abrupt\nchanges in decisions. This formulation captures real-world scenarios such as\nelastic and inelastic workload scheduling in AI clusters, where operators must\nbalance long-term service-level agreements (e.g., LLM training) against sudden\ndemand spikes (e.g., real-time inference). We first present BEST, a robust\nalgorithm with provable competitive guarantees for SOOTT. To enhance practical\nperformance, we introduce CoRT, a learning-augmented variant that incorporates\nuntrusted black-box predictions (e.g., from ML models) into its decision\nprocess. Our theoretical analysis shows that CoRT strictly improves over BEST\nwhen predictions are accurate, while maintaining robustness under arbitrary\nprediction errors. We validate our approach through a case study on workload\nscheduling, demonstrating that both algorithms effectively balance trajectory\ntracking, decision smoothness, and resilience to external disturbances.", "AI": {"tldr": "The paper introduces a new problem called Smoothed Online Optimization for Target Tracking (SOOTT), proposes two algorithms (BEST and CoRT) for solving it, and examines its application in workload scheduling.", "motivation": "The paper aims to address practical challenges in online decision-making under uncertainty by integrating tracking cost, adversarial perturbation cost, and switching cost into a unified framework.", "method": "The authors present BEST, a robust algorithm with provable guarantees, and CoRT, a learning-augmented version that incorporates potentially inaccurate black-box predictions to improve decision-making.", "result": "Theoretical and case study validations show that CoRT outperforms BEST when predictions are accurate and maintains robustness under errors, while both algorithms balance trajectory tracking, smoothness, and resilience effectively.", "conclusion": "BEST and CoRT offer scalable and robust solutions for SOOTT problems, providing theoretical improvements and practical feasibility in domains like workload scheduling."}}
{"id": "2509.06239", "pdf": "https://arxiv.org/pdf/2509.06239", "abs": "https://arxiv.org/abs/2509.06239", "authors": ["Manvi Jha", "Jiaxin Wan", "Deming Chen"], "title": "Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nautomated code generation but frequently produce code that fails formal\nverification, an essential requirement for hardware and safety-critical\ndomains. To overcome this fundamental limitation, we previously proposed\nPREFACE, a model-agnostic framework based on reinforcement learning (RL) that\niteratively repairs the prompts provided to frozen LLMs, systematically\nsteering them toward generating formally verifiable Dafny code without costly\nfine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis\nframework that embeds the previously proposed PREFACE flow to enable the\ngeneration of correctness-by-construction hardware directly from natural\nlanguage specifications. Proof2Silicon operates by: (1) leveraging PREFACE's\nverifier-driven RL agent to optimize prompt generation iteratively, ensuring\nDafny code correctness; (2) automatically translating verified Dafny programs\ninto synthesizable high-level C using Dafny's Python backend and PyLog; and (3)\nemploying Vivado HLS to produce RTL implementations. Evaluated rigorously on a\nchallenging 100-task benchmark, PREFACE's RL-guided prompt optimization\nconsistently improved Dafny verification success rates across diverse LLMs by\nup to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis\nsuccess rate of up to 72%, generating RTL designs through Vivado HLS synthesis\nflows. These results demonstrate a robust, scalable, and automated pipeline for\nLLM-driven, formally verified hardware synthesis, bridging natural-language\nspecification and silicon realization.", "AI": {"tldr": "Proof2Silicon introduces an automated framework for generating formally verified hardware from natural language specifications, leveraging reinforcement learning for prompt optimization and Dafny code translation to RTL implementation.", "motivation": "Large Language Models often produce code failing formal verification, a critical issue for hardware and safety-critical systems.", "method": "Using PREFACE, an RL-based framework for improving prompt quality, Proof2Silicon translates verified Dafny code into synthesizable C and generates RTL designs via Vivado HLS.", "result": "Proof2Silicon improved Dafny verification rates by 21% across LLMs and achieved up to 72% success in hardware synthesis for a 100-task benchmark.", "conclusion": "Proof2Silicon demonstrates scalable and automated LLM-driven synthesis of formally verified hardware from natural language inputs."}}
{"id": "2509.05513", "pdf": "https://arxiv.org/pdf/2509.05513", "abs": "https://arxiv.org/abs/2509.05513", "authors": ["Ahad Jawaid", "Yu Xiang"], "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "4 pages, 1 figure", "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.", "AI": {"tldr": "The paper introduces OpenEgo, a comprehensive egocentric manipulation dataset with unified hand-pose annotations and intention-aligned action primitives for imitation learning.", "motivation": "Existing egocentric video datasets lack fine-grained action descriptions or standardized hand annotations, limiting their utility for dexterous manipulation research.", "method": "The authors created OpenEgo by aggregating and unifying data from 6 public datasets, standardizing hand-pose layouts, and adding detailed, timestamped action primitives.", "result": "The researchers validated the dataset's utility by training language-conditioned imitation-learning policies to predict dexterous hand trajectories.", "conclusion": "OpenEgo aims to facilitate learning dexterous manipulation from egocentric videos and promote reproducible research in vision-language-action learning."}}
{"id": "2509.06119", "pdf": "https://arxiv.org/pdf/2509.06119", "abs": "https://arxiv.org/abs/2509.06119", "authors": ["Shiqi Xu", "Lihao Zhang", "Yuyang Du", "Qun Yang", "Soung Chang Liew"], "title": "A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Recent progress in robotics has underscored the demand for real-time control\nin applications such as manufacturing, healthcare, and autonomous systems,\nwhere the timely delivery of mission-critical commands under heterogeneous\nrobotic traffic is paramount for operational efficacy and safety. In these\nscenarios, mission-critical traffic follows a strict deadline-constrained\ncommunication pattern: commands must arrive within defined QoS deadlines,\notherwise late arrivals can degrade performance or destabilize control loops.In\nthis work, we demonstrate on a real-time SDR platform that CSMA, widely adopted\nin robotic communications,suffers severe degradation under high robot traffic\nloads, with contention-induced collisions and delays disrupting the on-time\narrival of mission-critical packets. To address this problem, we propose an\nIEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's\ndeterministic slot scheduling with CSMA's adaptability for heterogeneous robot\ntraffic.The protocol achieves collision-free, low-latency mission-critical\ncommand delivery and IEEE 802.11 compatibility through the synergistic\nintegration of sub-microsecond PTP-based slot synchronization-essential for\nestablishing precise timing for TDMA, a three-session superframe with dynamic\nTDMA allocation for structured and adaptable traffic management,and beacon-NAV\nprotection to preemptively secure these critical communication sessions from\ninterference. Emulation experiments on real-time SDR testbed and Robot\nOperating System (ROS) simulation show that the proposed protocol reduces\nmissed-deadline errors by 93% compared to the CSMA baseline. In high-speed\nrobot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)\ntrajectory error by up to 90% compared with a CSMA baseline, all while\nmaintaining throughput for non-critical traffic within +-2%.", "AI": {"tldr": "The study proposes a hybrid TDMA/CSMA protocol for robotic communications, demonstrating substantial improvements in delivering mission-critical commands under high-traffic loads compared to conventional CSMA.", "motivation": "To ensure reliable and timely delivery of mission-critical commands in robotic systems under high traffic loads, addressing the performance degradation caused by CSMA's contention-based issues.", "method": "Developed an IEEE 802.11-compatible hybrid TDMA/CSMA protocol combining TDMA for deterministic scheduling and CSMA for traffic adaptability. Key features include sub-microsecond PTP-based synchronization, dynamic TDMA allocation, and beacon-NAV protection to avoid communication interference.", "result": "The proposed protocol achieves a 93% reduction in missed-deadline errors and a 90% reduction in RMS trajectory errors in high-speed robotic simulations compared to CSMA. It maintains throughput for non-critical traffic within a 2% margin.", "conclusion": "The hybrid TDMA/CSMA protocol significantly improves the reliability and timeliness of mission-critical robotic communications under heterogeneous traffic, outperforming CSMA in both simulations and real-time scenarios."}}
{"id": "2509.05741", "pdf": "https://arxiv.org/pdf/2509.05741", "abs": "https://arxiv.org/abs/2509.05741", "authors": ["Fernando Gabriela Garc\u00eda", "Qiyang Shi", "Zilin Feng"], "title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "categories": ["cs.CL"], "comment": null, "summary": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation.", "AI": {"tldr": "The paper introduces VeriFact-CoT, a method aimed at improving accuracy and reliability in large language models (LLMs) by addressing hallucination and citation issues.", "motivation": "The motivation is to tackle the challenges of hallucinations and the lack of credible citations in LLMs during complex, fact-sensitive content generation.", "method": "The method involves a multi-stage approach of 'fact verification-reflection-citation integration,' enabling LLMs to self-examine and refine their reasoning and answers.", "result": "VeriFact-CoT significantly improves the accuracy, trustworthiness, and traceability of outputs produced by LLMs.", "conclusion": "By implementing VeriFact-CoT, LLMs become more dependable for high-fidelity applications such as scientific research, news reporting, and legal advisory tasks."}}
{"id": "2509.06120", "pdf": "https://arxiv.org/pdf/2509.06120", "abs": "https://arxiv.org/abs/2509.06120", "authors": ["Ambuj Tewari"], "title": "If generative AI is the answer, what is the question?", "categories": ["cs.LG", "stat.ML"], "comment": "To appear as a book chapter in a Springer book titled \"Statistical\n  Foundations and Applications of Artificial Intelligence, Machine Learning and\n  Deep Learning\" and edited by S. Ejaz Ahmed, Pierre Alquier, Yi Li, Shuangge\n  Ma", "summary": "Beginning with text and images, generative AI has expanded to audio, video,\ncomputer code, and molecules. Yet, if generative AI is the answer, what is the\nquestion? We explore the foundations of generation as a distinct machine\nlearning task with connections to prediction, compression, and decision-making.\nWe survey five major generative model families: autoregressive models,\nvariational autoencoders, normalizing flows, generative adversarial networks,\nand diffusion models. We then introduce a probabilistic framework that\nemphasizes the distinction between density estimation and generation. We review\na game-theoretic framework with a two-player adversary-learner setup to study\ngeneration. We discuss post-training modifications that prepare generative\nmodels for deployment. We end by highlighting some important topics in socially\nresponsible generation such as privacy, detection of AI-generated content, and\ncopyright and IP. We adopt a task-first framing of generation, focusing on what\ngeneration is as a machine learning problem, rather than only on how models\nimplement it.", "AI": {"tldr": "The paper explores generative AI across various domains, surveys major models, and introduces frameworks focusing on generation as a task.", "motivation": "Examining generative AI to understand its scope and propose systematic frameworks for advancements and socially responsible development.", "method": "Surveys five generative models; introduces probabilistic and game-theoretic frameworks; discusses deployment strategies and social considerations.", "result": "Proposed distinguished frameworks and insights into generative AI modeling and deployment, with social responsibility topics explored.", "conclusion": "Highlights generation as a distinct ML task and calls for a balanced approach between technical innovation and societal impact."}}
{"id": "2509.06269", "pdf": "https://arxiv.org/pdf/2509.06269", "abs": "https://arxiv.org/abs/2509.06269", "authors": ["Vishal Raman", "Vijai Aravindh R", "Abhijith Ragav"], "title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "categories": ["cs.AI"], "comment": "8 pages, 2 figures, Accepted at the OARS Workshop, KDD 2025, Paper\n  link: https://oars-workshop.github.io/papers/Raman2025.pdf", "summary": "Personalized AI assistants often struggle to incorporate complex personal\ndata and causal knowledge, leading to generic advice that lacks explanatory\npower. We propose REMI, a Causal Schema Memory architecture for a multimodal\nlifestyle agent that integrates a personal causal knowledge graph, a causal\nreasoning engine, and a schema based planning module. The idea is to deliver\nexplainable, personalized recommendations in domains like fashion, personal\nwellness, and lifestyle planning. Our architecture uses a personal causal graph\nof the user's life events and habits, performs goal directed causal traversals\nenriched with external knowledge and hypothetical reasoning, and retrieves\nadaptable plan schemas to generate tailored action plans. A Large Language\nModel orchestrates these components, producing answers with transparent causal\nexplanations. We outline the CSM system design and introduce new evaluation\nmetrics for personalization and explainability, including Personalization\nSalience Score and Causal Reasoning Accuracy, to rigorously assess its\nperformance. Results indicate that CSM based agents can provide more context\naware, user aligned recommendations compared to baseline LLM agents. This work\ndemonstrates a novel approach to memory augmented, causal reasoning in\npersonalized agents, advancing the development of transparent and trustworthy\nAI lifestyle assistants.", "AI": {"tldr": "Authors propose REMI, a Causal Schema Memory architecture for lifestyle agents capable of producing personalized and explainable recommendations based on users' life events, habits, and external knowledge.", "motivation": "Personalized AI assistants often fail at incorporating complex personal data and causal knowledge, which leads to generic and unexplainable advice.", "method": "REMI integrates a personal causal knowledge graph, a causal reasoning engine, and a schema planning module, enriched by a Large Language Model to perform causal reasoning, retrieve adaptable action plans, and deliver transparent causal explanations.", "result": "REMI provides more context-aware, personalized, and user-aligned recommendations compared to baseline LLM agents. New evaluation metrics (Personalization Salience Score and Causal Reasoning Accuracy) validate its explainability and personalization.", "conclusion": "This work advances memory-augmented causal reasoning in AI by developing transparent, trustworthy, and personalized lifestyle assistants leveraging novel evaluation techniques and architectures."}}
{"id": "2509.05515", "pdf": "https://arxiv.org/pdf/2509.05515", "abs": "https://arxiv.org/abs/2509.05515", "authors": ["Sen Wang", "Kunyi Li", "Siyun Liang", "Elena Alegret", "Jing Ma", "Nassir Navab", "Stefano Gasperini"], "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, distilling open-vocabulary language features from 2D images into 3D\nGaussians has attracted significant attention. Although existing methods\nachieve impressive language-based interactions of 3D scenes, we observe two\nfundamental issues: background Gaussians contributing negligibly to a rendered\npixel get the same feature as the dominant foreground ones, and multi-view\ninconsistencies due to view-specific noise in language embeddings. We introduce\nVisibility-Aware Language Aggregation (VALA), a lightweight yet effective\nmethod that computes marginal contributions for each ray and applies a\nvisibility-aware gate to retain only visible Gaussians. Moreover, we propose a\nstreaming weighted geometric median in cosine space to merge noisy multi-view\nfeatures. Our method yields a robust, view-consistent language feature\nembedding in a fast and memory-efficient manner. VALA improves open-vocabulary\nlocalization and segmentation across reference datasets, consistently\nsurpassing existing works.", "AI": {"tldr": "This paper addresses the issue of embedding open-vocabulary language features into 3D representations, proposing Visibility-Aware Language Aggregation (VALA) to improve visibility and view-consistency.", "motivation": "To tackle issues in existing methods, namely background Gaussians with disproportionate contributions and multi-view inconsistencies due to noise in language embeddings.", "method": "The authors introduce VALA, which uses marginal contribution computation and visibility-aware gates, along with a streaming weighted geometric median, to refine 3D language embeddings effectively.", "result": "Results show that VALA enhances open-vocabulary localization and segmentation performance across multiple datasets, outperforming current approaches.", "conclusion": "VALA demonstrates a fast, robust, and memory-efficient framework that addresses key limitations in 3D Gaussian language embeddings, improving interaction and feature consistency."}}
{"id": "2509.05779", "pdf": "https://arxiv.org/pdf/2509.05779", "abs": "https://arxiv.org/abs/2509.05779", "authors": ["Wei Chen", "Yuqian Wu", "Yuanshao Zhu", "Xixuan Hao", "Shiyu Wang", "Yuxuan Liang"], "title": "Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting", "categories": ["cs.LG"], "comment": "16 pages, 11 figures", "summary": "Spatio-temporal forecasting aims to predict the future state of dynamic\nsystems and plays an important role in multiple fields. However, existing\nsolutions only focus on modeling using a limited number of observed target\nvariables. In real-world scenarios, exogenous variables can be integrated into\nthe model as additional input features and associated with the target signal to\npromote forecast accuracy. Although promising, this still encounters two\nchallenges: the inconsistent effects of different exogenous variables to the\ntarget system, and the imbalance effects between historical variables and\nfuture variables. To address these challenges, this paper introduces \\model, a\nnovel framework for modeling \\underline{exo}genous variables in\n\\underline{s}patio-\\underline{t}emporal forecasting, which follows a ``select,\nthen balance'' paradigm. Specifically, we first construct a latent space gated\nexpert module, where fused exogenous information is projected into a latent\nspace to dynamically select and recompose salient signals via specialized\nsub-experts. Furthermore, we design a siamese network architecture in which\nrecomposed representations of past and future exogenous variables are fed into\ndual-branch spatio-temporal backbones to capture dynamic patterns. The outputs\nare integrated through a context-aware weighting mechanism to achieve dynamic\nbalance during the modeling process. Extensive experiments on real-world\ndatasets demonstrate the effectiveness, generality, robustness, and efficiency\nof our proposed framework.", "AI": {"tldr": "The paper addresses limitations in spatio-temporal forecasting by proposing a new framework, \\model, which effectively leverages exogenous variables using a 'select, then balance' approach for better prediction accuracy.", "motivation": "To improve spatio-temporal forecasting by addressing issues related to inconsistent and imbalanced effects of various exogenous variables in dynamic systems.", "method": "The method introduces \\model, which includes a latent space gated expert module for signal selection and a siamese network for dynamic pattern modeling using both historical and future variables.", "result": "Experiments on real-world datasets show that \\model improves forecasting performance in terms of accuracy, generality, robustness, and efficiency.", "conclusion": "The proposed framework is effective in dynamically modeling exogenous variables to enhance spatio-temporal forecasting performance."}}
{"id": "2509.06191", "pdf": "https://arxiv.org/pdf/2509.06191", "abs": "https://arxiv.org/abs/2509.06191", "authors": ["Yifei Ren", "Edward Johns"], "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project webpage with robot videos:\n  https://www.robot-learning.uk/op-gen", "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.", "AI": {"tldr": "This paper explores using 3D generative models to augment datasets for robot learning, enabling effective policy learning from fewer demonstrations.", "motivation": "Robots often require extensive demonstrations for policy learning. Data augmentation through 3D models can reduce this bottleneck and improve generalization.", "method": "The authors used 3D generative models to create augmented datasets from a single demonstration and trained omnidirectional policies to generalize across diverse initial states.", "result": "Robots successfully performed tasks far from the initial demonstration states, surpassing alternative data augmentation methods in tasks like grasping, drawer opening, and trash placement.", "conclusion": "Using 3D generative models for dataset augmentation boosts robotic policy learning efficiency, reducing required demonstrations and expanding learning versatility."}}
{"id": "2509.05863", "pdf": "https://arxiv.org/pdf/2509.05863", "abs": "https://arxiv.org/abs/2509.05863", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work.", "AI": {"tldr": "LatinX is a multilingual text-to-speech model designed for preserving speaker identity in speech-to-speech translation across languages, showing improved performance through Direct Preference Optimization.", "motivation": "The paper seeks to address the challenge of maintaining speaker identity while performing multilingual speech-to-speech translation.", "method": "The model employs a 12-layer decoder-only Transformer trained in three steps: text-to-audio pre-training, supervised fine-tuning for voice cloning, and alignment using Direct Preference Optimization considering Word Error Rate and speaker similarity.", "result": "LatinX significantly reduces Word Error Rate and enhances speaker similarity metrics compared to a fine-tuned baseline, as confirmed by both objective measures and human evaluations.", "conclusion": "LatinX demonstrates strong speaker similarity in multilingual TTS tasks, bridging gaps between objective and subjective evaluations, and suggests paths for improving latency and balance in evaluation signals."}}
{"id": "2509.06154", "pdf": "https://arxiv.org/pdf/2509.06154", "abs": "https://arxiv.org/abs/2509.06154", "authors": ["Dibyajyoti Nayak", "Somdatta Goswami"], "title": "Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "21 pages including references. Supplementary Information provided", "summary": "Neural operators (NOs) approximate mappings between infinite-dimensional\nfunction spaces but require large datasets and struggle with scarce training\ndata. Many NO formulations don't explicitly encode causal, local-in-time\nstructure of physical evolution. While autoregressive models preserve causality\nby predicting next time-steps, they suffer from rapid error accumulation. We\nemploy Graph Neural Simulators (GNS) - a message-passing graph neural network\nframework - with explicit numerical time-stepping schemes to construct accurate\nforward models that learn PDE solutions by modeling instantaneous time\nderivatives. We evaluate our framework on three canonical PDE systems: (1) 2D\nBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D\nAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantly\nimproves data efficiency, achieving higher generalization accuracy with\nsubstantially fewer training trajectories compared to neural operator baselines\nlike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors\nwith only 30 training samples out of 1000 (3% of available data) across all\nthree PDE systems. It substantially reduces error accumulation over extended\ntemporal horizons: averaged across all cases, GNS reduces autoregressive error\nby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a\nPCA+KMeans trajectory selection strategy enhancing low-data performance.\nResults indicate combining graph-based local inductive biases with conventional\ntime integrators yields accurate, physically consistent, and scalable surrogate\nmodels for time-dependent PDEs.", "AI": {"tldr": "The paper proposes using Graph Neural Simulators (GNS) with numerical time-stepping to model PDEs, achieving high accuracy and data efficiency compared to traditional neural operators. It reduces error accumulation and provides consistent performance.", "motivation": "To address the limitations of existing Neural Operators (NOs), which require large datasets and fail to explicitly incorporate the time-causal, local structure of physical systems.", "method": "The paper uses a Graph Neural Simulator framework combined with explicit numerical time-stepping to learn instantaneous time derivatives of PDEs. It incorporates a PCA+KMeans strategy for selecting training samples.", "result": "GNS demonstrated significantly better data efficiency and accuracy, achieving under 1% relative L2 errors with just 3% of training data and reducing error accumulation by 82.48% to 99.86% compared to FNO AR and DON AR in three different PDE systems.", "conclusion": "Graph-based inductive biases coupled with time integrators provide a promising, scalable approach for accurate and data-efficient PDE modeling. The method generalizes well in low-data scenarios and ensures physical consistency."}}
{"id": "2509.06278", "pdf": "https://arxiv.org/pdf/2509.06278", "abs": "https://arxiv.org/abs/2509.06278", "authors": ["Chuang Jiang", "Mingyue Cheng", "Xiaoyu Tao", "Qingyang Mao", "Jie Ouyang", "Qi Liu"], "title": "TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning", "categories": ["cs.AI"], "comment": "Comments: 10 pages, 6 figures. Submitted to WSDM 2026", "summary": "Table reasoning is crucial for leveraging structured data in domains such as\nfinance, healthcare, and scientific research. While large language models\n(LLMs) show promise in multi-step reasoning, purely text-based methods often\nstruggle with the complex numerical computations and fine-grained operations\ninherently required in this task. Tool-integrated reasoning improves\ncomputational accuracy via explicit code execution, yet existing systems\nfrequently rely on rigid patterns, supervised imitation, and lack true\nautonomous adaptability. In this paper, we present TableMind, an LLM-driven\ntable reasoning agent that (i) autonomously performs multi-turn tool\ninvocation, (ii) writes and executes data-analyzing code in a secure sandbox\nenvironment for data analysis and precise numerical reasoning, and (iii)\nexhibits high-level capabilities such as planning and self-reflection to adapt\nstrategies. To realize these capabilities, we adopt a two-stage fine-tuning\nparadigm built on top of a powerful pre-trained language model: supervised\nfine-tuning on high-quality reasoning trajectories to establish effective tool\nusage patterns, followed by reinforcement fine-tuning to optimize\nmulti-objective strategies. In particular, we propose Rank-Aware Policy\nOptimization (RAPO), which increases the update weight of high-quality\ntrajectories when their output probabilities are lower than those of\nlow-quality ones, thereby guiding the model more consistently toward better and\nmore accurate answers. Extensive experiments on several mainstream benchmarks\ndemonstrate that TableMind achieves superior performance compared to\ncompetitive baselines, yielding substantial gains in both reasoning accuracy\nand computational precision.", "AI": {"tldr": "TableMind, an advanced LLM-based table reasoning system, combines multi-turn tool usage and code execution in a secure environment with features like planning and self-reflection.", "motivation": "Existing text-based methods for table reasoning struggle with numerical calculations and complex operations, and current tool-integrated systems lack adaptability.", "method": "TableMind uses a two-stage fine-tuning approach: supervised fine-tuning for effective tool usage patterns followed by RAPO-based reinforcement fine-tuning to focus on high-quality outputs.", "result": "TableMind outperformed competitive baselines in reasoning accuracy and computational precision across multiple benchmarks.", "conclusion": "TableMind successfully demonstrates improved tool integration, adaptability, and reasoning capabilities, providing a step forward in table reasoning technology for structured data analysis."}}
{"id": "2509.05543", "pdf": "https://arxiv.org/pdf/2509.05543", "abs": "https://arxiv.org/abs/2509.05543", "authors": ["Haitao Tian", "Pierre Payeur"], "title": "DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation", "categories": ["cs.CV"], "comment": "ICCV 2025 accepted paper", "summary": "In this paper, a contrastive representation learning framework is proposed to\nenhance human action segmentation via pre-training using trimmed (single\naction) skeleton sequences. Unlike previous representation learning works that\nare tailored for action recognition and that build upon isolated sequence-wise\nrepresentations, the proposed framework focuses on exploiting multi-scale\nrepresentations in conjunction with cross-sequence variations. More\nspecifically, it proposes a novel data augmentation strategy, 'Shuffle and\nWarp', which exploits diverse multi-action permutations. The latter effectively\nassists two surrogate tasks that are introduced in contrastive learning: Cross\nPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). In\noptimization, CPC learns intra-class similarities by contrasting\nrepresentations of the same action class across different permutations, while\nROR reasons about inter-class contexts by predicting relative mapping between\ntwo permutations. Together, these tasks enable a Dual-Surrogate Contrastive\nLearning (DuoCLR) network to learn multi-scale feature representations\noptimized for action segmentation. In experiments, DuoCLR is pre-trained on a\ntrimmed skeleton dataset and evaluated on an untrimmed dataset where it\ndemonstrates a significant boost over state-the-art comparatives in both\nmulti-class and multi-label action segmentation tasks. Lastly, ablation studies\nare conducted to evaluate the effectiveness of each component of the proposed\napproach.", "AI": {"tldr": "The paper proposes a contrastive representation learning framework, DuoCLR, using novel data augmentation and dual surrogate tasks to improve human action segmentation. It outperforms existing methods in experiments.", "motivation": "Enhance human action segmentation by addressing limitations in existing representation learning frameworks that primarily focus on action recognition and isolated sequence-wise representations.", "method": "Introduces a new data augmentation strategy, 'Shuffle and Warp,' and two surrogate tasks (CPC and ROR) in a contrastive learning setup to optimize a DuoCLR network for extracting multi-scale feature representations.", "result": "DuoCLR achieves a significant performance boost in multi-class and multi-label action segmentation on untrimmed datasets compared to state-of-the-art methods.", "conclusion": "The proposed DuoCLR framework effectively leverages trimmed skeleton sequence data and distinct learning tasks for robust human action segmentation, showing promise for improved real-world applications."}}
{"id": "2509.05801", "pdf": "https://arxiv.org/pdf/2509.05801", "abs": "https://arxiv.org/abs/2509.05801", "authors": ["Debdeep Sanyal", "Aaryan Nagpal", "Dhruv Kumar", "Murari Mandal", "Saurabh Deshpande"], "title": "time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While transformer-based foundation models excel at forecasting routine\npatterns, two questions remain: do they internalize semantic concepts such as\nmarket regimes, or merely fit curves? And can their internal representations be\nleveraged to simulate rare, high-stakes events such as market crashes? To\ninvestigate this, we introduce activation transplantation, a causal\nintervention that manipulates hidden states by imposing the statistical moments\nof one event (e.g., a historical crash) onto another (e.g., a calm period)\nduring the forward pass. This procedure deterministically steers forecasts:\ninjecting crash semantics induces downturn predictions, while injecting calm\nsemantics suppresses crashes and restores stability. Beyond binary control, we\nfind that models encode a graded notion of event severity, with the latent\nvector norm directly correlating with the magnitude of systemic shocks.\nValidated across two architecturally distinct TSFMs, Toto (decoder only) and\nChronos (encoder-decoder), our results demonstrate that steerable, semantically\ngrounded representations are a robust property of large time series\ntransformers. Our findings provide evidence for a latent concept space that\ngoverns model predictions, shifting interpretability from post-hoc attribution\nto direct causal intervention, and enabling semantic \"what-if\" analysis for\nstrategic stress-testing.", "AI": {"tldr": "This paper demonstrates that transformer-based models encode semantically grounded representations useful for forecasting and manipulating high-stakes events like market crashes through a proposed technique called activation transplantation.", "motivation": "The paper seeks to determine whether transformer-based models internalize semantic concepts, such as market regimes, or merely fit patterns, and whether these representations can be manipulated to simulate high-stakes events.", "method": "The authors propose a method called activation transplantation, where hidden states are manipulated by imposing statistical moments of one event (e.g., a crash) onto another during the forward pass to steer forecasts deterministically.", "result": "The study shows that injection of event semantics steers model predictions effectively, such as inducing crash predictions by injecting crash semantics. Models encode event severity in a graded manner, with latent vector norms correlating with systemic shock magnitudes.", "conclusion": "The findings highlight the existence of a latent concept space in time-series transformers that allows for causally-driven intervention and 'what-if' analysis, advancing interpretability and strategic stress testing."}}
{"id": "2509.06201", "pdf": "https://arxiv.org/pdf/2509.06201", "abs": "https://arxiv.org/abs/2509.06201", "authors": ["Jun Yamada", "Adithyavairavan Murali", "Ajay Mandlekar", "Clemens Eppner", "Ingmar Posner", "Balakumar Sundaralingam"], "title": "Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "14 pages, 17 figures", "summary": "Grasping of diverse objects in unstructured environments remains a\nsignificant challenge. Open-loop grasping methods, effective in controlled\nsettings, struggle in cluttered environments. Grasp prediction errors and\nobject pose changes during grasping are the main causes of failure. In\ncontrast, closed-loop methods address these challenges in simplified settings\n(e.g., single object on a table) on a limited set of objects, with no path to\ngeneralization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping\npolicy designed for robust and reactive grasping of novel objects in cluttered\nenvironments. Grasp-MPC incorporates a value function, trained on visual\nobservations from a large-scale synthetic dataset of 2 million grasp\ntrajectories that include successful and failed attempts. We deploy this\nlearned value function in an MPC framework in combination with other cost terms\nthat encourage collision avoidance and smooth execution. We evaluate Grasp-MPC\non FetchBench and real-world settings across diverse environments. Grasp-MPC\nimproves grasp success rates by up to 32.6% in simulation and 33.3% in\nreal-world noisy conditions, outperforming open-loop, diffusion policy,\ntransformer policy, and IQL approaches. Videos and more at\nhttp://grasp-mpc.github.io.", "AI": {"tldr": "Grasp-MPC enhances the success rate in object grasping in cluttered, unstructured environments by using a closed-loop vision-based method combined with an MPC framework.", "motivation": "Grasping in unstructured environments is challenged by errors in grasp prediction and pose changes, which hinder the effectiveness of traditional open-loop methods, while existing closed-loop methods lack generalization and robustness.", "method": "The Grasp-MPC framework trains a value function on a synthetic dataset of 2 million grasp trajectories, integrating it into a Model Predictive Control framework with collision avoidance and smooth execution costs.", "result": "Grasp-MPC outperformed competing approaches, improving grasp success rates by up to 32.6% in simulations and 33.3% in real-world noisy settings.", "conclusion": "Grasp-MPC demonstrates superior robustness and generalization for 6-DoF grasping of diverse objects in cluttered environments, addressing key challenges with a closed-loop approach."}}
{"id": "2509.05867", "pdf": "https://arxiv.org/pdf/2509.05867", "abs": "https://arxiv.org/abs/2509.05867", "authors": ["ZiXuan Zhang", "Bowen Hao", "Yingjie Li", "Hongzhi Yin"], "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.", "AI": {"tldr": "A new framework, ZhiFangDanTai, combines Graph-based Retrieval-Augmented Generation (GraphRAG) and Large Language Model (LLM) fine-tuning to enhance the generation of explainable Traditional Chinese Medicine formulas, outperforming state-of-the-art models.", "motivation": "The study aims to address the limitations in existing TCM models that lack comprehensive outputs and detailed explanations, particularly in roles like sovereign, minister, assistant, and courier, as well as their efficacy and diagnostic guidance.", "method": "The researchers propose ZhiFangDanTai, a framework that integrates GraphRAG for structured knowledge retrieval and LLM fine-tuning. An enhanced instruction dataset is also created, alongside theoretical proofs demonstrating reductions in error rates.", "result": "ZhiFangDanTai outperforms state-of-the-art techniques in explainable TCM formula generation, as derived from experimental results on both collected and clinical datasets.", "conclusion": "ZhiFangDanTai offers a significant advancement in the analysis and explanation of TCM formulas through its novel integration of GraphRAG and fine-tuning methods, and the model is publicly available for research use."}}
{"id": "2509.06864", "pdf": "https://arxiv.org/pdf/2509.06864", "abs": "https://arxiv.org/abs/2509.06864", "authors": ["Ming-I Huang", "Chih-Duo Hong", "Fang Yu"], "title": "Concolic Testing on Individual Fairness of Neural Network Models", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "This paper introduces PyFair, a formal framework for evaluating and verifying\nindividual fairness of Deep Neural Networks (DNNs). By adapting the concolic\ntesting tool PyCT, we generate fairness-specific path constraints to\nsystematically explore DNN behaviors. Our key innovation is a dual network\narchitecture that enables comprehensive fairness assessments and provides\ncompleteness guarantees for certain network types. We evaluate PyFair on 25\nbenchmark models, including those enhanced by existing bias mitigation\ntechniques. Results demonstrate PyFair's efficacy in detecting discriminatory\ninstances and verifying fairness, while also revealing scalability challenges\nfor complex models. This work advances algorithmic fairness in critical domains\nby offering a rigorous, systematic method for fairness testing and verification\nof pre-trained DNNs.", "AI": {"tldr": "PyFair is a tool that evaluates and verifies individual fairness in Deep Neural Networks by generating fairness-specific path constraints.", "motivation": "Address the need for systematic evaluation and verification of fairness in Deep Neural Networks.", "method": "PyFair utilizes a concolic testing tool, PyCT, to create fairness-specific path constraints, paired with a dual network architecture for comprehensive assessments.", "result": "Tested on 25 benchmark models, PyFair successfully detects discriminative instances, verifies fairness, and highlights scalability issues in complex models.", "conclusion": "PyFair represents a significant advance in algorithmic fairness testing, offering a rigorous approach to evaluating pre-trained DNNs."}}
{"id": "2509.06213", "pdf": "https://arxiv.org/pdf/2509.06213", "abs": "https://arxiv.org/abs/2509.06213", "authors": ["Christo Mathew", "Wentian Wang", "Lazaros Gallos", "Paul Kantor", "Vladimir Menkov", "Hao Wang"], "title": "Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)\nenvironment, a complex puzzle in which an agent must infer and execute hidden\nrules to clear a 6$\\times$6 board by placing game pieces into buckets. We\nexplore two state representation strategies, namely Feature-Centric (FC) and\nObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic\n(A2C) algorithm for training. The agent has access only to partial observations\nand must simultaneously infer the governing rule and learn the optimal policy\nthrough experience. We evaluate our models across multiple rule-based and\ntrial-list-based experimental setups, analyzing transfer effects and the impact\nof representation on learning efficiency.", "AI": {"tldr": "This paper studies reinforcement learning in the Game of Hidden Rules (GOHR), focusing on rule inference and optimal decision-making using Feature-Centric and Object-Centric representations with a Transformer-based A2C algorithm.", "motivation": "Understanding how agents can learn and adapt to hidden rules in dynamic environments with only partial observations.", "method": "Employs Feature-Centric and Object-Centric state representations combined with a Transformer-based Advantage Actor-Critic (A2C) algorithm in the GOHR puzzle environment.", "result": "The study evaluates the models on various setups, assessing their transfer abilities and the impact of state representation on learning efficiency.", "conclusion": "Representation strategies significantly influence learning efficiency, offering insights into how agents thrive in environments with partial observations and hidden rules."}}
{"id": "2509.06283", "pdf": "https://arxiv.org/pdf/2509.06283", "abs": "https://arxiv.org/abs/2509.06283", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Revanth Gangi Reddy", "Austin Xu", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "categories": ["cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.", "AI": {"tldr": "This paper explores enhancing large language models (LLMs) for autonomous deep research tasks using continual reinforcement learning (RL) with synthetic data.", "motivation": "To empower large language models with advanced reasoning and tool-use capabilities, enabling applications like Deep Research, which demand dynamic search and reasoning.", "method": "The study implements continual reinforcement learning (RL) using synthetic data to optimize reasoning abilities in open-source LLMs for dynamic task execution.", "result": "The model variant SFR-DR-20B achieved a performance score of up to 28.7% on the Humanity's Last Exam benchmark.", "conclusion": "Using RL with synthetic data enhances reasoning and agentic skills in LLMs, demonstrating potential in autonomous single-agent systems for complex research tasks."}}
{"id": "2509.05554", "pdf": "https://arxiv.org/pdf/2509.05554", "abs": "https://arxiv.org/abs/2509.05554", "authors": ["Yihong Leng", "Siming Zheng", "Jinwei Chen", "Bo Li", "Jiaojiao Li", "Peng-Tao Jiang"], "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness.", "AI": {"tldr": "The paper proposes the RED network, which improves event-guided motion deblurring by addressing event incompleteness and enhancing robustness through perturbation and attention strategies.", "motivation": "Existing methods for motion deblurring using event cameras often overlook the incompleteness of event streams due to DVS sensitivity-noise trade-offs, compromising motion priors needed for effective deblurring.", "method": "The authors introduced the RED network featuring a Robustness-Oriented Perturbation Strategy (random event masking for robustness) and a disentangled OmniAttention to model correlations between blurry images and incomplete events. Additionally, interactive modules enhance motion and semantic context.", "result": "RED network achieves state-of-the-art performance in terms of both accuracy and robustness, validated by experiments on synthetic and real-world datasets.", "conclusion": "The paper concludes that RED can robustly handle incomplete event streams and improve motion deblurring effectiveness by integrating reliable representations from blurry images and event data."}}
{"id": "2509.05811", "pdf": "https://arxiv.org/pdf/2509.05811", "abs": "https://arxiv.org/abs/2509.05811", "authors": ["Ben Kretzu", "Karen Ullrich", "Yonathan Efroni"], "title": "Simple Optimizers for Convex Aligned Multi-Objective Optimization", "categories": ["cs.LG"], "comment": null, "summary": "It is widely recognized in modern machine learning practice that access to a\ndiverse set of tasks can enhance performance across those tasks. This\nobservation suggests that, unlike in general multi-objective optimization, the\nobjectives in many real-world settings may not be inherently conflicting. To\naddress this, prior work introduced the Aligned Multi-Objective Optimization\n(AMOO) framework and proposed gradient-based algorithms with provable\nconvergence guarantees. However, existing analysis relies on strong\nassumptions, particularly strong convexity, which implies the existence of a\nunique optimal solution. In this work, we relax this assumption and study\ngradient-descent algorithms for convex AMOO under standard smoothness or\nLipschitz continuity conditions-assumptions more consistent with those used in\ndeep learning practice. This generalization requires new analytical tools and\nmetrics to characterize convergence in the convex AMOO setting. We develop such\ntools, propose scalable algorithms for convex AMOO, and establish their\nconvergence guarantees. Additionally, we prove a novel lower bound that\ndemonstrates the suboptimality of naive equal-weight approaches compared to our\nmethods.", "AI": {"tldr": "This paper generalizes gradient-based algorithms for Aligned Multi-Objective Optimization (AMOO) by replacing strong convexity assumptions with more practical conditions like smoothness or Lipschitz continuity. It also proposes scalable algorithms with proven convergence guarantees and provides a novel proof showing the limitations of equal-weight approaches.", "motivation": "To improve task performance in machine learning by effectively leveraging Aligned Multi-Objective Optimization (AMOO) under less restrictive and more realistic assumptions than strong convexity.", "method": "The authors relax strong convexity assumptions and introduce smoothness or Lipschitz continuity for analyzing convex AMOO. They use new analytical tools and metrics to establish convergence and design scalable algorithms. They also derive a lower bound to highlight the inefficiency of naive equal-weight methods.", "result": "Scalable gradient-descent algorithms for convex AMOO were developed. The theoretical analysis proves their convergence under more realistic assumptions, and a lower bound indicates the suboptimality of simple equal-weight optimization strategies.", "conclusion": "The study advances the AMOO framework by making it applicable to real-world conditions in deep learning, enhancing its theoretical foundations and offering practical, efficient solutions."}}
{"id": "2509.06233", "pdf": "https://arxiv.org/pdf/2509.06233", "abs": "https://arxiv.org/abs/2509.06233", "authors": ["Tongxuan Tian", "Xuhui Kang", "Yen-Ling Kuo"], "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Conference on Robot Learning (CoRL) 2025. Project website:\n  https://o3afford.github.io/", "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.", "AI": {"tldr": "The paper introduces a novel one-shot 3D object-to-object affordance learning approach that combines semantic features, point cloud geometry, and large language models to enhance robotic manipulation.", "motivation": "Current methods focus too much on single-object affordance predictions, ignoring the common situation where object interactions involve pairs of objects.", "method": "The method combines semantic features from vision foundation models with 3D point cloud representations. It integrates these with large language models to improve reasoning and task-specific function generation for robotic manipulation.", "result": "Experiments show their approach, O$^3$Afford, significantly outperforms existing baselines in accuracy and generalization capabilities.", "conclusion": "This work provides a robust framework for leveraging one-shot learning and integration of LLMs for object-to-object affordance, advancing the field of robotic manipulation."}}
{"id": "2509.05878", "pdf": "https://arxiv.org/pdf/2509.05878", "abs": "https://arxiv.org/abs/2509.05878", "authors": ["Fran\u00e7ois Grolleau", "Emily Alsentzer", "Timothy Keyes", "Philip Chung", "Akshay Swaminathan", "Asad Aali", "Jason Hom", "Tridu Huynh", "Thomas Lew", "April S. Liang", "Weihan Chu", "Natasha Z. Steele", "Christina F. Lin", "Jingkun Yang", "Kameron C. Black", "Stephen P. Ma", "Fateme N. Haredasht", "Nigam H. Shah", "Kevin Schulman", "Jonathan H. Chen"], "title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows.", "AI": {"tldr": "The paper introduces MedFactEval, a clinician-defined evaluation framework, and MedAgentBrief, a multi-step workflow for generating discharge summaries, to enhance the factual accuracy of LLM-generated clinical texts.", "motivation": "Ensuring factual accuracy in LLM-generated clinical texts is crucial, as expert manual review is impractical for continuous verification.", "method": "Two main contributions: MedFactEval, using clinician-defined key facts and LLM Jury voting, and MedAgentBrief, a workflow for factual text generation.", "result": "MedFactEval aligns closely with clinician majority votes (81% agreement) and performs comparably to human experts in evaluating key facts.", "conclusion": "The paper presents scalable tools for evaluating and generating reliable clinical text, promoting responsible use of generative AI in healthcare."}}
{"id": "2509.06218", "pdf": "https://arxiv.org/pdf/2509.06218", "abs": "https://arxiv.org/abs/2509.06218", "authors": ["Shuowei Ma", "Junyu Liu"], "title": "The Efficiency Frontier: Classical Shadows versus Quantum Footage", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "comment": "23 pages, many figures", "summary": "Interfacing quantum and classical processors is an important subroutine in\nfull-stack quantum algorithms. The so-called \"classical shadow\" method\nefficiently extracts essential classical information from quantum states,\nenabling the prediction of many properties of a quantum system from only a few\nmeasurements. However, for a small number of highly non-local observables, or\nwhen classical post-processing power is limited, the classical shadow method is\nnot always the most efficient choice. Here, we address this issue\nquantitatively by performing a full-stack resource analysis that compares\nclassical shadows with ``quantum footage,\" which refers to direct quantum\nmeasurement. Under certain assumptions, our analysis illustrates a boundary of\ndownload efficiency between classical shadows and quantum footage. For\nobservables expressed as linear combinations of Pauli matrices, the classical\nshadow method outperforms direct measurement when the number of observables is\nlarge and the Pauli weight is small. For observables in the form of large\nHermitian sparse matrices, the classical shadow method shows an advantage when\nthe number of observables, the sparsity of the matrix, and the number of qubits\nfall within a certain range. The key parameters influencing this behavior\ninclude the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight\n$w$, accuracy requirement $\\epsilon$, and failure tolerance $\\delta$. We also\ncompare the resource consumption of the two methods on different types of\nquantum computers and identify break-even points where the classical shadow\nmethod becomes more efficient, which vary depending on the hardware. This paper\nopens a new avenue for quantitatively designing optimal strategies for hybrid\nquantum-classical tomography and provides practical insights for selecting the\nmost suitable quantum measurement approach in real-world applications.", "AI": {"tldr": "The study contrasts classical shadow methods and quantum footage for extracting information from quantum states, identifying specific conditions where each method excels.", "motivation": "To address inefficiencies in quantum-to-classical interfacing when classical shadow methods fail in scenarios with non-local observables or limited classical processing power.", "method": "A full-stack resource analysis comparing classical shadows and quantum footage, considering parameters like qubits, observables, sparsity, Pauli weight, and hardware capability.", "result": "Classical shadows outperform direct measurements for certain observable characteristics, while direct measurements may be more efficient under other conditions. Hardware-specific break-even points are identified.", "conclusion": "The paper offers a framework for optimizing hybrid quantum-classical tomography and provides guidance for measurement strategy selection based on system parameters."}}
{"id": "2509.06284", "pdf": "https://arxiv.org/pdf/2509.06284", "abs": "https://arxiv.org/abs/2509.06284", "authors": ["Jiaxiang Chen", "Zhuo Wang", "Mingxi Zou", "Zhucong Li", "Zhijian Zhou", "Song Wang", "Zenglin Xu"], "title": "From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have advanced general-purpose reasoning, showing\nstrong performance across diverse tasks. However, existing methods often rely\non implicit exploration, where the model follows stochastic and unguided\nreasoning paths-like walking without a map. This leads to unstable reasoning\npaths, lack of error correction, and limited learning from past experience. To\naddress these issues, we propose a framework that shifts from implicit\nexploration to structured reasoning through guideline and refinement. First, we\nextract structured reasoning patterns from successful trajectories and\nreflective signals from failures. During inference, the model follows these\nguidelines step-by-step, with refinement applied after each step to correct\nerrors and stabilize the reasoning process. Experiments on BBH and four\nadditional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method\nconsistently outperforms strong baselines across diverse reasoning tasks.\nStructured reasoning with stepwise execution and refinement improves stability\nand generalization, while guidelines transfer well across domains and flexibly\nsupport cross-model collaboration, matching or surpassing supervised\nfine-tuning in effectiveness and scalability.", "AI": {"tldr": "The paper proposes a framework for structured reasoning in large language models (LLMs) using guidelines and refinement, improving stability and performance across reasoning tasks.", "motivation": "To address the instability, lack of error correction, and limited learning from past experience in current implicit exploration methods used by LLMs.", "method": "The framework extracts structured reasoning patterns from successful paths and reflective signals from failures. During inference, it applies step-by-step guidelines with refinement at each step to correct errors.", "result": "The method outperforms strong baselines on benchmarks like BBH, GSM8K, MATH-500, MBPP, and HumanEval, demonstrating improved stability, generalization, and scalability.", "conclusion": "Structured reasoning with stepwise execution and refinement enhances stability, supports cross-model collaboration, and achieves effectiveness comparable to supervised fine-tuning."}}
{"id": "2509.05576", "pdf": "https://arxiv.org/pdf/2509.05576", "abs": "https://arxiv.org/abs/2509.05576", "authors": ["Zekang Zheng", "Haokun Li", "Yaofo Chen", "Mingkui Tan", "Qing Du"], "title": "Sensitivity-Aware Post-Training Quantization for Deep Neural Networks", "categories": ["cs.CV"], "comment": "Accepted by PRCV 2025", "summary": "Model quantization reduces neural network parameter precision to achieve\ncompression, but often compromises accuracy. Existing post-training\nquantization (PTQ) methods employ iterative parameter updates to preserve\naccuracy under high compression ratios, incurring significant computational\ncomplexity and resource overhead, which limits applicability in\nresource-constrained edge computing and real-time inference scenarios. This\npaper proposes an efficient PTQ method guided by parameter sensitivity\nanalysis. The approach prioritizes quantization of high-sensitivity parameters,\nleveraging unquantized low-sensitivity parameters to compensate for\nquantization errors, thereby mitigating accuracy degradation. Furthermore, by\nexploiting column-wise clustering of parameter sensitivity, the method\nintroduces a row-parallel quantization framework with a globally shared inverse\nHessian matrix update mechanism, reducing computational complexity by an order\nof magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a\n20-200-fold quantization speedup over the Optimal Brain Quantization baseline,\nwith mean accuracy loss below 0.3%, confirming the method's efficacy in\nbalancing efficiency and accuracy.", "AI": {"tldr": "This paper introduces an efficient post-training quantization (PTQ) method for neural networks that prioritizes parameter sensitivity to reduce computational overhead while maintaining accuracy.", "motivation": "To enable model compression via quantization while minimizing both accuracy losses and computational complexity, making it applicable for resource-constrained and real-time scenarios.", "method": "The method prioritizes the quantization of high-sensitivity parameters and uses low-sensitivity parameters for compensation. It employs column-wise clustering of sensitivity to inform a row-parallel quantization framework, optimized by a globally shared inverse Hessian matrix.", "result": "The proposed method achieves a 20-200x speedup in quantization compared to Optimal Brain Quantization while ensuring mean accuracy loss remains below 0.3% in experiments on ResNet-50 and YOLOv5s.", "conclusion": "This method significantly improves quantization efficiency by reducing computational complexity and demonstrates strong efficacy in preserving model accuracy, making it viable for edge computing and time-critical applications."}}
{"id": "2509.05826", "pdf": "https://arxiv.org/pdf/2509.05826", "abs": "https://arxiv.org/abs/2509.05826", "authors": ["Misgina Tsighe Hagos", "Claes Lundstr\u00f6m"], "title": "Performance of Conformal Prediction in Capturing Aleatoric Uncertainty", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conformal prediction is a model-agnostic approach to generating prediction\nsets that cover the true class with a high probability. Although its prediction\nset size is expected to capture aleatoric uncertainty, there is a lack of\nevidence regarding its effectiveness. The literature presents that prediction\nset size can upper-bound aleatoric uncertainty or that prediction sets are\nlarger for difficult instances and smaller for easy ones, but a validation of\nthis attribute of conformal predictors is missing. This work investigates how\neffectively conformal predictors quantify aleatoric uncertainty, specifically\nthe inherent ambiguity in datasets caused by overlapping classes. We perform\nthis by measuring the correlation between prediction set sizes and the number\nof distinct labels assigned by human annotators per instance. We further assess\nthe similarity between prediction sets and human-provided annotations. We use\nthree conformal prediction approaches to generate prediction sets for eight\ndeep learning models trained on four datasets. The datasets contain annotations\nfrom multiple human annotators (ranging from five to fifty participants) per\ninstance, enabling the identification of class overlap. We show that the vast\nmajority of the conformal prediction outputs show a very weak to weak\ncorrelation with human annotations, with only a few showing moderate\ncorrelation. These findings underscore the necessity of critically reassessing\nthe prediction sets generated using conformal predictors. While they can\nprovide a higher coverage of the true classes, their capability in capturing\naleatoric uncertainty remains limited.", "AI": {"tldr": "This paper investigates the correlation between conformal prediction set sizes and human annotations to evaluate their effectiveness in quantifying aleatoric uncertainty, revealing a weak performance across most methods.", "motivation": "To address the lack of evidence in validating conformal predictors' capability of capturing aleatoric uncertainty caused by overlapping class annotations.", "method": "The authors measured correlation between conformal prediction set sizes and the number of distinct human-labeled annotations per instance, and assessed the similarity between prediction sets and annotations using eight deep learning models and four datasets.", "result": "The study shows conformal prediction set sizes mostly exhibit weak correlation with human annotations, indicating limited effectiveness in capturing aleatoric uncertainty.", "conclusion": "Conformal predictors can offer higher class coverage but do not effectively quantify aleatoric uncertainty, necessitating critical reassessment of their outputs."}}
{"id": "2509.06285", "pdf": "https://arxiv.org/pdf/2509.06285", "abs": "https://arxiv.org/abs/2509.06285", "authors": ["Xiangcheng Hu", "Xieyuanli Chen", "Mingkai Jia", "Jin Wu", "Ping Tan", "Steven L. Waslander"], "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration", "categories": ["cs.RO"], "comment": "24 pages, 19 figures, 9 tables", "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.", "AI": {"tldr": "The paper introduces DCReg, a framework to address ill-conditioned LiDAR point cloud registration problems, achieving significant improvements in accuracy and efficiency compared to existing methods.", "motivation": "The authors aim to address the challenge of ill-conditioned LiDAR point cloud registration in narrow or degenerate environments, which leads to unstable solutions and degraded accuracy.", "method": "DCReg uses Schur complement decomposition for decoupling the problem into rotational and translational subspaces, develops eigenspace-mapping techniques for actionable insights, and introduces a preconditioner to stabilize ill-conditioned directions while preserving well-constrained information.", "result": "DCReg demonstrates 20%-50% improvement in localization accuracy and 5-100 times speedup over state-of-the-art methods in various environments.", "conclusion": "This study offers a robust framework for improving LiDAR point cloud registration accuracy and speed, addressing key challenges in ill-conditioned scenarios. The implementation will be made publicly available."}}
{"id": "2509.05882", "pdf": "https://arxiv.org/pdf/2509.05882", "abs": "https://arxiv.org/abs/2509.05882", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.", "AI": {"tldr": "The paper explores how alignment techniques influence LLM agents in multiturn, multiparty collaborations and introduces a friction-aware approach demonstrating improved task collaboration outcomes.", "motivation": "The motivation is to ensure Large Language Models (LLMs) behave predictably and reliably in complex multi-interaction settings, moving beyond simplified single-user scenarios.", "method": "The authors employ a roleplay-based methodology to evaluate the performance of differently-trained LLM friction agents during collaborative dialogues, introducing a counterfactual evaluation framework to measure intervention impacts.", "result": "Friction-aware alignment methods lead to better convergence on shared task goals and improved decision correctness in multiparty collaborations compared to standard alignment baselines.", "conclusion": "The findings highlight the effectiveness of friction-aware strategies in improving collaborative group dynamics and decision-making outcomes, suggesting a promising direction for training future LLMs."}}
{"id": "2509.06505", "pdf": "https://arxiv.org/pdf/2509.06505", "abs": "https://arxiv.org/abs/2509.06505", "authors": ["Yu-Jui Huang", "Hsin-Hua Shen", "Yu-Chih Huang", "Wan-Yi Lin", "Shih-Chun Lin"], "title": "On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "The generative adversarial network (GAN) aims to approximate an unknown\ndistribution via a parameterized neural network (NN). While GANs have been\nwidely applied in reinforcement and semisupervised learning as well as computer\nvision tasks, selecting their parameters often needs an exhaustive search and\nonly a few selection methods can be proved to be theoretically optimal. One of\nthe most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on\noptimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)\nsetting, where the NN is linear and the data is Gaussian. In this paper, we\nfocus on the characterization of optimal WGAN parameters beyond the LQG\nsetting. We derive closed-form optimal parameters for one-dimensional WGANs\nwhen the NN has non-linear activation functions and the data is non-Gaussian.\nTo extend this to high-dimensional WGANs, we adopt the sliced Wasserstein\nframework and replace the constraint on marginal distributions of the randomly\nprojected data by a constraint on the joint distribution of the original\n(unprojected) data. We show that the linear generator can be asymptotically\noptimal for sliced WGAN with non-Gaussian data. Empirical studies show that our\nclosed-form WGAN parameters have good convergence behavior with data under both\nGaussian and Laplace distributions. Also, compared to the r principal component\nanalysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve\nthe same performance while requiring less computational resources.", "AI": {"tldr": "This paper focuses on optimizing WGAN's parameters beyond conventional settings by deriving closed-form solutions for one-dimensional WGANs and proposing an advantageous approach for high-dimensional cases using the sliced Wasserstein framework.", "motivation": "While GANs are versatile and widely applied, there is a lack of efficient and theoretically optimal methods for parameter selection, particularly for WGANs in non-LQG settings.", "method": "The authors derive closed-form optimal parameters for one-dimensional WGANs under non-linear activation functions and non-Gaussian data. For high-dimensional cases, they utilize the sliced Wasserstein framework with a new constraint on the joint distribution of unprojected data.", "result": "They demonstrate that the linear generator can be asymptotically optimal for sliced WGANs with non-Gaussian data. Empirical tests confirm that their parameters converge well for Gaussian and Laplace distributions, and their approach is computationally more efficient than r-PCA.", "conclusion": "The paper provides an effective solution for optimizing WGAN parameters in non-conventional settings, balancing performance and computational efficiency through the sliced Wasserstein framework."}}
{"id": "2509.06307", "pdf": "https://arxiv.org/pdf/2509.06307", "abs": "https://arxiv.org/abs/2509.06307", "authors": ["Lei Shu", "Dong Zhao"], "title": "Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Conventional approaches to building energy retrofit decision making suffer\nfrom limited generalizability and low interpretability, hindering adoption in\ndiverse residential contexts. With the growth of Smart and Connected\nCommunities, generative AI, especially large language models (LLMs), may help\nby processing contextual information and producing practitioner readable\nrecommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,\nLlama, and Claude) on residential retrofit decisions under two objectives:\nmaximizing CO2 reduction (technical) and minimizing payback period\n(sociotechnical). Performance is assessed on four dimensions: accuracy,\nconsistency, sensitivity, and reasoning, using a dataset of 400 homes across 49\nUS states. LLMs generate effective recommendations in many cases, reaching up\nto 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.\nPerformance is stronger for the technical objective, while sociotechnical\ndecisions are limited by economic trade offs and local context. Agreement\nacross models is low, and higher performing models tend to diverge from others.\nLLMs are sensitive to location and building geometry but less sensitive to\ntechnology and occupant behavior. Most models show step by step, engineering\nstyle reasoning, but it is often simplified and lacks deeper contextual\nawareness. Overall, LLMs are promising assistants for energy retrofit decision\nmaking, but improvements in accuracy, consistency, and context handling are\nneeded for reliable practice.", "AI": {"tldr": "Paper evaluates the use of generative AI like LLMs for residential energy retrofit decision-making, finding potential but identifying challenges in accuracy, consistency, and context handling.", "motivation": "To improve the process of building energy retrofit decision-making, which traditionally suffers from poor generalizability and interpretability, hindering its application in diverse residential settings.", "method": "Seven large language models were tested on making decisions about residential retrofits. The evaluation focused on two goals \u2013 maximizing CO2 reduction and minimizing payback period \u2013 and assessed metrics like accuracy, consistency, sensitivity, and reasoning using data from 400 homes across the U.S.", "result": "LLMs demonstrated potential by producing recommendations with up to 54.5% top-1 match accuracy and 92.8% within top-5 accuracy. Performance varied between objectives, with stronger results for maximizing CO2 reduction.", "conclusion": "Large language models show promise as tools for energy retrofit decision-making, but they require improvements in areas like contextual understanding, economic trade-offs handling, and consistency to be widely reliable."}}
{"id": "2509.05582", "pdf": "https://arxiv.org/pdf/2509.05582", "abs": "https://arxiv.org/abs/2509.05582", "authors": ["Zhiling Ye", "Cong Zhou", "Xiubao Zhang", "Haifeng Shen", "Weihong Deng", "Quan Lu"], "title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian Head", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods.", "AI": {"tldr": "The paper introduces a framework for generating controllable 3D Gaussian head avatars using a single image, achieving high rendering performance and improved reconstruction quality.", "motivation": "To develop a framework that generates realistic and controllable 3D Gaussian head avatars from a single portrait image and addresses limitations of existing methods, such as generalization and texture reconstruction.", "method": "The approach involves creating a large-scale one-shot Gaussian head generator using WebSSL and a two-stage training strategy for enhanced generalization and texture quality. It demonstrates high-efficiency rendering driven by control signals.", "result": "The model achieves 90 FPS at a resolution of 512x512, adheres to scaling laws for improved reconstruction performance, and showcases superior results compared to state-of-the-art methods.", "conclusion": "The framework separates reconstruction and reenactment, providing efficient and high-quality 3D Gaussian avatars driven by signals, outperforming other methods in both qualitative and quantitative evaluations."}}
{"id": "2509.05830", "pdf": "https://arxiv.org/pdf/2509.05830", "abs": "https://arxiv.org/abs/2509.05830", "authors": ["Akaash Kolluri", "Shengguang Wu", "Joon Sung Park", "Michael S. Bernstein"], "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments", "categories": ["cs.LG", "cs.CY"], "comment": "16 pages, 5 figures", "summary": "Large language models (LLMs) offer a powerful opportunity to simulate the\nresults of social science experiments. In this work, we demonstrate that\nfinetuning LLMs directly on individual-level responses from past experiments\nmeaningfully improves the accuracy of such simulations across diverse social\nscience domains. We construct SocSci210 via an automatic pipeline, a dataset\ncomprising 2.9 million responses from 400,491 participants in 210 open-source\nsocial science experiments. Through finetuning, we achieve multiple levels of\ngeneralization. In completely unseen studies, our strongest model,\nSocrates-Qwen-14B, produces predictions that are 26% more aligned with\ndistributions of human responses to diverse outcome questions under varying\nconditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by\n13%. By finetuning on a subset of conditions in a study, generalization to new\nunseen conditions is particularly robust, improving by 71%. Since SocSci210\ncontains rich demographic information, we reduce demographic parity, a measure\nof bias, by 10.6% through finetuning. Because social sciences routinely\ngenerate rich, topic-specific datasets, our findings indicate that finetuning\non such data could enable more accurate simulations for experimental hypothesis\nscreening. We release our data, models and finetuning code at\nstanfordhci.github.io/socrates.", "AI": {"tldr": "The paper explores finetuning large language models (LLMs) on a dataset of individual-level responses from social science experiments, demonstrating improved accuracy for simulating human behavior in experiments.", "motivation": "The paper aims to address the challenge of accurately simulating social science experiments using large language models by leveraging individual-level responses from past experiments.", "method": "The authors create a dataset called SocSci210, containing 2.9 million responses from 210 open-source social science experiments, and finetune LLMs on this data to improve their predictive capabilities.", "result": "The finetuned model, Socrates-Qwen-14B, improved prediction accuracy by 26% compared to its base model and 13% compared to GPT-4o in unseen studies. Generalization to new conditions was robust, showing a 71% improvement, and demographic bias was reduced by 10.6%.", "conclusion": "Finetuning LLMs on rich, topic-specific social science datasets enhances their utility for experimental hypothesis screening, offering broader applications in social science research."}}
{"id": "2509.06296", "pdf": "https://arxiv.org/pdf/2509.06296", "abs": "https://arxiv.org/abs/2509.06296", "authors": ["Francisco Affonso", "Felipe Andrade G. Tommaselli", "Juliano Negri", "Vivian S. Medeiros", "Mateus V. Gasparino", "Girish Chowdhary", "Marcelo Becker"], "title": "Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": "Under review at IEEE Robotics and Automation Letters. 8 pages", "summary": "Traditional RL-based locomotion controllers often suffer from low data\nefficiency, requiring extensive interaction to achieve robust performance. We\npresent a model-based reinforcement learning (MBRL) framework that improves\nsample efficiency for quadrupedal locomotion by appending synthetic data to the\nend of standard rollouts in PPO-based controllers, following the Dyna-Style\nparadigm. A predictive model, trained alongside the policy, generates\nshort-horizon synthetic transitions that are gradually integrated using a\nscheduling strategy based on the policy update iterations. Through an ablation\nstudy, we identified a strong correlation between sample efficiency and rollout\nlength, which guided the design of our experiments. We validated our approach\nin simulation on the Unitree Go1 robot and showed that replacing part of the\nsimulated steps with synthetic ones not only mimics extended rollouts but also\nimproves policy return and reduces variance. Finally, we demonstrate that this\nimprovement transfers to the ability to track a wide range of locomotion\ncommands using fewer simulated steps.", "AI": {"tldr": "The paper proposes a model-based reinforcement learning (MBRL) framework to enhance the sample efficiency of quadrupedal locomotion control using synthetic data and a scheduling strategy. Experiments show boosted performance and reduced variance.", "motivation": "Traditional RL-based locomotion controllers require extensive data interactions to achieve robust performance, which limits their efficiency.", "method": "The proposed MBRL framework augments PPO-based controllers by appending short-horizon synthetic data to rollouts. A predictive model generates these synthetic transitions and integrates them using a policy update-based scheduling strategy. An ablation study determined the optimal synthetic rollout length for enhancing sample efficiency.", "result": "Validation in simulation on the Unitree Go1 robot showed that replacing some simulated steps with synthetic ones improved policy returns, reduced variance, and enabled wide-range locomotion command tracking with fewer steps.", "conclusion": "The MBRL approach enhances policy training efficiency through synthetic data integration, offering more effective and reliable locomotion control with reduced interaction requirements."}}
{"id": "2509.05908", "pdf": "https://arxiv.org/pdf/2509.05908", "abs": "https://arxiv.org/abs/2509.05908", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Shiliang Zhang", "Qian Chen", "Jiqing Han"], "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198", "summary": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.", "AI": {"tldr": "The authors address the problem of contextual ASR performance degradation with long biasing lists by introducing PSC-Joint, a method that models relevant semantic correlations at different granularities and filters irrelevant information. This improves F1 scores significantly.", "motivation": "The paper aims to address the issue of rapidly declining effectiveness of cross-attention methods in ASR when dealing with long biasing lists, by identifying that only a small subset of biasing information is relevant at any time.", "method": "The PSC-Joint approach defines semantic correlations at list-level, phrase-level, and token-level, then jointly models these to identify and focus on the most relevant biasing information. A grouped-and-competitive purification mechanism further reduces computational overhead by filtering out irrelevant phrases.", "result": "The proposed method achieves notable improvements in F1 scores: up to a 21.34% average relative improvement on AISHELL-1 and 28.46% on KeSpeech, outperforming baseline models across various biasing list lengths.", "conclusion": "Identifying and integrating only the most relevant biasing information improves contextual ASR performance, particularly under varying biasing list lengths. The PSC-Joint method demonstrates both computational efficiency and robustness."}}
{"id": "2509.06697", "pdf": "https://arxiv.org/pdf/2509.06697", "abs": "https://arxiv.org/abs/2509.06697", "authors": ["Tanujit Chakraborty", "Donia Besher", "Madhurima Panja", "Shovon Sengupta"], "title": "Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties", "categories": ["econ.EM", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Accurate forecasting of exchange rates remains a persistent challenge,\nparticularly for emerging economies such as Brazil, Russia, India, and China\n(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity\nproperties that conventional time series models struggle to capture.\nAdditionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US\nmonetary policy uncertainty, oil price growth rates, and country-specific\nshort-term interest rate differentials. These empirical complexities underscore\nthe need for a flexible modeling framework that can jointly accommodate long\nmemory, nonlinearity, and the influence of external drivers. To address these\nchallenges, we propose a Neural AutoRegressive Fractionally Integrated Moving\nAverage (NARFIMA) model that combines the long-memory representation of ARFIMA\nwith the nonlinear learning capacity of neural networks, while flexibly\nincorporating exogenous causal variables. We establish theoretical properties\nof the model, including asymptotic stationarity of the NARFIMA process using\nMarkov chains and nonlinear time series techniques. We quantify forecast\nuncertainty using conformal prediction intervals within the NARFIMA framework.\nEmpirical results across six forecast horizons show that NARFIMA consistently\noutperforms various state-of-the-art statistical and machine learning models in\nforecasting BRIC exchange rates. These findings provide new insights for\npolicymakers and market participants navigating volatile financial conditions.\nThe \\texttt{narfima} \\textbf{R} package provides an implementation of our\napproach.", "AI": {"tldr": "The paper introduces a novel NARFIMA model for forecasting BRIC exchange rates, addressing long-memory, nonlinearity, and non-stationarity challenges while outperforming existing models.", "motivation": "Accurate exchange rate forecasting for emerging economies faces challenges from complex dynamics including long memory, nonlinearity, non-stationarity, and external drivers such as global economic uncertainty and oil prices.", "method": "The study proposes the NARFIMA model, integrating ARFIMA's long-memory representation and neural networks' nonlinear learning capacity, while using Markov chains and nonlinear techniques for theoretical validation.", "result": "Empirical forecasting results across six horizons demonstrate that the NARFIMA model outperforms state-of-the-art statistical and machine learning methods for BRIC currencies.", "conclusion": "The findings highlight the effectiveness of NARFIMA for challenging financial predictions and stress its utility for policymakers and market participants. The method is implemented in the R package 'narfima'."}}
{"id": "2509.06337", "pdf": "https://arxiv.org/pdf/2509.06337", "abs": "https://arxiv.org/abs/2509.06337", "authors": ["Jianpeng Zhao", "Chenyu Yuan", "Weiming Luo", "Haoling Xie", "Guangwei Zhang", "Steven Jige Quan", "Zixuan Yuan", "Pengyang Wang", "Denghui Zhang"], "title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation", "categories": ["cs.AI"], "comment": null, "summary": "Questionnaire-based surveys are foundational to social science research and\npublic policymaking, yet traditional survey methods remain costly,\ntime-consuming, and often limited in scale. This paper explores a new paradigm:\nsimulating virtual survey respondents using Large Language Models (LLMs). We\nintroduce two novel simulation settings, namely Partial Attribute Simulation\n(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the\nability of LLMs to generate accurate and demographically coherent responses. In\nPAS, the model predicts missing attributes based on partial respondent\nprofiles, whereas FAS involves generating complete synthetic datasets under\nboth zero-context and context-enhanced conditions. We curate a comprehensive\nbenchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey\nSimulation), that spans 11 real-world public datasets across four sociological\ndomains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA\n3.0/3.1-8B) reveals consistent trends in prediction performance, highlights\nfailure modes, and demonstrates how context and prompt design impact simulation\nfidelity. This work establishes a rigorous foundation for LLM-driven survey\nsimulations, offering scalable and cost-effective tools for sociological\nresearch and policy evaluation. Our code and dataset are available at:\nhttps://github.com/dart-lab-research/LLM-S-Cube-Benchmark", "AI": {"tldr": "The paper investigates using Large Language Models (LLMs) to simulate virtual survey respondents for scalable and cost-effective sociological research.", "motivation": "Traditional survey methods are expensive, time-consuming, and limited in scale, prompting the need for innovative approaches.", "method": "The authors introduce Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS) for generating survey responses using LLMs, and develop a benchmark suite spanning 11 datasets.", "result": "Evaluations on multiple LLMs show consistent trends, identified failure modes, and demonstrated that context and prompt design influence simulation accuracy.", "conclusion": "LLM-driven survey simulations represent a promising, scalable, and budget-friendly alternative to traditional surveys for sociological studies and policymaking."}}
{"id": "2509.05592", "pdf": "https://arxiv.org/pdf/2509.05592", "abs": "https://arxiv.org/abs/2509.05592", "authors": ["Changtao Miao", "Yi Zhang", "Man Luo", "Weiwei Feng", "Kaiyuan Zheng", "Qi Chu", "Tao Gong", "Jianshu Li", "Yunfeng Diao", "Wei Zhou", "Joey Tianyi Zhou", "Xiaoshuai Hao"], "title": "MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Content (AIGC) have\nenabled increasingly sophisticated face forgeries, posing a significant threat\nto social security. However, current Deepfake detection methods are limited by\nconstraints in existing datasets, which lack the diversity necessary in\nreal-world scenarios. Specifically, these data sets fall short in four key\nareas: unknown of advanced forgery techniques, variability of facial scenes,\nrichness of real data, and degradation of real-world propagation. To address\nthese challenges, we propose the Multi-dimensional Face Forgery Image\n(\\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances\nrealism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied\nFacial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation\nOperations. MFFI integrates $50$ different forgery methods and contains $1024K$\nimage samples. Benchmark evaluations show that MFFI outperforms existing public\ndatasets in terms of scene complexity, cross-domain generalization capability,\nand detection difficulty gradients. These results validate the technical\nadvance and practical utility of MFFI in simulating real-world conditions. The\ndataset and additional details are publicly available at\n{https://github.com/inclusionConf/MFFI}.", "AI": {"tldr": "This paper introduces the Multi-dimensional Face Forgery Image (MFFI) dataset to address limitations in current Deepfake detection datasets and improve real-world application.", "motivation": "Current Deepfake detection methods are limited by datasets lacking diversity and realism in terms of advanced forgery techniques, varied scenes, authentic data, and degradation modeling, which compromises their effectiveness in real-world scenarios.", "method": "The authors developed the MFFI dataset by incorporating 50 different forgery methods and 1,024,000 image samples. The dataset focuses on four dimensions: wider forgery methods, varied facial scenes, diversified authentic data, and multi-level degradation operations.", "result": "Benchmark tests reveal that MFFI surpasses existing datasets by offering greater scene diversity, improved cross-domain generalization, and more challenging detection scenarios.", "conclusion": "MFFI provides a significant improvement over existing datasets by better simulating real-world conditions for face forgery detection. Its availability is expected to enhance research and practical applications in the field."}}
{"id": "2509.05833", "pdf": "https://arxiv.org/pdf/2509.05833", "abs": "https://arxiv.org/abs/2509.05833", "authors": ["Zeyu Song", "Sainyam Galhotra", "Shagufta Mehnaz"], "title": "Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "The rise of distributed and privacy-preserving machine learning has sparked\ninterest in decentralized gradient marketplaces, where participants trade\nintermediate artifacts like gradients. However, existing Federated Learning\n(FL) benchmarks overlook critical economic and systemic factors unique to such\nmarketplaces-cost-effectiveness, fairness to sellers, and market\nstability-especially when a buyer relies on a private baseline dataset for\nevaluation.\n  We introduce a comprehensive benchmark framework to holistically evaluate\nrobust gradient aggregation methods within these buyer-baseline-reliant\nmarketplaces. Our contributions include: (1) a simulation environment modeling\nmarketplace dynamics with a variable buyer baseline and diverse seller\ndistributions; (2) an evaluation methodology augmenting standard FL metrics\nwith marketplace-centric dimensions such as Economic Efficiency, Fairness, and\nSelection Dynamics; (3) an in-depth empirical analysis of the existing\nDistributed Gradient Marketplace framework, MartFL, including the integration\nand comparative evaluation of adapted FLTrust and SkyMask as alternative\naggregation strategies within it. This benchmark spans diverse datasets, local\nattacks, and Sybil attacks targeting the marketplace selection process; and (4)\nactionable insights into the trade-offs between model performance, robustness,\ncost, fairness, and stability.\n  This benchmark equips the community with essential tools and empirical\nevidence to evaluate and design more robust, equitable, and economically viable\ndecentralized gradient marketplaces.", "AI": {"tldr": "The paper introduces a benchmark framework to assess decentralized gradient marketplaces in privacy-preserving machine learning, focusing on cost-effectiveness, fairness, and stability.", "motivation": "Existing benchmarks in Federated Learning fail to address economic and systemic aspects critical to decentralized marketplaces, such as fairness and stability.", "method": "The authors created a simulation environment, developed evaluation metrics for marketplace factors, analyzed alternative aggregation methods, and provided insights into trade-offs in the system.", "result": "An empirical analysis of MartFL and adapted methods like FLTrust and SkyMask revealed marketplace dynamics under attacks, providing comprehensive benchmarks.", "conclusion": "The benchmark aids in the design and evaluation of decentralized gradient marketplaces, ensuring robustness, fairness, and economic viability."}}
{"id": "2509.06342", "pdf": "https://arxiv.org/pdf/2509.06342", "abs": "https://arxiv.org/abs/2509.06342", "authors": ["Filip Bjelonic", "Fabian Tischhauser", "Marco Hutter"], "title": "Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots", "categories": ["cs.RO"], "comment": "Submitted to The International Journal of Robotics Research (IJRR),\n  25 Figures, 7 Tables, Open Source Data available at ETH Research Collection.\n  Open Source software available soon", "summary": "Legged robots must achieve both robust locomotion and energy efficiency to be\npractical in real-world environments. Yet controllers trained in simulation\noften fail to transfer reliably, and most existing approaches neglect\nactuator-specific energy losses or depend on complex, hand-tuned reward\nformulations. We propose a framework that integrates sim-to-real reinforcement\nlearning with a physics-grounded energy model for permanent magnet synchronous\nmotors. The framework requires a minimal parameter set to capture the\nsimulation-to-reality gap and employs a compact four-term reward with a\nfirst-principle-based energetic loss formulation that balances electrical and\nmechanical dissipation. We evaluate and validate the approach through a\nbottom-up dynamic parameter identification study, spanning actuators,\nfull-robot in-air trajectories and on-ground locomotion. The framework is\ntested on three primary platforms and deployed on ten additional robots,\ndemonstrating reliable policy transfer without randomization of dynamic\nparameters. Our method improves energetic efficiency over state-of-the-art\nmethods, achieving a 32 percent reduction in the full Cost of Transport of\nANYmal (value 1.27). All code, models, and datasets will be released.", "AI": {"tldr": "Legged robots often struggle with real-world energy efficiency and control after training in simulation. The authors propose a solution that integrates sim-to-real reinforcement learning with a physics-informed energy model for motors, reducing inefficiencies and improving reliability.", "motivation": "The authors aim to address the challenges of transferring simulation-trained controllers to the real world while ensuring robust locomotion and better energy efficiency, especially focusing on actuator-specific energy loss.", "method": "The framework combines simulation-to-reality reinforcement learning with an energy model requiring minimal parameters. A compact, four-term reward system considers both electrical and mechanical energy losses.", "result": "The approach improved the energetic efficiency of robots, achieving a 32% Cost of Transport reduction for ANYmal, and demonstrated consistent policy transfer to ten additional robots without dynamic parameter randomization.", "conclusion": "The integrated framework enhances reliability and energy efficiency for legged robots in real-world scenarios. The authors will release all associated resources for further development."}}
{"id": "2509.05915", "pdf": "https://arxiv.org/pdf/2509.05915", "abs": "https://arxiv.org/abs/2509.05915", "authors": ["Sangmin Bae"], "title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "categories": ["cs.CL"], "comment": "PhD Dissertation", "summary": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model.", "AI": {"tldr": "The paper tackles the computational challenges of deploying large language models by co-designing adaptive computation methods and architectures for better efficiency and performance.", "motivation": "Large language models face significant computational costs during deployment, and adaptive methods like early-exiting often inadvertently reduce throughput in batched inference.", "method": "The study proposes efficient parallel decoding mechanisms, employs deep parameter sharing to mitigate synchronization issues, and introduces lightweight routers for dynamic token assignment.", "result": "The approach creates a new Pareto frontier, achieving improvements in adaptive computation and parameter efficiency simultaneously.", "conclusion": "The co-designed algorithms and architectures optimize the balance between computational dynamism and system-level efficiency for large language models."}}
{"id": "2509.06896", "pdf": "https://arxiv.org/pdf/2509.06896", "abs": "https://arxiv.org/abs/2509.06896", "authors": ["William Xu", "Yiwei Lu", "Yihan Wang", "Matthew Y. R. Yang", "Zuoqiu Liu", "Gautam Kamath", "Yaoliang Yu"], "title": "Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Targeted data poisoning attacks pose an increasingly serious threat due to\ntheir ease of deployment and high success rates. These attacks aim to\nmanipulate the prediction for a single test sample in classification models.\nUnlike indiscriminate attacks that aim to decrease overall test performance,\ntargeted attacks present a unique threat to individual test instances. This\nthreat model raises a fundamental question: what factors make certain test\nsamples more susceptible to successful poisoning than others? We investigate\nhow attack difficulty varies across different test instances and identify key\ncharacteristics that influence vulnerability. This paper introduces three\npredictive criteria for targeted data poisoning difficulty: ergodic prediction\naccuracy (analyzed through clean training dynamics), poison distance, and\npoison budget. Our experimental results demonstrate that these metrics\neffectively predict the varying difficulty of real-world targeted poisoning\nattacks across diverse scenarios, offering practitioners valuable insights for\nvulnerability assessment and understanding data poisoning attacks.", "AI": {"tldr": "The paper studies factors that make certain test samples more vulnerable to targeted data poisoning attacks, introducing three predictive criteria for assessing attack difficulty.", "motivation": "To understand why certain test instances are more prone to successful targeted data poisoning attacks and provide insights into assessing vulnerability.", "method": "The authors propose and analyze three criteria\u2014ergodic prediction accuracy, poison distance, and poison budget\u2014through experimental evaluations.", "result": "The study demonstrates that these criteria can effectively predict the difficulty of targeted attacks in diverse real-world scenarios.", "conclusion": "The findings offer valuable tools for understanding and assessing vulnerabilities in classification models against targeted data poisoning attacks."}}
{"id": "2509.06341", "pdf": "https://arxiv.org/pdf/2509.06341", "abs": "https://arxiv.org/abs/2509.06341", "authors": ["Issue Yishu Wang", "Kakam Chong", "Xiaofeng Wang", "Xu Yan", "DeXin Kong", "Chen Ju", "Ming Chen", "Shuai Xiao", "Shuguang Han", "jufeng chen"], "title": "Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent", "categories": ["cs.AI"], "comment": null, "summary": "In online second-hand marketplaces, multi-turn bargaining is a crucial part\nof seller-buyer interactions. Large Language Models (LLMs) can act as seller\nagents, negotiating with buyers on behalf of sellers under given business\nconstraints. A critical ability for such agents is to track and accurately\ninterpret cumulative buyer intents across long negotiations, which directly\nimpacts bargaining effectiveness. We introduce a multi-turn evaluation\nframework for measuring the bargaining ability of seller agents in e-commerce\ndialogues. The framework tests whether an agent can extract and track buyer\nintents. Our contributions are: (1) a large-scale e-commerce bargaining\nbenchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a\nturn-level evaluation framework grounded in Theory of Mind (ToM) with annotated\nbuyer intents, moving beyond outcome-only metrics; and (3) an automated\npipeline that extracts reliable intent from massive dialogue data.", "AI": {"tldr": "The paper presents a framework to evaluate seller agents that engage in multi-turn bargaining in online second-hand marketplaces, focusing on tracking buyer intents.", "motivation": "There is a need for seller agents (powered by LLMs) to more effectively interpret buyer intentions during negotiations in e-commerce settings, which existing methods do not adequately address.", "method": "The authors introduce a large-scale bargaining benchmark with annotated intent datasets and a turn-level evaluation framework based on Theory of Mind. They also provide an automated pipeline for extracting buyer intent from dialogues.", "result": "The proposed framework enables detailed tracking of buyer intents, moving beyond conventional outcome-only metrics, and supports comprehensive benchmarking.", "conclusion": "The research advances the study of LLM-powered seller agents by establishing a structured framework for evaluating their negotiation skills, particularly in capturing buyer intentions effectively."}}
{"id": "2509.05604", "pdf": "https://arxiv.org/pdf/2509.05604", "abs": "https://arxiv.org/abs/2509.05604", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IJCV, 29 pages, 14 figures, 11 tables", "summary": "Video summarization aims to select keyframes that are visually diverse and\ncan represent the whole story of a given video. Previous approaches have\nfocused on global interlinkability between frames in a video by temporal\nmodeling. However, fine-grained visual entities, such as objects, are also\nhighly related to the main content of the video. Moreover, language-guided\nvideo summarization, which has recently been studied, requires a comprehensive\nlinguistic understanding of complex real-world videos. To consider how all the\nobjects are semantically related to each other, this paper regards video\nsummarization as a language-guided spatiotemporal graph modeling problem. We\npresent recursive spatiotemporal graph networks, called VideoGraph, which\nformulate the objects and frames as nodes of the spatial and temporal graphs,\nrespectively. The nodes in each graph are connected and aggregated with graph\nedges, representing the semantic relationships between the nodes. To prevent\nthe edges from being configured with visual similarity, we incorporate language\nqueries derived from the video into the graph node representations, enabling\nthem to contain semantic knowledge. In addition, we adopt a recursive strategy\nto refine initial graphs and correctly classify each frame node as a keyframe.\nIn our experiments, VideoGraph achieves state-of-the-art performance on several\nbenchmarks for generic and query-focused video summarization in both supervised\nand unsupervised manners. The code is available at\nhttps://github.com/park-jungin/videograph.", "AI": {"tldr": "This paper introduces VideoGraph, a language-guided recursive spatiotemporal graph network for video summarization, achieving state-of-the-art performance on several benchmarks.", "motivation": "To improve video summarization by incorporating the semantic relationships of fine-grained visual entities like objects and leveraging language-guided queries for better understanding and summarization.", "method": "The proposed method uses recursive spatiotemporal graph networks where objects and frames are modeled as graph nodes, and semantic relationships are depicted as graph edges. Language queries are integrated to enrich semantic node representations for precise classification of keyframes.", "result": "VideoGraph demonstrates state-of-the-art performance in both generic and query-focused video summarization tasks across multiple benchmarks.", "conclusion": "The VideoGraph framework effectively integrates semantic knowledge with spatiotemporal graph modeling, refining video summarization tasks through recursive strategies and providing high accuracy in supervised and unsupervised settings."}}
{"id": "2509.05839", "pdf": "https://arxiv.org/pdf/2509.05839", "abs": "https://arxiv.org/abs/2509.05839", "authors": ["Daksh Mittal", "Shunri Zheng", "Jing Dong", "Hongseok Namkoong"], "title": "Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "While queueing network models are powerful tools for analyzing service\nsystems, they traditionally require substantial human effort and domain\nexpertise to construct. To make this modeling approach more scalable and\naccessible, we propose a data-driven framework for queueing network modeling\nand simulation based on autoregressive sequence models trained on event-stream\ndata. Instead of explicitly specifying arrival processes, service mechanisms,\nor routing logic, our approach learns the conditional distributions of event\ntypes and event times, recasting the modeling task as a problem of sequence\ndistribution learning. We show that Transformer-style architectures can\neffectively parameterize these distributions, enabling automated construction\nof high-fidelity simulators. As a proof of concept, we validate our framework\non event tables generated from diverse queueing networks, showcasing its\nutility in simulation, uncertainty quantification, and counterfactual\nevaluation. Leveraging advances in artificial intelligence and the growing\navailability of data, our framework takes a step toward more automated,\ndata-driven modeling pipelines to support broader adoption of queueing network\nmodels across service domains.", "AI": {"tldr": "The paper presents a data-driven framework using Transformer-style models to automate queueing network modeling and simulation based on event-stream data.", "motivation": "Current queueing network models require substantial human effort and domain expertise, limiting scalability and accessibility.", "method": "The proposed framework uses autoregressive sequence models to learn the conditional distributions of event types and event times, avoiding the need for explicitly defined processes.", "result": "Validation on diverse queueing networks demonstrates the framework's utility for simulation, uncertainty quantification, and counterfactual evaluation.", "conclusion": "The framework leverages AI to automate and democratize queueing network modeling, enabling broader adoption across service domains."}}
{"id": "2509.06375", "pdf": "https://arxiv.org/pdf/2509.06375", "abs": "https://arxiv.org/abs/2509.06375", "authors": ["Fujiang Yuan", "Zhen Tian", "Yangfan He", "Guojian Zou", "Chunhong Yuan", "Yanhong Peng", "Zhihao Lin"], "title": "Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "In recent years, ensuring safety, efficiency, and comfort in interactive\nautonomous driving has become a critical challenge. Traditional model-based\ntechniques, such as game-theoretic methods and robust control, are often overly\nconservative or computationally intensive. Conversely, learning-based\napproaches typically require extensive training data and frequently exhibit\nlimited interpretability and generalizability. Simpler strategies, such as Risk\nPotential Fields (RPF), provide lightweight alternatives with minimal data\ndemands but are inherently static and struggle to adapt effectively to dynamic\ntraffic conditions. To overcome these limitations, we propose the Evolutionary\nRisk Potential Field (ERPF), a novel approach that dynamically updates risk\nassessments in dynamical scenarios based on historical obstacle proximity data.\nWe introduce a Risk-Ellipse construct that combines longitudinal reach and\nlateral uncertainty into a unified spatial temporal collision envelope.\nAdditionally, we define an adaptive Evolution Factor metric, computed through\nsigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard\n(TWH), which dynamically adjusts the dimensions of the ellipse axes in real\ntime. This adaptive risk metric is integrated seamlessly into a Model\nPredictive Control (MPC) framework, enabling autonomous vehicles to proactively\naddress complex interactive driving scenarios in terms of uncertain driving of\nsurrounding vehicles. Comprehensive comparative experiments demonstrate that\nour ERPF-MPC approach consistently achieves smoother trajectories, higher\naverage speeds, and collision-free navigation, offering a robust and adaptive\nsolution suitable for complex interactive driving environments.", "AI": {"tldr": "The paper introduces an Evolutionary Risk Potential Field (ERPF) approach integrated with Model Predictive Control (MPC) for more dynamic risk assessment and adaptive autonomous driving.", "motivation": "Ensure autonomous driving systems can dynamically respond to uncertain and interactive driving environments while addressing limitations in traditional and learning-based approaches.", "method": "Developed ERPF, combining Risk-Ellipse constructs with adaptive metrics like Evolution Factor based on TTC and TWH. Integrated this dynamically updating risk mechanism into an MPC framework to enhance responsiveness.", "result": "Experiments show ERPF-MPC achieves smoother trajectories, better speeds, and collision-free navigation compared to traditional techniques.", "conclusion": "ERPF-MPC offers a robust solution with adaptive capabilities, making it highly suitable for dynamic and complex autonomous driving scenarios."}}
{"id": "2509.06065", "pdf": "https://arxiv.org/pdf/2509.06065", "abs": "https://arxiv.org/abs/2509.06065", "authors": ["Lorenzo Alfred Nery", "Ronald Dawson Catignas", "Thomas James Tiam-Lee"], "title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "categories": ["cs.CL"], "comment": "14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of\n  NLPIR 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage.", "AI": {"tldr": "A Filipino version of the TruthfulQA benchmark, called KatotohananQA, is introduced to evaluate large language model truthfulness in low-resource languages, revealing performance gaps and multilingual transfer issues.", "motivation": "The motivation behind this paper is to address the lack of benchmarks for evaluating the truthfulness of large language models (LLMs) in low-resource languages, as existing benchmarks are primarily in English.", "method": "The authors translated the TruthfulQA benchmark into Filipino, named KatotohananQA, and assessed seven free-tier proprietary models using a binary-choice evaluation framework.", "result": "Performance gaps were identified between English and Filipino truthfulness, with newer OpenAI models such as GPT-5 and GPT-5 mini showing stronger multilingual robustness. Disparities in question characteristics were also noted.", "conclusion": "The study highlights the need for broader multilingual evaluations to ensure fairness and reliability of LLMs, as some question types and topics are less robust in multilingual contexts."}}
{"id": "2509.06355", "pdf": "https://arxiv.org/pdf/2509.06355", "abs": "https://arxiv.org/abs/2509.06355", "authors": ["Yunzhe Wang", "Volkan Ustun", "Chris McGroarty"], "title": "A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at the Winter Simulation Conference 2025, December, Seattle\n  USA", "summary": "Modern simulation environments for complex multi-agent interactions must\nbalance high-fidelity detail with computational efficiency. We present DECOY, a\nnovel multi-agent simulator that abstracts strategic, long-horizon planning in\n3D terrains into high-level discretized simulation while preserving low-level\nenvironmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a\ntestbed, our framework accurately simulates gameplay using only movement\ndecisions as tactical positioning -- without explicitly modeling low-level\nmechanics such as aiming and shooting. Central to our approach is a waypoint\nsystem that simplifies and discretizes continuous states and actions, paired\nwith neural predictive and generative models trained on real CS:GO tournament\ndata to reconstruct event outcomes. Extensive evaluations show that replays\ngenerated from human data in DECOY closely match those observed in the original\ngame. Our publicly available simulation environment provides a valuable tool\nfor advancing research in strategic multi-agent planning and behavior\ngeneration.", "AI": {"tldr": "DECOY is a multi-agent simulation framework for modeling long-term strategies in 3D terrains, demonstrated using CS:GO gameplay data.", "motivation": "To create a simulation tool that balances long-horizon strategic modeling and environmental detail for multi-agent interactions.", "method": "The method involves waypoint-based state-action discretization and neural models trained on CS:GO data to predict and generate outcomes.", "result": "Replays generated by DECOY closely match human gameplay data from CS:GO.", "conclusion": "DECOY provides a versatile, open-source environment for studying strategic multi-agent behavior with high validation accuracy."}}
{"id": "2509.05606", "pdf": "https://arxiv.org/pdf/2509.05606", "abs": "https://arxiv.org/abs/2509.05606", "authors": ["Juan Yeo", "Ijun Jang", "Taesup Kim"], "title": "Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Dense representations are essential for vision tasks that require spatial\nprecision and fine-grained detail. While most self-supervised representation\nlearning methods focus on global representations that summarize the image as a\nwhole, such approaches often fall short in capturing the localized semantics\nnecessary for dense prediction tasks. To overcome these limitations, we propose\na framework that builds on pretrained representations through additional\nself-supervised learning, aiming to transfer existing semantic knowledge into\nthe dense feature space. Our method aligns the distributions of dense features\nbetween a teacher and a student model. Specifically, we introduce Patch-level\nKernel Alignment (PaKA), a simple yet effective alignment objective that\ncaptures statistical dependencies, thereby matching the structural\nrelationships of dense patches across the two models. In addition, we\ninvestigate augmentation strategies specifically designed for dense\nrepresentation learning. Our framework achieves state-of-the-art results across\na variety of dense vision benchmarks, demonstrating the effectiveness of our\napproach.", "AI": {"tldr": "The paper introduces a self-supervised learning framework for dense representations using Patch-level Kernel Alignment (PaKA) and custom augmentation techniques, achieving state-of-the-art results.", "motivation": "Dense vision tasks demand representations that preserve spatial precision and local semantics, which current self-supervised methods fail to sufficiently address.", "method": "The framework utilizes self-supervised learning to refine pretrained representations for dense tasks. PaKA aligns statistical dependencies between dense patches of teacher and student models. Furthermore, augmentation strategies tailored for dense learning are proposed.", "result": "The framework achieved state-of-the-art performance across several benchmarks for dense vision tasks.", "conclusion": "PaKA and specialized augmentation strategies are effective tools for transferring semantic knowledge into dense representations, improving dense vision task outcomes."}}
{"id": "2509.06404", "pdf": "https://arxiv.org/pdf/2509.06404", "abs": "https://arxiv.org/abs/2509.06404", "authors": ["Kaikai Wang", "Tianxun Li", "Liang Xu", "Qinglei Hu", "Keyou You"], "title": "Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining", "categories": ["cs.RO"], "comment": "12 pages, 9 figures, accepted to RA-L", "summary": "While Model Predictive Control (MPC) enforces safety via constraints, its\nreal-time execution can exceed embedded compute budgets. We propose a\nBarrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework\nthat synergizes neural networks' fast computation with MPC's\nconstraint-handling capability. To ensure strict safety, we replace traditional\nEuclidean distance with Control Barrier Functions (CBFs) for collision\navoidance. We integrate an offline-learned neural value function into the\noptimization objective of a Short-horizon MPC, substantially reducing online\ncomputational complexity. Additionally, we use a second neural network to learn\nthe sensitivity of the value function to system parameters, and adaptively\nadjust the neural value function based on this neural sensitivity when model\nparameters change, eliminating the need for retraining and reducing offline\ncomputation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano\nshow that BAN-MPC solves 200 times faster than traditional MPC, enabling\ncollision-free navigation with control error below 5\\% under model parameter\nvariations within 15\\%, making it an effective embedded MPC alternative.", "AI": {"tldr": "This paper presents a Barrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework that combines fast neural network computation with Model Predictive Control's constraint-handling to ensure safe and efficient real-time control.", "motivation": "Develop a control framework that ensures safety constraints while addressing the computational inefficiencies of traditional Model Predictive Control (MPC) for embedded systems.", "method": "The approach integrates Control Barrier Functions for safety, a neural network-based value function for reducing MPC computation, and another neural network for sensitivity analysis to adapt the control model dynamically without retraining.", "result": "Hardware experiments demonstrate BAN-MPC achieves a computational speed-up of 200 times compared to traditional MPC, while maintaining collision-free navigation and low control error under parameter variation.", "conclusion": "BAN-MPC offers a robust and computationally efficient solution for embedded system control, ensuring safety and adaptability without extensive offline retraining."}}
{"id": "2509.06074", "pdf": "https://arxiv.org/pdf/2509.06074", "abs": "https://arxiv.org/abs/2509.06074", "authors": ["Zhenqi Jia", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.", "AI": {"tldr": "The paper introduces MFCIG-CSS, a system utilizing fine-grained multimodal dialogue interaction graphs to improve conversational speech synthesis, outperforming existing models in prosodic expressiveness.", "motivation": "Existing conversational speech synthesis methods fail to account for the fine-grained semantic and prosodic interactions at the word level in multimodal dialogue history, leading to suboptimal prosody prediction.", "method": "The proposed system, MFCIG-CSS, constructs two multimodal fine-grained dialogue interaction graphs (semantic and prosody interaction graphs) to encode word-level interactions and their contextual impact, enhancing prosody in synthesized speech.", "result": "MFCIG-CSS demonstrated superior performance in capturing prosodic expressiveness compared to baseline models when evaluated on the DailyTalk dataset.", "conclusion": "Fine-grained modeling of semantic and prosodic interactions improves natural prosody in conversational speech synthesis, making MFCIG-CSS a significant advancement in the field."}}
{"id": "2509.06409", "pdf": "https://arxiv.org/pdf/2509.06409", "abs": "https://arxiv.org/abs/2509.06409", "authors": ["Yihong Luo", "Wenwu He", "Zhuo-Xu Cui", "Dong Liang"], "title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning", "categories": ["cs.AI"], "comment": null, "summary": "This study presents DiagCoT, a multi-stage framework that applies supervised\nfine-tuning to general-purpose vision-language models (VLMs) to emulate\nradiologists' stepwise diagnostic reasoning using only free-text reports.\nDiagCoT combines contrastive image-report tuning for domain alignment,\nchain-of-thought supervision to capture inferential logic, and reinforcement\ntuning with clinical reward signals to enhance factual accuracy and fluency. On\nthe MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC\nfrom 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08\nto 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33\n(absolute gain of 0.22). It outperformed state-of-the-art models including\nLLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By\nconverting unstructured clinical narratives into structured supervision,\nDiagCoT offers a scalable approach for developing interpretable and\ndiagnostically competent AI systems for radiology.", "AI": {"tldr": "DiagCoT is a framework that fine-tunes vision-language models using free-text reports to mimic radiologists' stepwise diagnostic reasoning.", "motivation": "To develop interpretable and diagnostically competent AI systems for radiology using unstructured clinical narratives.", "method": "The framework includes contrastive image-report tuning, chain-of-thought supervision, and reinforcement tuning with clinical reward signals.", "result": "On the MIMIC-CXR benchmark, DiagCoT achieved substantial improvements in disease classification, pathology grounding, and report generation metrics compared to state-of-the-art models.", "conclusion": "DiagCoT provides a scalable and effective approach to training AI systems for radiology, outperforming existing models in several key metrics and long-tailed disease diagnosis."}}
{"id": "2509.05614", "pdf": "https://arxiv.org/pdf/2509.05614", "abs": "https://arxiv.org/abs/2509.05614", "authors": ["Hanzhen Wang", "Jiaming Xu", "Jiayi Pan", "Yongkang Zhou", "Guohao Dai"], "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "8pages, 10 figures,", "summary": "Pruning accelerates compute-bound models by reducing computation. Recently\napplied to Vision-Language-Action (VLA) models, existing methods prune tokens\nusing only local info from current action, ignoring global context from prior\nactions, causing >20% success rate drop and limited speedup. We observe high\nsimilarity across consecutive actions and propose leveraging both local\n(current) and global (past) info for smarter token selection. We introduce\nSpecPrune-VLA, a training-free method with two-level pruning and heuristic\ncontrol: (1) Static pruning at action level: uses global history and local\ncontext to reduce visual tokens per action; (2) Dynamic pruning at layer level:\nprunes tokens per layer based on layer-specific importance; (3) Lightweight\naction-aware controller: classifies actions as coarse/fine-grained (by speed),\nadjusting pruning aggressiveness since fine-grained actions are\npruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times\nspeedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.\nOpenVLA-OFT, with negligible success rate loss.", "AI": {"tldr": "SpecPrune-VLA enhances token pruning in Vision-Language-Action models by leveraging both local and global context, achieving speedups with minimal loss in success rates.", "motivation": "To address inefficiencies in token pruning methods for Vision-Language-Action models that ignore global context and lead to poor performance and limited computational acceleration.", "method": "SpecPrune-VLA introduces a two-level pruning approach: static action-level pruning utilizing global history and local context, dynamic layer-level pruning based on layer-specific importance, and an action-aware controller that adjusts pruning aggressiveness.", "result": "The model achieved a 1.46x computational speedup on NVIDIA A800 and 1.57x on NVIDIA GeForce RTX 3090 compared to OpenVLA-OFT, with negligible reduction in success rates.", "conclusion": "By integrating global history, local context, and adaptive pruning strategies, SpecPrune-VLA improves computational efficiency while maintaining performance for Vision-Language-Action models."}}
{"id": "2509.05874", "pdf": "https://arxiv.org/pdf/2509.05874", "abs": "https://arxiv.org/abs/2509.05874", "authors": ["Shao-An Yin"], "title": "Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IR", "I.2.6"], "comment": "8 pages, 2 figures", "summary": "The rapid expansion of scientific literature makes it increasingly difficult\nto acquire new knowledge, particularly in specialized domains where reasoning\nis complex, full-text access is restricted, and target references are sparse\namong a large set of candidates. We present a Deep Reinforcement Learning\nframework for sparse reference selection that emulates human knowledge\nconstruction, prioritizing which papers to read under limited time and cost.\nEvaluated on drug--gene relation discovery with access restricted to titles and\nabstracts, our approach demonstrates that both humans and machines can\nconstruct knowledge effectively from partial information.", "AI": {"tldr": "A Deep Reinforcement Learning framework to select sparse references for prioritizing reading, aiding knowledge construction from limited information.", "motivation": "The expansion of scientific literature increases the challenge of acquiring knowledge due to restricted access and sparse relevant references.", "method": "Deep Reinforcement Learning framework to simulate human-like knowledge construction by selecting key papers with limited data access.", "result": "The framework successfully demonstrated effective knowledge construction from partial information in drug--gene relation discovery.", "conclusion": "Humans and machines can effectively construct knowledge with limited information through the proposed system, optimizing literature review efforts."}}
{"id": "2509.06433", "pdf": "https://arxiv.org/pdf/2509.06433", "abs": "https://arxiv.org/abs/2509.06433", "authors": ["Ian Page", "Pierre Susbielle", "Olivier Aycard", "Pierre-Brice Wieber"], "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation", "categories": ["cs.RO"], "comment": null, "summary": "Achieving efficient remote teleoperation is particularly challenging in\nunknown environments, as the teleoperator must rapidly build an understanding\nof the site's layout. Online 3D mapping is a proven strategy to tackle this\nchallenge, as it enables the teleoperator to progressively explore the site\nfrom multiple perspectives. However, traditional online map-based teleoperation\nsystems struggle to generate visually accurate 3D maps in real-time due to the\nhigh computational cost involved, leading to poor teleoperation performances.\nIn this work, we propose a solution to improve teleoperation efficiency in\nunknown environments. Our approach proposes a novel, modular and efficient\nGPU-based integration between recent advancement in gaussian splatting SLAM and\nexisting online map-based teleoperation systems. We compare the proposed\nsolution against state-of-the-art teleoperation systems and validate its\nperformances through real-world experiments using an aerial vehicle. The\nresults show significant improvements in decision-making speed and more\naccurate interaction with the environment, leading to greater teleoperation\nefficiency. In doing so, our system enhances remote teleoperation by seamlessly\nintegrating photorealistic mapping generation with real-time performances,\nenabling effective teleoperation in unfamiliar environments.", "AI": {"tldr": "The paper presents a GPU-based method combining recent Gaussian splatting SLAM with teleoperation systems to enhance real-time 3D mapping and improve remote teleoperation efficiency.", "motivation": "Address the inefficiency of teleoperation in unknown environments caused by computational challenges in generating accurate real-time 3D maps.", "method": "Develop a modular GPU-based system that integrates Gaussian splatting SLAM with online map-based teleoperation systems to enhance real-time mapping performance.", "result": "Proposed system surpasses state-of-the-art teleoperation systems, improving decision-making speed and interaction accuracy, proven via real-world aerial vehicle experiments.", "conclusion": "The solution enables effective teleoperation in unknown environments by improving photorealistic 3D mapping in real-time, enhancing teleoperator efficiency."}}
{"id": "2509.06079", "pdf": "https://arxiv.org/pdf/2509.06079", "abs": "https://arxiv.org/abs/2509.06079", "authors": ["Hao Liang", "Ruitao Wu", "Bohan Zeng", "Junbo Niu", "Wentao Zhang", "Bin Dong"], "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.", "AI": {"tldr": "The paper introduces a caption-assisted reasoning framework to improve multimodal reasoning by bridging visual and textual modalities. It achieved high performance in competitions and benchmarks.", "motivation": "Current state-of-the-art AI models struggle with multimodal reasoning scenarios, despite advances in text-based reasoning. Addressing this gap is crucial for advancing AI capabilities.", "method": "The proposed framework integrates captions to link visual and textual information, enhancing multimodal reasoning and tackling challenges in scenarios involving diverse modalities.", "result": "The framework achieved 1st place in the ICML 2025 AI for Math Workshop Challenge 2: SeePhys and demonstrated strong generalization on the MathVerse benchmark, especially for geometric reasoning.", "conclusion": "The caption-assisted reasoning framework is effective and robust, bridging the gap between textual and visual reasoning. The code's public availability fosters further research and development in this domain."}}
{"id": "2509.06436", "pdf": "https://arxiv.org/pdf/2509.06436", "abs": "https://arxiv.org/abs/2509.06436", "authors": ["Song Yu", "Xiaofei Xu", "Ke Deng", "Li Li", "Lin Tian"], "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning", "categories": ["cs.AI"], "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.", "AI": {"tldr": "The paper introduces Tree of Agents (TOA), a multi-agent reasoning framework aimed at addressing the challenges faced by large language models (LLMs) in long-context tasks like position bias and lost information.", "motivation": "The study aims to solve the issues LLMs encounter in long-context tasks, particularly the 'lost in the middle' problem where critical information from mid-sections of long input is underutilized.", "method": "They propose the Tree of Agents (TOA), which segments input data into chunks processed by independent agents. The agents perform collaborative reasoning in a tree structure and apply strategies like prefix-hash caching and adaptive pruning to boost efficiency.", "result": "Experimental results show that TOA, implemented with LLaMA3.1-8B, surpasses multiple baselines and matches the performance of larger commercial models such as Gemini1.5-pro on long-context tasks.", "conclusion": "TOA effectively mitigates position bias and hallucination issues in long-context tasks while being efficient in terms of API overhead, proving to be a promising alternative to larger models."}}
{"id": "2509.05625", "pdf": "https://arxiv.org/pdf/2509.05625", "abs": "https://arxiv.org/abs/2509.05625", "authors": ["Kien Nguyen", "Anh Tran", "Cuong Pham"], "title": "SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of text-to-image diffusion models has raised concerns about\ntheir potential misuse in generating harmful or unauthorized contents. To\naddress these issues, several Concept Erasure methods have been proposed.\nHowever, most of them fail to achieve both robustness, i.e., the ability to\nrobustly remove the target concept., and effectiveness, i.e., maintaining image\nquality. While few recent techniques successfully achieve these goals for NSFW\nconcepts, none could handle narrow concepts such as copyrighted characters or\ncelebrities. Erasing these narrow concepts is critical in addressing copyright\nand legal concerns. However, erasing them is challenging due to their close\ndistances to non-target neighboring concepts, requiring finer-grained\nmanipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel\nmethod specifically designed to achieve both robustness and effectiveness in\neasing these narrow concepts. SuMa first derives a target subspace representing\nthe concept to be erased and then neutralizes it by mapping it to a reference\nsubspace that minimizes the distance between the two. This mapping ensures the\ntarget concept is robustly erased while preserving image quality. We conduct\nextensive experiments with SuMa across four tasks: subclass erasure, celebrity\nerasure, artistic style erasure, and instance erasure and compare the results\nwith current state-of-the-art methods. Our method achieves image quality\ncomparable to approaches focused on effectiveness, while also yielding results\nthat are on par with methods targeting completeness.", "AI": {"tldr": "The manuscript introduces SuMa, a novel Subspace Mapping method for erasing narrow concepts like copyrighted figures or celebrities in diffusion models while maintaining image quality.", "motivation": "The increasing misuse of text-to-image diffusion models for generating unauthorized or harmful content highlights the need for robust concept erasure methods. Current techniques underperform particularly in addressing narrow concepts critical for copyright and legal concerns.", "method": "This paper proposes Subspace Mapping (SuMa), which involves deriving a target subspace for the concept needing erasure and then neutralizing this space by mapping it to a reference subspace that minimizes distance between them. This ensures robustness in erasure and quality preservation.", "result": "SuMa demonstrated effectiveness across tasks like subclass, celebrity, artistic style, and instance erasure, achieving comparable image quality and erasure robustness against state-of-the-art methods.", "conclusion": "SuMa successfully addresses the finer-grained manipulation challenges in erasing narrow concepts without compromising the generated image quality, making it a robust and effective solution compared to existing frameworks."}}
{"id": "2509.05886", "pdf": "https://arxiv.org/pdf/2509.05886", "abs": "https://arxiv.org/abs/2509.05886", "authors": ["Reza Pirayeshshirazinezhad"], "title": "SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework", "categories": ["cs.LG"], "comment": null, "summary": "A surrogate model is developed to predict the convective heat transfer\ncoefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.\nInitially, kernel-based machine learning techniques and shallow neural network\nare applied to a dataset with 87 Nusselt numbers for liquid sodium in\nrectangular miniature heat sinks. Subsequently, a self-supervised\nphysics-informed neural network and transfer learning approach are used to\nincrease the estimation performance. In the self-supervised physics-informed\nneural network, an additional layer determines the weight the of physics in the\nloss function to balance data and physics based on their uncertainty for a\nbetter estimation. For transfer learning, a shallow neural network trained on\nwater is adapted for use with Na. Validation results show that the\nself-supervised physics-informed neural network successfully estimate the heat\ntransfer rates of Na with an error margin of approximately +8%. Using only\nphysics for regression, the error remains between 5% to 10%. Other machine\nlearning methods specify the prediction mostly within +8%. High-fidelity\nmodeling of turbulent forced convection of liquid metals using computational\nfluid dynamics (CFD) is both time-consuming and computationally expensive.\nTherefore, machine learning based models offer a powerful alternative tool for\nthe design and optimization of liquid-metal-cooled miniature heat sinks.", "AI": {"tldr": "The paper develops surrogate models (including machine learning approaches and a physics-informed neural network) to predict the convective heat transfer coefficient for liquid sodium in rectangular miniature heat sinks.", "motivation": "To provide an efficient and accurate alternative to computationally expensive CFD analysis for turbulent forced convection of liquid sodium in miniature heat sinks.", "method": "The study applies kernel-based machine learning, shallow neural networks, a self-supervised physics-informed neural network with uncertainty weighting, and transfer learning from water data to sodium data.", "result": "Validation shows that the self-supervised physics-informed model estimates the heat transfer coefficient for liquid sodium with an error margin of ~8%, while physics-only regressions have errors between 5%-10%. Other ML methods maintain prediction errors mostly within +8%.", "conclusion": "The proposed machine learning models, notably the physics-informed approach, provide a viable alternative for high-fidelity modeling in the design and optimization of liquid-metal-cooled miniature heat sinks."}}
{"id": "2509.06469", "pdf": "https://arxiv.org/pdf/2509.06469", "abs": "https://arxiv.org/abs/2509.06469", "authors": ["Benedikt Kreis", "Malte Mosbach", "Anny Ripke", "Muhammad Ehsan Ullah", "Sven Behnke", "Maren Bennewitz"], "title": "Interactive Shaping of Granular Media Using Reinforcement Learning", "categories": ["cs.RO"], "comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025", "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, outperforming two baseline\napproaches.", "AI": {"tldr": "The paper explores a reinforcement learning (RL) framework that enables robotic arms to autonomously shape granular materials into desired structures.", "motivation": "Challenges in manipulating granular materials arise from their complex dynamics and high-dimensional configuration space, making traditional methods less effective.", "method": "A reinforcement learning framework with a robotic arm equipped with a cubic end-effector and stereo camera, focusing on compact observations and concise rewards, validated through an ablation study.", "result": "The RL approach successfully trains visual policies for manipulation tasks, outperforming two baseline methods and achieving real-world deployment.", "conclusion": "The study showcases the potential of RL for efficient granular material manipulation, providing insights into observation and reward design for complex tasks."}}
{"id": "2509.06100", "pdf": "https://arxiv.org/pdf/2509.06100", "abs": "https://arxiv.org/abs/2509.06100", "authors": ["Kefan Cao", "Shuaicheng Wu"], "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "categories": ["cs.CL"], "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.", "AI": {"tldr": "This paper introduces OLieRA, a method leveraging Lie group theory for fine-tuning large language models (LLMs) to address catastrophic forgetting, achieving state-of-the-art results.", "motivation": "LLMs suffer from catastrophic forgetting in multi-task settings, and existing solutions that use parameter regularization do not preserve the geometric structure of LLM parameters, limiting their effectiveness.", "method": "The authors propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which uses multiplicative updates to maintain the geometric structure of model parameters while applying orthogonality constraints to task subspaces.", "result": "OLieRA achieves state-of-the-art results on the Standard CL benchmark and performs competitively in settings with a large number of tasks.", "conclusion": "The method demonstrates that preserving geometric structure alongside orthogonality constraints can significantly enhance fine-tuning in large language models, addressing task interference more effectively."}}
{"id": "2509.06444", "pdf": "https://arxiv.org/pdf/2509.06444", "abs": "https://arxiv.org/abs/2509.06444", "authors": ["Cheng Qian", "Hainan Zhang", "Yongxin Tong", "Hong-Wei Zheng", "Zhiming Zheng"], "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data", "categories": ["cs.AI"], "comment": "9 pages, 7 figures", "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.", "AI": {"tldr": "The paper introduces HyFedRAG, a Federated Retrieval-Augmented Generation (RAG) framework tailored for privacy-sensitive and heterogeneous healthcare data.", "motivation": "Centralized RAG pipelines face limitations in handling distributed, heterogeneous, and privacy-sensitive data in healthcare, especially when operating across various formats such as SQL databases, knowledge graphs, and clinical notes.", "method": "The proposed HyFedRAG framework utilizes an edge-cloud collaborative approach, with edge-side LLMs standardizing and anonymizing data and server-side LLMs integrating the outputs for comprehensive reasoning. The framework also incorporates a three-tier caching strategy to enhance efficiency.", "result": "Experimental results on a healthcare dataset demonstrate that HyFedRAG significantly improves upon existing baselines in retrieval quality, generation consistency, and system efficiency.", "conclusion": "HyFedRAG presents a scalable, privacy-compliant solution for RAG tasks in environments characterized by heterogeneous and sensitive data, bridging gaps in current systems and unlocking LLMs' potential in healthcare and similar domains."}}
{"id": "2509.05630", "pdf": "https://arxiv.org/pdf/2509.05630", "abs": "https://arxiv.org/abs/2509.05630", "authors": ["Moqsadur Rahman", "Saurav Kumar", "Santosh S. Palmate", "M. Shahriar Hossain"], "title": "Self-supervised Learning for Hyperspectral Images of Trees", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Aerial remote sensing using multispectral and RGB imagers has provided a\ncritical impetus to precision agriculture. Analysis of the hyperspectral images\nwith limited or no labels is challenging. This paper focuses on self-supervised\nlearning to create neural network embeddings reflecting vegetation properties\nof trees from aerial hyperspectral images of crop fields. Experimental results\ndemonstrate that a constructed tree representation, using a vegetation\nproperty-related embedding space, performs better in downstream machine\nlearning tasks compared to the direct use of hyperspectral vegetation\nproperties as tree representations.", "AI": {"tldr": "This paper explores self-supervised learning for neural network embeddings of vegetation properties from aerial hyperspectral images, resulting in improved tree representations for downstream tasks.", "motivation": "The aim is to address challenges in analyzing hyperspectral images for precision agriculture when there are limited or no labeled data.", "method": "Self-supervised learning is applied to create neural network embeddings reflecting vegetation properties from aerial hyperspectral images.", "result": "Tree representations using the embedding space outperform the direct use of hyperspectral vegetation properties in downstream machine learning tasks.", "conclusion": "Self-supervised learning-based tree representations offer a significant improvement in utilizing hyperspectral images for better performance in agricultural analysis tasks."}}
{"id": "2509.05899", "pdf": "https://arxiv.org/pdf/2509.05899", "abs": "https://arxiv.org/abs/2509.05899", "authors": ["Dazhi Peng"], "title": "X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "With Large Language Models' (LLMs) emergent abilities on code generation\ntasks, Text-to-SQL has become one of the most popular downstream applications.\nDespite the strong results of multiple recent LLM-based Text-to-SQL frameworks,\nthe research community often overlooks the importance of database schema\ninformation for generating high-quality SQL queries. We find that such schema\ninformation plays a significant or even dominant role in the Text-to-SQL task.\nTo tackle this challenge, we propose a novel database schema expert with two\ncomponents. We first introduce X-Linking, an LLM Supervised Finetuning\n(SFT)-based method that achieves superior Schema Linking results compared to\nexisting open-source Text-to-SQL methods. In addition, we innovatively propose\nan X-Admin component that focuses on Schema Understanding by bridging the gap\nbetween abstract schema information and the user's natural language question.\nAside from better learning with schema information, we experiment with\nMulti-LLMs for different components within the system to further boost its\nperformance. By incorporating these techniques into our end-to-end framework,\nX-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset\nand 82.5% on the Spider-Test dataset. This outstanding performance establishes\nX-SQL as the leading Text-to-SQL framework based on open-source models.", "AI": {"tldr": "The paper introduces X-SQL, a Text-to-SQL framework leveraging database schema information and multi-LLMs, achieving high performance on Spider datasets.", "motivation": "To address the overlooked significance of database schema information in Text-to-SQL tasks and enhance SQL query generation quality using LLMs.", "method": "The proposed X-SQL framework introduces two components: X-Linking (LLM Supervised Fine-tuning for Schema Linking) and X-Admin (bridging schema information and natural language queries), alongside the use of Multi-LLMs for performance optimization.", "result": "X-SQL achieves Execution Accuracies of 84.9% on Spider-Dev and 82.5% on Spider-Test datasets, setting a benchmark for open-source Text-to-SQL models.", "conclusion": "The X-SQL framework demonstrates the critical role of schema information, combined with advanced LLM strategies, in delivering state-of-the-art Text-to-SQL performance."}}
{"id": "2509.06481", "pdf": "https://arxiv.org/pdf/2509.06481", "abs": "https://arxiv.org/abs/2509.06481", "authors": ["Vinita Sao", "Tu Dac Ho", "Sujoy Bhore", "P. B. Sujit"], "title": "Event Driven CBBA with Reduced Communication", "categories": ["cs.RO"], "comment": null, "summary": "In various scenarios such as multi-drone surveillance and search-and-rescue\noperations, deploying multiple robots is essential to accomplish multiple tasks\nat once. Due to the limited communication range of these vehicles, a\ndecentralised task allocation algorithm is crucial for effective task\ndistribution among robots. The consensus-based bundle algorithm (CBBA) has been\npromising for multi-robot operation, offering theoretical guarantees. However,\nCBBA demands continuous communication, leading to potential congestion and\npacket loss that can hinder performance. In this study, we introduce an\nevent-driven communication mechanism designed to address these communication\nchallenges while maintaining the convergence and performance bounds of CBBA. We\ndemonstrate theoretically that the solution quality matches that of CBBA and\nvalidate the approach with Monte-Carlo simulations across varying targets,\nagents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can\nreduce message transmissions by up to 52%.", "AI": {"tldr": "The research introduces an event-driven communication enhancement (ED-CBBA) to the Consensus-Based Bundle Algorithm (CBBA) for multi-robot task allocation, reducing message transmissions by 52% while maintaining solution quality.", "motivation": "To address communication challenges in multi-robot task allocation caused by limited communication range and congestion in the conventional CBBA approach.", "method": "The researchers developed an event-driven communication mechanism for CBBA, which activates communication updates only when certain events occur, reducing dependency on continuous communication.", "result": "The event-driven CBBA (ED-CBBA) maintains the theoretical performance quality of standard CBBA while decreasing communication overhead significantly\u2014cutting message transmissions by up to 52% as validated through Monte-Carlo simulations.", "conclusion": "ED-CBBA demonstrates that event-driven communication can mitigate communication challenges in decentralized multi-robot systems without compromising task allocation quality or performance guarantees."}}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164", "abs": "https://arxiv.org/abs/2509.06164", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "title": "Benchmarking Gender and Political Bias in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "AI": {"tldr": "The paper presents EuroParlVote, a benchmark dataset for testing language model biases in political settings, evaluating gender classification and vote prediction tasks.", "motivation": "To address the need for understanding bias and fairness of large language models in politically sensitive applications.", "method": "The researchers linked European Parliament speeches to voting outcomes and demographic metadata, then tested state-of-the-art LLMs on gender classification and vote prediction.", "result": "LLMs showed gender and political biases, such as misclassifying female MEPs and favoring centrist political groups over extremes. GPT-4 performed better than open-weight models.", "conclusion": "EuroParlVote advances the study of bias and accountability in NLP, and its public release aims to foster research in political fairness."}}
{"id": "2509.06463", "pdf": "https://arxiv.org/pdf/2509.06463", "abs": "https://arxiv.org/abs/2509.06463", "authors": ["Chengwei Wu", "Li Du", "Hanyu Zhao", "Yiming Ju", "Jiapu Wang", "Tengfei Pan"], "title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set", "categories": ["cs.AI"], "comment": null, "summary": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}.", "AI": {"tldr": "The authors study how to select informative instructions for better alignment of large language models and propose a method that improves alignment by focusing on instruction depth and semantic space coverage.", "motivation": "As the demand for large language models grows, there is a need to optimize their alignment with instructions to enhance performance and efficiency. Current methods falter as instruction pools grow in size.", "method": "The authors analyze dataset distribution factors influencing model performance, identifying instruction depth and semantic space coverage as key. They develop an algorithm to optimize these factors in instruction selection.", "result": "Their instruction selection method explains over 70% of model performance variance and demonstrates faster and sustainable improvement compared to existing methods.", "conclusion": "The proposed method achieves better model alignment by optimizing instruction selection to accelerate performance scaling."}}
{"id": "2509.05652", "pdf": "https://arxiv.org/pdf/2509.05652", "abs": "https://arxiv.org/abs/2509.05652", "authors": ["Ha Meem Hossain", "Pritam Nath", "Mahitun Nesa Mahi", "Imtiaz Uddin", "Ishrat Jahan Eiste", "Syed Nasibur Rahman Ratul", "Md Naim Uddin Mozumdar", "Asif Mohammed Saad"], "title": "Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh", "categories": ["cs.CV"], "comment": null, "summary": "Vehicle detection systems trained on Non-Bangladeshi datasets struggle to\naccurately identify local vehicle types in Bangladesh's unique road\nenvironments, creating critical gaps in autonomous driving technology for\ndeveloping regions. This study evaluates six YOLO model variants on a custom\ndataset featuring 29 distinct vehicle classes, including region-specific\nvehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and\n``CNG''. The dataset comprises high-resolution images (1920x1080) captured\nacross various Bangladeshi roads using mobile phone cameras and manually\nannotated using LabelImg with YOLO format bounding boxes. Performance\nevaluation revealed YOLOv11x as the top performer, achieving 63.7\\% mAP@0.5,\n43.8\\% mAP@0.5:0.95, 61.4\\% recall, and 61.6\\% F1-score, though requiring 45.8\nmilliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)\nstruck an optimal balance, delivering robust detection performance with mAP@0.5\nvalues of 62.5\\% and 61.8\\% respectively, while maintaining moderate inference\ntimes around 14-15 milliseconds. The study identified significant detection\nchallenges for rare vehicle classes, with Construction Vehicles and Desi\nNosimons showing near-zero accuracy due to dataset imbalances and insufficient\ntraining samples. Confusion matrices revealed frequent misclassifications\nbetween visually similar vehicles, particularly Mini Trucks versus Mini Covered\nVans. This research provides a foundation for developing robust object\ndetection systems specifically adapted to Bangladesh traffic conditions,\naddressing critical needs in autonomous vehicle technology advancement for\ndeveloping regions where conventional generic-trained models fail to perform\nadequately.", "AI": {"tldr": "This paper evaluates six YOLO variants on a custom dataset for vehicle detection in Bangladesh, highlighting the strengths and weaknesses of the models with region-specific vehicles.", "motivation": "Generic vehicle detection models trained on non-local datasets struggle in Bangladesh's unique traffic environments, necessitating specialized detection systems.", "method": "The study used a custom dataset with 29 vehicle classes, captured from Bangladeshi roads, annotated, and evaluated on YOLO model variants measuring mAP, recall, and F1-score.", "result": "YOLOv11x achieved the highest detection performance with a mAP@0.5 of 63.7%, but medium YOLO variants offered a balance between performance and inference time. Rare classes faced detection challenges.", "conclusion": "The study emphasizes the need for region-specific training datasets and system development to improve autonomous driving technology in developing countries like Bangladesh."}}
{"id": "2509.06582", "pdf": "https://arxiv.org/pdf/2509.06582", "abs": "https://arxiv.org/abs/2509.06582", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gr\u00f6ne", "Mathias G\u00fcnther", "Oliver Deussen"], "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions.", "AI": {"tldr": "The paper introduces a VR co-location framework combining motion capture and SLAM for accurate and smooth, shared environments.", "motivation": "To overcome limitations in VR co-location systems, such as latency and drift, and improve natural multi-user interactions.", "method": "Integrating motion capture with SLAM-based inside-out tracking to enable smooth tracking with real-time pose sharing and spatial realignment.", "result": "The framework ensures consistent spatial accuracy for multi-user interactions while enhancing comfort, scalability, and robustness.", "conclusion": "The proposed approach provides a high-performing solution for natural and scalable multi-user VR experiences by addressing spatial alignment challenges effectively."}}
{"id": "2509.06184", "pdf": "https://arxiv.org/pdf/2509.06184", "abs": "https://arxiv.org/abs/2509.06184", "authors": ["Jacob Mitchell Springer", "Vaibhav Adlakha", "Siva Reddy", "Aditi Raghunathan", "Marius Mosbach"], "title": "Understanding the Influence of Synthetic Data for Text Embedders", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.", "AI": {"tldr": "This paper studies the role of synthetic data in developing general-purpose text embedders, releasing high-quality synthetic datasets, and analyzing their localized benefits and trade-offs across tasks.", "motivation": "The motivation behind this work is the lack of publicly available synthetic datasets, despite their significant role in training general-purpose text embedders, which limits further exploration and understanding of their impact on model generalization.", "method": "The authors replicate and release synthetic data that reproduce earlier work, analyze the data's impact on model generalization, and investigate how and where it aids performance, particularly focusing on trade-offs between different tasks.", "result": "The synthetic data provides improvement in performance, but the benefits are limited to specific datasets and tasks, with certain data negatively affecting performance in others.", "conclusion": "Current approaches to synthetic data have limitations for building general-purpose embedders, and relying on it does not necessarily lead to models that are robust across various tasks."}}
{"id": "2509.06477", "pdf": "https://arxiv.org/pdf/2509.06477", "abs": "https://arxiv.org/abs/2509.06477", "authors": ["Pengxiang Zhao", "Guangyi Liu", "Yaozhen Liang", "Weiqing He", "Zhengxi Lu", "Yuehao Huang", "Yaxuan Guo", "Kexin Zhang", "Hao Wang", "Liang Liu", "Yong Liu"], "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.", "AI": {"tldr": "MAS-Bench introduces a benchmark to evaluate GUI-shortcut hybrid agents, focusing on mobile applications. It combines GUI operations with shortcuts, enabling agents to generate efficient workflows autonomously and achieve higher success rates compared to GUI-only methods.", "motivation": "To address the lack of systematic benchmarking for GUI-shortcut hybrid agents, especially in the mobile domain.", "method": "MAS-Bench evaluates agents through tasks across real-world applications, using a combination of predefined shortcuts and metrics to assess the creation of reusable workflows.", "result": "Hybrid agents demonstrated significantly better success rates and efficiency compared to GUI-only agents during experiments.", "conclusion": "MAS-Bench establishes a foundational framework for assessing and advancing GUI-shortcut hybrid agents, marking a pivotal step in developing intelligent systems."}}
{"id": "2509.05659", "pdf": "https://arxiv.org/pdf/2509.05659", "abs": "https://arxiv.org/abs/2509.05659", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditIDv2, a tuning-free solution specifically designed for\nhigh-complexity narrative scenes and long text inputs. Existing character\nediting methods perform well under simple prompts, but often suffer from\ndegraded editing capabilities, semantic understanding biases, and identity\nconsistency breakdowns when faced with long text narratives containing multiple\nsemantic layers, temporal logic, and complex contextual relationships. In\nEditID, we analyzed the impact of the ID integration module on editability. In\nEditIDv2, we further explore and address the influence of the ID feature\nintegration module. The core of EditIDv2 is to discuss the issue of editability\ninjection under minimal data lubrication. Through a sophisticated decomposition\nof PerceiverAttention, the introduction of ID loss and joint dynamic training\nwith the diffusion model, as well as an offline fusion strategy for the\nintegration module, we achieve deep, multi-level semantic editing while\nmaintaining identity consistency in complex narrative environments using only a\nsmall amount of data lubrication. This meets the demands of long prompts and\nhigh-quality image generation, and achieves excellent results in the IBench\nevaluation.", "AI": {"tldr": "EditIDv2 improves character editing in complex narratives with minimal data, maintaining identity consistency even with long text inputs.", "motivation": "Existing methods struggle with editing accuracy and consistency in complex narrative scenes and long text inputs.", "method": "EditIDv2 incorporates PerceiverAttention decomposition, ID loss, dynamic training with diffusion models, and an offline integration module strategy.", "result": "The method achieves high-quality image generation, deep semantic editing, and identity consistency, validated through IBench evaluation.", "conclusion": "EditIDv2 successfully addresses editability challenges in complex narratives, providing advanced solutions for high-quality image editing under demanding prompts."}}
{"id": "2509.06025", "pdf": "https://arxiv.org/pdf/2509.06025", "abs": "https://arxiv.org/abs/2509.06025", "authors": ["Vignesh Ethiraj", "Subhash Talluri"], "title": "Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior", "categories": ["cs.LG", "cs.AI", "68T07, 62M20", "I.2.6; H.2.8; H.3.3"], "comment": null, "summary": "A central goal of artificial intelligence is to build systems that can\nunderstand and predict complex, evolving sequences of events. However, current\nfoundation models, designed for natural language, fail to grasp the holistic\nnature of structured interactions found in domains like telecommunications,\ne-commerce and finance. By serializing events into text, they disassemble them\ninto semantically fragmented parts, losing critical context. In this work, we\nintroduce the Unified Interaction Foundation Model (UIFM), a foundation model\nengineered for genuine behavioral understanding. At its core is the principle\nof composite tokenization, where each multi-attribute event is treated as a\nsingle, semantically coherent unit. This allows UIFM to learn the underlying\n\"grammar\" of user behavior, perceiving entire interactions rather than a\ndisconnected stream of data points. We demonstrate that this architecture is\nnot just more accurate, but represents a fundamental step towards creating more\nadaptable and intelligent predictive systems.", "AI": {"tldr": "The paper introduces the Unified Interaction Foundation Model (UIFM), a system aimed at understanding complex, evolving events in structured domains by overcoming the limitations of current models that rely on text serialization.", "motivation": "Current foundation models struggle to accurately grasp the holistic and structured nature of interactions in domains like telecommunications, finance, and e-commerce. Serialization into text fragments loses essential contextual information required for understanding user behavior.", "method": "The UIFM utilizes composite tokenization, treating each multi-attribute event as a unified, coherent unit. This approach allows the model to learn the \"grammar\" of user behavior and understand entire interactions holistically.", "result": "UIFM is demonstrated to be more accurate in behavioral understanding compared to traditional methods, suggesting improved adaptability and intelligence in predictive systems.", "conclusion": "The research provides a significant step forward in creating foundational models capable of genuine behavioral understanding by considering structured interactions as semantically unified events."}}
{"id": "2509.06593", "pdf": "https://arxiv.org/pdf/2509.06593", "abs": "https://arxiv.org/abs/2509.06593", "authors": ["Meher V. R. Malladi", "Tiziano Guadagnino", "Luca Lobefaro", "Cyrill Stachniss"], "title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling", "categories": ["cs.RO"], "comment": null, "summary": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.", "AI": {"tldr": "This paper presents a robust LiDAR-inertial odometry system, leveraging a simplified motion model and novel regularization for enhanced performance across diverse sensors and environments.", "motivation": "Accurate odometry is essential for robotic navigation, and existing sensor-based methods often rely on sensor-specific modeling, limiting their adaptability across diverse domains.", "method": "The proposed method integrates IMU data using a simplified motion model and employs scan-to-map LiDAR registration with a novel regularization to enhance odometry performance.", "result": "Extensive experiments across various datasets and sensor configurations demonstrate the robustness and adaptability of the system in different scenarios.", "conclusion": "The system achieves robust and accurate odometry performance with a universal configuration, and the open-sourcing of the implementation invites further development by the community."}}
{"id": "2509.06196", "pdf": "https://arxiv.org/pdf/2509.06196", "abs": "https://arxiv.org/abs/2509.06196", "authors": ["Mohamed T. Younes", "Omar Walid", "Khaled Shaban", "Ali Hamdi", "Mai Hassan"], "title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching.", "AI": {"tldr": "This paper focuses on enhancing recruitment automation using fine-tuned Large Language Models (LLMs) tailored for recruitment tasks, achieving notable improvements in metrics like F1 score.", "motivation": "The paper aims to address limitations of generic LLMs in recruitment automation and improve the precision and recall of candidate-job matching.", "method": "The authors developed a new framework leveraging synthetic and parsed resume datasets in standardized JSON format. They fine-tuned LLMs, including the Phi-4 model, for recruitment-specific tasks.", "result": "The fine-tuned Phi-4 model achieved an F1 score of 90.62%, demonstrating significant improvement over baseline and state-of-the-art models in recruitment workflows.", "conclusion": "Fine-tuned LLMs, particularly the Phi-4 model, show promise in revolutionizing recruitment processes by enhancing accuracy and efficiency in candidate-job matching."}}
{"id": "2509.06490", "pdf": "https://arxiv.org/pdf/2509.06490", "abs": "https://arxiv.org/abs/2509.06490", "authors": ["Niki Kotecha", "Ehecatl Antonio del Rio Chanona"], "title": "MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization", "categories": ["cs.AI"], "comment": null, "summary": "In supply chain management, decision-making often involves balancing multiple\nconflicting objectives, such as cost reduction, service level improvement, and\nenvironmental sustainability. Traditional multi-objective optimization methods,\nsuch as linear programming and evolutionary algorithms, struggle to adapt in\nreal-time to the dynamic nature of supply chains. In this paper, we propose an\napproach that combines Reinforcement Learning (RL) and Multi-Objective\nEvolutionary Algorithms (MOEAs) to address these challenges for dynamic\nmulti-objective optimization under uncertainty. Our method leverages MOEAs to\nsearch the parameter space of policy neural networks, generating a Pareto front\nof policies. This provides decision-makers with a diverse population of\npolicies that can be dynamically switched based on the current system\nobjectives, ensuring flexibility and adaptability in real-time decision-making.\nWe also introduce Conditional Value-at-Risk (CVaR) to incorporate\nrisk-sensitive decision-making, enhancing resilience in uncertain environments.\nWe demonstrate the effectiveness of our approach through case studies,\nshowcasing its ability to respond to supply chain dynamics and outperforming\nstate-of-the-art methods in an inventory management case study. The proposed\nstrategy not only improves decision-making efficiency but also offers a more\nrobust framework for managing uncertainty and optimizing performance in supply\nchains.", "AI": {"tldr": "This paper combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to handle dynamic supply chain optimization, incorporating a risk-sensitive approach using Conditional Value-at-Risk (CVaR).", "motivation": "To tackle challenges in balancing conflicting objectives like cost, service, and sustainability in supply chain management, particularly under dynamic and uncertain conditions, where traditional methods fail to adapt in real-time.", "method": "The paper integrates MOEAs with RL, using MOEAs to optimize policy neural networks and generate a Pareto front of policies. Dynamic policy-switching and CVaR are used for risk-sensitive decision-making.", "result": "The approach shows superior performance compared to state-of-the-art techniques in responding to supply chain dynamics, demonstrated through an inventory management case study.", "conclusion": "The proposed method improves the efficiency and robustness of real-time decision-making in supply chains, enhancing both adaptability to uncertainties and overall performance."}}
{"id": "2509.05661", "pdf": "https://arxiv.org/pdf/2509.05661", "abs": "https://arxiv.org/abs/2509.05661", "authors": ["Xiaomeng Zhu", "Changwei Wang", "Haozhe Wang", "Xinyu Liu", "Fangzhen Lin"], "title": "OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation", "categories": ["cs.CV"], "comment": null, "summary": "A scene graph is a structured represention of objects and their relationships\nin a scene. Scene Graph Anticipation (SGA) involves predicting future scene\ngraphs from video clips, enabling applications as intelligent surveillance and\nhuman-machine collaboration. Existing SGA approaches primarily leverage visual\ncues, often struggling to integrate valuable commonsense knowledge, thereby\nlimiting long-term prediction robustness. To explicitly leverage such\ncommonsense knowledge, we propose a new approach to better understand the\nobjects, concepts, and relationships in a scene graph. Our approach decouples\nthe SGA task in two steps: first a scene graph capturing model is used to\nconvert a video clip into a sequence of scene graphs, then a pure text-based\nmodel is used to predict scene graphs in future frames. Our focus in this work\nis on the second step, and we call it Linguistic Scene Graph Anticipation\n(LSGA) and believes it should have independent interest beyond the use in SGA\ndiscussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method\n(OOTSM) where an Large Language Model (LLM) first forecasts object appearances\nand disappearances before generating detailed human-object relations. We\nconduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we\nevaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,\nGPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome\nannotations. For SGA, we combine our OOTSM with STTran++ from, and our\nexperiments demonstrate effective state-of-the-art performance: short-term\nmean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves\ndramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.", "AI": {"tldr": "The paper proposes a scene graph anticipation (SGA) method that integrates commonsense knowledge through a linguistic approach, improving both short-term and long-term predictions.", "motivation": "To address the limitations of existing SGA methods that struggle with long-term prediction robustness due to underutilization of commonsense knowledge.", "method": "A two-step approach called Object-Oriented Two-Staged Method (OOTSM): First, a scene graph capturing model translates video clips into scene graphs. Next, a linguistic model forecasts and details object relationships.", "result": "OOTSM shows state-of-the-art performance. In the short-term, mean-Recall (@10) improves by 3.4%. Long-term mean-Recall (@50) sees a substantial improvement of 21.9%, tested on a benchmark from Action Genome annotations.", "conclusion": "The proposed approach successfully integrates linguistic insights into SGA, demonstrating its independent interest and improving prediction accuracy significantly."}}
{"id": "2509.06053", "pdf": "https://arxiv.org/pdf/2509.06053", "abs": "https://arxiv.org/abs/2509.06053", "authors": ["Mingrui Lv", "Hangzhi Liu", "Zhi Luo", "Hongjie Zhang", "Jie Ou"], "title": "PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-agent reinforcement learning (MARL) has achieved significant progress\nin solving complex multi-player games through self-play. However, training\neffective adversarial policies requires millions of experience samples and\nsubstantial computational resources. Moreover, these policies lack\ninterpretability, hindering their practical deployment. Recently, researchers\nhave successfully leveraged Large Language Models (LLMs) to generate\nprogrammatic policies for single-agent tasks, transforming neural network-based\npolicies into interpretable rule-based code with high execution efficiency.\nInspired by this, we propose PolicyEvolve, a general framework for generating\nprogrammatic policies in multi-player games. PolicyEvolve significantly reduces\nreliance on manually crafted policy code, achieving high-performance policies\nwith minimal environmental interactions. The framework comprises four modules:\nGlobal Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool\npreserves elite policies accumulated during iterative training. The Local Pool\nstores temporary policies for the current iteration; only sufficiently\nhigh-performing policies from this pool are promoted to the Global Pool. The\nPolicy Planner serves as the core policy generation module. It samples the top\nthree policies from the Global Pool, generates an initial policy for the\ncurrent iteration based on environmental information, and refines this policy\nusing feedback from the Trajectory Critic. Refined policies are then deposited\ninto the Local Pool. This iterative process continues until the policy achieves\na sufficiently high average win rate against the Global Pool, at which point it\nis integrated into the Global Pool. The Trajectory Critic analyzes interaction\ndata from the current policy, identifies vulnerabilities, and proposes\ndirectional improvements to guide the Policy Planner", "AI": {"tldr": "Researchers propose PolicyEvolve, a framework for generating efficient and interpretable programmatic policies for multi-agent reinforcement learning tasks, leveraging a modular approach to achieve high performance with minimal interactions.", "motivation": "The authors aim to address the challenges of high computational costs, inefficiency, and lack of interpretability in adversarial policies generated by traditional multi-agent reinforcement learning methods.", "method": "PolicyEvolve consists of four modules: Global Pool and Local Pool for policy storage, a Policy Planner for iterative policy generation and refinement, and a Trajectory Critic for analyzing data to improve policies. The system promotes an iterative process for developing high-quality programmatic policies.", "result": "PolicyEvolve achieves high-performance policies with fewer environmental interactions compared to traditional methods, while simultaneously maintaining interpretable and efficient rule-based code.", "conclusion": "PolicyEvolve demonstrates a novel approach to bridging the gap between efficiency, effectiveness, and interpretability in multi-agent systems, offering a viable alternative to existing reinforcement learning frameworks."}}
{"id": "2509.06597", "pdf": "https://arxiv.org/pdf/2509.06597", "abs": "https://arxiv.org/abs/2509.06597", "authors": ["Frederik Plahl", "Georgios Katranis", "Ilshat Mamaev", "Andrey Morozov"], "title": "LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods", "categories": ["cs.RO"], "comment": "Preprint of final paper that will appear in the Proceedings of the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\n  2025)", "summary": "We present LiHRA, a novel dataset designed to facilitate the development of\nautomated, learning-based, or classical risk monitoring (RM) methods for\nHuman-Robot Interaction (HRI) scenarios. The growing prevalence of\ncollaborative robots in industrial environments has increased the need for\nreliable safety systems. However, the lack of high-quality datasets that\ncapture realistic human-robot interactions, including potentially dangerous\nevents, slows development. LiHRA addresses this challenge by providing a\ncomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body\nkeypoints, and robot joint states, capturing the complete spatial and dynamic\ncontext of human-robot collaboration. This combination of modalities allows for\nprecise tracking of human movement, robot actions, and environmental\nconditions, enabling accurate RM during collaborative tasks. The LiHRA dataset\ncovers six representative HRI scenarios involving collaborative and coexistent\ntasks, object handovers, and surface polishing, with safe and hazardous\nversions of each scenario. In total, the data set includes 4,431 labeled point\nclouds recorded at 10 Hz, providing a rich resource for training and\nbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrate\nLiHRA's utility, we introduce an RM method that quantifies the risk level in\neach scenario over time. This method leverages contextual information,\nincluding robot states and the dynamic model of the robot. With its combination\nof high-resolution LiDAR data, precise human tracking, robot state data, and\nrealistic collision events, LiHRA offers an essential foundation for future\nresearch into real-time RM and adaptive safety strategies in human-robot\nworkspaces.", "AI": {"tldr": "LiHRA is a new dataset designed to advance risk monitoring methods in Human-Robot Interaction by providing detailed multi-modal data, including LiDAR point clouds, human keypoints, and robot states.", "motivation": "The growing adoption of collaborative robots in industrial environments demands reliable safety systems, but progress is hindered by the lack of realistic datasets capturing human-robot interactions.", "method": "LiHRA combines 3D LiDAR point clouds, human keypoints, and robot states across six HRI scenarios, offering labeled data for both safe and hazardous versions to train and benchmark RM algorithms.", "result": "The dataset contains 4,431 labeled point clouds recorded at 10 Hz, and an accompanying RM method quantifies risk levels dynamically using contextual data.", "conclusion": "LiHRA provides an essential resource for advancing real-time risk monitoring and adaptive safety in collaborative human-robot environments, addressing critical gaps in the field."}}
{"id": "2509.06200", "pdf": "https://arxiv.org/pdf/2509.06200", "abs": "https://arxiv.org/abs/2509.06200", "authors": ["Omar Walid", "Mohamed T. Younes", "Khaled Shaban", "Mai Hassan", "Ali Hamdi"], "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.", "AI": {"tldr": "MSLEF is a multi-segment ensemble framework that fine-tunes large language models for improved resume parsing.", "motivation": "To address limitations of single-model systems in resume parsing, especially with diverse formats and structures.", "method": "The framework involves weighted voting of fine-tuned LLMs specialized for different resume segments and uses Gemini-2.5-Flash as an aggregator.", "result": "MSLEF significantly improved metrics like EM, F1, BLEU, ROUGE, and Recruitment Similarity, outperforming single models by up to +7% in RS.", "conclusion": "MSLEF succeeds in enhancing generalization and accuracy in resume parsing, demonstrating adaptability to diverse hiring scenarios."}}
{"id": "2509.06493", "pdf": "https://arxiv.org/pdf/2509.06493", "abs": "https://arxiv.org/abs/2509.06493", "authors": ["Ran Xin", "Zeyu Zheng", "Yanchen Nie", "Kun Yuan", "Xia Xiao"], "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers", "categories": ["cs.AI"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.", "AI": {"tldr": "The paper proposes BFS-Prover-V2, an enhanced theorem proving system using RL frameworks and planning techniques to improve training and inference efficiency, achieving state-of-the-art results.", "motivation": "To overcome scalability challenges in integrating LLMs into automated theorem proving, specifically related to training-time reinforcement learning and inference-time compute.", "method": "The paper introduces a novel multi-turn off-policy RL framework inspired by AlphaZero for training and a planner-enhanced multi-agent search architecture for inference to handle complex reasoning tasks.", "result": "BFS-Prover-V2 achieves state-of-the-art results with 95.08% on MiniF2F and 41.4% on ProofNet benchmarks, showcasing improved efficiency in theorem proving.", "conclusion": "The innovations presented effectively address scalability issues in LLM-based theorem proving, with broader applicability in domains requiring long-horizon reasoning and complex searches."}}
{"id": "2509.05662", "pdf": "https://arxiv.org/pdf/2509.05662", "abs": "https://arxiv.org/abs/2509.05662", "authors": ["Wasikul Islam"], "title": "WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising", "categories": ["cs.CV", "hep-ex"], "comment": "13 pages, 4 figures", "summary": "In high-energy particle physics, collider measurements are contaminated by\n\"pileup\", overlapping soft interactions that obscure the hard-scatter signal of\ninterest. Dedicated subtraction strategies exploit physical priors such as\nconservation, locality, and isolation. Inspired by this analogy, we investigate\nhow such principles can inform image denoising by embedding physics-guided\ninductive biases into neural architectures. This paper is a proof of concept:\nrather than targeting state-of-the-art (SOTA) benchmarks, we ask whether\nphysics-inspired priors improve robustness under strong corruption.\n  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with\nconservation constraints, its Gaussian-noise variants, and the Weighted\nInductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which\nintegrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at\n$\\sigma\\in\\{15,25,50,75,100\\}$, PU-inspired CNNs are competitive with standard\nbaselines, while WIPUNet shows a \\emph{widening margin} at higher noise.\nComplementary BSD500 experiments show the same trend, suggesting\nphysics-inspired priors provide stability where purely data-driven models\ndegrade. Our contributions are: (i) translating pileup-mitigation principles\ninto modular inductive biases; (ii) integrating them into UNet; and (iii)\ndemonstrating robustness gains at high noise without relying on heavy SOTA\nmachinery.", "AI": {"tldr": "This paper explores physics-guided inductive biases for image denoising, inspired by pileup mitigation in particle physics, and demonstrates improved robustness under strong noise conditions.", "motivation": "The motivation is to integrate physics-inspired principles, like conservation and locality, into neural network architectures for robust image denoising, particularly under high noise levels.", "method": "The authors propose a hierarchy of pileup-inspired denoisers, including a residual CNN, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network (WIPUNet) built on a UNet backbone.", "result": "The proposed methods demonstrate competitive performance on CIFAR-10 with Gaussian noise across various levels and show widening margins of improvement at higher noise levels. Experiments on BSD500 corroborate these findings.", "conclusion": "Physics-inspired priors enhance robustness in image denoising under strong corruption and offer stability where data-driven models falter, without requiring state-of-the-art techniques."}}
{"id": "2509.06056", "pdf": "https://arxiv.org/pdf/2509.06056", "abs": "https://arxiv.org/abs/2509.06056", "authors": ["Chun Wang"], "title": "A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation", "categories": ["cs.LG"], "comment": null, "summary": "A coupling model of biomass fluidized bed gasification based on machine\nlearning and computational fluid dynamics is proposed to improve the prediction\naccuracy and computational efficiency of complex thermochemical reaction\nprocess. By constructing a high-quality data set based on experimental data and\nhigh fidelity simulation results, the agent model used to describe the\ncharacteristics of reaction kinetics was trained and embedded into the\ncomputational fluid dynamics (CFD) framework to realize the real-time update of\nreaction rate and composition evolution.", "AI": {"tldr": "The paper proposes a coupling model combining machine learning and CFD to enhance biomass fluidized bed gasification predictions.", "motivation": "Conventional approaches to modeling biomass gasification struggle with accurately predicting thermochemical reactions and are computationally demanding.", "method": "Develop a model using machine learning trained on experimental and simulation data, embedding it into CFD for dynamic updates of reaction rates and compositions.", "result": "Achieved improved prediction accuracy and computational efficiency for biomass fluidized bed gasification processes.", "conclusion": "Coupling machine learning with CFD significantly enhances gasification modeling, demonstrating its potential in advancing thermochemical reaction analyses."}}
{"id": "2509.06644", "pdf": "https://arxiv.org/pdf/2509.06644", "abs": "https://arxiv.org/abs/2509.06644", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating\nthe state-of-the-art performance in the agricultural domain. Code:\nhttps://github.com/AlexTraveling/T-araVLN.", "AI": {"tldr": "This paper proposes T-araVLN, a refined method for agricultural robotic navigation using Vision-and-Language Navigation (VLN), improving performance on the A2A benchmark.", "motivation": "Agricultural robots increasingly assist in tasks but struggle with autonomous navigation using natural language instructions, especially for complex commands.", "method": "T-araVLN employs an Instruction Translator module to refine and clarify natural language instructions for better navigation performance.", "result": "On the A2A benchmark, T-araVLN enhances Success Rate (SR) from 0.47 to 0.63 and reduces Navigation Error (NE) from 2.91m to 2.28m, showcasing improved accuracy and efficiency.", "conclusion": "The T-araVLN method achieves state-of-the-art navigation performance for agricultural robots and bridges limitations in understanding complex instructions."}}
{"id": "2509.06277", "pdf": "https://arxiv.org/pdf/2509.06277", "abs": "https://arxiv.org/abs/2509.06277", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "title": "No Encore: Unlearning as Opt-Out in Music Generation", "categories": ["cs.CL"], "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "AI": {"tldr": "This paper explores the use of machine unlearning techniques in AI-driven text-to-music generation models to address copyright and ethical concerns.", "motivation": "To reduce ethical and legal issues arising from the potential misuse of copyrighted content by AI music generation models.", "method": "Machine unlearning techniques were applied to pre-trained Text-to-Music (TTM) models to test the effectiveness of unlearning datasets while preserving model performance.", "result": "Preliminary insights were obtained about the challenges of applying machine unlearning to music generation models, demonstrating promising directions.", "conclusion": "This study lays the groundwork for future research involving machine unlearning in generative music models while addressing copyright concerns."}}
{"id": "2509.06503", "pdf": "https://arxiv.org/pdf/2509.06503", "abs": "https://arxiv.org/abs/2509.06503", "authors": ["Eser Ayg\u00fcn", "Anastasiya Belyaeva", "Gheorghe Comanici", "Marc Coram", "Hao Cui", "Jake Garrison", "Renee Johnston Anton Kast", "Cory Y. McLean", "Peter Norgaard", "Zahra Shamsi", "David Smalling", "James Thompson", "Subhashini Venugopalan", "Brian P. Williams", "Chujun He", "Sarah Martinson", "Martyna Plomecka", "Lai Wei", "Yuchen Zhou", "Qian-Ze Zhu", "Matthew Abraham", "Erica Brand", "Anna Bulanova", "Jeffrey A. Cardille", "Chris Co", "Scott Ellsworth", "Grace Joseph", "Malcolm Kane", "Ryan Krueger", "Johan Kartiwa", "Dan Liebling", "Jan-Matthis Lueckmann", "Paul Raccuglia", "Xuefei", "Wang", "Katherine Chou", "James Manyika", "Yossi Matias", "John C. Platt", "Lizzie Dorfman", "Shibl Mourad", "Michael P. Brenner"], "title": "An AI system to help scientists write expert-level empirical software", "categories": ["cs.AI", "q-bio.QM"], "comment": "71 pages, 26 figures", "summary": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.", "AI": {"tldr": "The paper presents an AI system combining a Large Language Model (LLM) with Tree Search (TS) to autonomously create expert-level scientific software that outperforms human approaches in various fields.", "motivation": "Scientific progress is slowed by the manual development of computational tools, necessitating a faster, automated alternative.", "method": "The system pairs a Large Language Model (LLM) with Tree Search (TS) to systematically navigate solution spaces and optimize a quality metric for software development. External knowledge is incorporated for advanced problem-solving.", "result": "The system excelled across diverse fields: outperformed human methods in bioinformatics, epidemiology, geospatial analysis, zebrafish neural activity prediction, time series forecasting, and integral solutions.", "conclusion": "This AI-driven system significantly accelerates scientific discovery by efficiently solving a wide range of research challenges with novel software solutions."}}
{"id": "2509.05669", "pdf": "https://arxiv.org/pdf/2509.05669", "abs": "https://arxiv.org/abs/2509.05669", "authors": ["Weijie Shen", "Xinrui Wang", "Yuanqi Nie", "Apiradee Boonmee"], "title": "Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)\nexcel in single-turn tasks but face significant challenges in multi-turn\ninteractions requiring deep contextual understanding and complex visual\nreasoning, often leading to fragmented reasoning, context loss, and\nhallucinations. To address these limitations, we propose Context-Aware\nMulti-Turn Visual Reasoning (CAMVR), a novel framework designed to empower\nLVLMs with robust and coherent multi-turn visual-textual inference\ncapabilities. CAMVR introduces two key innovations: a Visual-Textual Context\nMemory Unit (VCMU), a dynamic read-write memory network that stores and manages\ncritical visual features, textual semantic representations, and their\ncross-modal correspondences from each interaction turn; and an Adaptive Visual\nFocus Guidance (AVFG) mechanism, which leverages the VCMU's context to\ndynamically adjust the visual encoder's attention to contextually relevant\nimage regions. Our multi-level reasoning integration strategy ensures that\nresponse generation is deeply coherent with both current inputs and accumulated\nhistorical context. Extensive experiments on challenging datasets, including\nVisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following\n(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art\nperformance.", "AI": {"tldr": "The paper introduces CAMVR, a framework enhancing LVLMs to improve multi-turn visual and textual reasoning using dynamic memory and adaptive focusing mechanisms.", "motivation": "The research addresses significant limitations of current LLMs and LVLMs, particularly their struggles with multi-turn interactions, fragmented reasoning, loss of context, and hallucination issues.", "method": "The authors propose a novel framework, CAMVR, incorporating two innovations: a Visual-Textual Context Memory Unit (VCMU) for managing and storing visual-textual correspondences dynamically, and an Adaptive Visual Focus Guidance (AVFG) for adjusting encoder focus based on context.", "result": "Experiments on datasets like VisDial, an adapted A-OKVQA, and the authors' own MTIF dataset show that CAMVR outperforms existing methods with state-of-the-art results.", "conclusion": "CAMVR effectively enhances LVLMs' capabilities in managing complex, multi-turn interactions, improving reasoning coherence and reducing issues like context loss and hallucinations."}}
{"id": "2509.06060", "pdf": "https://arxiv.org/pdf/2509.06060", "abs": "https://arxiv.org/abs/2509.06060", "authors": ["Fei Wang", "Yujie Li", "Zezhi Shao", "Chengqing Yu", "Yisong Fu", "Zhulin An", "Yongjun Xu", "Xueqi Cheng"], "title": "ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in deep learning models for time series forecasting have\nbeen significant. These models often leverage fundamental time series\nproperties such as seasonality and non-stationarity, which may suggest an\nintrinsic link between model performance and data properties. However, existing\nbenchmark datasets fail to offer diverse and well-defined temporal patterns,\nrestricting the systematic evaluation of such connections. Additionally, there\nis no effective model recommendation approach, leading to high time and cost\nexpenditures when testing different architectures across different downstream\napplications. For those reasons, we propose ARIES, a framework for assessing\nrelation between time series properties and modeling strategies, and for\nrecommending deep forcasting models for realistic time series. First, we\nconstruct a synthetic dataset with multiple distinct patterns, and design a\ncomprehensive system to compute the properties of time series. Next, we conduct\nan extensive benchmarking of over 50 forecasting models, and establish the\nrelationship between time series properties and modeling strategies. Our\nexperimental results reveal a clear correlation. Based on these findings, we\npropose the first deep forecasting model recommender, capable of providing\ninterpretable suggestions for real-world time series. In summary, ARIES is the\nfirst study to establish the relations between the properties of time series\ndata and modeling strategies, while also implementing a model recommendation\nsystem. The code is available at: https://github.com/blisky-li/ARIES.", "AI": {"tldr": "The paper introduces ARIES, a framework that correlates time series properties with deep forecasting models and provides recommendations for real-world applications.", "motivation": "Existing benchmarks lack diverse temporal patterns to assess the link between time series data properties and model performance, and there's no effective model recommendation system, leading to inefficiency.", "method": "ARIES constructs synthetic datasets with distinct patterns, calculates time series properties, benchmarks 50+ forecasting models, and develops a recommendation system for model selection.", "result": "The study found a clear correlation between time series properties and modeling strategies, enabling the development of a deep forecasting model recommender.", "conclusion": "ARIES establishes the relationship between time series properties and modeling strategies, providing interpretable and efficient model recommendations for practical forecasting applications."}}
{"id": "2509.06682", "pdf": "https://arxiv.org/pdf/2509.06682", "abs": "https://arxiv.org/abs/2509.06682", "authors": ["Sajad Ahmadi", "Mohammadreza Davoodi", "Javad Mohammadpour Velni"], "title": "An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents an adaptive coverage control method for a fleet of\noff-road and Unmanned Ground Vehicles (UGVs) operating in dynamic\n(time-varying) agricultural environments. Traditional coverage control\napproaches often assume static conditions, making them unsuitable for\nreal-world farming scenarios where obstacles, such as moving machinery and\nuneven terrains, create continuous challenges. To address this, we propose a\nreal-time path planning framework that integrates Unmanned Aerial Vehicles\n(UAVs) for obstacle detection and terrain assessment, allowing UGVs to\ndynamically adjust their coverage paths. The environment is modeled as a\nweighted directed graph, where the edge weights are continuously updated based\non the UAV observations to reflect obstacle motion and terrain variations. The\nproposed approach incorporates Voronoi-based partitioning, adaptive edge weight\nassignment, and cost-based path optimization to enhance navigation efficiency.\nSimulation results demonstrate the effectiveness of the proposed method in\nimproving path planning, reducing traversal costs, and maintaining robust\ncoverage in the presence of dynamic obstacles and muddy terrains.", "AI": {"tldr": "The paper introduces a real-time adaptive coverage control system for UGVs in dynamic agricultural environments, utilizing UAVs for obstacle detection and terrain assessment.", "motivation": "Existing coverage control methods assume static conditions, which are ineffective for agricultural fields where dynamic obstacles and terrain variability are common.", "method": "The study models the environment as a weighted directed graph updated by UAV observations, employing Voronoi partitioning, adaptive weight assignment, and cost-based path optimization for UGV path planning.", "result": "Simulations show improved path planning, lower traversal costs, and enhanced coverage efficiency despite dynamic challenges like obstacles and muddy terrains.", "conclusion": "The proposed method effectively enables UGVs to handle real-world agricultural challenges by integrating UAV-assisted dynamic environmental mapping and adaptive planning."}}
{"id": "2509.06350", "pdf": "https://arxiv.org/pdf/2509.06350", "abs": "https://arxiv.org/abs/2509.06350", "authors": ["Junjie Mu", "Zonghao Ying", "Zhekui Fan", "Zonglei Jing", "Yaoyuan Zhang", "Zhengmin Yu", "Wenxin Zhang", "Quanchen Zou", "Xiangzheng Zhang"], "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks.", "AI": {"tldr": "The paper introduces Mask-GCG, a method to optimize jailbreak attacks on LLMs by reducing redundancy in suffixes, delivering similar attack success rates with reduced computational effort.", "motivation": "To address redundancy in suffixes used for jailbreak attacks on LLMs, improving their efficiency and interpretability.", "method": "Mask-GCG uses learnable token masking to identify high-impact tokens and prune low-impact ones, reducing redundancy and computational overhead in these attacks.", "result": "Experimental results show Mask-GCG maintains attack success rates by focusing on impactful tokens, confirming redundancy in suffixes.", "conclusion": "Mask-GCG improves the efficiency of jailbreak attacks on LLMs without compromising success rates, paving the way for more interpretable and efficient LLM developments."}}
{"id": "2509.06641", "pdf": "https://arxiv.org/pdf/2509.06641", "abs": "https://arxiv.org/abs/2509.06641", "authors": ["Zhou-Peng Shou", "Zhi-Qiang You", "Fang Wang", "Hai-Bo Liu"], "title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.", "AI": {"tldr": "The paper introduces a zero-shot reasoning component for multimodal models inspired by human cognitive strategies, suppressing shortcuts and improving accuracy.", "motivation": "The work aims to address issues of 'shortcuts' and inadequate contextual reasoning in complex cross-modal tasks in large multimodal reasoning models.", "method": "The authors propose a plug-and-play three-module pipeline \u2013 Intent Perceiver, Strategy Generator, and Strategy Selector \u2013 to follow a cognitive process of 'understand-plan-select' using 'intent sketch.'", "result": "Experiments show improvements of up to 9.51 percentage points in reasoning tasks on multiple benchmarks without parameter fine-tuning.", "conclusion": "This 'intent sketch'-based zero-shot approach improves reasoning generality, portability, contextual understanding, and reduces shortcut reliance."}}
{"id": "2509.05670", "pdf": "https://arxiv.org/pdf/2509.05670", "abs": "https://arxiv.org/abs/2509.05670", "authors": ["Ga\u0161per Podobnik", "Toma\u017e Vrtovec"], "title": "MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics", "categories": ["cs.CV"], "comment": null, "summary": "The surge of research in image segmentation has yielded remarkable\nperformance gains but also exposed a reproducibility crisis. A major\ncontributor is performance evaluation, where both selection and implementation\nof metrics play critical roles. While recent efforts have improved the former,\nthe reliability of metric implementation has received far less attention.\nPitfalls in distance-based metric implementation can lead to considerable\ndiscrepancies between common open-source tools, for instance, exceeding 100 mm\nfor the Hausdorff distance and 30%pt for the normalized surface distance for\nthe same pair of segmentations. To address these pitfalls, we introduce\nMeshMetrics, a mesh-based framework that provides a more precise computation of\ndistance-based metrics than conventional grid-based approaches. Through\ntheoretical analysis and empirical validation, we demonstrate that MeshMetrics\nachieves higher accuracy and precision than established tools, and is\nsubstantially less affected by discretization artifacts, such as distance\nquantization. We release MeshMetrics as an open-source Python package,\navailable at https://github.com/gasperpodobnik/MeshMetrics.", "AI": {"tldr": "The paper introduces MeshMetrics, a mesh-based framework to resolve inaccuracies in distance-based metric evaluation for image segmentation.", "motivation": "Reproducibility in image segmentation research is hindered by inconsistent and inaccurate metric implementations.", "method": "MeshMetrics uses a mesh-based approach for precise computation of distance-based metrics, avoiding common grid-based discretization errors.", "result": "The framework demonstrates higher accuracy, better precision, and reduced discretization artifacts compared to conventional tools.", "conclusion": "MeshMetrics improves reliability in image segmentation evaluations and is available as an open-source Python package."}}
{"id": "2509.06067", "pdf": "https://arxiv.org/pdf/2509.06067", "abs": "https://arxiv.org/abs/2509.06067", "authors": ["Mianjun Xiao", "Peng Song", "Yulong Liu", "Cedric Korte", "Ziyang Xu", "Jiale Gao", "Jiaqi Lu", "Haoyang Nie", "Qiantong Deng", "Timing Qu"], "title": "A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "Finite element method (FEM) is widely used in high-temperature\nsuperconducting (HTS) magnets, but its computational cost increases with magnet\nsize and becomes time-consuming for meter-scale magnets, especially when\nmulti-physics couplings are considered, which limits the fast design of\nlarge-scale REBCO magnet systems. In this work, a surrogate model based on a\nfully connected residual neural network (FCRN) is developed to predict the\nspace-time current density distribution in REBCO solenoids. Training datasets\nwere generated from FEM simulations with varying numbers of turns and pancakes.\nThe results demonstrate that, for deeper networks, the FCRN architecture\nachieves better convergence than conventional fully connected network (FCN),\nwith the configuration of 12 residual blocks and 256 neurons per layer\nproviding the most favorable balance between training accuracy and\ngeneralization capability. Extrapolation studies show that the model can\nreliably predict magnetization losses for up to 50% beyond the training range,\nwith maximum errors below 10%. The surrogate model achieves predictions several\norders of magnitude faster than FEM and still remains advantageous when\ntraining costs are included. These results indicate that the proposed\nFCRN-based surrogate model provides both accuracy and efficiency, offering a\npromising tool for the rapid analysis of large-scale HTS magnets.", "AI": {"tldr": "The paper introduces a fully connected residual neural network (FCRN) as a surrogate model for faster and accurate prediction of current density in REBCO solenoids, significantly reducing computational time compared to the finite element method (FEM).", "motivation": "Finite Element Method (FEM) is computationally expensive, especially for large-scale HTS magnets with multi-physics couplings, thus hindering fast design processes.", "method": "A surrogate model using Fully Connected Residual Neural Network (FCRN) was trained on datasets from FEM simulations with varying geometrical configurations of REBCO solenoids.", "result": "The FCRN outperforms conventional neural networks in convergence and efficiently predicts beyond the training range with errors under 10%, offering a speed-up of several orders of magnitude compared to FEM.", "conclusion": "The FCRN-based surrogate model is accurate and highly efficient, making it a practical tool for rapid analysis and design of large-scale high-temperature superconducting magnets."}}
{"id": "2509.06687", "pdf": "https://arxiv.org/pdf/2509.06687", "abs": "https://arxiv.org/abs/2509.06687", "authors": ["Sajad Ahmadi", "Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Deploying self-navigating surface vessels in inland waterways offers a\nsustainable alternative to reduce road traffic congestion and emissions.\nHowever, navigating confined waterways presents unique challenges, including\nnarrow channels, higher traffic density, and hydrodynamic disturbances.\nExisting methods for autonomous vessel navigation often lack the robustness or\nprecision required for such environments. This paper presents a new motion\nplanning approach for Automated Surface Vessels (ASVs) using Robust Model\nPredictive Control (RMPC) combined with Control Barrier Functions (CBFs). By\nincorporating channel borders and obstacles as safety constraints within the\ncontrol design framework, the proposed method ensures both collision avoidance\nand robust navigation on complex waterways. Simulation results demonstrate the\nefficacy of the proposed method in safely guiding ASVs under realistic\nconditions, highlighting its improved safety and adaptability compared to the\nstate-of-the-art.", "AI": {"tldr": "The paper introduces a motion planning approach for Automated Surface Vessels (ASVs) in narrow waterways, using Robust Model Predictive Control combined with Control Barrier Functions for safer and more adaptable navigation.", "motivation": "To address the challenges in navigating confined inland waterways with ASVs, such as narrow channels, high traffic density, and hydrodynamic disturbances, as existing methods lack robustness and precision for these environments.", "method": "The authors proposed integrating Robust Model Predictive Control (RMPC) with Control Barrier Functions (CBFs), incorporating channel borders and obstacles as constraints in the control framework to ensure collision avoidance and robust navigation.", "result": "Simulation results show the proposed method's effectiveness in safely guiding ASVs under realistic conditions, demonstrating improvements in safety and adaptability compared to existing methodologies.", "conclusion": "The proposed approach enhances autonomous vessel navigation in complex waterways by ensuring collision avoidance and robust performance, advancing ASV deployment in sustainable transport solutions."}}
{"id": "2509.06356", "pdf": "https://arxiv.org/pdf/2509.06356", "abs": "https://arxiv.org/abs/2509.06356", "authors": ["Ao Chang", "Yubo Chen", "Jun Zhao"], "title": "PL-CA: A Parametric Legal Case Augmentation Framework", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.", "AI": {"tldr": "The paper proposes PL-CA, which incorporates a parametric RAG framework to encode legal knowledge efficiently, alleviating context pressure. It also introduces a multi-task legal dataset to evaluate its effectiveness.", "motivation": "To address the limitations of conventional RAG methods, which include context window constraints, computational overhead, and the inadequacy of existing legal benchmarks to handle real-world multi-task scenarios.", "method": "Introduces the P-RAG framework for encoding corpus knowledge as parametric vectors, and integrates this knowledge into LLMs using LoRA. Also constructs an expert-annotated multi-task legal dataset.", "result": "The proposed PL-CA approach reduces context overhead while maintaining competitive performance on legal downstream tasks compared to traditional RAG methods.", "conclusion": "PL-CA improves the efficiency and capabilities of LLMs in legal contexts by addressing context limitations and benchmark inadequacies, demonstrating its practical advantages on a new expert-verified dataset."}}
{"id": "2509.06733", "pdf": "https://arxiv.org/pdf/2509.06733", "abs": "https://arxiv.org/abs/2509.06733", "authors": ["Wenjun Li", "Zhi Chen", "Jingru Lin", "Hannan Cao", "Wei Han", "Sheng Liang", "Zhi Zhang", "Kuicai Dong", "Dexun Li", "Chen Zhang", "Yong Liu"], "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "38 pages, first version", "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.", "AI": {"tldr": "This survey examines the RL foundations of agentic AI systems for complex, multi-step tasks, focusing on data synthesis, RL methods, and training systems.", "motivation": "To address the limitations of current AI training methods\u2014specifically SFT and DPO\u2014and the need for robust reinforcement learning approaches in deep research systems.", "method": "The paper systematizes RL advancements across data synthesis, RL techniques for agentic research, and RL training systems, while examining agent architecture, coordination, benchmarks, and evaluation methods.", "result": "The survey categorizes work in RL-based agentic AI systems, identifies recurring patterns, and highlights challenges such as infrastructure bottlenecks.", "conclusion": "RL enhances deep research systems by offering frameworks for principled credit assignment, recovery behaviors, and reduced reliance on human biases, while highlighting practical guidance for future improvements."}}
{"id": "2509.05695", "pdf": "https://arxiv.org/pdf/2509.05695", "abs": "https://arxiv.org/abs/2509.05695", "authors": ["Jingwei Peng", "Zhixuan Qiu", "Boyu Jin", "Surasakdi Siripong"], "title": "Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition often struggles with deep semantic understanding,\ncomplex contextual information, and fine-grained distinction, limitations that\ntraditional methods frequently encounter when dealing with diverse video data.\nInspired by the remarkable capabilities of large language models, this paper\nintroduces LVLM-VAR, a novel framework that pioneers the application of\npre-trained Vision-Language Large Models (LVLMs) to video action recognition,\nemphasizing enhanced accuracy and interpretability. Our method features a\nVideo-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video\nsequences into discrete, semantically and temporally consistent \"semantic\naction tokens,\" effectively crafting an \"action narrative\" that is\ncomprehensible to an LVLM. These tokens, combined with natural language\ninstructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)\nfor robust action classification and semantic reasoning. LVLM-VAR not only\nachieves state-of-the-art or highly competitive performance on challenging\nbenchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant\nimprovements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),\nbut also substantially boosts model interpretability by generating natural\nlanguage explanations for its predictions.", "AI": {"tldr": "The paper presents LVLM-VAR, a novel framework using Vision-Language Large Models (LVLMs) for video action recognition, improving accuracy and interpretability.", "motivation": "Traditional methods struggle with semantic understanding, contextual information, and fine-grained distinctions in video action recognition.", "method": "Introduces the Video-to-Semantic-Tokens module to generate semantic action tokens from video sequences, processed by LoRA-fine-tuned LVLMs for classification and reasoning.", "result": "Achieves state-of-the-art performance on benchmarks like NTU RGB+D, with significant accuracy improvements (e.g., 94.1% and 90.0% on respective datasets).", "conclusion": "LVLM-VAR enhances video action recognition by combining semantic tokenization and LVLMs, improving both performance and interpretability."}}
{"id": "2509.06094", "pdf": "https://arxiv.org/pdf/2509.06094", "abs": "https://arxiv.org/abs/2509.06094", "authors": ["S. R. Eshwar"], "title": "Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time-inconsistent preferences, where agents favor smaller-sooner over\nlarger-later rewards, are a key feature of human and animal decision-making.\nQuasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this\nbehavior, but its integration into the reinforcement learning (RL) framework\nhas been limited. This paper addresses key theoretical and algorithmic gaps for\nprecommitted agents with QH preferences. We make two primary contributions: (i)\nwe formally characterize the structure of the optimal policy, proving for the\nfirst time that it reduces to a simple one-step non-stationary form; and (ii)\nwe design the first practical, model-free algorithms for both policy evaluation\nand Q-learning in this setting, both with provable convergence guarantees. Our\nresults provide foundational insights for incorporating QH preferences in RL.", "AI": {"tldr": "The paper explores Quasi-Hyperbolic (QH) discounting in reinforcement learning, providing theoretical foundations and practical algorithms.", "motivation": "To address the integration gap between Quasi-Hyperbolic discounting and reinforcement learning, and understand time-inconsistent preferences in decision-making.", "method": "Characterization of optimal policy as one-step non-stationary, and development of model-free algorithms for policy evaluation and Q-learning with convergence guarantees.", "result": "Theoretical proof of policy structure and practical algorithms for QH discounting in RL, with proven convergence.", "conclusion": "Establishes foundational insights making QH preferences applicable in the RL framework effectively."}}
{"id": "2509.06768", "pdf": "https://arxiv.org/pdf/2509.06768", "abs": "https://arxiv.org/abs/2509.06768", "authors": ["Oluwadamilola Sotomi", "Devika Kodi", "Kiruthiga Chandra Shekar", "Aliasghar Arab"], "title": "Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous robots operating in dynamic environments should identify and\nreport anomalies. Embodying proactive mitigation improves safety and\noperational continuity. This paper presents a multimodal anomaly detection and\nmitigation system that integrates vision-language models and large language\nmodels to identify and report hazardous situations and conflicts in real-time.\nThe proposed system enables robots to perceive, interpret, report, and if\npossible respond to urban and environmental anomalies through proactive\ndetection mechanisms and automated mitigation actions. A key contribution in\nthis paper is the integration of Hazardous and Conflict states into the robot's\ndecision-making framework, where each anomaly type can trigger specific\nmitigation strategies. User studies (n = 30) demonstrated the effectiveness of\nthe system in anomaly detection with 91.2% prediction accuracy and relatively\nlow latency response times using edge-ai architecture.", "AI": {"tldr": "This paper introduces a multimodal system leveraging vision-language models and large language models to enable autonomous robots to detect and mitigate environmental anomalies in real-time.", "motivation": "To improve safety and operational continuity of autonomous robots in dynamic environments by enabling proactive anomaly detection and mitigation.", "method": "The paper integrates vision-language models and large language models to build a robot decision-making framework, designing specific mitigation strategies for hazardous and conflict states.", "result": "The system achieved 91.2% prediction accuracy with low-latency responses, validated through user studies involving 30 participants using edge-ai architecture.", "conclusion": "Integrating multimodal AI ensures efficient anomaly detection and proactive mitigation, contributing to safer and autonomous robotic operations in urban and dynamic environments."}}
{"id": "2509.06401", "pdf": "https://arxiv.org/pdf/2509.06401", "abs": "https://arxiv.org/abs/2509.06401", "authors": ["Ivan Mart\u00ednez-Murillo", "Elena Lloret", "Paloma Moreda", "Albert Gatt"], "title": "Do LLMs exhibit the same commonsense capabilities across languages?", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM.", "AI": {"tldr": "The study investigates how Large Language Models handle multilingual commonsense generation using a benchmark called MULTICOM, finding strengths in English but limitations in less-resourced languages.", "motivation": "The paper aims to evaluate the boundaries and limitations of LLMs in generating commonsense knowledge across multiple languages.", "method": "They utilized the MULTICOM benchmark, extending it to four languages, and evaluated LLMs with automatic metrics, AI evaluations, and human assessments.", "result": "English showed superior performance, while less-resourced languages had significantly lower results; evidence was mixed on contextual support benefits.", "conclusion": "LLMs currently show limitations in handling multilingual commonsense generation, particularly in less-resourced languages."}}
{"id": "2509.06736", "pdf": "https://arxiv.org/pdf/2509.06736", "abs": "https://arxiv.org/abs/2509.06736", "authors": ["Jie Yang", "Jiajun Chen", "Zhangyue Yin", "Shuo Chen", "Yuxin Wang", "Yiran Guo", "Yuan Li", "Yining Zheng", "Xuanjing Huang", "Xipeng Qiu"], "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.", "AI": {"tldr": "This paper introduces VehicleWorld, a comprehensive automotive environment with real-time state features, proposing a novel State-based Function Call (SFC) approach that improves execution accuracy and efficiency over traditional Function Calling methods.", "motivation": "To address inefficiency and limited error recovery in traditional stateless Function Calling methods in complex intelligent vehicle environments.", "method": "The authors developed VehicleWorld featuring fully executable implementations of 30 modules, 250 APIs, and 680 properties, enabling state-based analysis. They proposed State-based Function Call (SFC) for explicit system state awareness and direct transitions.", "result": "Experimental results show SFC achieves superior execution accuracy and reduced latency compared to traditional Function Calling methods.", "conclusion": "The paper concludes that SFC is a significant improvement for intelligent vehicle systems, making the execution more efficient. They have also publicly shared the implementation code to foster further research."}}
{"id": "2509.05696", "pdf": "https://arxiv.org/pdf/2509.05696", "abs": "https://arxiv.org/abs/2509.05696", "authors": ["Hongyu Zhou", "Yunzhou Zhang", "Tingsong Huang", "Fawei Ge", "Man Qi", "Xichen Zhang", "Yizhong Zhang"], "title": "JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle\n(UAV) localization and navigation. However, significant challenges arise from\nthe drastic viewpoint differences and appearance variations between images.\nExisting methods predominantly rely on semantic features from RGB images, often\nneglecting the importance of spatial structural information in capturing\nviewpoint-invariant features. To address this issue, we incorporate geometric\nstructural information from normal images and introduce a Joint perception\nnetwork to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a\ndual-branch feature extraction framework, leveraging a Difference-Aware Fusion\nModule (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to\nenable deep fusion and joint-constrained semantic and structural information\nrepresentation. Furthermore, we propose a 3D geographic augmentation technique\nto generate potential viewpoint variation samples, enhancing the network's\nability to learn viewpoint-invariant features. Extensive experiments on the\nUniversity-1652 and SUES-200 datasets validate the robustness of our method\nagainst complex viewpoint ariations, achieving state-of-the-art performance.", "AI": {"tldr": "The paper introduces a dual-branch network (JRN-Geo) for UAV cross-view geo-localization, combining RGB and normal images to address viewpoint changes and achieve better performance.", "motivation": "To overcome challenges in cross-view geo-localization caused by drastic viewpoint differences and appearance variations between images, especially when spatial structural information is often overlooked.", "method": "The paper proposes JRN-Geo, a dual-branch network combining RGB and normal images using a Difference-Aware Fusion Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy for joint semantic-structural representation. Additionally, a 3D geographic augmentation is introduced to simulate viewpoint variations.", "result": "Experiments on University-1652 and SUES-200 datasets demonstrate the proposed method's robustness to viewpoint variations and its state-of-the-art performance.", "conclusion": "Integrating geometric structural information significantly improves viewpoint-invariant feature learning for UAV localization, offering a robust solution for cross-view geo-localization tasks."}}
{"id": "2509.06819", "pdf": "https://arxiv.org/pdf/2509.06819", "abs": "https://arxiv.org/abs/2509.06819", "authors": ["Daniel San Jos\u00e9 Pro", "Oliver Hausd\u00f6rfer", "Ralf R\u00f6mer", "Maximilian D\u00f6sch", "Martin Schuck", "Angela P. Sch\u00f6llig"], "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation", "categories": ["cs.RO"], "comment": "5 pages, 5 figures", "summary": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.", "AI": {"tldr": "The paper introduces CRISP, a lightweight controller framework for ROS2-compatible robotic manipulators, which facilitates smooth tracking, real-time performance, and integration of learning-based high-level policies.", "motivation": "To address the challenge of integrating learning-based high-level controllers\u2014which often exhibit low-frequency or discontinuous outputs\u2014with hardware-level robot controllers while enabling compliant and smooth motion.", "method": "CRISP is a C++ implementation of Cartesian and joint-space controllers for the ROS2 control standard, providing Python and Gymnasium interfaces for seamless data collection and deployment of high-level learning-based policies. It has been validated on various hardware and simulation platforms.", "result": "The framework has been successfully validated on the Franka Robotics FR3 hardware, as well as in simulations using Kuka IIWA14 and Kinova Gen3 manipulators, demonstrating rapid integration and real-time performance.", "conclusion": "CRISP offers a unified and flexible solution for integrating learning-based methods with ROS2 manipulators, lowering barriers for research and deployment. Detailed documentation further facilitates accessibility."}}
{"id": "2509.06501", "pdf": "https://arxiv.org/pdf/2509.06501", "abs": "https://arxiv.org/abs/2509.06501", "authors": ["Junteng Liu", "Yunji Li", "Chi Zhang", "Jingyang Li", "Aili Chen", "Ke Ji", "Weiyu Cheng", "Zijia Wu", "Chengyu Du", "Qidi Xu", "Jiayuan Song", "Zhengmao Zhu", "Wenhu Chen", "Pengyu Zhao", "Junxian He"], "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.", "AI": {"tldr": "The paper introduces WebExplorer, a method to create challenging query-answer pairs for training advanced web agents. Using this, they developed WebExplorer-8B, which achieves state-of-the-art performance in various benchmarks.", "motivation": "To enhance the information-seeking capabilities of web agents by overcoming the limited availability of challenging training data for complex tasks.", "method": "Proposed a data generation approach (WebExplorer) involving model-based exploration and iterative query evolution to produce challenging query-answer pairs. Developed WebExplorer-8B through supervised fine-tuning and reinforcement learning.", "result": "WebExplorer-8B exhibits superior performance across diverse information-seeking tasks, outperforming larger models like WebSailor-72B and achieving the best results among smaller models. Additionally, it generalizes effectively on unrelated benchmarks despite being trained on a narrow dataset.", "conclusion": "WebExplorer's methodology significantly improves long-horizon web agents, demonstrating the practicality of the proposed approach for complex, multi-step web-based information retrieval tasks."}}
{"id": "2509.06770", "pdf": "https://arxiv.org/pdf/2509.06770", "abs": "https://arxiv.org/abs/2509.06770", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.", "AI": {"tldr": "This paper introduces a framework to evaluate iterative refinement in tasks involving ideation, coding, and math. It examines metrics and outcomes across multiple turns to measure when iteration improves or worsens outcomes.", "motivation": "To address the lack of clear evaluation methods for iterative workflows with large language models in diverse domains.", "method": "The framework involves 12-turn conversations per task with different feedback types and tracks outputs using domain-specific scoring and metrics such as semantic movement, turn-level change, and output size growth.", "result": "Findings show domain-dependent gains: early improvements in ideas and code, late-turn gains in math guided by elaboration, and plateau or regression with vague feedback. It outlines behavior patterns like size growth in code and semantic shifts in ideation.", "conclusion": "The proposed framework makes iteration measurable across domains, highlights its domain-specific impact, and offers insights on when to modify or stop iterative workflows."}}
{"id": "2509.05703", "pdf": "https://arxiv.org/pdf/2509.05703", "abs": "https://arxiv.org/abs/2509.05703", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.", "AI": {"tldr": "The paper examines whether Vision Language Models (VLMs) can interpret marine mammal bioacoustic spectrograms without requiring retraining or manual annotations.", "motivation": "The motivation lies in overcoming the challenge of interpreting domain-specific visual data (bioacoustic spectrograms) using preexisting models (VLMs) not explicitly trained for this purpose.", "method": "The approach combines VLM analysis of spectrograms with validation using Large Language Models (LLMs), aiming to integrate interpretation into domain knowledge.", "result": "The framework successfully enables VLMs to adapt to acoustic data without requiring manual tasks such as annotation or model retraining.", "conclusion": "VLMs, when paired with LLM-based validation, exhibit potential for meaningful visual interpretation in specialized fields like marine bioacoustics, streamlining data analysis."}}
{"id": "2509.06882", "pdf": "https://arxiv.org/pdf/2509.06882", "abs": "https://arxiv.org/abs/2509.06882", "authors": ["Zhiheng Chen", "Wei Wang"], "title": "Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles", "categories": ["cs.RO"], "comment": "This work has been accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.", "AI": {"tldr": "The study proposes a data-driven optimal control method for enhancing Micro Autonomous Surface Vehicles (MicroASVs) by refining a physics-driven model in real time, achieving improved trajectory accuracy and robustness.", "motivation": "To address the challenges of controlling MicroASVs in constrained and dynamic environments, including nonlinear hydrodynamic forces and unpredictable disturbances.", "method": "A hybrid approach combining a physics-driven dynamics model of an over-actuated MicroASV with a data-driven weak formulation-based online learning control mechanism for adaptive system refinement.", "result": "Simulation results confirm improved trajectory tracking and system robustness despite unknown payloads and external disturbances.", "conclusion": "The integration of data-driven online learning enhances the reliability and precision of MicroASV operations, showcasing its potential for future autonomous systems."}}
{"id": "2509.06518", "pdf": "https://arxiv.org/pdf/2509.06518", "abs": "https://arxiv.org/abs/2509.06518", "authors": ["Andrei Baroian", "Kasper Notebomer"], "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero", "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.", "AI": {"tldr": "This study explores new ways to redistribute computational capacity across Transformer layers using Layer-Wise Scaling (LWS) methods, resulting in enhanced performance without reducing training efficiency.", "motivation": "Traditional Transformer-based language models use uniform layer sizes, ignoring diverse roles and computational needs for different depths. The paper seeks to address this inefficiency by redistributing FFN widths and attention heads.", "method": "Three Layer-Wise Scaling variants (Framed, Reverse, Crown) were proposed to optimize layer designs via two or three-point linear interpolation during pre-training. Models were trained with 180M parameters on 5B tokens, with comparisons against isotropic baselines.", "result": "All models converged similarly in loss, outperforming equal-cost isotropic baselines without compromising throughput during training.", "conclusion": "Initial results highlight potential improvement in performance through layer-wise architectural changes, urging future exploration at larger scales in pre-training."}}
{"id": "2509.06822", "pdf": "https://arxiv.org/pdf/2509.06822", "abs": "https://arxiv.org/abs/2509.06822", "authors": ["Chenyang Zhu", "Spencer Hong", "Jingyu Wu", "Kushal Chawla", "Charlotte Tang", "Youbing Yin", "Nathan Wolfe", "Erin Babinsky", "Daben Liu"], "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.", "AI": {"tldr": "This paper introduces RAFFLES, an iterative evaluation architecture for diagnosing faults in long-horizon, multi-component LLM agentic systems, significantly improving fault identification accuracy over baselines.", "motivation": "To address the challenge of identifying breakdowns and faults in long-horizon, multi-component LLM agentic systems, which current evaluation methods struggle with due to their limited scope and grounding.", "method": "RAFFLES utilizes an iterative evaluation architecture with a central Judge and specialized Evaluators to systematically investigate faults and assess reasoning quality, employing a hypothesis-building approach.", "result": "RAFFLES achieved over 43% fault-pair accuracy on the Algorithmically-Generated dataset (up from 16.6%) and over 20% accuracy on the Hand-Crafted dataset (up from 8.8%), outperforming prior methods.", "conclusion": "RAFFLES represents a significant step towards automated fault detection for autonomous systems, reducing the reliance on manual reviews and enhancing evaluation capabilities for complex logic systems."}}
{"id": "2509.05728", "pdf": "https://arxiv.org/pdf/2509.05728", "abs": "https://arxiv.org/abs/2509.05728", "authors": ["Niels Balemans", "Ali Anwar", "Jan Steckel", "Siegfried Mercelis"], "title": "LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper extends LiDAR-BIND, a modular multi-modal fusion framework that\nbinds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,\nwith mechanisms that explicitly enforce temporal consistency. We introduce\nthree contributions: (i) temporal embedding similarity that aligns consecutive\nlatents, (ii) a motion-aligned transformation loss that matches displacement\nbetween predictions and ground truth LiDAR, and (iii) windows temporal fusion\nusing a specialised temporal module. We further update the model architecture\nto better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR\ntranslation demonstrate improved temporal and spatial coherence, yielding lower\nabsolute trajectory error and better occupancy map accuracy in\nCartographer-based SLAM (Simultaneous Localisation and Mapping). We propose\ndifferent metrics based on the Fr\\'echet Video Motion Distance (FVMD) and a\ncorrelation-peak distance metric providing practical temporal quality\nindicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or\nLiDAR-BIND-T, maintains plug-and-play modality fusion while substantially\nenhancing temporal stability, resulting in improved robustness and performance\nfor downstream SLAM.", "AI": {"tldr": "LiDAR-BIND-T improves temporal and spatial coherence in radar/sonar-to-LiDAR translation with new metrics and methods.", "motivation": "To enhance temporal stability and modality fusion in radar/sonar-to-LiDAR translation frameworks for better SLAM performance.", "method": "Three core techniques: temporal embedding similarity, motion-aligned transformation loss, and windowed temporal fusion, along with updates to model architecture.", "result": "Improved temporal/spatial coherence, lower trajectory error, and better occupancy map accuracy in SLAM evaluations.", "conclusion": "LiDAR-BIND-T enhances robustness and performance in SLAM while maintaining modular plug-and-play fusion."}}
{"id": "2509.06161", "pdf": "https://arxiv.org/pdf/2509.06161", "abs": "https://arxiv.org/abs/2509.06161", "authors": ["Aurora Polo-Rodr\u00edguez", "Juan Carlos Valera", "Jes\u00fas Peral", "David Gil", "Javier Medina-Quero"], "title": "Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models", "categories": ["cs.LG", "cs.AI", "I.2.0"], "comment": "25 pages, 14 figures", "summary": "The field of human activity recognition has evolved significantly, driven\nlargely by advancements in Internet of Things (IoT) device technology,\nparticularly in personal devices. This study investigates the use of\nultra-wideband (UWB) technology for tracking inhabitant paths in home\nenvironments using deep learning models. UWB technology estimates user\nlocations via time-of-flight and time-difference-of-arrival methods, which are\nsignificantly affected by the presence of walls and obstacles in real\nenvironments, reducing their precision. To address these challenges, we propose\na fingerprinting-based approach utilizing received signal strength indicator\n(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while\nperforming daily activities. We compare the performance of convolutional neural\nnetwork (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as\nwell as the use of Bluetooth technology. Additionally, we evaluate the impact\nof the type and duration of the temporal window (future, past, or a combination\nof both). Our results demonstrate a mean absolute error close to 50 cm,\nhighlighting the superiority of the hybrid model in providing accurate location\nestimates, thus facilitating its application in daily human activity\nrecognition in residential settings.", "AI": {"tldr": "This paper explores ultra-wideband (UWB) technology combined with deep learning to improve indoor human activity tracking, achieving high-location accuracy using hybrid CNN+LSTM models.", "motivation": "To address the challenges in precise user location tracking indoors using UWB technology, affected by obstacles and walls, and to enhance human activity recognition in residences.", "method": "The authors proposed a fingerprinting-based approach using RSSI data and evaluated CNN, LSTM, and hybrid CNN+LSTM models in two residential setups, incorporating temporal window variations.", "result": "The hybrid model demonstrated superior performance, achieving a mean absolute error close to 50 cm for inhabitant location tracking.", "conclusion": "The findings underscore the hybrid model's potential for accurate human activity recognition in home environments, advancing UWB-based tracking technologies."}}
{"id": "2509.06932", "pdf": "https://arxiv.org/pdf/2509.06932", "abs": "https://arxiv.org/abs/2509.06932", "authors": ["Yuqing Wen", "Hebei Li", "Kefan Gu", "Yucheng Zhao", "Tiancai Wang", "Xiaoyan Sun"], "title": "LLaDA-VLA: Vision Language Diffusion Action Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.", "AI": {"tldr": "This paper introduces LLaDA-VLA, a vision-language-action model utilizing diffusion-based vision-language models for robotic manipulation. The model demonstrates superior performance compared to existing approaches.", "motivation": "While auto-regressive models have seen advancements in vision-language-action tasks for robotics, leveraging diffusion-based vision-language models (d-VLMs) for robotic policy remains unexplored.", "method": "The authors adapt d-VLMs to robotics through two strategies: (1) localized special-token classification for easier adaptation, and (2) hierarchical action-structured decoding to model dependencies in action sequences effectively.", "result": "The proposed LLaDA-VLA outperforms state-of-the-art vision-language-action models in both simulated and real-world robotic tasks.", "conclusion": "LLaDA-VLA demonstrates that diffusion-based approaches are effective for robotic manipulation, marking a significant step forward in the domain of vision-language-action models."}}
{"id": "2509.06524", "pdf": "https://arxiv.org/pdf/2509.06524", "abs": "https://arxiv.org/abs/2509.06524", "authors": ["Jian Wu", "Hang Yu", "Bingchang Liu", "Wenjie Yang", "Peng Di", "Jianguo Li", "Yue Zhang"], "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.", "AI": {"tldr": "The paper proposes LAMDAS, a method for domain-specific data selection leveraging pre-trained LLMs as implicit classifiers to overcome data scarcity issues and improve fine-tuning performance.", "motivation": "The need to adapt large language models (LLMs) to specific domains despite a scarcity of high-quality, curated data, and the challenges posed by indiscriminate data selection leading to degraded model performance.", "method": "LAMDAS introduces a data selection strategy, treating the process as a one-class classification problem where pre-trained LLMs identify domain-specific data using a small reference dataset.", "result": "Experiments show that LAMDAS outperforms full-data training and nine SOTA baselines under various conditions, achieving significant performance gains with improved computational efficiency.", "conclusion": "LAMDAS is a highly efficient and accurate data selection method that improves domain adaptation for LLMs, balancing high performance with reduced computational costs."}}
{"id": "2509.06861", "pdf": "https://arxiv.org/pdf/2509.06861", "abs": "https://arxiv.org/abs/2509.06861", "authors": ["James Xu Zhao", "Bryan Hooi", "See-Kiong Ng"], "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "20 pages, 4 figures, 6 tables", "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge", "AI": {"tldr": "Test-time scaling, which enhances reasoning by increasing computation during inference, shows inconsistent results, particularly for knowledge-intensive tasks. Instead of improving accuracy, it sometimes causes more hallucinations.", "motivation": "Understanding the effectiveness and drawbacks of test-time scaling in reasoning tasks, especially for scenarios requiring high factual accuracy and low hallucination rates.", "method": "A comprehensive evaluation was conducted using 12 reasoning models on two benchmarks. Analysis of hallucination behaviors and detailed case studies were also performed.", "result": "Increasing test-time computation did not consistently improve factual accuracy and often heightened hallucination rates. Abstaining and attempting unanswered questions during extended reasoning influenced hallucination patterns.", "conclusion": "While extended reasoning shows promise, its limitations\u2014like inducing confirmation bias and overconfident hallucinations\u2014must be addressed to make it truly effective for tasks demanding factual precision."}}
{"id": "2509.05740", "pdf": "https://arxiv.org/pdf/2509.05740", "abs": "https://arxiv.org/abs/2509.05740", "authors": ["Xinyu Zhang", "Kai Huang", "Junqiao Zhao", "Zihan Yuan", "Tiantian Feng"], "title": "Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras", "categories": ["cs.CV"], "comment": null, "summary": "We propose a multi-camera LiDAR-visual-inertial odometry framework,\nMulti-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and\ninertial sensors for highly accurate and robust state estimation. To enable\nefficient and consistent integration of visual information from multiple\nfisheye cameras, we introduce a panoramic visual feature model that unifies\nmulti-camera observations into a single representation. The panoramic model\nserves as a global geometric optimization framework that consolidates\nmulti-view constraints, enabling seamless loop closure and global pose\noptimization, while simplifying system design by avoiding redundant handling of\nindividual cameras. To address the triangulation inconsistency caused by the\nmisalignment between each camera's frame and the panoramic model's frame, we\npropose an extrinsic compensation method. This method improves feature\nconsistency across views and significantly reduces triangulation and\noptimization errors, leading to more accurate pose estimation. We integrate the\npanoramic visual feature model into a tightly coupled LiDAR-visual-inertial\nsystem based on a factor graph. Extensive experiments on public datasets\ndemonstrate that the panoramic visual feature model enhances the quality and\nconsistency of multi-camera constraints, resulting in higher accuracy and\nrobustness than existing multi-camera LiDAR-visual-inertial systems.", "AI": {"tldr": "The paper introduces Multi-LVI-SAM, a state estimation framework that integrates fisheye cameras, LiDAR, and inertial sensors. A novel panoramic visual feature model is proposed for unifying multi-camera data efficiently, enhancing accuracy and robustness.", "motivation": "To improve the robustness and accuracy of state estimation by efficiently integrating data from multiple fisheye cameras, LiDAR, and inertial sensors.", "method": "The method introduces a panoramic visual feature model that consolidates multi-camera observations into a unified representation. It incorporates extrinsic compensation to address triangulation inconsistencies and adopts a tightly coupled LiDAR-visual-inertial system using a factor graph.", "result": "Experiments on public datasets show that the proposed model improves the quality and consistency of multi-camera constraints, achieving higher accuracy and robustness compared to existing systems.", "conclusion": "The panoramic visual feature model successfully simplifies multi-camera integration, resolves inconsistency issues, and enhances the overall state estimation framework, positioning it as superior to existing systems."}}
{"id": "2509.06951", "pdf": "https://arxiv.org/pdf/2509.06951", "abs": "https://arxiv.org/abs/2509.06951", "authors": ["Qi Lv", "Weijie Kong", "Hao Li", "Jia Zeng", "Zherui Qiu", "Delin Qu", "Haoming Song", "Qizhi Chen", "Xiang Deng", "Jiangmiao Pang"], "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.", "AI": {"tldr": "This paper introduces F1, a pretrained Vision-Language-Action (VLA) model integrating visual foresight for better decision-making in dynamic environments, achieving state-of-the-art performance by forecasting future visual states and reformulating action generation.", "motivation": "Address the shortcomings of reactive Vision-Language-Action models that struggle with short-sighted behaviors and lack robustness in dynamic visual environments.", "method": "Develop F1, a Mixture-of-Transformer architecture with modules for perception, foresight generation, and control. F1 uses a next-scale prediction mechanism for visual foresight integration into decision-making, trained on a dataset with 330,000 trajectories across 136 tasks.", "result": "Evaluations in real-world and simulated tasks reveal F1's superior performance in task success rates and generalization compared to existing models.", "conclusion": "F1 demonstrates that integrating visual foresight as planning targets can robustly improve decision-making and transferability in complex and dynamic environments."}}
{"id": "2509.06531", "pdf": "https://arxiv.org/pdf/2509.06531", "abs": "https://arxiv.org/abs/2509.06531", "authors": ["Mengxue Yang", "Chun Yang", "Jiaqi Zhu", "Jiafan Li", "Jingqi Zhang", "Yuyang Li", "Ying Li"], "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP Findings 2025", "summary": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.", "AI": {"tldr": "The paper introduces SLiNT, a framework that improves link prediction in knowledge graphs by injecting structural context into large language models using modular techniques like SGNE, DHCL, and GDDI. SLiNT performs well compared to existing methods.", "motivation": "Link prediction in knowledge graphs is challenging due to structural sparsity and semantic ambiguity, especially in incomplete or zero-shot scenarios.", "method": "The SLiNT framework uses three techniques: SGNE enriches sparse entities with pseudo-neighbors; DHCL interpolates hard positives/negatives for supervision; and GDDI applies structure-aware intervention to frozen LLM parameters.", "result": "Experiments on WN18RR and FB15k-237 datasets show SLiNT achieves superior or competitive performance against baseline methods.", "conclusion": "SLiNT demonstrates the value of integrating structural signals into language models, enhancing the scalability and accuracy of knowledge graph completion."}}
{"id": "2509.06917", "pdf": "https://arxiv.org/pdf/2509.06917", "abs": "https://arxiv.org/abs/2509.06917", "authors": ["Jiacheng Miao", "Joe R. Davis", "Jonathan K. Pritchard", "James Zou"], "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.", "AI": {"tldr": "Paper2Agent is an AI framework that converts research papers into interactive agents, enabling easier understanding and application.", "motivation": "Research papers require significant effort for readers to understand and adapt methods, creating barriers to dissemination and reuse.", "method": "Paper2Agent develops Model Context Protocols (MCPs) by analyzing papers and their codebases, testing for refinement, and connecting them with chat agents.", "result": "The framework successfully created agents for genomic and transcriptomics analyses, reproducing results and enabling novel user queries.", "conclusion": "Paper2Agent offers a dynamic and collaborative platform for knowledge dissemination, transforming static papers into active AI research assistants."}}
{"id": "2509.05746", "pdf": "https://arxiv.org/pdf/2509.05746", "abs": "https://arxiv.org/abs/2509.05746", "authors": ["Tianhao Guo", "Bingjie Lu", "Feng Wang", "Zhengyang Lu"], "title": "Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation", "categories": ["cs.CV"], "comment": null, "summary": "Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.", "AI": {"tldr": "This paper proposes a novel super-resolution framework that incorporates spatially adaptive and depth-aware techniques to address limitations in real-world imaging conditions like distance-dependent effects.", "motivation": "Existing single image super-resolution models often rely on spatially-invariant degradation assumptions, which fail to accommodate real-world imaging distortions caused by atmospheric scattering, depth-of-field, and perspective effects.", "method": "The authors developed a variational framework that models distance-dependent degradation as a pseudodifferential operator. They implemented a neural architecture using depth-conditional convolution kernels and spectral constraints derived from atmospheric theory to dynamically adjust reconstructions based on depth-specific characteristics.", "result": "The proposed model achieves state-of-the-art super-resolution results on benchmark datasets like KITTI, showing 36.89/0.9516 PSNR/SSIM at 2\u00d7 and 30.54/0.8721 at 4\u00d7 scales while surpassing existing methods by up to 0.44dB.", "conclusion": "This study introduces the first theoretically-grounded framework for distance-adaptive super-resolution, demonstrating notable advancements in handling depth-variant degradation in real-world imaging scenarios and achieving competitive or superior outcomes on conventional benchmarks."}}
{"id": "2509.06167", "pdf": "https://arxiv.org/pdf/2509.06167", "abs": "https://arxiv.org/abs/2509.06167", "authors": ["Ximena Pocco", "Waqar Hassan", "Karelia Salinas", "Vladimir Molchanov", "Luis G. Nonato"], "title": "Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Urban analytics utilizes extensive datasets with diverse urban information to\nsimulate, predict trends, and uncover complex patterns within cities. While\nthese data enables advanced analysis, it also presents challenges due to its\ngranularity, heterogeneity, and multimodality. To address these challenges,\nvisual analytics tools have been developed to support the exploration of latent\nrepresentations of fused heterogeneous and multimodal data, discretized at a\nstreet-level of detail. However, visualization-assisted tools seldom explore\nthe extent to which fused data can offer deeper insights than examining each\ndata source independently within an integrated visualization framework. In this\nwork, we developed a visualization-assisted framework to analyze whether fused\nlatent data representations are more effective than separate representations in\nuncovering patterns from dynamic and static urban data. The analysis reveals\nthat combined latent representations produce more structured patterns, while\nseparate ones are useful in particular cases.", "AI": {"tldr": "The paper presents a visualization-assisted framework to analyze the effectiveness of fused latent data over separate representations in enhancing urban pattern identification.", "motivation": "The paper seeks to address the challenges posed by granularity, heterogeneity, and multimodality in urban data by investigating whether fused latent representations provide deeper insights into urban dynamics compared to analyzing separate data sources.", "method": "The authors developed a framework using visualization to compare and analyze fused latent data representations against separate representations, focusing on dynamic and static urban data.", "result": "The findings demonstrate that fused latent representations help produce more structured patterns, while separate data representations are beneficial in some specific cases.", "conclusion": "The study concludes that fused data representations generally achieve superior results in discovering structured urban patterns, though single-source data analysis remains valuable for certain scenarios."}}
{"id": "2509.06953", "pdf": "https://arxiv.org/pdf/2509.06953", "abs": "https://arxiv.org/abs/2509.06953", "authors": ["Jiahui Yang", "Jason Jingzhou Liu", "Yulong Li", "Youssef Khaky", "Kenneth Shaw", "Deepak Pathak"], "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Website at \\url{deep-reactive-policy.com}", "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com", "AI": {"tldr": "The paper presents Deep Reactive Policy (DRP), a neural motion policy for robotic manipulators that improves motion generation in dynamic and partially observable environments, outperforming previous methods in simulations and real-world tests.", "motivation": "The study aims to address the limitations of classical motion planners and neural motion policies in generating collision-free motion in dynamic, partially observable environments. Classical methods require full knowledge of the environment and are computationally slow, while neural methods struggle to generalize in complex settings.", "method": "The researchers propose DRP, which leverages a transformer-based neural motion policy called IMPACT, pretrained on 10 million expert trajectories. Static obstacle avoidance is refined using iterative student-teacher fine-tuning, and dynamic obstacle avoidance at inference is enhanced with DCP-RMP, a locally reactive goal-proposal module. The system operates directly on point cloud sensory inputs.", "result": "DRP was extensively evaluated in tasks with cluttered scenes, dynamic moving obstacles, and goal obstructions. It demonstrated exceptional generalization capability, achieving higher success rates compared to prior classical and neural motion-planning methods in both simulated and real-world scenarios.", "conclusion": "Deep Reactive Policy (DRP) proves to be a significant advancement for robotic manipulators, offering a robust solution for motion planning in dynamic, partially observable environments. Its success suggests strong potential for deployment in real-world applications."}}
{"id": "2509.06596", "pdf": "https://arxiv.org/pdf/2509.06596", "abs": "https://arxiv.org/abs/2509.06596", "authors": ["Xin Tong", "Zhi Lin", "Jingya Wang", "Bo Jin"], "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.", "AI": {"tldr": "This paper introduces HAVE, a decoding framework that reduces hallucinations in LLMs by reweighing attention heads and calibrating value contributions without requiring fine-tuning.", "motivation": "The motivation is to address hallucinations in LLMs during retrieval-augmented or long-context generation, caused by input-agnostic head importance and poorly reflective attention weights.", "method": "The paper proposes HAVE, which includes head-adaptive gating for instance-level head reweighing and value calibration to consider the magnitude of value vectors. HAVE is parameter-free, operates in a single forward pass, and integrates easily with LLMs.", "result": "HAVE consistently reduces hallucinations in experiments across multiple QA benchmarks and LLMs, outperforming strong baselines like DAGCD with modest computational overhead.", "conclusion": "HAVE enhances the trustworthiness of LLM-generated content by reducing hallucinations efficiently and transparently, and it is broadly applicable to off-the-shelf models in real-world applications."}}
{"id": "2509.06942", "pdf": "https://arxiv.org/pdf/2509.06942", "abs": "https://arxiv.org/abs/2509.06942", "authors": ["Xiangwei Shen", "Zhimin Li", "Zhantao Yang", "Shiyi Zhang", "Yingfang Zhang", "Donghao Li", "Chunyu Wang", "Qinglin Lu", "Yansong Tang"], "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference", "categories": ["cs.AI", "cs.LG"], "comment": "15 pages", "summary": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.", "AI": {"tldr": "The paper introduces Direct-Align and Semantic Relative Preference Optimization (SRPO) to improve diffusion models, addressing computational problems and aesthetic adaptability for better realism.", "motivation": "Diffusion models face challenges in computational efficiency and achieving desired aesthetic qualities, requiring better optimization methods.", "method": "Direct-Align uses predefined noise priors for interpolation to simplify denoising, and SRPO formulates rewards as text-conditioned signals for online adaptability.", "result": "The fine-tuned FLUX.1.dev model achieves over 3x improvement in human-evaluated realism and aesthetic quality.", "conclusion": "The proposed methods optimize diffusion models for efficient denoising and adaptable aesthetics, greatly enhancing their utility and effectiveness in alignment with human preferences."}}
{"id": "2509.05747", "pdf": "https://arxiv.org/pdf/2509.05747", "abs": "https://arxiv.org/abs/2509.05747", "authors": ["Leo Ho", "Yinghao Huang", "Dafei Qin", "Mingyi Shi", "Wangpok Tse", "Wei Liu", "Junichi Yamagishi", "Taku Komura"], "title": "InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO", "I.5.4"], "comment": "The first two authors contributed equally to this work", "summary": "We address the problem of accurate capture of interactive behaviors between\ntwo people in daily scenarios. Most previous works either only consider one\nperson or solely focus on conversational gestures of two people, assuming the\nbody orientation and/or position of each actor are constant or barely change\nover each interaction. In contrast, we propose to simultaneously model two\npeople's activities, and target objective-driven, dynamic, and semantically\nconsistent interactions which often span longer duration and cover bigger\nspace. To this end, we capture a new multi-modal dataset dubbed InterAct, which\nis composed of 241 motion sequences where two people perform a realistic and\ncoherent scenario for one minute or longer over a complete interaction. For\neach sequence, two actors are assigned different roles and emotion labels, and\ncollaborate to finish one task or conduct a common interaction activity. The\naudios, body motions, and facial expressions of both persons are captured.\nInterAct contains diverse and complex motions of individuals and interesting\nand relatively long-term interaction patterns barely seen before. We also\ndemonstrate a simple yet effective diffusion-based method that estimates\ninteractive face expressions and body motions of two people from speech inputs.\nOur method regresses the body motions in a hierarchical manner, and we also\npropose a novel fine-tuning mechanism to improve the lip accuracy of facial\nexpressions. To facilitate further research, the data and code is made\navailable at https://hku-cg.github.io/interact/ .", "AI": {"tldr": "The paper introduces InterAct, a dataset capturing realistic, dynamic two-person interactions, and presents a diffusion-based method for estimating body motions and facial expressions from speech inputs.", "motivation": "To address the lack of datasets and models capturing realistic, long-duration, and semantically rich interactions between two people in complex scenarios.", "method": "The paper introduces the InterAct dataset, containing multimodal data (audio, motion, facial expressions) for two individuals. Additionally, it proposes a diffusion-based method for estimating interactions, with hierarchical body motion regression and fine-tuning for lip synchronization.", "result": "The InterAct dataset showcases diverse and complex interaction patterns, and the proposed method effectively estimates two-person interactions from speech inputs.", "conclusion": "The dataset and method offer new opportunities for studying and modeling realistic two-person interactions, filling a gap in prior research."}}
{"id": "2509.06169", "pdf": "https://arxiv.org/pdf/2509.06169", "abs": "https://arxiv.org/abs/2509.06169", "authors": ["Chuang Niu", "Ge Wang"], "title": "Reasoning Language Model for Personalized Lung Cancer Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate risk assessment in lung cancer screening is critical for enabling\nearly cancer detection and minimizing unnecessary invasive procedures. The Lung\nCT Screening Reporting and Data System (Lung-RADS) has been widely used as the\nstandard framework for patient management and follow-up. Nevertheless,\nLung-RADS faces trade-offs between sensitivity and specificity, as it\nstratifies risk solely based on lung nodule characteristics without\nincorporating various risk factors. Here we propose a reasoning language model\n(RLM) to integrate radiology findings with longitudinal medical records for\nindividualized lung cancer risk assessment. Through a systematic study\nincluding dataset construction and distillation, supervised fine-tuning,\nreinforcement learning, and comprehensive evaluation, our model makes\nsignificant improvements in risk prediction performance on datasets in the\nnational lung screening trial. Notably, RLM can decompose the risk evaluation\ntask into sub-components, analyze the contributions of diverse risk factors,\nand synthesize them into a final risk score computed using our data-driven\nsystem equation. Our approach improves both predictive accuracy and\nmonitorability through the chain of thought reasoning process, thereby\nfacilitating clinical translation into lung cancer screening.", "AI": {"tldr": "The paper introduces a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for better lung cancer risk assessment, outperforming existing methods in predictive accuracy.", "motivation": "Current lung cancer risk assessment systems like Lung-RADS have limitations due to their reliance solely on lung nodule characteristics, neglecting other risk factors.", "method": "The proposed RLM utilizes data from radiology and longitudinal medical records, undergoing steps such as dataset construction, supervised fine-tuning, reinforcement learning, and evaluation. It employs a chain-of-thought reasoning process for better interpretability.", "result": "The RLM method showed significant improvements in risk prediction accuracy and the ability to analyze diverse contributing factors, outperforming existing systems on national lung screening trial datasets.", "conclusion": "The study demonstrates that the RLM approach enhances both predictive accuracy and interpretability, improving the clinical applicability of lung cancer screening."}}
{"id": "2509.06631", "pdf": "https://arxiv.org/pdf/2509.06631", "abs": "https://arxiv.org/abs/2509.06631", "authors": ["\u00d6zg\u00fcr U\u011fur", "Musa Y\u0131lmaz", "Esra \u015eavirdi", "\u00d6zay Ezerceli", "Mahmut El Huseyni", "Selva Ta\u015f", "Reyhan Bayraktar"], "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.", "AI": {"tldr": "The paper evaluates methods for guiding the output of Retrieval-Augmented Generation (RAG) systems, exploring structured output and minimizing hallucinations.", "motivation": "Ensuring structured and reliable outputs from Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) systems while minimizing hallucinations.", "method": "Three guided decoding methods (Outlines, XGrammar, and LM Format Enforcer) were compared using different multi-turn prompting setups: 0-turn, 1-turn, and 2-turn.", "result": "The study identifies how multi-turn interactions affect guided decoding, uncovering notable performance variations.", "conclusion": "The paper advances understanding of guided decoding in RAG systems, offering insights for choosing methods based on specific needs for structured LLM deployment."}}
{"id": "2505.00275", "pdf": "https://arxiv.org/pdf/2505.00275", "abs": "https://arxiv.org/abs/2505.00275", "authors": ["Md Asaduzzaman Jabin", "Hanqi Jiang", "Yiwei Li", "Patrick Kaggwa", "Eugene Douglass", "Juliet N. Sekandi", "Tianming Liu"], "title": "AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,\nepilepsy, and tuberculosis, necessitate rigorous adherence to medication to\navert disease progression, manage symptoms, and decrease mortality rates.\nAdherence is frequently undermined by factors including patient behavior,\ncaregiver support, elevated medical costs, and insufficient healthcare\ninfrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based\nmultimodal large vision language model (LVLM) aimed at visual question\nanswering (VQA) concerning medication adherence through patient videos. We\nemploy a private dataset comprising 806 custom-annotated tuberculosis (TB)\nmedication monitoring videos, which have been labeled by clinical experts, to\nfine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a\ndetailed medical adherence VQA dataset that encompasses positive, negative, and\nambiguous adherence cases. Our method identifies correlations between visual\nfeatures, such as the clear visibility of the patient's face, medication, water\nintake, and the act of ingestion, and their associated medical concepts in\ncaptions. This facilitates the integration of aligned visual-linguistic\nrepresentations and improves multimodal interactions. Experimental results\nindicate that our method surpasses parameter-efficient fine-tuning (PEFT)\nenabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute\nimprovements ranging from 3.1% to 3.54% across pre-trained, regular, and\nlow-rank adaptation (LoRA) configurations. Comprehensive ablation studies and\nattention map visualizations substantiate our approach, enhancing\ninterpretability.", "AI": {"tldr": "The paper introduces AdCare-VLM, a Video-LLaVA-based multimodal model for analyzing medication adherence through videos, achieving better performance than existing models.", "motivation": "Improving medication adherence in chronic diseases due to significant health risks when adherence is low, especially given existing challenges like patient behavior and insufficient healthcare infrastructure.", "method": "Developing AdCare-VLM, a multimodal vision language model trained on a private medical video dataset to detect and analyze medication adherence visually and linguistically, supported by fine-tuning and detailed VQA dataset creation.", "result": "AdCare-VLM outperforms other leading models like LLaVA-V1.5 and Chat-UniVi by 3.1-3.54% through improved correlations between visual and linguistic representations in adherence detection.", "conclusion": "The model advances the state-of-the-art in medication adherence analysis by employing an innovative multimodal approach, validated through experiments and interpretability studies."}}
{"id": "2509.05751", "pdf": "https://arxiv.org/pdf/2509.05751", "abs": "https://arxiv.org/abs/2509.05751", "authors": ["Bingrui Zhao", "Lin Yuanbo Wu", "Xiangtian Fan", "Deyin Liu", "Lu Zhang", "Ruyi He", "Jialie Shen", "Ximing Li"], "title": "Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment an object of\ninterest throughout a video based on a language description. The prominent\nchallenge lies in aligning static text with dynamic visual content,\nparticularly when objects exhibiting similar appearances with inconsistent\nmotion and poses. However, current methods often rely on a holistic\nvisual-language fusion that struggles with complex, compositional descriptions.\nIn this paper, we propose \\textbf{PARSE-VOS}, a novel, training-free framework\npowered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine\nreasoning across text and video domains. Our approach begins by parsing the\nnatural language query into structured semantic commands. Next, we introduce a\nspatio-temporal grounding module that generates all candidate trajectories for\nall potential target objects, guided by the parsed semantics. Finally, a\nhierarchical identification module select the correct target through a\ntwo-stage reasoning process: it first performs coarse-grained motion reasoning\nwith an LLM to narrow down candidates; if ambiguity remains, a fine-grained\npose verification stage is conditionally triggered to disambiguate. The final\noutput is an accurate segmentation mask for the target object.\n\\textbf{PARSE-VOS} achieved state-of-the-art performance on three major\nbenchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.", "AI": {"tldr": "PARSE-VOS is a new framework for referring video object segmentation that combines hierarchical reasoning with large language models to achieve state-of-the-art results without training.", "motivation": "The key challenge in RVOS is aligning static natural language descriptions with dynamic and visually complex video content, especially for objects with similar appearances but varying motion and poses.", "method": "PARSE-VOS employs a hierarchical, coarse-to-fine reasoning approach. It starts by parsing the language query into structured semantic commands, grounds all potential object trajectories, and hierarchically identifies the correct target using motion reasoning and conditional pose verification, powered by LLMs.", "result": "PARSE-VOS outperformed existing methods and set new benchmarks on Ref-YouTube-VOS, Ref-DAVIS17, and MeViS datasets.", "conclusion": "By leveraging structured reasoning via LLMs and dynamically handling coarse-to-fine disambiguation, PARSE-VOS provides a robust, training-free solution for complex RVOS tasks."}}
{"id": "2509.05380", "pdf": "https://arxiv.org/pdf/2509.05380", "abs": "https://arxiv.org/abs/2509.05380", "authors": ["Yoana Pita Lorenzo"], "title": "Cumplimiento del Reglamento (UE) 2024/1689 en rob\u00f3tica y sistemas aut\u00f3nomos: una revisi\u00f3n sistem\u00e1tica de la literatura", "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.RO"], "comment": "in Spanish language", "summary": "This systematic literature review analyzes the current state of compliance\nwith Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on\ncybersecurity frameworks and methodologies. Using the PRISMA protocol, 22\nstudies were selected from 243 initial records across IEEE Xplore, ACM DL,\nScopus, and Web of Science. Findings reveal partial regulatory alignment: while\nprogress has been made in risk management and encrypted communications,\nsignificant gaps persist in explainability modules, real-time human oversight,\nand knowledge base traceability. Only 40% of reviewed solutions explicitly\naddress transparency requirements, and 30% implement failure intervention\nmechanisms. The study concludes that modular approaches integrating risk,\nsupervision, and continuous auditing are essential to meet the AI Act mandates\nin autonomous robotics.", "AI": {"tldr": "The review finds that many autonomous robotic systems partially comply with Regulation (EU) 2024/1689, notably lacking in transparency and failure intervention mechanisms.", "motivation": "To evaluate the compliance of autonomous robotic systems with Regulation (EU) 2024/1689, particularly in cybersecurity frameworks and methodologies.", "method": "A systematic review was conducted following the PRISMA protocol, selecting 22 studies from 243 records sourced from IEEE Xplore, ACM DL, Scopus, and Web of Science.", "result": "Findings indicate progress in areas like risk management and encrypted communications but emphasize significant gaps in explainability, real-time human oversight, and traceability. Only 40% of the solutions address transparency, and just 30% implement failure intervention mechanisms.", "conclusion": "Modular frameworks integrating risk management, supervision, and continuous auditing are needed to ensure compliance with the AI Act's mandates for autonomous robotics."}}
{"id": "2509.06637", "pdf": "https://arxiv.org/pdf/2509.06637", "abs": "https://arxiv.org/abs/2509.06637", "authors": ["Yi Xing"], "title": "Modelling Intertextuality with N-gram Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships.", "AI": {"tldr": "The paper develops a quantitative model to measure intertextuality using embeddings of n-grams, validated through tests and network analysis.", "motivation": "The study aims to provide a scalable and quantifiable method of analyzing intertextuality, addressing the lack of computational tools for such investigations.", "method": "The model calculates intertextuality through pairwise comparisons of embeddings of n-grams from texts, averaging results to determine overall intertextuality. Validation and scalability tests are conducted.", "result": "The quantitative model effectively identifies intertextuality in texts, confirmed through tests on known intertextual works and a large corpus of 267 texts. Network analysis uncovers community structures and centrality.", "conclusion": "The approach proves successful for scalable and efficient quantification of intertextuality, enabling new network-based insights into literary connections."}}
{"id": "2508.11849", "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Yinuo Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "comment": "13 pages", "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget.", "AI": {"tldr": "This paper introduces LocoMamba, a vision-driven framework employing Mamba for efficient deep reinforcement learning with long-range dependencies and reduced training latency.", "motivation": "To address the challenges of efficient state representation, long-range dependency modeling, and overfitting for training neural networks in complex environments.", "method": "The method uses selective state-space models with Mamba layers for token fusion, employing Proximal Policy Optimization with tailored state-centric rewards in challenging simulations.", "result": "LocoMamba outperforms state-of-the-art baselines in success rates, generalization, collision reduction, and training efficiency across challenging terrains.", "conclusion": "LocoMamba demonstrates the effectiveness of combining efficient state-space models and vision-driven state representations for robust and generalizable reinforcement learning outcomes."}}
{"id": "2509.05773", "pdf": "https://arxiv.org/pdf/2509.05773", "abs": "https://arxiv.org/abs/2509.05773", "authors": ["Zijian Chen", "Wenjie Hua", "Jinhao Li", "Lirong Deng", "Fan Du", "Tingzhu Chen", "Guangtao Zhai"], "title": "PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters", "categories": ["cs.CV"], "comment": "6 pages, 6 figures", "summary": "Deciphering oracle bone characters (OBCs), the oldest attested form of\nwritten Chinese, has remained the ultimate, unwavering goal of scholars,\noffering an irreplaceable key to understanding humanity's early modes of\nproduction. Current decipherment methodologies of OBC are primarily constrained\nby the sporadic nature of archaeological excavations and the limited corpus of\ninscriptions. With the powerful visual perception capability of large\nmultimodal models (LMMs), the potential of using LMMs for visually deciphering\nOBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed\nto evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It\nincludes 20k meticulously collected OBC and real object images, forming over\n15k multi-choice questions. We also conduct subjective annotations to\ninvestigate the consistency of the reference point between humans and LMMs in\nvisual reasoning. Experiments indicate that general LMMs possess preliminary\nvisual decipherment skills, and LMMs are not effectively using visual\ninformation, while most of the time they are limited by language priors. We\nhope that our dataset can facilitate the evaluation and optimization of visual\nattention in future OBC-oriented LMMs. The code and dataset will be available\nat https://github.com/OBI-Future/PictOBI-20k.", "AI": {"tldr": "This paper introduces PictOBI-20k, a dataset for testing large multimodal models (LMMs) in visually deciphering oracle bone characters (OBCs). Initial findings show that while LMMs demonstrate basic deciphering abilities, their performance is influenced more by language priors than visual analysis.", "motivation": "Deciphering Oracle Bone Characters (OBCs) is essential for understanding early Chinese historical contexts, but current methods are limited by fragmented archaeological data and small inscription datasets. Leveraging LMMs may overcome these constraints.", "method": "The authors developed a dataset named PictOBI-20k comprising 20k OBC and real-world object images paired with 15k multi-choice questions. They used this dataset to evaluate LMMs' deciphering capabilities and conducted human annotations for consistency checks.", "result": "Initial experiments reveal that LMMs exhibit basic visual deciphering capabilities but rely heavily on language models and fail to effectively utilize visual information.", "conclusion": "PictOBI-20k provides a valuable tool for evaluating and refining visual attention in LMMs for OBC decipherment. The findings highlight areas for optimization in LMMs' visual reasoning for historical scripts."}}
{"id": "2509.06214", "pdf": "https://arxiv.org/pdf/2509.06214", "abs": "https://arxiv.org/abs/2509.06214", "authors": ["Haochen You", "Baojing Liu"], "title": "Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering", "categories": ["cs.LG"], "comment": "Accepted as a conference paper at KSEM 2025", "summary": "Graph clustering under the framework of differential privacy, which aims to\nprocess graph-structured data while protecting individual privacy, has been\nreceiving increasing attention. Despite significant achievements in current\nresearch, challenges such as high noise, low efficiency and poor\ninterpretability continue to severely constrain the development of this field.\nIn this paper, we construct a differentially private and interpretable graph\nclustering approach based on metric embedding initialization. Specifically, we\nconstruct an SDP optimization, extract the key set and provide a\nwell-initialized clustering configuration using an HST-based initialization\nmethod. Subsequently, we apply an established k-median clustering strategy to\nderive the cluster results and offer comparative explanations for the query set\nthrough differences from the cluster centers. Extensive experiments on public\ndatasets demonstrate that our proposed framework outperforms existing methods\nin various clustering metrics while strictly ensuring privacy.", "AI": {"tldr": "This paper presents a differentially private graph clustering framework that enhances interpretability and efficiency by leveraging metric embedding initialization.", "motivation": "The research aims to address challenges in differentially private graph clustering, including high noise, low efficiency, and poor interpretability.", "method": "The approach consists of SDP optimization, an HST-based clustering initialization, and a k-median clustering strategy to ensure privacy and interpretability.", "result": "The proposed method demonstrated superior performance on public datasets across various clustering metrics while maintaining strict differential privacy.", "conclusion": "The study provides a more effective and interpretable framework for differentially private graph clustering, overcoming major challenges in the field."}}
{"id": "2509.06650", "pdf": "https://arxiv.org/pdf/2509.06650", "abs": "https://arxiv.org/abs/2509.06650", "authors": ["Hao Lin", "Peitong Xie", "Jingxue Chen", "Jie Lin", "Qingkun Tang", "Qianchun Lu"], "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.", "AI": {"tldr": "The paper introduces MoLER, a method combining pre-training and reinforcement learning to enhance retrieval and generation in RAG systems, achieving state-of-the-art results.", "motivation": "To overcome challenges in balancing domain-specific knowledge and query optimization in RAG retrieval systems.", "method": "MoLER employs continual pre-training with a Mixture of Losses and Group Relative Policy Optimization during reinforcement learning. It introduces strategies like Multi-query Single-passage Late Fusion for training efficiency.", "result": "MoLER significantly outperforms baseline methods in benchmark evaluations, achieving state-of-the-art retrieval performance.", "conclusion": "MoLER effectively addresses retrieval challenges in RAG systems, making them more robust and scalable for specialized domains."}}
{"id": "2509.05776", "pdf": "https://arxiv.org/pdf/2509.05776", "abs": "https://arxiv.org/abs/2509.05776", "authors": ["Jonathan Aellen", "Florian Burkhardt", "Thomas Vetter", "Marcel L\u00fcthi"], "title": "Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models", "categories": ["cs.CV"], "comment": null, "summary": "In medical imaging, point distribution models are often used to reconstruct\nand complete partial shapes using a statistical model of the full shape. A\ncommonly overlooked, but crucial factor in this reconstruction process, is the\npose of the training data relative to the partial target shape. A difference in\npose alignment of the training and target shape leads to biased solutions,\nparticularly when observing small parts of a shape. In this paper, we\ndemonstrate the importance of pose alignment for partial shape reconstructions\nand propose an efficient method to adjust an existing model to a specific\ntarget. Our method preserves the computational efficiency of linear models\nwhile significantly improving reconstruction accuracy and predicted variance.\nIt exactly recovers the intended aligned model for translations, and provides a\ngood approximation for small rotations, all without access to the original\ntraining data. Hence, existing shape models in reconstruction pipelines can be\nadapted by a simple preprocessing step, making our approach widely applicable\nin plug-and-play scenarios.", "AI": {"tldr": "This paper addresses the crucial impact of pose alignment in partial shape reconstructions in medical imaging and proposes an efficient method for pose adjustment to improve accuracy.", "motivation": "The motivation is to tackle the overlooked problem of pose misalignment in training and target data for statistical shape reconstruction, which leads to biased solutions, especially when using small parts of shapes.", "method": "The authors propose an efficient preprocessing method to adjust pose alignment of existing shape models without needing the original training data, preserving computational efficiency while improving reconstruction accuracy.", "result": "The proposed method effectively recovers aligned models for translations and approximates small rotations. It enhances accuracy and predicted variance compared to conventional models.", "conclusion": "The method provides a computationally efficient, broadly applicable improvement to shape reconstruction pipelines, enabling plug-and-play integration and better results without requiring original training datasets."}}
{"id": "2509.06219", "pdf": "https://arxiv.org/pdf/2509.06219", "abs": "https://arxiv.org/abs/2509.06219", "authors": ["Haochen You", "Baojing Liu"], "title": "MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning", "categories": ["cs.LG", "cs.MM"], "comment": "Accepted as a conference paper at KSEM 2025", "summary": "Exemplar-free class-incremental learning enables models to learn new classes\nover time without storing data from old ones. As multimodal graph-structured\ndata becomes increasingly prevalent, existing methods struggle with challenges\nlike catastrophic forgetting, distribution bias, memory limits, and weak\ngeneralization. We propose MCIGLE, a novel framework that addresses these\nissues by extracting and aligning multimodal graph features and applying\nConcatenated Recursive Least Squares for effective knowledge retention. Through\nmulti-channel processing, MCIGLE balances accuracy and memory preservation.\nExperiments on public datasets validate its effectiveness and generalizability.", "AI": {"tldr": "The paper introduces MCIGLE, a framework for class-incremental learning on multimodal graph data, tackling memory and generalization issues.", "motivation": "To address the challenges in class-incremental learning for multimodal graph data, including catastrophic forgetting and weak generalization.", "method": "The framework uses feature extraction, multimodal alignment, and Concatenated Recursive Least Squares for knowledge retention and memory balance.", "result": "Experiments on public datasets demonstrate MCIGLE's effectiveness and broad applicability.", "conclusion": "MCIGLE effectively balances accuracy and memory preservation in class-incremental learning, showcasing strong generalization and scalability in multimodal graph data scenarios."}}
{"id": "2509.06652", "pdf": "https://arxiv.org/pdf/2509.06652", "abs": "https://arxiv.org/abs/2509.06652", "authors": ["Xingwei Tan", "Mahathi Parvatham", "Chiara Gambi", "Gabriele Pergola"], "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings camera-ready, 9+7 pages", "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.", "AI": {"tldr": "The paper introduces \u2018IntrEx,\u2019 a novel dataset supporting the study of engagement in teacher-student conversations and evaluates large language models to predict and enhance interestingness judgments in dialogues.", "motivation": "The challenge lies in keeping learners engaged during second-language acquisition conversations, with limited understanding of the linguistic elements that foster conversational interest.", "method": "The authors created the IntrEx dataset, adding sequence-level interestingness annotations to dialogue data, and gathered ratings via comparison-based methods inspired by reinforcement learning from human feedback (RLHF). They test large language models, fine-tuned using these annotations, to predict human judgments about engagement.", "result": "Specialized large language models fine-tuned on interestingness ratings outperformed larger, proprietary models (e.g., GPT-4o), highlighting the effectiveness of targeted datasets. Analysis revealed factors like concreteness and readability influence engagement.", "conclusion": "Specialized datasets like IntrEx hold potential for modeling and improving engagement in educational conversations by helping large language models better understand and predict human interest."}}
{"id": "2509.05298", "pdf": "https://arxiv.org/pdf/2509.05298", "abs": "https://arxiv.org/abs/2509.05298", "authors": ["Rui Xi", "Xianghan Wang"], "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression", "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 2025). \\c{opyright} 2025\n  Springer. This is the author-accepted manuscript. Rui Xi and Xianghan Wang\n  contributed equally to this work. The final version will be available via\n  SpringerLink", "summary": "Loneliness and social isolation pose significant emotional and health\nchallenges, prompting the development of technology-based solutions for\ncompanionship and emotional support. This paper introduces Livia, an\nemotion-aware augmented reality (AR) companion app designed to provide\npersonalized emotional support by combining modular artificial intelligence\n(AI) agents, multimodal affective computing, progressive memory compression,\nand AR driven embodied interaction. Livia employs a modular AI architecture\nwith specialized agents responsible for emotion analysis, dialogue generation,\nmemory management, and behavioral orchestration, ensuring robust and adaptive\ninteractions. Two novel algorithms-Temporal Binary Compression (TBC) and\nDynamic Importance Memory Filter (DIMF)-effectively manage and prioritize\nlong-term memory, significantly reducing storage requirements while retaining\ncritical context. Our multimodal emotion detection approach achieves high\naccuracy, enhancing proactive and empathetic engagement. User evaluations\ndemonstrated increased emotional bonds, improved satisfaction, and\nstatistically significant reductions in loneliness. Users particularly valued\nLivia's adaptive personality evolution and realistic AR embodiment. Future\nresearch directions include expanding gesture and tactile interactions,\nsupporting multi-user experiences, and exploring customized hardware\nimplementations.", "AI": {"tldr": "The paper presents Livia, an emotion-aware augmented reality companion app designed to alleviate loneliness through advanced AI modules, emotion detection algorithms, and AR interaction.", "motivation": "Addressing the emotional and health challenges posed by loneliness and social isolation by leveraging technology-based solutions.", "method": "Development of Livia using modular AI agents for emotion analysis, dialogue generation, memory optimization, and AR-based embodied interaction. Included introduction of TBC and DIMF algorithms for effective long-term memory management.", "result": "Livia demonstrated enhanced user emotional bonds, satisfaction, and statistically significant reductions in loneliness through evaluations. Users appreciated its adaptive personality and realistic AR interactions.", "conclusion": "Livia effectively combines multimodal emotion detection and memory management with AR to combat loneliness. Future improvements could include gesture and tactile interactions, multi-user capabilities, and hardware optimization."}}
{"id": "2509.05780", "pdf": "https://arxiv.org/pdf/2509.05780", "abs": "https://arxiv.org/abs/2509.05780", "authors": ["Jongyoun Noh", "Junghyup Lee", "Hyekang Park", "Bumsub Ham"], "title": "3DPillars: Pillar-based two-stage 3D object detection", "categories": ["cs.CV"], "comment": "19 pages, 11 figures", "summary": "PointPillars is the fastest 3D object detector that exploits pseudo image\nrepresentations to encode features for 3D objects in a scene. Albeit efficient,\nPointPillars is typically outperformed by state-of-the-art 3D detection methods\ndue to the following limitations: 1) The pseudo image representations fail to\npreserve precise 3D structures, and 2) they make it difficult to adopt a\ntwo-stage detection pipeline using 3D object proposals that typically shows\nbetter performance than a single-stage approach. We introduce in this paper the\nfirst two-stage 3D detection framework exploiting pseudo image representations,\nnarrowing the performance gaps between PointPillars and state-of-the-art\nmethods, while retaining its efficiency. Our framework consists of two novel\ncomponents that overcome the aforementioned limitations of PointPillars: First,\nwe introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D\nvoxel-based features from the pseudo image representation efficiently using 2D\nconvolutions. The basic idea behind 3DPillars is that 3D features from voxels\ncan be viewed as a stack of pseudo images. To implement this idea, we propose a\nseparable voxel feature module that extracts voxel-based features without using\n3D convolutions. Second, we introduce an RoI head with a sparse scene context\nfeature module that aggregates multi-scale features from 3DPillars to obtain a\nsparse scene feature. This enables adopting a two-stage pipeline effectively,\nand fully leveraging contextual information of a scene to refine 3D object\nproposals. Experimental results on the KITTI and Waymo Open datasets\ndemonstrate the effectiveness and efficiency of our approach, achieving a good\ncompromise in terms of speed and accuracy.", "AI": {"tldr": "PointPillars is enhanced with a two-stage 3D object detection framework called 3DPillars, addressing limitations in preserving 3D structures and adopting two-stage detection.", "motivation": "PointPillars is fast but struggles with capturing precise 3D features and implementing two-stage detection, thus falling behind state-of-the-art methods.", "method": "The study introduces 3DPillars leveraging pseudo image stacks efficiently with 2D convolutions. It features a separable voxel module and an RoI head with sparse scene context.", "result": "Experiments on KITTI and Waymo datasets showcase that 3DPillars balances speed and accuracy while refining 3D proposals effectively.", "conclusion": "3DPillars significantly enhances 3D detection efficiency and accuracy, narrowing performance gaps compared to state-of-the-art methods."}}
{"id": "2509.06270", "pdf": "https://arxiv.org/pdf/2509.06270", "abs": "https://arxiv.org/abs/2509.06270", "authors": ["Honggang Jia", "Xiucheng Wang", "Nan Cheng", "Ruijin Sun", "Changle Li"], "title": "UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IEEE Global Communications Conference (GLOBECOM) 2025", "summary": "Sixth generation (6G) systems require environment-aware communication, driven\nby native artificial intelligence (AI) and integrated sensing and communication\n(ISAC). Radio maps (RMs), providing spatially continuous channel information,\nare key enablers. However, generating high-fidelity RM ground truth via\nelectromagnetic (EM) simulations is computationally intensive, motivating\nmachine learning (ML)-based RM construction. The effectiveness of these\ndata-driven methods depends on large-scale, high-quality training data. Current\npublic datasets often focus on single-input single-output (SISO) and limited\ninformation, such as path loss, which is insufficient for advanced multi-input\nmulti-output (MIMO) systems requiring detailed channel state information (CSI).\nTo address this gap, this paper presents UrbanMIMOMap, a novel large-scale\nurban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap\noffers comprehensive complex CSI matrices across a dense spatial grid, going\nbeyond traditional path loss data. This rich CSI is vital for constructing\nhigh-fidelity RMs and serves as a fundamental resource for data-driven RM\ngeneration, including deep learning. We demonstrate the dataset's utility\nthrough baseline performance evaluations of representative ML methods for RM\nconstruction. This work provides a crucial dataset and reference for research\nin high-precision RM generation, MIMO spatial performance, and ML for 6G\nenvironment awareness. The code and data for this work are available at:\nhttps://github.com/UNIC-Lab/UrbanMIMOMap.", "AI": {"tldr": "The paper introduces UrbanMIMOMap, a large-scale MIMO CSI dataset for aiding 6G environment-aware communication through machine learning techniques.", "motivation": "6G systems demand high-quality radio maps (RMs) for environment-aware communication, but generating such maps is computationally expensive and existing datasets lack the detailed information needed for advanced MIMO systems.", "method": "The authors created UrbanMIMOMap, a dataset built using high-precision ray tracing, which provides detailed complex channel state information (CSI) matrices across dense spatial grids.", "result": "UrbanMIMOMap includes comprehensive data that surpasses traditional path loss-focused datasets. Demonstrations show its utility in enabling machine learning-based RM generation and evaluating MIMO performance in 6G networks.", "conclusion": "UrbanMIMOMap is a valuable resource for advancing high-fidelity RM generation, improving MIMO system performance, and supporting machine learning applications in 6G, filling critical gaps in current datasets."}}
{"id": "2509.06675", "pdf": "https://arxiv.org/pdf/2509.06675", "abs": "https://arxiv.org/abs/2509.06675", "authors": ["Vladislav Stankov", "Maty\u00e1\u0161 Kopp", "Ond\u0159ej Bojar"], "title": "ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0\ncorpus, targeted at speech modeling tasks with the largest variant containing\n2,695 hours. We combined the sound recordings of the Czech parliamentary\nspeeches with the official transcripts. The recordings were processed with\nWhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our\nprocessing pipeline improves upon the ParCzech 3.0 speech recognition version\nby extracting more data with higher alignment reliability. The dataset is\noffered in three flexible variants: (1) sentence-segmented for automatic speech\nrecognition and speech synthesis tasks with clean boundaries, (2) unsegmented\npreserving original utterance flow across sentences, and (3) a raw-alignment\nfor further custom refinement for other possible tasks. All variants maintain\nthe original metadata and are released under a permissive CC-BY license. The\ndataset is available in the LINDAT repository, with the sentence-segmented and\nunsegmented variants additionally available on Hugging Face.", "AI": {"tldr": "The paper introduces ParCzech4Speech 1.0, an extensive Czech speech modeling corpus combining parliamentary speech recordings with official transcripts, processed for enhanced audio-text alignment and usability.", "motivation": "To improve speech modeling tasks by creating a reliable, flexible, and publicly accessible Czech parliamentary speech corpus with advanced audio-text alignment.", "method": "Sound recordings of Czech parliamentary speeches were processed using WhisperX and Wav2Vec 2.0 to achieve automated audio-text alignment, improving upon previous datasets.", "result": "The dataset provides 2,695 hours of Czech speech data in three variants: sentence-segmented, unsegmented, and raw-alignment, all maintaining metadata and released under CC-BY license.", "conclusion": "ParCzech4Speech 1.0 is a robust, flexible, and enhanced speech dataset aimed at aiding automatic speech recognition, synthesis, and other customization-oriented tasks."}}
{"id": "2509.05302", "pdf": "https://arxiv.org/pdf/2509.05302", "abs": "https://arxiv.org/abs/2509.05302", "authors": ["Ra\u00fal Mi\u00f1\u00e1n", "Carles Perez-Lopez", "Javier Iglesias", "\u00c1lvaro Ciudad", "Alexis Molina"], "title": "Sesame: Opening the door to protein pockets", "categories": ["q-bio.BM", "cs.AI"], "comment": "Published at the Proceedings of the 2nd Workshop on Generative and\n  Experimental Perspectives for Biomolecular Design. ICLR 2025", "summary": "Molecular docking is a cornerstone of drug discovery, relying on\nhigh-resolution ligand-bound structures to achieve accurate predictions.\nHowever, obtaining these structures is often costly and time-intensive,\nlimiting their availability. In contrast, ligand-free structures are more\naccessible but suffer from reduced docking performance due to pocket geometries\nbeing less suited for ligand accommodation in apo structures. Traditional\nmethods for artificially inducing these conformations, such as molecular\ndynamics simulations, are computationally expensive. In this work, we introduce\nSesame, a generative model designed to predict this conformational change\nefficiently. By generating geometries better suited for ligand accommodation at\na fraction of the computational cost, Sesame aims to provide a scalable\nsolution for improving virtual screening workflows.", "AI": {"tldr": "The paper introduces 'Sesame,' a generative model to improve ligand docking predictions by modifying ligand-free structures efficiently.", "motivation": "Obtaining high-resolution ligand-bound structures is costly and limits docking performance. Ligand-free structures are accessible but less ideal for accommodating ligands, and existing simulation methods are computationally expensive.", "method": "The authors developed Sesame, a generative model capable of efficiently predicting conformational changes in apo structures to better suit ligand accommodation.", "result": "Sesame generates suitable geometries for ligand docking at significantly reduced computational costs, creating a scalable solution.", "conclusion": "Sesame enhances virtual screening workflows by addressing limitations of ligand-free structures, improving the accessibility and efficiency of drug discovery."}}
{"id": "2509.05785", "pdf": "https://arxiv.org/pdf/2509.05785", "abs": "https://arxiv.org/abs/2509.05785", "authors": ["In-Jae Lee", "Sihwan Hwang", "Youngseok Kim", "Wonjune Kim", "Sanmin Kim", "Dongsuk Kum"], "title": "CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Recently, camera-radar fusion-based 3D object detection methods in bird's eye\nview (BEV) have gained attention due to the complementary characteristics and\ncost-effectiveness of these sensors. Previous approaches using forward\nprojection struggle with sparse BEV feature generation, while those employing\nbackward projection overlook depth ambiguity, leading to false positives. In\nthis paper, to address the aforementioned limitations, we propose a novel\ncamera-radar fusion-based 3D object detection and segmentation model named CRAB\n(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based\nview transformation), using a backward projection that leverages radar to\nmitigate depth ambiguity. During the view transformation, CRAB aggregates\nperspective view image context features into BEV queries. It improves depth\ndistinction among queries along the same ray by combining the dense but\nunreliable depth distribution from images with the sparse yet precise depth\ninformation from radar occupancy. We further introduce spatial cross-attention\nwith a feature map containing radar context information to enhance the\ncomprehension of the 3D scene. When evaluated on the nuScenes open dataset, our\nproposed approach achieves a state-of-the-art performance among backward\nprojection-based camera-radar fusion methods with 62.4\\% NDS and 54.0\\% mAP in\n3D object detection.", "AI": {"tldr": "The paper proposes CRAB, a novel 3D object detection and segmentation model that fuses camera and radar data to address depth ambiguity issues in backward projection for BEV transformations.", "motivation": "Camera-radar fusion for 3D object detection has the potential to leverage complementary sensor traits, but existing methods face challenges with sparse BEV features or depth ambiguity, leading to inaccuracies.", "method": "CRAB employs backward projection with radar data to mitigate depth ambiguity and introduces spatial cross-attention for enhanced 3D scene comprehension during view transformation.", "result": "The method achieved state-of-the-art performance on the nuScenes dataset with 62.4% NDS and 54.0% mAP in 3D object detection.", "conclusion": "CRAB effectively reduces depth ambiguity in backward projection-based camera-radar fusion, improving accuracy and performance in 3D object detection tasks."}}
{"id": "2509.06274", "pdf": "https://arxiv.org/pdf/2509.06274", "abs": "https://arxiv.org/abs/2509.06274", "authors": ["Aosong Feng", "Zhichao Xu", "Xian Wu", "Kang Zhou", "Sheng Guan", "Yueyan Chen", "Ninad Kulkarni", "Yun Zhou", "Balasubramaniam Srinivasan", "Haibo Ding", "Lin Lee Cheong"], "title": "IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs", "categories": ["cs.LG"], "comment": null, "summary": "Routing incoming queries to the most cost-effective LLM while maintaining\nresponse quality poses a fundamental challenge in optimizing performance-cost\ntrade-offs for large-scale commercial systems. We present IPR\\, a\nquality-constrained Intelligent Prompt Routing framework that dynamically\nselects optimal models based on predicted response quality and user-specified\ntolerance levels. IPR introduces three key innovations: (1) a modular\narchitecture with lightweight quality estimators trained on 1.5M prompts\nannotated with calibrated quality scores, enabling fine-grained quality\nprediction across model families; (2) a user-controlled routing mechanism with\ntolerance parameter $\\tau \\in [0,1]$ that provides explicit control over\nquality-cost trade-offs; and (3) an extensible design using frozen encoders\nwith model-specific adapters, reducing new model integration from days to\nhours. To rigorously train and evaluate IPR, we curate an industrial-level\ndataset IPRBench\\footnote{IPRBench will be released upon legal approval.}, a\ncomprehensive benchmark containing 1.5 million examples with response quality\nannotations across 11 LLM candidates. Deployed on a major cloud platform, IPR\nachieves 43.9\\% cost reduction while maintaining quality parity with the\nstrongest model in the Claude family and processes requests with sub-150ms\nlatency.", "AI": {"tldr": "The paper introduces IPR, an intelligent routing framework designed to optimize cost while maintaining response quality in deploying large language models (LLMs).", "motivation": "To address the need for balancing cost-effectiveness and quality in selecting Large Language Models (LLMs) for large-scale systems.", "method": "The proposed IPR framework predicts the quality of LLM responses using modular quality estimators, employs a user-controlled routing mechanism for cost-quality trade-offs, and integrates frozen encoders with adapters for adaptability.", "result": "The system achieves a 43.9% cost reduction while maintaining comparable quality to advanced models in the Claude family, with low latency.", "conclusion": "IPR provides a scalable and efficient solution for dynamically routing inputs while achieving significant cost savings and maintaining high response quality."}}
{"id": "2509.06704", "pdf": "https://arxiv.org/pdf/2509.06704", "abs": "https://arxiv.org/abs/2509.06704", "authors": ["Amir Homayounirad", "Enrico Liscio", "Tong Wang", "Catholijn M. Jonker", "Luciano C. Siebert"], "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments", "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process.", "AI": {"tldr": "The paper examines approaches for identifying subjectivity in recognizing human values driving arguments, finding direct subjectivity identification improves performance.", "motivation": "Understanding and handling annotator disagreement in subjective tasks, such as recognizing human values behind arguments.", "method": "Two main approaches: value prediction to infer subjectivity and direct identification of subjectivity. Experiments tested combining contrastive loss with binary cross-entropy loss.", "result": "Direct subjectivity identification approaches significantly outperform other methods in flagging subjective arguments. Combining loss functions doesn't improve performance but reduces dependency.", "conclusion": "Directly identifying subjectivity advances model reliability and helps highlight arguments subject to diverse individual interpretations, improving annotation processes."}}
{"id": "2509.05796", "pdf": "https://arxiv.org/pdf/2509.05796", "abs": "https://arxiv.org/abs/2509.05796", "authors": ["Julio Zanon Diaz", "Georgios Siogkas", "Peter Corcoran"], "title": "Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 5 figures, 13 tables", "summary": "Automating visual inspection in medical device manufacturing remains\nchallenging due to small and imbalanced datasets, high-resolution imagery, and\nstringent regulatory requirements. This work proposes two attention-guided\nautoencoder architectures for deep anomaly detection designed to address these\nconstraints. The first employs a structural similarity-based anomaly score\n(4-MS-SSIM), offering lightweight and accurate real-time defect detection,\nyielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised\nthresholding) on the - Surface Seal Image - Test split with only 10% of\ndefective samples. The second applies a feature-distance approach using\nMahalanobis scoring on reduced latent features, providing high sensitivity to\ndistributional shifts for supervisory monitoring, achieving ACC 0.722 with\nsupervised thresholding. Together, these methods deliver complementary\ncapabilities: the first supports reliable inline inspection, while the second\nenables scalable post-production surveillance and regulatory compliance\nmonitoring. Experimental results demonstrate that both approaches surpass\nre-implemented baselines and provide a practical pathway for deploying deep\nanomaly detection in regulated manufacturing environments, aligning accuracy,\nefficiency, and the regulatory obligations defined for high-risk AI systems\nunder the EU AI Act.", "AI": {"tldr": "The paper proposes two attention-guided autoencoder architectures for deep anomaly detection in medical device manufacturing, addressing constraints like small datasets and regulatory requirements.", "motivation": "Challenges in automating visual inspection in medical device manufacturing due to the need for high accuracy, regulatory compliance, small datasets, and high-resolution imagery.", "method": "Two novel autoencoder architectures: one with structural similarity-based anomaly scoring (4-MS-SSIM) and another using Mahalanobis scoring based on reduced latent features.", "result": "The first method achieves ACC of 0.931 supervised thresholding and the second achieves ACC of 0.722 supervised thresholding, both outperforming baseline methods.", "conclusion": "The methods deliver complementary strengths, supporting inline inspection and post-production surveillance, aligning with accuracy, efficiency, and regulatory requirements of EU AI Act."}}
{"id": "2509.06286", "pdf": "https://arxiv.org/pdf/2509.06286", "abs": "https://arxiv.org/abs/2509.06286", "authors": ["Chang Xue", "Youwei Lu", "Chen Yang", "Jinming Xing"], "title": "RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations", "categories": ["cs.LG"], "comment": null, "summary": "Personalization is a core capability across consumer technologies, streaming,\nshopping, wearables, and voice, yet it remains challenged by sparse\ninteractions, fast content churn, and heterogeneous textual signals. We present\nRecMind, an LLM-enhanced graph recommender that treats the language model as a\npreference prior rather than a monolithic ranker. A frozen LLM equipped with\nlightweight adapters produces text-conditioned user/item embeddings from\ntitles, attributes, and reviews; a LightGCN backbone learns collaborative\nembeddings from the user-item graph. We align the two views with a symmetric\ncontrastive objective and fuse them via intra-layer gating, allowing language\nto dominate in cold/long-tail regimes and graph structure to stabilize rankings\nelsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on\nall eight reported metrics, with relative improvements up to +4.53\\%\n(Recall@40) and +4.01\\% (NDCG@40) over strong baselines. Ablations confirm both\nthe necessity of cross-view alignment and the advantage of gating over late\nfusion and LLM-only variants.", "AI": {"tldr": "RecMind, an advanced recommender system, integrates language models and graph-based methods for personalized recommendations, achieving state-of-the-art performance on Yelp and Amazon benchmarks.", "motivation": "The paper aims to address challenges in personalization, such as sparse interactions, frequent content turnover, and diverse textual signals, which limit the accuracy of current recommender systems.", "method": "RecMind combines a frozen language model (enhanced with adapters) to derive text-based embeddings and a LightGCN model for collaborative graph embeddings. These components are aligned using a contrastive objective and fused with intra-layer gating to leverage advantages of both approaches.", "result": "RecMind outperformed strong baselines on Yelp and Amazon-Electronics datasets, showing significant improvements in eight metrics, including Recall@40 (+4.53%) and NDCG@40 (+4.01%).", "conclusion": "The cross-view alignment and intra-layer gating techniques are critical for RecMind's success, outperforming both standalone methods and simpler fusion strategies."}}
{"id": "2509.05645", "pdf": "https://arxiv.org/pdf/2509.05645", "abs": "https://arxiv.org/abs/2509.05645", "authors": ["Yan-Shan Lu", "Miguel Arana-Catania", "Saurabh Upadhyay", "Leonard Felicetti"], "title": "Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.RO"], "comment": "8 pages, 6 figures, 2 tables. ESA ASTRA 2025", "summary": "Mars exploration requires precise and reliable terrain models to ensure safe\nrover navigation across its unpredictable and often hazardous landscapes.\nStereoscopic vision serves a critical role in the rover's perception, allowing\nscene reconstruction by generating precise depth maps through stereo matching.\nState-of-the-art Martian planetary exploration uses traditional local\nblock-matching, aggregates cost over square windows, and refines disparities\nvia smoothness constraints. However, this method often struggles with\nlow-texture images, occlusion, and repetitive patterns because it considers\nonly limited neighbouring pixels and lacks a wider understanding of scene\ncontext. This paper uses Semi-Global Matching (SGM) with superpixel-based\nrefinement to mitigate the inherent block artefacts and recover lost details.\nThe approach balances the efficiency and accuracy of SGM and adds context-aware\nsegmentation to support more coherent depth inference. The proposed method has\nbeen evaluated in three datasets with successful results: In a Mars analogue,\nthe terrain maps obtained show improved structural consistency, particularly in\nsloped or occlusion-prone regions. Large gaps behind rocks, which are common in\nraw disparity outputs, are reduced, and surface details like small rocks and\nedges are captured more accurately. Another two datasets, evaluated to test the\nmethod's general robustness and adaptability, show more precise disparity maps\nand more consistent terrain models, better suited for the demands of autonomous\nnavigation on Mars, and competitive accuracy across both non-occluded and\nfull-image error metrics. This paper outlines the entire terrain modelling\nprocess, from finding corresponding features to generating the final 2D\nnavigation maps, offering a complete pipeline suitable for integration in\nfuture planetary exploration missions.", "AI": {"tldr": "The paper proposes a more accurate terrain modeling method for Mars exploration using Semi-Global Matching with superpixel-based refinement, improving depth maps and navigation safety.", "motivation": "The paper aims to address challenges in terrain modeling for Mars exploration, where traditional methods struggle with low-texture images, occlusions, and repetitive patterns.", "method": "The method integrates Semi-Global Matching with superpixel-based refinement, leveraging context-aware segmentation for more coherent and detailed depth inference.", "result": "The proposed method demonstrated improved structural consistency in Martian terrain maps, reduced gaps around rocks, and provided better accuracy across multiple datasets tested.", "conclusion": "The approach presents a robust and efficient pipeline for terrain modeling, making it suitable for future autonomous navigation missions on Mars."}}
{"id": "2509.06795", "pdf": "https://arxiv.org/pdf/2509.06795", "abs": "https://arxiv.org/abs/2509.06795", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Qika Lin", "Kai He", "Ting Liu", "Bing Qin", "Mengling Feng"], "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint", "categories": ["cs.CL"], "comment": null, "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.", "AI": {"tldr": "The paper identifies and addresses issues in Instruction Fine-Tuning (IFT) of Large Language Models (LLMs) by mitigating drift in refusal direction during training using a method called ProCon.", "motivation": "IFT improves LLM performance but raises safety concerns, particularly compromising the models' ability to refuse malicious instructions.", "method": "The proposed ProCon method utilizes a projection-constrained loss term to regulate the magnitude of training samples' hidden state projections onto the refusal direction and introduces a warm-up strategy for better effectiveness.", "result": "ProCon effectively mitigates safety risks by stabilizing refusal direction drift while maintaining task performance, outperforming strong baselines across various datasets and scenarios.", "conclusion": "ProCon not only enhances LLM safety but also provides an interpretability-driven approach to understanding their internal mechanisms, paving the way for future research in LLM safety."}}
{"id": "2509.05306", "pdf": "https://arxiv.org/pdf/2509.05306", "abs": "https://arxiv.org/abs/2509.05306", "authors": ["Enis Karaarslan", "Esin G\u00fcler", "Efe Emir Y\u00fcce", "Cagatay Coban"], "title": "Towards Log Analysis with AI Agents: Cowrie Case Study", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "The scarcity of real-world attack data significantly hinders progress in\ncybersecurity research and education. Although honeypots like Cowrie\neffectively collect live threat intelligence, they generate overwhelming\nvolumes of unstructured and heterogeneous logs, rendering manual analysis\nimpractical. As a first step in our project on secure and efficient AI\nautomation, this study explores the use of AI agents for automated log\nanalysis. We present a lightweight and automated approach to process Cowrie\nhoneypot logs. Our approach leverages AI agents to intelligently parse,\nsummarize, and extract insights from raw data, while also considering the\nsecurity implications of deploying such an autonomous system. Preliminary\nresults demonstrate the pipeline's effectiveness in reducing manual effort and\nidentifying attack patterns, paving the way for more advanced autonomous\ncybersecurity analysis in future work.", "AI": {"tldr": "This study develops an AI-driven approach for automated analysis of Cowrie honeypot logs to tackle issues of scalability and efficiency in cybersecurity.", "motivation": "The research aims to address the challenge of analyzing overwhelming, unstructured, and heterogeneous attack logs collected by honeypots, which limits advancements in cybersecurity.", "method": "It utilizes AI agents to parse, summarize, and extract insights from raw logs collected by Cowrie honeypots, creating a lightweight and automated pipeline for log processing.", "result": "Preliminary findings show that the proposed approach successfully minimizes manual effort while identifying attack patterns effectively.", "conclusion": "The pipeline demonstrates potential as a foundational step towards advanced autonomous cybersecurity solutions, emphasizing the importance of secure AI deployment."}}
{"id": "2509.05809", "pdf": "https://arxiv.org/pdf/2509.05809", "abs": "https://arxiv.org/abs/2509.05809", "authors": ["Tyler Ward", "Abdullah Imran"], "title": "A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Recent advances in promptable segmentation, such as the Segment Anything\nModel (SAM), have enabled flexible, high-quality mask generation across a wide\nrange of visual domains. However, SAM and similar models remain fundamentally\ndeterministic, producing a single segmentation per object per prompt, and fail\nto capture the inherent ambiguity present in many real-world tasks. This\nlimitation is particularly troublesome in medical imaging, where multiple\nplausible segmentations may exist due to annotation uncertainty or inter-expert\nvariability. In this paper, we introduce Probabilistic SAM, a probabilistic\nextension of SAM that models a distribution over segmentations conditioned on\nboth the input image and prompt. By incorporating a latent variable space and\ntraining with a variational objective, our model learns to generate diverse and\nplausible segmentation masks reflecting the variability in human annotations.\nThe architecture integrates a prior and posterior network into the SAM\nframework, allowing latent codes to modulate the prompt embeddings during\ninference. The latent space allows for efficient sampling during inference,\nenabling uncertainty-aware outputs with minimal overhead. We evaluate\nProbabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate\nits ability to produce diverse outputs that align with expert disagreement,\noutperforming existing probabilistic baselines on uncertainty-aware metrics.\nOur code is available at: https://github.com/tbwa233/Probabilistic-SAM/.", "AI": {"tldr": "The paper introduces Probabilistic SAM, an extension of the Segment Anything Model (SAM), which captures variability in segmentation by modeling a distribution over outputs, particularly addressing ambiguity in medical imaging.", "motivation": "The motivation is to address the deterministic nature of existing segmentation models like SAM that fail to capture inherent segmentation ambiguity in real-world and medical imaging tasks.", "method": "The paper introduces a probabilistic framework by incorporating a latent variable space and a variational objective into SAM, enabling diverse mask generation. A prior and posterior network modify the prompt embeddings during inference, facilitating efficient uncertainty-aware outputs.", "result": "Probabilistic SAM was evaluated on the LIDC-IDRI lung nodule dataset and demonstrated the capability to generate diverse segmentations that reflect expert disagreements, outperforming other methods on uncertainty-aware metrics.", "conclusion": "Probabilistic SAM successfully generates diverse, plausible segmentations representative of annotation variability, enhancing the robustness of segmentation models in ambiguous tasks."}}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806", "abs": "https://arxiv.org/abs/2509.06806", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.", "AI": {"tldr": "The paper introduces a methodology, MachineLearningLM, to integrate machine learning capabilities into large language models (LLMs) for robust in-context learning while maintaining their general knowledge and reasoning skills.", "motivation": "The motivation is to enhance the ability of LLMs to effectively learn from many-shot demonstrations in standard ML tasks, a weakness they currently exhibit.", "method": "The method involves continued pretraining of LLMs by synthesizing ML tasks from millions of structural causal models, incorporating decision-making strategies from random-forest teachers, and using token-efficient prompts to handle large examples within limited context windows.", "result": "MachineLearningLM achieves about 15% higher accuracy compared to strong LLM baselines on out-of-distribution tabular classification tasks across various domains and demonstrates a consistent improvement in accuracy with increased in-context demonstrations. Additionally, it achieves random-forest-level performance without task-specific training.", "conclusion": "MachineLearningLM successfully enhances LLMs with strong in-context ML task capabilities while preserving their general knowledge and reasoning abilities, making them broadly applicable across tasks and domains."}}
{"id": "2509.05309", "pdf": "https://arxiv.org/pdf/2509.05309", "abs": "https://arxiv.org/abs/2509.05309", "authors": ["Xiangyu Liu", "Haodi Lei", "Yi Liu", "Yang Liu", "Wei Hu"], "title": "ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders", "categories": ["q-bio.QM", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic\ninterpretability of large language models. Recent works apply SAE to protein\nlanguage models (PLMs), aiming to extract and analyze biologically meaningful\nfeatures from their latent spaces. However, SAE suffers from semantic\nentanglement, where individual neurons often mix multiple nonlinear concepts,\nmaking it difficult to reliably interpret or manipulate model behaviors. In\nthis paper, we propose a semantically-guided SAE, called ProtSAE. Unlike\nexisting SAE which requires annotation datasets to filter and interpret\nactivations, we guide semantic disentanglement during training using both\nannotation datasets and domain knowledge to mitigate the effects of entangled\nattributes. We design interpretability experiments showing that ProtSAE learns\nmore biologically relevant and interpretable hidden features compared to\nprevious methods. Performance analyses further demonstrate that ProtSAE\nmaintains high reconstruction fidelity while achieving better results in\ninterpretable probing. We also show the potential of ProtSAE in steering PLMs\nfor downstream generation tasks.", "AI": {"tldr": "ProtSAE introduces a semantically-guided sparse autoencoder to improve interpretability in protein language models by disentangling semantic attributes using both datasets and domain knowledge, outperforming existing methods in interpretability and reconstruction fidelity.", "motivation": "To address the issue of semantic entanglement in sparse autoencoders applied to protein language models, which hinders reliable interpretation and manipulation of model behaviors.", "method": "The paper introduces ProtSAE, a semantically-guided sparse autoencoder, that incorporates annotation datasets and domain knowledge during training to achieve better semantic disentanglement.", "result": "ProtSAE demonstrates superior biological interpretability of learned features, retains high reconstruction accuracy, and achieves better performance in probing tasks compared to existing approaches.", "conclusion": "ProtSAE effectively mitigates semantic entanglement, enhances interpretability in protein language models, and shows potential in guiding downstream generation tasks while maintaining reconstruction fidelity."}}
{"id": "2509.05887", "pdf": "https://arxiv.org/pdf/2509.05887", "abs": "https://arxiv.org/abs/2509.05887", "authors": ["Caleb Gates", "Patrick Moorhead", "Jayden Ferguson", "Omar Darwish", "Conner Stallman", "Pablo Rivas", "Paapa Quansah"], "title": "Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data", "categories": ["cs.CV", "cs.LG", "eess.IV", "68T07, 86A32", "I.2.6; I.5.4"], "comment": "29th International Conference on Image Processing, Computer Vision, &\n  Pattern Recognition (IPCV'25)", "summary": "Dust storms harm health and reduce visibility; quick detection from\nsatellites is needed. We present a near real-time system that flags dust at the\npixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D\nconvolutional network learns patterns across all 36 bands, plus split thermal\nbands, to separate dust from clouds and surface features. Simple normalization\nand local filling handle missing data. An improved version raises training\nspeed by 21x and supports fast processing of full scenes. On 17 independent\nMODIS scenes, the model reaches about 0.92 accuracy with a mean squared error\nof 0.014. Maps show strong agreement in plume cores, with most misses along\nedges. These results show that joint band-and-space learning can provide timely\ndust alerts at global scale; using wider input windows or attention-based\nmodels may further sharpen edges.", "AI": {"tldr": "The paper introduces a system for real-time detection of dust storms using NASA's satellite data and advanced deep learning methods.", "motivation": "Dust storms adversely impact health and visibility, necessitating quick detection solutions to address these issues in a global context.", "method": "The study employs a 3D convolutional network analyzing 36 MODIS bands to segregate dust from clouds and surfaces, with preprocessing techniques tackling data gaps. Enhancements boost training speed by 21x.", "result": "On 17 independent datasets, the model achieved approximately 0.92 accuracy, with minor misses identified along plume edges.", "conclusion": "Joint learning of spectral and spatial data facilitates efficient global dust detection, with future potential improvements noted through extended windows or attention-based techniques."}}
{"id": "2509.06297", "pdf": "https://arxiv.org/pdf/2509.06297", "abs": "https://arxiv.org/abs/2509.06297", "authors": ["Li Lin", "Xiaojun Wan"], "title": "LoaQ: Layer-wise Output Approximation Quantization", "categories": ["cs.LG"], "comment": "7 pages, under review", "summary": "A natural and intuitive idea in model quantization is to approximate each\ncomponent's quantized output to match its original. Layer-wise post-training\nquantization (PTQ), though based on this idea, adopts a strictly local view and\ncan achieve, at best, only activation-aware approximations of weights. As a\nresult, it often leads to insufficient approximations and practical deviations\nfrom this guiding intuition. Recent work has achieved a more accurate\napproximation of linear-layer outputs within the framework of layer-wise PTQ,\nbut such refinements remain inadequate for achieving alignment with the full\nmodel output. Based on a deeper understanding of the structural characteristics\nof mainstream LLMs, we propose $LoaQ$, an output-approximation method for\nlayer-wise PTQ that explicitly targets output-level consistency. It better\naligns with this intuition and can feature a simple closed-form solution,\nmaking it orthogonal to existing techniques and readily integrable into\nexisting quantization pipelines. Experiments on the LLaMA and Qwen model\nfamilies demonstrate that LoaQ performs effectively in both weight-only and\nweight-activation joint quantization. By integrating seamlessly with existing\nquantization strategies, it further enhances overall quantization quality and\nshows strong potential to advance the frontier of post-training quantization.", "AI": {"tldr": "Layer-wise post-training quantization often fails to align quantized outputs with their originals due to a strictly local approach. LoaQ introduces an output-level consistency method addressing this issue.", "motivation": "Existing layer-wise PTQ methods provide local approximations that do not align well with full model outputs, leading to insufficient quantization results.", "method": "LoaQ employs an output-approximation technique targeting consistency at the model output level, featuring a simple closed-form solution and integrability into existing quantization pipelines.", "result": "Experiments on LLaMA and Qwen model families show LoaQ effectively improves quantization quality in both weight-only and weight-activation joint quantization.", "conclusion": "LoaQ enhances overall quantization and holds potential for advancing post-training quantization techniques further."}}
{"id": "2509.06807", "pdf": "https://arxiv.org/pdf/2509.06807", "abs": "https://arxiv.org/abs/2509.06807", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Ting Liu", "Bing Qin"], "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.", "AI": {"tldr": "The paper introduces MoGU and MoGU_v2 frameworks aimed at improving the balance between the security and usability of Large Language Models (LLMs). The MoGU_v2 framework demonstrates stable performance gains and adaptability across various scenarios, offering a robust solution for mitigating LLM security risks without sacrificing task usability.", "motivation": "The paper aims to address the critical problem of security in Large Language Models (LLMs) without requiring a trade-off between usability and security, which has been a significant limitation in existing solutions.", "method": "The authors propose the MoGU framework, which uses an intra-layer router to balance contributions of security-optimized and usability-optimized variants. They then enhance this with MoGU_v2, which tightly integrates routers and hidden states, selectively embeds routers in layers encoding classifiable security features, and activates backbone modules during optimization for better adaptability.", "result": "MoGU_v2 provides stable improvements across multiple LLM scenarios, including mainstream, on-device, and reasoning-focused models. It also successfully restores security when dealing with risks introduced by Instruction Fine-tuning, all without compromising task performance.", "conclusion": "The MoGU_v2 framework is presented as a robust, adaptable, and effective solution for enhancing LLM security while maintaining usability in a variety of real-world contexts."}}
{"id": "2509.05311", "pdf": "https://arxiv.org/pdf/2509.05311", "abs": "https://arxiv.org/abs/2509.05311", "authors": ["Konur Tholl", "Fran\u00e7ois Rivest", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) has shown great potential for autonomous\ndecision-making in the cybersecurity domain, enabling agents to learn through\ndirect environment interaction. However, RL agents in Autonomous Cyber\nOperations (ACO) typically learn from scratch, requiring them to execute\nundesirable actions to learn their consequences. In this study, we integrate\nexternal knowledge in the form of a Large Language Model (LLM) pretrained on\ncybersecurity data that our RL agent can directly leverage to make informed\ndecisions. By guiding initial training with an LLM, we improve baseline\nperformance and reduce the need for exploratory actions with obviously negative\noutcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity\nenvironment, and demonstrate that our guided agent achieves over 2x higher\nrewards during early training and converges to a favorable policy approximately\n4,500 episodes faster than the baseline.", "AI": {"tldr": "The paper explores integrating a Large Language Model (LLM) with reinforcement learning (RL) in the cybersecurity domain to reduce negative exploratory actions and improve early training performance.", "motivation": "Traditional RL agents in cybersecurity learn from scratch, which requires executing undesirable actions to understand their outcomes, potentially leading to inefficiencies and risks.", "method": "The authors propose integrating an LLM pretrained on cybersecurity data with an RL agent, allowing the RL agent to leverage external knowledge for more informed decision-making during training.", "result": "The LLM-guided RL agent achieves over 2x higher rewards during early training and converges to an optimal policy approximately 4,500 episodes faster compared to the baseline method.", "conclusion": "Integrating an LLM with an RL agent enhances early training performance and reduces reliance on risky exploratory actions, demonstrating its potential to improve decision-making in cybersecurity environments."}}
{"id": "2509.05892", "pdf": "https://arxiv.org/pdf/2509.05892", "abs": "https://arxiv.org/abs/2509.05892", "authors": ["Phongsakon Mark Konrad", "Andrei-Alexandru Popa", "Yaser Sabzehmeidani", "Liang Zhong", "Elisa A. Liehn", "Serkan Ayvaz"], "title": "Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation of carotid artery structures in histopathological\nimages is vital for advancing cardiovascular disease research and diagnosis.\nHowever, deep learning model development in this domain is constrained by the\nscarcity of annotated cardiovascular histopathological data. This study\ninvestigates a systematic evaluation of state-of-the-art deep learning\nsegmentation models, including convolutional neural networks (U-Net,\nDeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models\n(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology\nimages. Despite employing an extensive hyperparameter optimization strategy\nwith Bayesian search, our findings reveal that model performance is highly\nsensitive to data splits, with minor differences driven more by statistical\nnoise than by true algorithmic superiority. This instability exposes the\nlimitations of standard benchmarking practices in low-data clinical settings\nand challenges the assumption that performance rankings reflect meaningful\nclinical utility.", "AI": {"tldr": "The paper evaluates deep learning segmentation models on limited cardiovascular histopathological data, revealing performance sensitivity to data splits and questioning current benchmarking methods.", "motivation": "The scarcity of annotated cardiovascular histopathological data limits the development of segmentation models for advancing cardiovascular disease diagnosis and research.", "method": "The paper systematically evaluates multiple state-of-the-art deep learning segmentation approaches, including CNNs, vision transformers, and foundation models, utilizing Bayesian hyperparameter optimization.", "result": "Results show that model performance is highly sensitive to data splits, with differences largely attributed to statistical noise rather than intrinsic superiority of the algorithms.", "conclusion": "Standard benchmarking practices in low-data clinical settings may be unreliable, necessitating a reevaluation of assumptions about performance rankings and clinical utility improvements."}}
{"id": "2509.06311", "pdf": "https://arxiv.org/pdf/2509.06311", "abs": "https://arxiv.org/abs/2509.06311", "authors": ["Hang Fan", "Yu Shi", "Zongliang Fu", "Shuo Chen", "Wei Wei", "Wei Xu", "Jian Li"], "title": "WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "High-quality wind power forecasting is crucial for the operation of modern\npower grids. However, prevailing data-driven paradigms either train a\nsite-specific model which cannot generalize to other locations or rely on\nfine-tuning of general-purpose time series foundation models which are\ndifficult to incorporate domain-specific data in the energy sector. This paper\nintroduces WindFM, a lightweight and generative Foundation Model designed\nspecifically for probabilistic wind power forecasting. WindFM employs a\ndiscretize-and-generate framework. A specialized time-series tokenizer first\nconverts continuous multivariate observations into discrete, hierarchical\ntokens. Subsequently, a decoder-only Transformer learns a universal\nrepresentation of wind generation dynamics by autoregressively pre-training on\nthese token sequences. Using the comprehensive WIND Toolkit dataset comprising\napproximately 150 billion time steps from more than 126,000 sites, WindFM\ndevelops a foundational understanding of the complex interplay between\natmospheric conditions and power output. Extensive experiments demonstrate that\nour compact 8.1M parameter model achieves state-of-the-art zero-shot\nperformance on both deterministic and probabilistic tasks, outperforming\nspecialized models and larger foundation models without any fine-tuning. In\nparticular, WindFM exhibits strong adaptiveness under out-of-distribution data\nfrom a different continent, demonstrating the robustness and transferability of\nits learned representations. Our pre-trained model is publicly available at\nhttps://github.com/shiyu-coder/WindFM.", "AI": {"tldr": "WindFM is a compact generative Foundation Model for probabilistic wind power forecasting, achieving state-of-the-art results without fine-tuning.", "motivation": "Existing methods for wind power forecasting either lack generalization across locations or struggle to incorporate domain-specific data efficiently.", "method": "WindFM uses a discretize-and-generate framework with a specialized tokenizer and a Transformer model trained on large-scale WIND Toolkit data.", "result": "WindFM achieves top zero-shot performance in both deterministic and probabilistic forecasting tasks, showing robustness to out-of-distribution data.", "conclusion": "WindFM provides an innovative, lightweight solution for wind power forecasting with high adaptability and transferability."}}
{"id": "2509.05855", "pdf": "https://arxiv.org/pdf/2509.05855", "abs": "https://arxiv.org/abs/2509.05855", "authors": ["Thijs Masmeijer", "Caleb Swain", "Jeff Hill", "Ed Habtour"], "title": "Programming tension in 3D printed networks inspired by spiderwebs", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Each element in tensioned structural networks -- such as tensegrity,\narchitectural fabrics, or medical braces/meshes -- requires a specific tension\nlevel to achieve and maintain the desired shape, stability, and compliance.\nThese structures are challenging to manufacture, 3D print, or assemble because\nflattening the network during fabrication introduces multiplicative\ninaccuracies in the network's final tension gradients. This study overcomes\nthis challenge by offering a fabrication algorithm for direct 3D printing of\nsuch networks with programmed tension gradients, an approach analogous to the\nspinning of spiderwebs. The algorithm: (i) defines the desired network and\nprescribes its tension gradients using the force density method; (ii) converts\nthe network into an unstretched counterpart by numerically optimizing vertex\nlocations toward target element lengths and converting straight elements into\narcs to resolve any remaining error; and (iii) decomposes the network into\nprintable toolpaths; Optional additional steps are: (iv) flattening curved 2D\nnetworks or 3D networks to ensure 3D printing compatibility; and (v)\nautomatically resolving any unwanted crossings introduced by the flattening\nprocess. The proposed method is experimentally validated using 2D unit cells of\nviscoelastic filaments, where accurate tension gradients are achieved with an\naverage element strain error of less than 1.0\\%. The method remains effective\nfor networks with element minimum length and maximum stress of 5.8 mm and 7.3\nMPa, respectively. The method is used to demonstrate the fabrication of three\ncomplex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The\nprogrammable tension gradient algorithm can be utilized to produce compact,\nintegrated cable networks, enabling novel applications such as moment-exerting\nstructures in medical braces and splints.", "AI": {"tldr": "The paper proposes a fabrication algorithm for 3D printing tensioned structural networks with accurate programmed tension gradients, validated experimentally with minimal error.", "motivation": "Ensuring accurate tension gradients in manufactured structural networks is challenging, as inaccuracies arise during fabrication processes like flattening. This hinders their effectiveness in applications requiring precise tension control.", "method": "A new fabrication algorithm is introduced, which utilizes the force density method to design tension gradients, optimizes vertex locations and converts elements into arcs, decomposes the network into printable toolpaths, and resolves flattening-induced issues for 3D printing.", "result": "The experimental validation showed that the method achieves tension gradients with an average strain error of less than 1.0%, and is effective for elements with lengths as small as 5.8 mm and stresses up to 7.3 MPa.", "conclusion": "The algorithm enables accurate and efficient production of complex tensioned networks, with potential applications in creating advanced structures like medical braces, curved meshes, and tensegrity systems."}}
{"id": "2509.06809", "pdf": "https://arxiv.org/pdf/2509.06809", "abs": "https://arxiv.org/abs/2509.06809", "authors": ["Valentin Quesnel", "Damien Sileo"], "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1", "AI": {"tldr": "The paper proposes a scalable framework for generating high-quality mathematical reasoning data for Large Language Models (LLMs), using E-prover to ensure logical validity without relying on error-prone LLMs or complex syntax.", "motivation": "There is a scarcity of high-quality and logically sound data needed to advance the mathematical reasoning capabilities of LLMs.", "method": "The proposed framework uses E-prover's saturation feature on the TPTP axiom library to generate a corpus of guaranteed-valid theorems, then filters and converts them into three types of reasoning challenges: entailment verification, premise selection, and proof reconstruction.", "result": "Zero-shot experiments on advanced models reveal that they perform poorly on tasks requiring deep structural reasoning, highlighting the gap in LLM capabilities.", "conclusion": "The framework offers a diagnostic tool for measuring weaknesses in LLM reasoning and a scalable way to generate symbolic data for training them. The authors provide publicly available data and code to support further research."}}
{"id": "2509.05895", "pdf": "https://arxiv.org/pdf/2509.05895", "abs": "https://arxiv.org/abs/2509.05895", "authors": ["Yujie Li", "Wenjia Xu", "Yuanben Zhang", "Zhiwei Wei", "Mugen Peng"], "title": "BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": "5 pages, 2 figures Submitted to ICASSP 2026", "summary": "Bi-temporal satellite imagery supports critical applications such as urban\ndevelopment monitoring and disaster assessment. Although powerful multimodal\nlarge language models (MLLMs) have been applied in bi-temporal change analysis,\nprevious methods process image pairs through direct concatenation, inadequately\nmodeling temporal correlations and spatial semantic changes. This deficiency\nhampers visual-semantic alignment in change understanding, thereby constraining\nthe overall effectiveness of current approaches. To address this gap, we\npropose BTCChat, a multi-temporal MLLM with advanced bi-temporal change\nunderstanding capability. BTCChat supports bi-temporal change captioning and\nretains single-image interpretation capability. To better capture temporal\nfeatures and spatial semantic changes in image pairs, we design a Change\nExtraction module. Moreover, to enhance the model's attention to spatial\ndetails, we introduce a Prompt Augmentation mechanism, which incorporates\ncontextual clues into the prompt to enhance model performance. Experimental\nresults demonstrate that BTCChat achieves state-of-the-art performance on\nchange captioning and visual question answering tasks.", "AI": {"tldr": "BTCChat is a novel multi-temporal large language model designed for bi-temporal satellite imagery analysis, outperforming current models in change captioning and visual question answering.", "motivation": "Existing approaches inadequately model temporal correlations and semantic changes in bi-temporal satellite imagery, hampering change analysis effectiveness.", "method": "BTCChat incorporates a Change Extraction module for better temporal and semantic modeling and a Prompt Augmentation mechanism to enhance spatial attention.", "result": "BTCChat delivers state-of-the-art performance on bi-temporal change captioning and visual question answering tasks.", "conclusion": "BTCChat advances bi-temporal change analysis by improving temporal and spatial feature extraction and achieving superior performance in relevant tasks."}}
{"id": "2509.06314", "pdf": "https://arxiv.org/pdf/2509.06314", "abs": "https://arxiv.org/abs/2509.06314", "authors": ["Mehmet Can Yavuz", "Berrin Yanikoglu"], "title": "Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "A central challenge in representation learning is constructing latent\nembeddings that are both expressive and efficient. In practice, deep networks\noften produce redundant latent spaces where multiple coordinates encode\noverlapping information, reducing effective capacity and hindering\ngeneralization. Standard metrics such as accuracy or reconstruction loss\nprovide only indirect evidence of such redundancy and cannot isolate it as a\nfailure mode. We introduce a redundancy index, denoted rho(C), that directly\nquantifies inter-dimensional dependencies by analyzing coupling matrices\nderived from latent representations and comparing their off-diagonal statistics\nagainst a normal distribution via energy distance. The result is a compact,\ninterpretable, and statistically grounded measure of representational quality.\nWe validate rho(C) across discriminative and generative settings on MNIST\nvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple\narchitectures and hyperparameter optimization strategies. Empirically, low\nrho(C) reliably predicts high classification accuracy or low reconstruction\nerror, while elevated redundancy is associated with performance collapse.\nEstimator reliability grows with latent dimension, yielding natural lower\nbounds for reliable analysis. We further show that Tree-structured Parzen\nEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)\ncan guide neural architecture search and serve as a redundancy-aware\nregularization target. By exposing redundancy as a universal bottleneck across\nmodels and tasks, rho(C) offers both a theoretical lens and a practical tool\nfor evaluating and improving the efficiency of learned representations.", "AI": {"tldr": "The paper introduces a redundancy index (rho(C)) to measure inefficiencies in latent representations, particularly caused by overlapping information among dimensions. This metric directly quantifies redundancy and serves as a tool for improving representational quality.", "motivation": "Redundant latent spaces, which arise in deep networks, limit model capacity and hinder generalization. Existing metrics only indirectly address redundancy, prompting the need for a direct approach.", "method": "The authors propose rho(C), a redundancy index based on analyzing coupling matrices of latent representations and comparing their off-diagonal statistics to a normal distribution using energy distance.", "result": "Low rho(C) correlates with high classification accuracy or low reconstruction error, whereas high rho(C) predicts performance collapse. The reliability improves with larger latent dimensions, and rho(C) aids neural architecture search using Tree-structured Parzen Estimators (TPE).", "conclusion": "The redundancy index rho(C) offers a theoretical and practical framework for identifying and addressing redundancy in latent representations, improving efficiency and generalization across models and tasks."}}
{"id": "2509.06103", "pdf": "https://arxiv.org/pdf/2509.06103", "abs": "https://arxiv.org/abs/2509.06103", "authors": ["Divij Gupta", "Arkajit Aich"], "title": "Advancing Resource Extraction Systems in Martian Volcanic Terrain: Rover Design, Power Consumption and Hazard Analysis", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.RO", "physics.space-ph"], "comment": "23 pages, 5 figures", "summary": "This study proposes a schematic plan for in-situ resource utilization (ISRU)\nin Martian volcanic terrains. The work investigated the complexity of volcanic\nterrains and Martian environmental hazards and suggested comprehensive\nengineering strategies to overcome the odds and establish a successful mining\nprogram in Martian volcanic regions. Slope stabilization methods - such as\nterracing and anchored drilling rigs - with terrain-adaptive rovers capable of\nautonomous operations on steep unstable slopes has been suggested as feasible\nsolutions to navigate the complex geological terrains of Martian volcanoes. The\nmid range rover design with a mass of approximately 2.1 t, proposed here for\nmining operations, incorporates a six-wheel rocker-bogie suspension,\nanchoring-enabled drilling arm, dust-mitigation solar arrays, and advanced\nsensing systems for hazard detection and navigation. A comparative analysis\nregarding choice of roads and rails for building transport infrastructure has\nalso been performed. We have also looked into the energy requirement of the\nrover to work under extreme environmental conditions of Mars and suggested a\ncombination of solar and nuclear power to account for the huge energy\nrequirements of sustained operations on Mars. The results demonstrate that\nmission success in these environments depends on integrating mechanical\nresilience, environmental adaptability, and operational autonomy, enabling\nsustainable access to resources in one of Mars' most geologically challenging\nsettings.", "AI": {"tldr": "The study presents a plan for mining resources in Martian volcanic terrains using adaptive rovers and integrated engineering strategies.", "motivation": "To address the challenges posed by Martian volcanic terrains and environmental hazards, facilitating resource utilization.", "method": "Development of terrain-adaptive rovers and strategies, including slope stabilization methods, rover design, transport infrastructure analysis, and energy solutions using solar and nuclear power.", "result": "The proposed mid-range rover design enhances navigation and mining capability in Mars\u2019 volcanic terrains, supported by anchoring systems, solar arrays, and sensing systems.", "conclusion": "Successful ISRU on Mars requires mechanical resilience, adaptability to harsh environments, and operational autonomy for sustained resource access."}}
{"id": "2509.06813", "pdf": "https://arxiv.org/pdf/2509.06813", "abs": "https://arxiv.org/abs/2509.06813", "authors": ["Max Malyi", "Jonathan Shek", "Alasdair McDonald", "Andre Biscaya"], "title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs", "categories": ["cs.CL"], "comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms", "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.", "AI": {"tldr": "The paper proposes a framework for evaluating Large Language Models (LLMs) in classifying wind turbine maintenance logs, a task hindered by their unstructured nature. Results show model performance varies with task ambiguity, suggesting human-assisted LLM use is ideal.", "motivation": "Reducing the Levelised Cost of Energy (LCOE) from wind power is crucial, but analyzing turbine maintenance logs is challenging due to their unstructured, free-text format.", "method": "The authors developed an open-source, reproducible benchmarking framework for evaluating various proprietary and open-source LLMs in classifying industrial maintenance records.", "result": "Findings reveal a performance hierarchy among LLMs, with better alignment in objective tasks than interpretive ones. Calibration and accuracy remain inconsistent across models.", "conclusion": "While no LLM achieves perfect accuracy, integrating them in Human-in-the-Loop systems can assist human experts, improving O&M data labeling, enhancing data quality, and reliability analysis."}}
{"id": "2509.05913", "pdf": "https://arxiv.org/pdf/2509.05913", "abs": "https://arxiv.org/abs/2509.05913", "authors": ["Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Tamanna Shermin", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "title": "A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features", "categories": ["cs.CV"], "comment": "16 pages, 6 figures, 8 tables", "summary": "Musculoskeletal disorders pose significant risks to athletes, and assessing\nrisk early is important for prevention. However, most existing methods are\ndesigned for controlled settings and fail to reliably assess risk in complex\nenvironments due to their reliance on a single type of data. This research\nproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel\nmultimodal deep learning framework designed to classify musculoskeletal risk\nusing visual and skeletal coordinate-based features. In addition, a custom\nmultimodal dataset is constructed by combining visual data and skeletal\ncoordinates for risk assessment. Each sample is labeled into eight risk\ncategories based on the Rapid Entire Body Assessment system. ViSK-GAT combines\na Residual Block with a Lightweight Transformer Block to learn spatial and\ntemporal dependencies jointly. It incorporates two novel modules: the\nFine-Grained Attention Module (FGAM), which enables precise inter-modal feature\nrefinement through cross-attention between visual and skeletal inputs, and the\nMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modal\ncoherence by aligning image features with coordinate-based representations.\nViSK-GAT achieved strong performance with validation and test accuracies of\n93.55\\% and 93.89\\%, respectively; a precision of 93.86\\%; an F1 score of\n93.85\\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\\%. The\nregression results also indicated a low Root Mean Square Error of the predicted\nprobability distribution of 0.1205 and a corresponding Mean Absolute Error of\n0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT\nconsistently outperformed previous methods. The ViSK-GAT model advances\nartificial intelligence implementation and application, transforming\nmusculoskeletal risk classification and enabling impactful early interventions\nin sports.", "AI": {"tldr": "The paper introduces a multimodal deep learning model, ViSK-GAT, for musculoskeletal risk classification using visual and skeletal data. The model shows strong performance metrics and surpasses existing methods.", "motivation": "Musculoskeletal disorders are critical for athletes, but existing risk assessment methods are unsuitable for complex environments due to reliance on single data types, necessitating a more robust model.", "method": "The ViSK-GAT model combines visual and skeletal coordinate data with novel modules like FGAM for inter-modal feature refinement and MGCM for cross-modal coherence. The framework leverages a Residual Block and a Lightweight Transformer Block to capture spatial and temporal dependencies.", "result": "ViSK-GAT achieved accuracies of 93.55% (validation) and 93.89% (test), along with strong precision, F1 scores, and low error rates across metrics. It outperformed nine transfer learning backbones in the experiment.", "conclusion": "ViSK-GAT advances musculoskeletal risk classification through its superior performance, enabling effective early interventions in sports with a robust multimodal AI framework."}}
{"id": "2509.06322", "pdf": "https://arxiv.org/pdf/2509.06322", "abs": "https://arxiv.org/abs/2509.06322", "authors": ["Jiajun Bao", "Nicolas Boull\u00e9", "Toni J. B. Liu", "Rapha\u00ebl Sarfati", "Christopher J. Earls"], "title": "Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated emergent in-context learning\n(ICL) capabilities across a range of tasks, including zero-shot time-series\nforecasting. We show that text-trained foundation models can accurately\nextrapolate spatiotemporal dynamics from discretized partial differential\nequation (PDE) solutions without fine-tuning or natural language prompting.\nPredictive accuracy improves with longer temporal contexts but degrades at\nfiner spatial discretizations. In multi-step rollouts, where the model\nrecursively predicts future spatial states over multiple time steps, errors\ngrow algebraically with the time horizon, reminiscent of global error\naccumulation in classical finite-difference solvers. We interpret these trends\nas in-context neural scaling laws, where prediction quality varies predictably\nwith both context length and output length. To better understand how LLMs are\nable to internally process PDE solutions so as to accurately roll them out, we\nanalyze token-level output distributions and uncover a consistent ICL\nprogression: beginning with syntactic pattern imitation, transitioning through\nan exploratory high-entropy phase, and culminating in confident, numerically\ngrounded predictions.", "AI": {"tldr": "The paper explores large language models (LLMs) for solving discretized partial differential equations (PDEs) and analyzes their in-context learning (ICL) behavior.", "motivation": "To investigate whether text-trained LLMs can accurately predict spatiotemporal dynamics from PDE solutions without fine-tuning and to understand the mechanism underlying their ICL abilities.", "method": "The authors evaluate LLMs' ability to perform multi-step rollouts of PDE solutions, analyzing predictive accuracy against spatiotemporal contexts. They also examine token-level output distributions to uncover trends in how models process PDE solutions over time.", "result": "LLMs can extrapolate spatiotemporal dynamics with accuracy that improves with longer temporal contexts but decreases at finer spatial resolutions. Errors in multi-step rollouts grow algebraically with time, exhibiting trends analogous to classical solvers.", "conclusion": "LLMs exhibit predictable scaling behaviors in context learning for PDEs, transitioning through phases of pattern imitation, exploratory behavior, and confident predictions. This provides insights into their capability for numerical tasks despite being trained on text."}}
{"id": "2509.06333", "pdf": "https://arxiv.org/pdf/2509.06333", "abs": "https://arxiv.org/abs/2509.06333", "authors": ["Penelope Brown", "Julie Stephany Berrio Perez", "Mao Shan", "Stewart Worrall"], "title": "Multi-Modal Camera-Based Detection of Vulnerable Road Users", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists\nrepresent more than half of global traffic deaths, yet their detection remains\nchallenging in poor lighting, adverse weather, and unbalanced data sets. This\npaper presents a multimodal detection framework that integrates RGB and thermal\ninfrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,\nBDD100K, and Teledyne FLIR datasets, with class re-weighting and light\naugmentations to improve minority-class performance and robustness, experiments\nshow that 640-pixel resolution and partial backbone freezing optimise accuracy\nand efficiency, while class-weighted losses enhance recall for rare VRUs.\nResults highlight that thermal models achieve the highest precision, and\nRGB-to-thermal augmentation boosts recall, demonstrating the potential of\nmultimodal detection to improve VRU safety at intersections.", "AI": {"tldr": "This paper develops a multimodal framework combining RGB and thermal imaging with YOLOv8, enhancing vulnerable road user (VRU) detection in challenging scenarios.", "motivation": "The primary motivation is to address the challenges in detecting vulnerable road users (VRUs) in poor lighting, adverse weather, and unbalanced datasets, as these users account for over half of global traffic deaths.", "method": "The method integrates RGB and thermal imaging with a fine-tuned YOLOv8 model. It uses datasets such as KITTI, BDD100K, and Teledyne FLIR, with strategies like class re-weighting, light augmentations, 640-pixel resolution optimization, and partial backbone freezing to improve accuracy and recall.", "result": "Experiments demonstrate the superiority of thermal models in precision and highlight that RGB-to-thermal augmentation enhances recall, particularly for rare VRU classes.", "conclusion": "The study concludes that multimodal detection frameworks integrating thermal and RGB modalities can significantly enhance VRU detection accuracy and safety, especially in challenging conditions and at intersections."}}
{"id": "2509.06836", "pdf": "https://arxiv.org/pdf/2509.06836", "abs": "https://arxiv.org/abs/2509.06836", "authors": ["Eugene Kwek", "Wenpeng Yin"], "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.", "AI": {"tldr": "The paper introduces COMPACT, a novel method for improving the efficiency of large language models (LLMs) by pruning rarely used vocabulary and FFN channels to maintain performance while reducing memory, latency, and costs.", "motivation": "The study aims to make LLMs more memory- and cost-efficient for edge deployment, interactive applications, and large-scale sustainable inference, addressing limitations of existing pruning methods.", "method": "COMPACT prunes rare vocabulary to shrink embeddings and unembedding matrices and optimally prunes FFN intermediate channels by aligning pruning importance with token distribution, all while preserving a transformer-friendly layout.", "result": "COMPACT demonstrated state-of-the-art performance on Qwen, LLaMA, and Gemma models spanning 0.5B to 70B parameters, achieving high pruning ratios with significant reductions in memory usage, latency, and serving costs.", "conclusion": "COMPACT offers a training-free, scalable, and deployment-friendly approach, achieving competitive or superior pruning outcomes without compromising downstream task accuracy in large-scale LLMs."}}
{"id": "2509.05925", "pdf": "https://arxiv.org/pdf/2509.05925", "abs": "https://arxiv.org/abs/2509.05925", "authors": ["Ruiqi Shen", "Haotian Wu", "Wenjing Zhang", "Jiangjing Hu", "Deniz Gunduz"], "title": "Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models", "categories": ["cs.CV", "cs.IT", "math.IT"], "comment": "Published as a conference paper at IEEE 35th Workshop on Machine\n  Learning for Signal Processing (MLSP)", "summary": "Recent deep learning-based methods for lossy image compression achieve\ncompetitive rate-distortion performance through extensive end-to-end training\nand advanced architectures. However, emerging applications increasingly\nprioritize semantic preservation over pixel-level reconstruction and demand\nrobust performance across diverse data distributions and downstream tasks.\nThese challenges call for advanced semantic compression paradigms. Motivated by\nthe zero-shot and representational capabilities of multimodal foundation\nmodels, we propose a novel semantic compression method based on the contrastive\nlanguage-image pretraining (CLIP) model. Rather than compressing images for\nreconstruction, we propose compressing the CLIP feature embeddings into minimal\nbits while preserving semantic information across different tasks. Experiments\nshow that our method maintains semantic integrity across benchmark datasets,\nachieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This\nis less than 5% of the bitrate required by mainstream image compression\napproaches for comparable performance. Remarkably, even under extreme\ncompression, the proposed approach exhibits zero-shot robustness across diverse\ndata distributions and downstream tasks.", "AI": {"tldr": "The paper introduces a semantic compression method leveraging CLIP models to preserve semantic information, achieving significantly lower bitrates compared to traditional methods.", "motivation": "Emerging applications require compression methods that focus on semantic preservation over pixel-perfect reconstruction for robust performance in diverse contexts.", "method": "The authors propose compressing CLIP feature embeddings instead of images, ensuring semantic integrity by prioritizing semantic information preservation.", "result": "The method achieves semantic preservation with less than 5% of the bitrate required by standard techniques, delivering robust zero-shot performance across tasks and datasets.", "conclusion": "This approach advances semantic compression research, highlighting the benefits of multimodal foundation models for efficient and robust representation in diverse applications."}}
{"id": "2509.06330", "pdf": "https://arxiv.org/pdf/2509.06330", "abs": "https://arxiv.org/abs/2509.06330", "authors": ["Guanlan Hu", "Adit Anand", "Pooja M. Desai", "I\u00f1igo Urteaga", "Lena Mamykina"], "title": "Exploring approaches to computational representation and classification of user-generated meal logs", "categories": ["cs.LG"], "comment": null, "summary": "This study examined the use of machine learning and domain specific\nenrichment on patient generated health data, in the form of free text meal\nlogs, to classify meals on alignment with different nutritional goals. We used\na dataset of over 3000 meal records collected by 114 individuals from a\ndiverse, low income community in a major US city using a mobile app. Registered\ndietitians provided expert judgement for meal to goal alignment, used as gold\nstandard for evaluation. Using text embeddings, including TFIDF and BERT, and\ndomain specific enrichment information, including ontologies, ingredient\nparsers, and macronutrient contents as inputs, we evaluated the performance of\nlogistic regression and multilayer perceptron classifiers using accuracy,\nprecision, recall, and F1 score against the gold standard and self assessment.\nEven without enrichment, ML outperformed self assessments of individuals who\nlogged meals, and the best performing combination of ML classifier with\nenrichment achieved even higher accuracies. In general, ML classifiers with\nenrichment of Parsed Ingredients, Food Entities, and Macronutrients information\nperformed well across multiple nutritional goals, but there was variability in\nthe impact of enrichment and classification algorithm on accuracy of\nclassification for different nutritional goals. In conclusion, ML can utilize\nunstructured free text meal logs and reliably classify whether meals align with\nspecific nutritional goals, exceeding self assessments, especially when\nincorporating nutrition domain knowledge. Our findings highlight the potential\nof ML analysis of patient generated health data to support patient centered\nnutrition guidance in precision healthcare.", "AI": {"tldr": "The study uses patient-generated free-text meal logs, enhanced with domain-specific information, to classify meal alignment with nutritional goals via machine learning, achieving high accuracy beyond self-assessments.", "motivation": "To explore how machine learning can analyze free-text meal logs to support nutrition guidance by replacing or enhancing self-assessments with automated classification based on nutritional goals.", "method": "Applied a combination of text embeddings (e.g., TFIDF, BERT) and domain-specific enrichments such as ontologies, ingredient parsers, and macronutrient data as inputs to machine learning classifiers (logistic regression and multilayer perceptron). Evaluated their accuracy against a gold standard provided by dietitians.", "result": "Machine learning models outperformed self-assessments and achieved higher accuracy, especially when enriched with nutrition-specific data, across diverse nutritional goals and on patient-generated meal logs.", "conclusion": "Machine learning, especially when combined with domain-specific dietary enrichments, effectively classifies meals' alignment with nutritional goals, showing promise for enhancing patient-centered nutrition guidance in precision healthcare."}}
{"id": "2509.06374", "pdf": "https://arxiv.org/pdf/2509.06374", "abs": "https://arxiv.org/abs/2509.06374", "authors": ["Hiroya Makino", "Seigo Ito"], "title": "MAPF-HD: Multi-Agent Path Finding in High-Density Environments", "categories": ["cs.MA", "cs.RO"], "comment": "9 pages, 12 figures", "summary": "Multi-agent path finding (MAPF) involves planning efficient paths for\nmultiple agents to move simultaneously while avoiding collisions. In typical\nwarehouse environments, agents are often sparsely distributed along aisles.\nHowever, increasing the agent density can improve space efficiency. When the\nagent density is high, we must optimize the paths not only for goal-assigned\nagents but also for those obstructing them. This study proposes a novel MAPF\nframework for high-density environments (MAPF-HD). Several studies have\nexplored MAPF in similar settings using integer linear programming (ILP).\nHowever, ILP-based methods require substantial computation time to optimize all\nagent paths simultaneously. Even in small grid-based environments with fewer\nthan $100$ cells, these computations can incur tens to hundreds of seconds.\nThese high computational costs render these methods impractical for large-scale\napplications such as automated warehouses and valet parking. To address these\nlimitations, we introduce the phased null-agent swapping (PHANS) method. PHANS\nemploys a heuristic approach to incrementally swap positions between agents and\nempty vertices. This method solves the MAPF-HD problem within seconds to tens\nof seconds, even in large environments containing more than $700$ cells. The\nproposed method can potentially improve efficiency in various real-world\napplications such as warehouse logistics, traffic management, or crowd control.\nCode is available at https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs.", "AI": {"tldr": "The study introduces a new framework (MAPF-HD) and method (PHANS) aimed at solving multi-agent pathfinding in high-density environments quickly and efficiently.", "motivation": "The growing need for efficient logistics in high-density environments like warehouses requires better MAPF methods as current ILP-based approaches are computationally prohibitive for large-scale scenarios.", "method": "The paper proposes the phased null-agent swapping (PHANS) technique, which uses heuristics to incrementally swap positions between agents and empty vertices, enabling faster path optimization.", "result": "PHANS dramatically reduces computation time for MAPF-HD problems, from tens to hundreds of seconds (previous methods) down to mere seconds or tens of seconds in environments exceeding 700 cells.", "conclusion": "PHANS and the MAPF-HD framework significantly enhance the efficiency and scalability of MAPF problems, making them applicable in practical contexts such as automated warehouses, traffic management, and crowd dynamics."}}
{"id": "2509.06838", "pdf": "https://arxiv.org/pdf/2509.06838", "abs": "https://arxiv.org/abs/2509.06838", "authors": ["Mohammad Reza Mirbagheri", "Mohammad Mahdi Mirkamali", "Zahra Motoshaker Arani", "Ali Javeri", "Amir Mahdi Sadeghzadeh", "Rasool Jalili"], "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.", "AI": {"tldr": "This paper introduces EPT, a Persian-specific metric to assess LLM trustworthiness across six dimensions and evaluates top models, revealing deficiencies in safety.", "motivation": "To address the critical challenge of ensuring LLM trustworthiness, especially in alignment with cultural and ethical values.", "method": "Developed the EPT metric, curated a labeled Persian dataset, and assessed models using LLM-based and human evaluations across six dimensions.", "result": "The study found significant shortcomings in the safety aspect of leading LLMs and provided insights into their alignment with Persian cultural values.", "conclusion": "Critical gaps in trustworthiness and cultural alignment exist in LLMs, presenting opportunities to improve their alignment with ethical and cultural standards."}}
{"id": "2509.05949", "pdf": "https://arxiv.org/pdf/2509.05949", "abs": "https://arxiv.org/abs/2509.05949", "authors": ["Qiqi Zhan", "Shiwei Li", "Qingjie Liu", "Yunhong Wang"], "title": "AttriPrompt: Dynamic Prompt Composition Learning for CLIP", "categories": ["cs.CV"], "comment": null, "summary": "The evolution of prompt learning methodologies has driven exploration of\ndeeper prompt designs to enhance model performance. However, current deep text\nprompting approaches suffer from two critical limitations: Over-reliance on\nconstrastive learning objectives that prioritize high-level semantic alignment,\nneglecting fine-grained feature optimization; Static prompts across all input\ncategories, preventing content-aware adaptation. To address these limitations,\nwe propose AttriPrompt-a novel framework that enhances and refines textual\nsemantic representations by leveraging the intermediate-layer features of\nCLIP's vision encoder. We designed an Attribute Retrieval module that first\nclusters visual features from each layer. The aggregated visual features\nretrieve semantically similar prompts from a prompt pool, which are then\nconcatenated to the input of every layer in the text encoder. Leveraging\nhierarchical visual information embedded in prompted text features, we\nintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.\nFurthermore, we introduce a Self-Regularization mechanism by applying explicit\nregularization constraints between the prompted and non-prompted text features\nto prevent overfitting on limited training data. Extensive experiments across\nthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-art\nmethods, achieving up to 7.37\\% improvement in the base-to-novel setting. The\nobserved strength of our method in cross-domain knowledge transfer positions\nvision-language pre-trained models as more viable solutions for real-world\nimplementation.", "AI": {"tldr": "The paper presents AttriPrompt, a novel framework that enhances performance in model prompting by addressing limitations in current methods, leveraging CLIP's vision encoder, and introducing innovative mechanisms to improve alignment and prevent overfitting.", "motivation": "Current prompt learning methods rely heavily on contrastive learning objectives, neglecting fine-grained feature optimization, and use static prompts that fail to adapt to varied input categories.", "method": "The authors propose AttriPrompt, which uses an Attribute Retrieval module to cluster visual features and retrieve adaptive prompts for text encoders. It incorporates hierarchical visual features and uses Dual-stream Contrastive Learning along with a Self-Regularization mechanism to improve semantic alignment and prevent overfitting.", "result": "AttriPrompt demonstrates superiority in experiments, with up to 7.37% improvement in base-to-novel settings and strong performance in cross-domain knowledge transfer.", "conclusion": "AttriPrompt addresses key limitations in text prompt methodologies, enhances semantic representation, and establishes vision-language pre-trained models as effective tools for practical applications."}}
{"id": "2509.06332", "pdf": "https://arxiv.org/pdf/2509.06332", "abs": "https://arxiv.org/abs/2509.06332", "authors": ["Roussel Rahman", "Aashwin Ananda Mishra"], "title": "A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable emergent\ncapabilities, yet the robustness of their numerical reasoning remains an open\nquestion. While standard benchmarks evaluate LLM reasoning on complex problem\nsets using aggregated metrics, they often obscure foundational weaknesses. In\nthis work, we probe LLM mathematical numeracy by evaluating performance on\nproblems of escalating complexity, from constituent operations to combinatorial\npuzzles. We test several state-of-the-art LLM-based agents on a 100-problem\nchallenge comprising four categories: (1) basic arithmetic, (2) advanced\noperations, (3) primality checking, and (4) the Game of 24 number puzzle. Our\nresults show that while the agents achieved high accuracy on the first three\ncategories, which require deterministic algorithmic execution, they\nconsistently failed at the number puzzle, underlining its demand for a\nheuristic search over a large combinatorial space to be a significant\nbottleneck. These findings reveal that the agents' proficiency is largely\nconfined to recalling and executing known algorithms, rather than performing\ngenerative problem-solving. This suggests their apparent numerical reasoning is\nmore akin to sophisticated pattern-matching than flexible, analytical thought,\nlimiting their potential for tasks that require novel or creative numerical\ninsights.", "AI": {"tldr": "The paper evaluates the numerical reasoning abilities of Large Language Models (LLMs), finding they excel in algorithmic tasks but struggle with generative problem-solving.", "motivation": "To better understand the robustness and limitations of LLMs in numerical reasoning, a critical component of their reasoning capabilities.", "method": "The study tests state-of-the-art LLM-based agents on a 100-problem challenge over four categories ranging from basic arithmetic to complex combinatorial puzzles.", "result": "LLMs perform well in deterministic algorithmic tasks but fail consistently at combinatorial puzzles, highlighting a lack of generative problem-solving ability.", "conclusion": "LLMs rely on recalling known algorithms and pattern-matching rather than flexible, analytical reasoning, which limits their potential in tasks requiring creative numerical insights."}}
{"id": "2509.06870", "pdf": "https://arxiv.org/pdf/2509.06870", "abs": "https://arxiv.org/abs/2509.06870", "authors": ["Wenting Zhao", "Pranjal Aggarwal", "Swarnadeep Saha", "Asli Celikyilmaz", "Jason Weston", "Ilia Kulikov"], "title": "The Majority is not always right: RL training for solution aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.", "AI": {"tldr": "The paper introduces a method for aggregating multiple solutions to reasoning tasks using reinforcement learning, termed AggLM, which outperforms existing majority voting and reward-based methods.", "motivation": "Current methods for combining multiple solutions to reasoning tasks, like majority voting or reward model rankings, have limited effectiveness.", "method": "The proposal involves an aggregator model using reinforcement learning. It is trained on verifiable rewards with balanced examples to reconcile and synthesize a final answer.", "result": "AggLM outperformed rule-based and reward-model-based baselines on various benchmarks, working effectively across solutions from different models and requiring fewer tokens.", "conclusion": "Learning aggregation as a reasoning skill presents a more effective method for improving performance on reasoning tasks, especially under constraints like fewer tokens used."}}
{"id": "2509.05318", "pdf": "https://arxiv.org/pdf/2509.05318", "abs": "https://arxiv.org/abs/2509.05318", "authors": ["Zuquan Peng", "Jianming Fu", "Lixin Zou", "Li Zheng", "Yanzhen Ren", "Guojun Peng"], "title": "Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 9 figures, 8 tables, journal", "summary": "The use of unvetted third-party and internet data renders pre-trained models\nsusceptible to backdoor attacks. Detecting backdoor samples is critical to\nprevent backdoor activation during inference or injection during training.\nHowever, existing detection methods often require the defender to have access\nto the poisoned models, extra clean samples, or significant computational\nresources to detect backdoor samples, limiting their practicality. To address\nthis limitation, we propose a backdoor sample detection method based on\nperturbatio\\textbf{N} discr\\textbf{E}pancy consis\\textbf{T}ency\n\\textbf{E}valuation (\\NETE). This is a novel detection method that can be used\nboth pre-training and post-training phases. In the detection process, it only\nrequires an off-the-shelf pre-trained model to compute the log probability of\nsamples and an automated function based on a mask-filling strategy to generate\nperturbations. Our method is based on the interesting phenomenon that the\nchange in perturbation discrepancy for backdoor samples is smaller than that\nfor clean samples. Based on this phenomenon, we use curvature to measure the\ndiscrepancy in log probabilities between different perturbed samples and input\nsamples, thereby evaluating the consistency of the perturbation discrepancy to\ndetermine whether the input sample is a backdoor sample. Experiments conducted\non four typical backdoor attacks and five types of large language model\nbackdoor attacks demonstrate that our detection strategy outperforms existing\nzero-shot black-box detection methods.", "AI": {"tldr": "The paper introduces NETE, a novel method to detect backdoor attacks in pre-trained models without requiring extensive resources or model access.", "motivation": "Pre-trained models are increasingly susceptible to backdoor attacks, and existing detection methods often demand significant resources, limiting their practicality.", "method": "The proposed NETE method utilizes perturbation discrepancy consistency, log probability calculations from a pre-trained model, and automated mask-filling perturbations to identify backdoor samples, usable both pre- and post-training.", "result": "Experimental results on four typical backdoor attacks and five large language model backdoor attacks show that NETE is more effective than existing zero-shot black-box detection methods.", "conclusion": "NETE provides a practical and efficient means to detect backdoor attacks, reducing reliance on computational and data resources."}}
{"id": "2509.05952", "pdf": "https://arxiv.org/pdf/2509.05952", "abs": "https://arxiv.org/abs/2509.05952", "authors": ["Feng Wang", "Zihao Yu"], "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching", "categories": ["cs.CV"], "comment": "work in progress", "summary": "Reinforcement Learning (RL) has recently emerged as a powerful technique for\nimproving image and video generation in Diffusion and Flow Matching models,\nspecifically for enhancing output quality and alignment with prompts. A\ncritical step for applying online RL methods on Flow Matching is the\nintroduction of stochasticity into the deterministic framework, commonly\nrealized by Stochastic Differential Equation (SDE). Our investigation reveals a\nsignificant drawback to this approach: SDE-based sampling introduces pronounced\nnoise artifacts in the generated images, which we found to be detrimental to\nthe reward learning process. A rigorous theoretical analysis traces the origin\nof this noise to an excess of stochasticity injected during inference. To\naddress this, we draw inspiration from Denoising Diffusion Implicit Models\n(DDIM) to reformulate the sampling process. Our proposed method,\nCoefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This\nleads to more accurate reward modeling, ultimately enabling faster and more\nstable convergence for reinforcement learning-based optimizers like Flow-GRPO\nand Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS", "AI": {"tldr": "The paper identifies noise artifact issues in SDE-based sampling for RL in Flow Matching models and introduces Coefficients-Preserving Sampling (CPS) to eliminate this, enhancing image generation quality.", "motivation": "Recent applications of Reinforcement Learning to improve image and video quality in Diffusion and Flow Matching models face challenges from noise caused by SDE-based sampling, hindering reward learning.", "method": "The authors analyze the cause of noise artifacts associated with SDE sampling in Flow Matching, propose a new sampling method inspired by Denoising Diffusion Implicit Models (DDIM), and introduce Coefficients-Preserving Sampling (CPS).", "result": "CPS eliminates noise artifacts, enables more accurate reward modeling, and ensures faster and more stable convergence in RL-based optimizations such as Flow-GRPO and Dance-GRPO.", "conclusion": "The introduction of CPS improves the alignment of Flow Matching with RL objectives, providing a significant advancement in image generation tasks. The method will be made publicly available."}}
{"id": "2509.06346", "pdf": "https://arxiv.org/pdf/2509.06346", "abs": "https://arxiv.org/abs/2509.06346", "authors": ["Yuanteng Chen", "Peisong Wang", "Yuantian Shao", "Jian Cheng"], "title": "Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 9 figures", "summary": "Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling\nlarge language models (LLMs) efficiently. Recent fine-grained MoE designs\nintroduce hundreds of experts per layer, with multiple experts activated per\ntoken, enabling stronger specialization. However, during pre-training, routers\nare optimized mainly for stability and robustness: they converge prematurely\nand enforce balanced usage, limiting the full potential of model performance\nand efficiency. In this work, we uncover two overlooked issues: (i) a few\nhighly influential experts are underutilized due to premature and balanced\nrouting decisions; and (ii) enforcing a fixed number of active experts per\ntoken introduces substantial redundancy. Instead of retraining models or\nredesigning MoE architectures, we introduce Ban&Pick, a post-training,\nplug-and-play strategy for smarter MoE routing. Pick discovers and reinforces\nkey experts-a small group with outsized impact on performance-leading to\nnotable accuracy gains across domains. Ban complements this by dynamically\npruning redundant experts based on layer and token sensitivity, delivering\nfaster inference with minimal accuracy loss. Experiments on fine-grained\nMoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks\ndemonstrate that Ban&Pick delivers free performance gains and inference\nacceleration without retraining or architectural changes. For instance, on\nQwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from\n65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the\nvLLM.", "AI": {"tldr": "The paper introduces Ban&Pick, a post-training strategy for optimizing Sparse Mixture-of-Experts (MoE) language models, resolving issues with premature routing decisions and redundant expert activations to improve performance and efficiency.", "motivation": "To address inefficiencies in current fine-grained MoE architectures, specifically the premature convergence of routers and unnecessary redundancy in activating a fixed number of experts per token, which limit model performance and inference efficiency.", "method": "The proposed Ban&Pick strategy involves two components: (1) \"Pick,\" which identifies and reinforces the most impactful experts to enhance accuracy, and (2) \"Ban,\" which dynamically prunes redundant experts to expedite inference without retraining or architectural modifications.", "result": "Experiments on MoE-based large language models, such as DeepSeek and Qwen3, show that Ban&Pick improves accuracy across math, coding, and reasoning benchmarks by notable margins while also accelerating inference by up to 1.25x.", "conclusion": "Ban&Pick is an effective, plug-and-play strategy for enhancing model performance and inference speed in fine-grained MoE-LLMs, solving routing inefficiencies without the need for retraining or architectural changes."}}
{"id": "2509.06660", "pdf": "https://arxiv.org/pdf/2509.06660", "abs": "https://arxiv.org/abs/2509.06660", "authors": ["Cailei Liang", "Adrian Bodenmann", "Emma J Curtis", "Samuel Simmons", "Kazunori Nagano", "Stan Brown", "Adam Riese", "Blair Thornton"], "title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.", "AI": {"tldr": "The paper examines how location metadata can enhance self-supervised learning (SSL) frameworks for analyzing seafloor images, finding consistent performance improvement with location-based regularization, particularly for low-dimensional representations.", "motivation": "Marine monitoring and exploration through seafloor imagery requires efficient image interpretation methods. Incorporating location metadata into SSL may enhance feature learning but its effects across SSL strategies and datasets remain unclear.", "method": "Researchers tested the influence of location-based regularization on six SSL frameworks using CNN and ViT models with varying latent-space dimensionalities. They evaluated performance across three seafloor image datasets.", "result": "Location regularization improved classification performance: CNNs F1-score gain 4.9 \u00b1 4.0% and ViTs 6.3 \u00b1 8.9%. Low-dimensional latent representations especially benefitted, with CNN gains up to 10.1 \u00b1 9.4%. High-dimensional ViTs showed strong generalization even without regularization.", "conclusion": "Location metadata significantly boosts SSL performance, especially for models with low-dimensional representations. Pre-trained and high-dimensional ViTs exhibit robust generalization, making them competitive even without location-based enhancements."}}
{"id": "2509.06883", "pdf": "https://arxiv.org/pdf/2509.06883", "abs": "https://arxiv.org/abs/2509.06883", "authors": ["Joe Wilder", "Nikhil Kadapala", "Benji Xu", "Mohammed Alsaadi", "Aiden Parsons", "Mitchell Rogers", "Palash Agarwal", "Adam Hassick", "Laura Dietz"], "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain", "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.", "AI": {"tldr": "The paper explores different approaches to extract check-worthy claims from social media using various prompting and LLM techniques. Best results are obtained using FLAN-T5 fine-tuning, but some methods produce higher-quality claims despite scoring lower.", "motivation": "To improve the extraction of check-worthy claims from social media passages for better fact-checking.", "method": "Exploring few-shot prompting, fine-tuning with different LLMs, and evaluating using METEOR score.", "result": "Best METEOR score achieved with fine-tuned FLAN-T5. However, other methods sometimes provide better qualitative results.", "conclusion": "Different approaches to LLM utilization trade-off between METEOR scores and qualitative claim quality. Fine-tuned FLAN-T5 offers the best quantitative performance, but careful consideration of other methods is valuable."}}
{"id": "2509.05953", "pdf": "https://arxiv.org/pdf/2509.05953", "abs": "https://arxiv.org/abs/2509.05953", "authors": ["Jeonghyun Noh", "Wangsu Jeon", "Jinsun Park"], "title": "Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "16pages", "summary": "Medical image segmentation is a crucial method for assisting professionals in\ndiagnosing various diseases through medical imaging. However, various factors\nsuch as noise, blurriness, and low contrast often hinder the accurate diagnosis\nof diseases. While numerous image enhancement techniques can mitigate these\nissues, they may also alter crucial information needed for accurate diagnosis\nin the original image. Conventional image fusion strategies, such as feature\nconcatenation can address this challenge. However, they struggle to fully\nleverage the advantages of both original and enhanced images while suppressing\nthe side effects of the enhancements. To overcome the problem, we propose a\ndual interactive fusion module (DIFM) that effectively exploits mutual\ncomplementary information from the original and enhanced images. DIFM employs\ncross-attention bidirectionally to simultaneously attend to corresponding\nspatial information across different images, subsequently refining the\ncomplementary features via global spatial attention. This interaction leverages\nlow- to high-level features implicitly associated with diverse structural\nattributes like edges, blobs, and object shapes, resulting in enhanced features\nthat embody important spatial characteristics. In addition, we introduce a\nmulti-scale boundary loss based on gradient extraction to improve segmentation\naccuracy at object boundaries. Experimental results on the ACDC and Synapse\ndatasets demonstrate the superiority of the proposed method quantitatively and\nqualitatively. Code available at: https://github.com/JJeong-Gari/DIN", "AI": {"tldr": "The paper presents a method to enhance medical image segmentation using a Dual Interactive Fusion Module (DIFM) and introduces multi-scale boundary loss to address boundary accuracy issues.", "motivation": "To improve medical image segmentation accuracy hindered by factors like noise, blurriness, and low contrast, while addressing limitations in existing image enhancement and fusion techniques.", "method": "The proposed method introduces the Dual Interactive Fusion Module (DIFM), which uses bidirectional cross-attention to combine complementary information from original and enhanced images, and incorporates a multi-scale boundary loss leveraging gradient extraction.", "result": "The proposed approach showed superior performance on the ACDC and Synapse datasets both quantitatively and qualitatively.", "conclusion": "The study demonstrates that DIFM and the multi-scale boundary loss can effectively enhance image segmentation by addressing challenges related to noisy and low-contrast features, while preserving crucial diagnostic details."}}
{"id": "2509.06371", "pdf": "https://arxiv.org/pdf/2509.06371", "abs": "https://arxiv.org/abs/2509.06371", "authors": ["Victor Guyomard", "Mathis Mauvisseau", "Marie Paindavoine"], "title": "Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment", "categories": ["cs.LG"], "comment": null, "summary": "Due to hardware and software improvements, an increasing number of AI models\nare deployed on-device. This shift enhances privacy and reduces latency, but\nalso introduces security risks distinct from traditional software. In this\narticle, we examine these risks through the real-world case study of\nSafetyCore, an Android system service incorporating sensitive image content\ndetection. We demonstrate how the on-device AI model can be extracted and\nmanipulated to bypass detection, effectively rendering the protection\nineffective. Our analysis exposes vulnerabilities of on-device AI models and\nprovides a practical demonstration of how adversaries can exploit them.", "AI": {"tldr": "This paper investigates security vulnerabilities in on-device AI models, using the Android SafetyCore system as a case study, showing how adversaries can bypass sensitive content detection.", "motivation": "To explore the security risks introduced by deploying AI models on devices instead of traditional cloud-based approaches.", "method": "The study analyzed the SafetyCore Android system service to extract and manipulate its on-device AI model, demonstrating real-world exploitation.", "result": "It was shown that on-device AI models can be manipulated to bypass detection mechanisms, rendering the security protections ineffective.", "conclusion": "On-device AI models enhance privacy and reduce latency but have unique security vulnerabilities that adversaries can exploit, highlighting the need for improved protection mechanisms."}}
{"id": "2509.06678", "pdf": "https://arxiv.org/pdf/2509.06678", "abs": "https://arxiv.org/abs/2509.06678", "authors": ["Cailei Liang", "Adrian Bodenmann", "Sam Fenton", "Blair Thornton"], "title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.", "AI": {"tldr": "The paper proposes an online clustering framework (OCF) for real-time, unsupervised interpretation of seafloor imagery in adaptive AUV missions.", "motivation": "To address the lack of tools for real-time interpretation of seafloor imagery in AUVs, where offline image analysis methods relying on complete datasets and human labeling fall short.", "method": "The OCF identifies and maintains representative samples from data streams, enabling dynamic clustering in real-time while avoiding the need to reprocess the entire image history. It supports dynamic cluster splitting/merging and is scalable.", "result": "OCF outperformed other online clustering approaches, achieving an average F1 score of 0.68 across three datasets with bounded computational time, demonstrating robustness to trajectory variation.", "conclusion": "The OCF framework offers a scalable, efficient solution for real-time seafloor imagery analysis, benefiting adaptive missions and long-term autonomous marine exploration."}}
{"id": "2509.06888", "pdf": "https://arxiv.org/pdf/2509.06888", "abs": "https://arxiv.org/abs/2509.06888", "authors": ["Marc Marone", "Orion Weller", "William Fleshman", "Eugene Yang", "Dawn Lawrie", "Benjamin Van Durme"], "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.", "AI": {"tldr": "The paper introduces mmBERT, a multilingual encoder-only language model that significantly improves classification and retrieval tasks, particularly for low-resource languages.", "motivation": "To address the lack of recent advancements and research focus on multilingual encoder-only language models, particularly for low-resource languages.", "method": "The authors pretrain mmBERT on 3T tokens across 1800 languages, introducing innovations like an inverse mask ratio schedule and inverse temperature sampling ratio. They strategically add low-resource languages during a decay phase to maximize performance improvements.", "result": "mmBERT achieves comparable results to prominent models like OpenAI's o3 and Google's Gemini 2.5 Pro in classification while outperforming previous-generation multilingual models on classification and retrieval tasks. It also demonstrates significant improvements in low-resource language tasks.", "conclusion": "The study proposes an effective strategy for multilingual language model training by incorporating novel methods to optimize performance, particularly for low-resource languages."}}
{"id": "2509.05326", "pdf": "https://arxiv.org/pdf/2509.05326", "abs": "https://arxiv.org/abs/2509.05326", "authors": ["Logan Nye"], "title": "Zero-Knowledge Proofs in Sublinear Space", "categories": ["cs.CR", "cs.AI"], "comment": "21 pages", "summary": "Modern zero-knowledge proof (ZKP) systems, essential for privacy and\nverifiable computation, suffer from a fundamental limitation: the prover\ntypically uses memory that scales linearly with the computation's trace length\nT, making them impractical for resource-constrained devices and prohibitively\nexpensive for large-scale tasks. This paper overcomes this barrier by\nconstructing, to our knowledge, the first sublinear-space ZKP prover. Our core\ncontribution is an equivalence that reframes proof generation as an instance of\nthe classic Tree Evaluation problem. Leveraging a recent space-efficient\ntree-evaluation algorithm, we design a streaming prover that assembles the\nproof without ever materializing the full execution trace. The approach reduces\nprover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)\nwhile preserving proof size, verifier time, and the transcript/security\nguarantees of the underlying system. This enables a shift from specialized,\nserver-bound proving to on-device proving, opening applications in\ndecentralized systems, on-device machine learning, and privacy-preserving\ntechnologies.", "AI": {"tldr": "The paper introduces a sublinear-space ZKP prover reducing memory usage from linear to O(sqrt(T)), enabling on-device applications.", "motivation": "The aim is to address the impractical memory requirements of ZKP systems, which hinder their use in resource-constrained devices and large-scale computations.", "method": "The authors leverage an equivalence to the Tree Evaluation problem and utilize a space-efficient tree-evaluation algorithm to design a streaming prover.", "result": "The proposed method decreases prover memory usage to O(sqrt(T)) while preserving proof size, verifier time, and security guarantees.", "conclusion": "This sublinear-space approach facilitates on-device proving and broadens the applications of ZKP systems in decentralized systems, ML, and privacy technologies."}}
{"id": "2509.05954", "pdf": "https://arxiv.org/pdf/2509.05954", "abs": "https://arxiv.org/abs/2509.05954", "authors": ["Weichao Wang", "Wendong Mao", "Zhongfeng Wang"], "title": "StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud", "categories": ["cs.CV"], "comment": null, "summary": "The deployment of high-accuracy 3D object detection models from point cloud\nremains a significant challenge due to their substantial computational and\nmemory requirements. To address this, we introduce StripDet, a novel\nlightweight framework designed for on-device efficiency. First, we propose the\nnovel Strip Attention Block (SAB), a highly efficient module designed to\ncapture long-range spatial dependencies. By decomposing standard 2D\nconvolutions into asymmetric strip convolutions, SAB efficiently extracts\ndirectional features while reducing computational complexity from quadratic to\nlinear. Second, we design a hardware-friendly hierarchical backbone that\nintegrates SAB with depthwise separable convolutions and a simple multiscale\nfusion strategy, achieving end-to-end efficiency. Extensive experiments on the\nKITTI dataset validate StripDet's superiority. With only 0.65M parameters, our\nmodel achieves a 79.97% mAP for car detection, surpassing the baseline\nPointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms\nrecent lightweight and knowledge distillation-based methods, achieving a\nsuperior accuracy-efficiency trade-off while establishing itself as a practical\nsolution for real-world 3D detection on edge devices.", "AI": {"tldr": "StripDet introduces a lightweight framework for 3D object detection utilizing point clouds, focusing on efficiency via the novel Strip Attention Block and hierarchical backbone, achieving superior performance with reduced computational load.", "motivation": "The paper addresses the challenge of deploying high-accuracy 3D object detection models on edge devices, constrained by computational and memory requirements.", "method": "The study proposes the Strip Attention Block (SAB), which uses asymmetric strip convolutions to efficiently capture long-range spatial dependencies, reducing computational complexity. Additionally, it employs a hierarchical backbone with depthwise separable convolutions and a multiscale fusion method.", "result": "Experiments on the KITTI dataset show StripDet achieving 79.97% mAP for car detection using only 0.65M parameters, a significant improvement over PointPillars with 7x fewer parameters.", "conclusion": "StripDet provides a robust and efficient solution for 3D object detection on edge devices, outperforming existing methods and establishing a new standard for practical applications in accuracy-efficiency trade-offs."}}
{"id": "2509.06383", "pdf": "https://arxiv.org/pdf/2509.06383", "abs": "https://arxiv.org/abs/2509.06383", "authors": ["Hyungjoon Soh", "Dongha Lee", "Vipul Periwal", "Junghyo Jo"], "title": "Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection", "categories": ["cs.LG", "physics.data-an"], "comment": "11 pages, 4 figures", "summary": "Selecting key variables from high-dimensional data is increasingly important\nin the era of big data. Sparse regression serves as a powerful tool for this\npurpose by promoting model simplicity and explainability. In this work, we\nrevisit a valuable yet underutilized method, the statistical physics-based\nVariational Garrote (VG), which introduces explicit feature selection spin\nvariables and leverages variational inference to derive a tractable loss\nfunction. We enhance VG by incorporating modern automatic differentiation\ntechniques, enabling scalable and efficient optimization. We evaluate VG on\nboth fully controllable synthetic datasets and complex real-world datasets. Our\nresults demonstrate that VG performs especially well in highly sparse regimes,\noffering more consistent and robust variable selection than Ridge and LASSO\nregression across varying levels of sparsity. We also uncover a sharp\ntransition: as superfluous variables are admitted, generalization degrades\nabruptly and the uncertainty of the selection variables increases. This\ntransition point provides a practical signal for estimating the correct number\nof relevant variables, an insight we successfully apply to identify key\npredictors in real-world data. We expect that VG offers strong potential for\nsparse modeling across a wide range of applications, including compressed\nsensing and model pruning in machine learning.", "AI": {"tldr": "This paper revisits and enhances the Variational Garrote (VG) method for sparse regression, demonstrating its effectiveness for variable selection in high-dimensional data.", "motivation": "To improve variable selection in high-dimensional datasets by leveraging an underutilized statistical physics-based method, the Variational Garrote, and addressing its scalability and efficiency issues.", "method": "Enhanced the Variational Garrote (VG) method by incorporating modern automatic differentiation techniques to optimize its performance and scalability. Evaluated using both synthetic and real-world datasets.", "result": "VG demonstrated superior performance in highly sparse regimes compared to Ridge and LASSO regression, with consistent and robust variable selection. Also, discovered a transition point where superfluous variables degrade generalization and increase uncertainty metrics.", "conclusion": "The enhanced VG method offers robust and scalable sparse regression, with practical applications in detecting key predictors in datasets. It holds potential for various tasks, such as compressed sensing and model pruning."}}
{"id": "2509.06902", "pdf": "https://arxiv.org/pdf/2509.06902", "abs": "https://arxiv.org/abs/2509.06902", "authors": ["Aivin V. Solatorio"], "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "AI": {"tldr": "The paper introduces Proof-Carrying Numbers (PCN), a system ensuring numeric fidelity by verifying presented numbers through structured claims and policies.", "motivation": "There is a growing problem of numeric hallucination in large language models (LLMs), where generated numbers deviate from factual data, undermining trust in numeric output.", "method": "The authors propose Proof-Carrying Numbers (PCN), a presentation-layer protocol that ties numeric outputs to structured claims and enforces their verification via mechanical checks through predefined policies.", "result": "PCN is demonstrated to be lightweight, model-agnostic, and capable of ensuring numeric fidelity by marking verified numbers and treating others with uncertainty. It supports sound validation and fail-safe behavior.", "conclusion": "By enforcing mandatory verification of numeric claims before displaying output, PCN provides a reliable mechanism for ensuring numeric accuracy in LLM-generated content, promoting trust through provable correctness."}}
{"id": "2509.05963", "pdf": "https://arxiv.org/pdf/2509.05963", "abs": "https://arxiv.org/abs/2509.05963", "authors": ["Rafal Karp", "Dawid Gruszka", "Tomasz Trzcinski"], "title": "Neural Bloom: A Deep Learning Approach to Real-Time Lighting", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel method to generate bloom lighting effect in real time\nusing neural networks. Our solution generate brightness mask from given 3D\nscene view up to 30% faster than state-of-the-art methods. The existing\ntraditional techniques rely on multiple blur appliances and texture sampling,\nalso very often have existing conditional branching in its implementation.\nThese operations occupy big portion of the execution time. We solve this\nproblem by proposing two neural network-based bloom lighting methods, Neural\nBloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on\ntheir quality and performance. Both methods were tested on a variety of 3D\nscenes, with evaluations conducted on brightness mask accuracy and inference\nspeed. The main contribution of this work is that both methods produce\nhigh-quality bloom effects while outperforming the standard state-of-the-art\nbloom implementation, with FastNBL being faster by 28% and NBL faster by 12%.\nThese findings highlight that we can achieve realistic bloom lighting phenomena\nfaster, moving us towards more realism in real-time environments in the future.\nThis improvement saves computational resources, which is a major bottleneck in\nreal-time rendering. Furthermore, it is crucial for sustaining immersion and\nensuring smooth experiences in high FPS environments, while maintaining\nhigh-quality realism.", "AI": {"tldr": "This paper introduces two neural network methods for generating bloom lighting in real-time, achieving up to 30% speed improvements over traditional techniques.", "motivation": "To address the inefficiencies of traditional bloom lighting techniques, which heavily rely on computationally expensive operations like multiple blur passes and texture sampling.", "method": "Two methods\u2014Neural Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL)\u2014are proposed, leveraging neural networks to achieve higher accuracy and faster computation speeds.", "result": "FastNBL offers a 28% speed boost and NBL a 12% speed boost over current state-of-the-art without sacrificing quality.", "conclusion": "The proposed methods offer a significant step towards achieving realistic bloom lighting faster, saving computational resources and improving immersion in high FPS environments."}}
{"id": "2509.06385", "pdf": "https://arxiv.org/pdf/2509.06385", "abs": "https://arxiv.org/abs/2509.06385", "authors": ["Senhao Liu", "Zhiyu Guo", "Zhiyuan Ji", "Yueguo Chen", "Yateng Tang", "Yunhai Wang", "Xuehao Zheng", "Xiang Ao"], "title": "Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IEEE ICDM 2025", "summary": "Typical financial risk management involves distinct phases for pre-service\nrisk assessment and in-service default detection, often modeled separately.\nThis paper proposes a novel framework, Multi-Granularity Knowledge Distillation\n(abbreviated as MGKD), aimed at improving pre-service risk prediction through\nthe integration of in-service user behavior data. MGKD follows the idea of\nknowledge distillation, where the teacher model, trained on historical\nin-service data, guides the student model, which is trained on pre-service\ndata. By using soft labels derived from in-service data, the teacher model\nhelps the student model improve its risk prediction prior to service\nactivation. Meanwhile, a multi-granularity distillation strategy is introduced,\nincluding coarse-grained, fine-grained, and self-distillation, to align the\nrepresentations and predictions of the teacher and student models. This\napproach not only reinforces the representation of default cases but also\nenables the transfer of key behavioral patterns associated with defaulters from\nthe teacher to the student model, thereby improving the overall performance of\npre-service risk assessment. Moreover, we adopt a re-weighting strategy to\nmitigate the model's bias towards the minority class. Experimental results on\nlarge-scale real-world datasets from Tencent Mobile Payment demonstrate the\neffectiveness of our proposed approach in both offline and online scenarios.", "AI": {"tldr": "The paper introduces the Multi-Granularity Knowledge Distillation (MGKD) framework, which enhances pre-service financial risk prediction by integrating behavioral data from in-service phases, using a teacher-student model structure.", "motivation": "To overcome the limitations of separate modeling phases for pre-service risk assessment and in-service default detection by leveraging the behavioral data observed during in-service phases.", "method": "The MGKD framework employs knowledge distillation where a teacher model uses in-service data to guide a student model trained on pre-service data. It incorporates a multi-granularity strategy and re-weighting to improve prediction and mitigate bias towards minority classes.", "result": "Experiments on large-scale datasets from Tencent Mobile Payment show MGKD improves pre-service risk assessment performance effectively in both offline and online scenarios.", "conclusion": "Integrating multi-granularity distillation and behavioral data transfer enhances financial risk prediction, enabling more accurate pre-service risk assessment and addressing data imbalance issues."}}
{"id": "2509.06741", "pdf": "https://arxiv.org/pdf/2509.06741", "abs": "https://arxiv.org/abs/2509.06741", "authors": ["Christian Geckeler", "Niklas Neugebauer", "Manasi Muglikar", "Davide Scaramuzza", "Stefano Mintchev"], "title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light", "categories": ["cs.CV", "cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.", "AI": {"tldr": "This paper introduces a novel event spectroscopy system for UAVs, offering high-resolution and low-latency depth reconstruction and multispectral imaging to enhance navigation and data collection in dense forest environments.", "motivation": "Current UAV sensing systems in forested environments struggle with issues like latency, poor depth resolution, and ambient light dependence, making efficient navigation and precise data collection difficult.", "method": "The system uses structured light for depth reconstruction and modulates light wavelengths to capture multispectral data. It was validated against commercial sensors and spectrometers, and a portable RGB version was used in a Masoala Rainforest.", "result": "The new system showed a 60% RMSE improvement in depth sensing, comparable spectral accuracy to commercial tools, and over 30% accuracy improvement in material differentiation when combining depth and spectral data.", "conclusion": "This system effectively enhances UAV perception by integrating depth and spectral imaging, proving its robustness and utility in complex forest environments."}}
{"id": "2509.06948", "pdf": "https://arxiv.org/pdf/2509.06948", "abs": "https://arxiv.org/abs/2509.06948", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.", "AI": {"tldr": "The paper addresses the inefficiency of traditional reinforcement learning (RL) approaches for training large language models (LLMs) and proposes a bilevel optimization method to better integrate supervised fine-tuning (SFT) with RL. This improves both effectiveness and efficiency in reasoning tasks.", "motivation": "The motivation is to overcome efficiency challenges and enhance reasoning performance in large language models (LLMs) by addressing the limitations of the traditional two-stage SFT and RL approach.", "method": "The proposed method uses bilevel optimization, where the SFT objective is conditioned on the optimal RL policy. SFT learns to guide RL's optimization process in a cooperative training setup, with RL updates and SFT supervision integrated during training.", "result": "The method achieves superior performance on five reasoning benchmarks compared to traditional RL and SFT approaches, showcasing improved balance between effectiveness and efficiency.", "conclusion": "The study proves that a cooperative bilevel optimization framework can effectively integrate SFT and RL, leading to better reasoning capabilities in LLMs with fewer inefficiencies."}}
{"id": "2509.05331", "pdf": "https://arxiv.org/pdf/2509.05331", "abs": "https://arxiv.org/abs/2509.05331", "authors": ["Youssef Chakir", "Iyad Lahsen-Cherif"], "title": "ForensicsData: A Digital Forensics Dataset for Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted to WiMob 2025 (21st International Conference on Wireless and\n  Mobile Computing, Networking and Communications), Marrakesh, Morocco, Oct\n  20-22, 2025. 6 pages, 5 figures, 5 tables. IEEEtran conference format", "summary": "The growing complexity of cyber incidents presents significant challenges for\ndigital forensic investigators, especially in evidence collection and analysis.\nPublic resources are still limited because of ethical, legal, and privacy\nconcerns, even though realistic datasets are necessary to support research and\ntool developments. To address this gap, we introduce ForensicsData, an\nextensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware\nanalysis reports. It consists of more than 5,000 Q-C-A triplets. A unique\nworkflow was used to create the dataset, which extracts structured data, uses\nlarge language models (LLMs) to transform it into Q-C-A format, and then uses a\nspecialized evaluation process to confirm its quality. Among the models\nevaluated, Gemini 2 Flash demonstrated the best performance in aligning\ngenerated content with forensic terminology. ForensicsData aims to advance\ndigital forensics by enabling reproducible experiments and fostering\ncollaboration within the research community.", "AI": {"tldr": "This paper introduces ForensicsData, a large Question-Context-Answer (Q-C-A) dataset derived from malware analysis reports to address the lack of realistic datasets for digital forensics research.", "motivation": "The paper is motivated by the need to overcome challenges in digital forensic investigations, particularly the lack of sufficient public datasets due to ethical, legal, and privacy concerns.", "method": "The dataset of over 5,000 Q-C-A triplets was created via a workflow that extracts structured data, transforms it using large language models (LLMs), and validates its quality through specialized evaluations.", "result": "The dataset, ForensicsData, was validated for quality, and among the models used, Gemini 2 Flash showed the best performance in aligning generated content with forensic terminology.", "conclusion": "ForensicsData is designed to advance the field of digital forensics by facilitating reproducible experiments and enhancing collaboration within the forensic research community."}}
{"id": "2509.05967", "pdf": "https://arxiv.org/pdf/2509.05967", "abs": "https://arxiv.org/abs/2509.05967", "authors": ["Yiqin Zhang", "Meiling Chen", "Zhengjie Zhang"], "title": "Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks", "categories": ["cs.CV"], "comment": null, "summary": "The application of self-supervised techniques has become increasingly\nprevalent within medical visualization tasks, primarily due to its capacity to\nmitigate the data scarcity prevalent in the healthcare sector. The majority of\ncurrent works are influenced by designs originating in the generic 2D visual\ndomain, which lack the intuitive demonstration of the model's learning process\nregarding 3D spatial knowledge. Consequently, these methods often fall short in\nterms of medical interpretability. We propose a method consisting of three\nsub-tasks to capture the spatially relevant semantics in medical 3D imaging.\nTheir design adheres to observable principles to ensure interpretability, and\nminimize the performance loss caused thereby as much as possible. By leveraging\nthe enhanced semantic depth offered by the extra dimension in 3D imaging, this\napproach incorporates multi-granularity spatial relationship modeling to\nmaintain training stability. Experimental findings suggest that our approach is\ncapable of delivering performance that is on par with current methodologies,\nwhile facilitating an intuitive understanding of the self-supervised learning\nprocess.", "AI": {"tldr": "The paper introduces a method to improve self-supervised learning for medical 3D imaging, ensuring interpretability and maintaining training performance.", "motivation": "The motivation is to address data scarcity in healthcare and improve the learning of 3D spatial knowledge in medical contexts, which is often lacking in current methods adapted from 2D designs.", "method": "The authors propose a method with three sub-tasks designed to capture spatial semantics in 3D medical imaging, adhering to principles that enhance interpretability and ensure stable training.", "result": "The proposed approach achieves competitive performance compared to existing methods while enhancing the intuitive understanding of the self-supervised process.", "conclusion": "This method provides a balance between interpretability and performance, utilizing 3D imaging's advantages for effective self-supervised learning in medical visualization tasks."}}
{"id": "2509.06395", "pdf": "https://arxiv.org/pdf/2509.06395", "abs": "https://arxiv.org/abs/2509.06395", "authors": ["Lili Chen", "Changyang She", "Jingge Zhu", "Jamie Evans"], "title": "Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Meeting minimum data rate constraints is a significant challenge in wireless\ncommunication systems, particularly as network complexity grows. Traditional\ndeep learning approaches often address these constraints by incorporating\npenalty terms into the loss function and tuning hyperparameters empirically.\nHowever, this heuristic treatment offers no theoretical convergence guarantees\nand frequently fails to satisfy QoS requirements in practical scenarios.\nBuilding upon the structure of the WMMSE algorithm, we first extend it to a\nmulti-channel setting with QoS constraints, resulting in the enhanced WMMSE\n(eWMMSE) algorithm, which is provably convergent to a locally optimal solution\nwhen the problem is feasible. To further reduce computational complexity and\nimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of\nsupporting simultaneous multi-channel allocation per user. To overcome the\nlimitations of traditional deep learning methods, we propose a principled\nframework that integrates GNN with a Lagrangian-based primal-dual optimization\nmethod. By training the GNN within the Lagrangian framework, we ensure\nsatisfaction of QoS constraints and convergence to a stationary point.\nExtensive simulations demonstrate that JCPGNN-M matches the performance of\neWMMSE while offering significant gains in inference speed, generalization to\nlarger networks, and robustness under imperfect channel state information. This\nwork presents a scalable and theoretically grounded solution for constrained\nresource allocation in future wireless networks.", "AI": {"tldr": "The paper addresses QoS constraints in wireless communication using eWMMSE and introduces a scalable GNN-based solution, JCPGNN-M, for improved speed and robustness.", "motivation": "To overcome the challenges in meeting minimum data rate constraints in complex wireless networks, where traditional deep learning methods lack theoretical convergence guarantees and sometimes fail in practical scenarios.", "method": "The authors enhance the WMMSE algorithm to eWMMSE for multi-channel settings with QoS constraints. Additionally, they propose a GNN-based JCPGNN-M algorithm integrated with a Lagrangian-based primal-dual framework for scalable optimization.", "result": "Extensive simulations show that JCPGNN-M matches the performance of eWMMSE while offering faster inference, better scalability to larger networks, and robustness under imperfect data.", "conclusion": "The work provides a theoretically grounded and scalable solution for constrained resource allocation in future wireless networks, ensuring QoS satisfaction and improved efficiency."}}
{"id": "2509.06893", "pdf": "https://arxiv.org/pdf/2509.06893", "abs": "https://arxiv.org/abs/2509.06893", "authors": ["Noble Harasha", "Nancy Lynch"], "title": "Nanobot Algorithms for Treatment of Diffuse Cancer", "categories": ["cs.MA", "cs.RO", "q-bio.QM"], "comment": "Abridged abstract shown here; 34 pages, 9 figures", "summary": "Motile nanosized particles, or \"nanobots\", promise more effective and less\ntoxic targeted drug delivery because of their unique scale and precision. We\nconsider the case in which the cancer is \"diffuse\", dispersed such that there\nare multiple distinct cancer sites. We investigate the problem of a swarm of\nnanobots locating these sites and treating them by dropping drug payloads at\nthe sites. To improve the success of the treatment, the drug payloads must be\nallocated between sites according to their \"demands\"; this requires extra\nnanobot coordination. We present a mathematical model of the behavior of the\nnanobot agents and of their colloidal environment. This includes a movement\nmodel for agents based upon experimental findings from actual nanoparticles in\nwhich bots noisily ascend and descend chemical gradients. We present three\nalgorithms: The first algorithm, called KM, is the most representative of\nreality, with agents simply following naturally existing chemical signals that\nsurround each cancer site. The second algorithm, KMA, includes an additional\nchemical payload which amplifies the existing natural signals. The third\nalgorithm, KMAR, includes another additional chemical payload which counteracts\nthe other signals, instead inducing negative chemotaxis in agents such that\nthey are repelled from sites that are already sufficiently treated. We present\nsimulation results for all algorithms across different types of cancer\narrangements. For KM, we show that the treatment is generally successful unless\nthe natural chemical signals are weak, in which case the treatment progresses\ntoo slowly. For KMA, we demonstrate a significant improvement in treatment\nspeed but a drop in eventual success, except for concentrated cancer patterns.\nFor KMAR, our results show great performance across all types of cancer\npatterns, demonstrating robustness and adaptability.", "AI": {"tldr": "The paper discusses algorithms for nanobot-driven drug delivery systems targeting diffuse cancer sites, comparing their effectiveness in locating and treating cancer sites through simulation results.", "motivation": "Targeted drug delivery via nanosized particles promises efficient and less toxic treatment, particularly for diffuse cancers dispersed across multiple sites.", "method": "Three algorithms were developed for nanobots navigating chemical signals: KM (agents following natural signals), KMA (signal amplification), and KMAR (negative chemotaxis to discourage treatment oversaturation).", "result": "Simulation results show KM works well with strong natural signals but fails with weak ones; KMA speeds up treatment but sacrifices success in some patterns, while KMAR outperforms in robustness and adaptability.", "conclusion": "KMAR is the most effective algorithm, offering robust performance across various cancer patterns, enhancing the precision and adaptability of nanobot-based drug delivery systems."}}
{"id": "2509.06949", "pdf": "https://arxiv.org/pdf/2509.06949", "abs": "https://arxiv.org/abs/2509.06949", "authors": ["Yinjie Wang", "Ling Yang", "Bowen Li", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "categories": ["cs.CL"], "comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL", "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL", "AI": {"tldr": "The paper presents TraceRL, a new reinforcement learning framework tailored for Diffusion Language Models (DLMs), optimizing inference trajectory during post-training for enhanced performance, especially in reasoning tasks.", "motivation": "The motivation behind this paper is the need to improve reasoning accuracy in complex tasks like mathematics and coding, and adapt diffusion-based models for broader applicability and better sampling flexibility.", "method": "The method involves developing TraceRL, which incorporates preferred inference trajectories into post-training using a diffusion-based value model. It also employs curriculum learning strategies and open-source tools to enable reproducible research.", "result": "TraceRL-derived models, specifically TraDo models, demonstrate superior performance in math reasoning benchmarks, achieving significant accuracy improvements over leading models like Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct.", "conclusion": "The effectiveness of TraceRL underscores the potential of trajectory-aware RL frameworks in advancing DLMs' capabilities, with open-source access enabling broader applications and further research."}}
{"id": "2509.05332", "pdf": "https://arxiv.org/pdf/2509.05332", "abs": "https://arxiv.org/abs/2509.05332", "authors": ["Christos Anagnostopoulos", "Ioulia Kapsali", "Alexandros Gkillas", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages, 2 figures", "summary": "Autonomous vehicles (AVs) rely on complex perception and communication\nsystems, making them vulnerable to adversarial attacks that can compromise\nsafety. While simulation offers a scalable and safe environment for robustness\ntesting, existing frameworks typically lack comprehensive supportfor modeling\nmulti-domain adversarial scenarios. This paper introduces a novel, open-source\nintegrated simulation framework designed to generate adversarial attacks\ntargeting both perception and communication layers of AVs. The framework\nprovides high-fidelity modeling of physical environments, traffic dynamics, and\nV2X networking, orchestrating these components through a unified core that\nsynchronizes multiple simulators based on a single configuration file. Our\nimplementation supports diverse perception-level attacks on LiDAR sensor data,\nalong with communication-level threats such as V2X message manipulation and GPS\nspoofing. Furthermore, ROS 2 integration ensures seamless compatibility with\nthird-party AV software stacks. We demonstrate the framework's effectiveness by\nevaluating the impact of generated adversarial scenarios on a state-of-the-art\n3D object detector, revealing significant performance degradation under\nrealistic conditions.", "AI": {"tldr": "The paper presents a novel open-source simulation framework to test autonomous vehicles (AVs) against adversarial attacks targeting their perception and communication layers.", "motivation": "The study aims to address the vulnerability of autonomous vehicles to adversarial attacks by providing better tools for testing resilience in multi-domain scenarios.", "method": "The authors developed a unified simulation framework that integrates high-fidelity modeling of environments, traffic dynamics, V2X networking, and attacks on perception and communication layers, synchronized via a single configuration file.", "result": "The framework effectively simulates adversarial conditions and reveals significant performance degradation in a state-of-the-art 3D object detector under realistic attack scenarios.", "conclusion": "This framework provides a robust tool for evaluating and improving the resilience of autonomous vehicles to multi-domain adversarial attacks, facilitating safer AV deployment."}}
{"id": "2509.05970", "pdf": "https://arxiv.org/pdf/2509.05970", "abs": "https://arxiv.org/abs/2509.05970", "authors": ["Ye Wang", "Zili Yi", "Yibo Zhang", "Peng Zheng", "Xuping Xie", "Jiang Lin", "Yilin Wang", "Rui Ma"], "title": "OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization", "categories": ["cs.CV"], "comment": "Project Page: https://wangyephd.github.io/projects/omnistyle2.html", "summary": "OmniStyle2 introduces a novel approach to artistic style transfer by\nreframing it as a data problem. Our key insight is destylization, reversing\nstyle transfer by removing stylistic elements from artworks to recover natural,\nstyle-free counterparts. This yields DST-100K, a large-scale dataset that\nprovides authentic supervision signals by aligning real artistic styles with\ntheir underlying content. To build DST-100K, we develop (1) DST, a text-guided\ndestylization model that reconstructs stylefree content, and (2) DST-Filter, a\nmulti-stage evaluation model that employs Chain-of-Thought reasoning to\nautomatically discard low-quality pairs while ensuring content fidelity and\nstyle accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward\nmodel based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently\nsurpasses state-of-the-art methods across both qualitative and quantitative\nbenchmarks. Our results demonstrate that scalable data generation via\ndestylization provides a reliable supervision paradigm, overcoming the\nfundamental challenge posed by the lack of ground-truth data in artistic style\ntransfer.", "AI": {"tldr": "OmniStyle2 proposes destylization as a new paradigm in artistic style transfer, creating a large dataset (DST-100K) for improved training. The model outperforms state-of-the-art methods.", "motivation": "The lack of ground-truth data for artistic style transfer is a significant challenge. This paper aims to address this by reframing style transfer as a data problem and creating a dataset via destylization.", "method": "The authors developed DST for reconstructing style-free content and DST-Filter for filtering low-quality data. Using these, they built DST-100K and trained a feed-forward model (OmniStyle2) based on FLUX.1-dev.", "result": "OmniStyle2 demonstrates superior performance over existing methods in both qualitative and quantitative benchmarks.", "conclusion": "Destylization enables scalable and authentic data generation, resolving the issue of missing ground truth in artistic style transfer and advancing the field significantly."}}
{"id": "2509.06402", "pdf": "https://arxiv.org/pdf/2509.06402", "abs": "https://arxiv.org/abs/2509.06402", "authors": ["Yilin Li", "Guozhu Meng", "Mingyang Sun", "Yanzhong Wang", "Kun Sun", "Hailong Chang", "Yuekang Li"], "title": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers.", "AI": {"tldr": "NeuroDeX is a new tool that decompiles deep neural network executables into high-level models using semantic LLMs and dynamic analysis, achieving significant accuracy for both quantized and non-quantized models.", "motivation": "The growing deployment of deep learning models on edge devices creates a need to address the threat of reverse engineering for compiled executables. Existing decompilation methods face challenges in handling optimizations and quantized models.", "method": "NeuroDeX combines the semantic understanding of large language models (LLMs) with dynamic analysis to identify operator types, recover attributes, and reconstruct models from compiled DNN executables.", "result": "Tests on 96 DNN executables across 12 models show that NeuroDeX accurately reconstructs almost identical high-level models for non-quantized executables and achieves 72% top-1 accuracy for quantized models.", "conclusion": "NeuroDeX provides a more robust, accurate, and versatile solution for decompiling DNN executables than previous methods, addressing challenges in both optimized and quantized models."}}
{"id": "2509.06934", "pdf": "https://arxiv.org/pdf/2509.06934", "abs": "https://arxiv.org/abs/2509.06934", "authors": ["Agam Oberlender", "Hadas Erel"], "title": "\"It was Tragic\": Exploring the Impact of a Robot's Shutdown", "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025", "summary": "It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities.", "AI": {"tldr": "The study examines how people perceive robotic gestures during shutdown, revealing that a 'designed' shutdown gesture akin to 'falling asleep' leads to more positive interpretations than a typical robotic shutdown.", "motivation": "The paper explores the human tendency to anthropomorphize robots and aims to understand how robot shutdown gestures influence people's perceptions, emphasizing the need for social consideration in robotic design.", "method": "The experiment involved participants interacting with a robotic arm and turning it off under two conditions: (1) 'Non-designed,' a typical sudden shutdown, and (2) 'Designed,' a gradual shutdown mimicking falling asleep.", "result": "Participants anthropomorphized the robot's shutdown gestures. In the 'Non-designed' condition, responses were mostly negative (interpreting the robot as 'dying'), while the 'Designed' condition evoked neutral and positive responses (interpreting as 'falling asleep'). The latter also enhanced likeability, perceived intelligence, and animacy of the robot.", "conclusion": "Common interactions with robots, like shutdowns, significantly impact user perception. Designers should account for humans' tendency to socially interpret robotic behaviors to foster better robot-human interactions."}}
{"id": "2509.06952", "pdf": "https://arxiv.org/pdf/2509.06952", "abs": "https://arxiv.org/abs/2509.06952", "authors": ["Linlu Qiu", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Yoon Kim", "Roger P. Levy"], "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts", "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main)", "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.", "AI": {"tldr": "This paper evaluates language models' pragmatic reasoning abilities using a communication game framework, highlighting performance differences based on model size and inference strategies.", "motivation": "To assess the capabilities of language models in pragmatic reasoning, conversational contexts, and their alignment with human language processing.", "method": "Evaluation of language models using a framework derived from the communication game Wavelength, employing direct and Chain-of-Thought (CoT) prompting alongside Bayesian pragmatic reasoning (RSA).", "result": "State-of-the-art language models excel in language comprehension with human-like accuracy, while RSA and CoT prompting boost performance in language production tasks.", "conclusion": "LMs display strong pragmatic reasoning abilities at larger scales, with RSA methodology showing promise for enhancing their inference strategies and understanding contextual communication norms further."}}
{"id": "2509.05975", "pdf": "https://arxiv.org/pdf/2509.05975", "abs": "https://arxiv.org/abs/2509.05975", "authors": ["Nam Duong Tran", "Nam Nguyen Phuong", "Hieu H. Pham", "Phi Le Nguyen", "My T. Thai"], "title": "ConstStyle: Robust Domain Generalization with Unified Style Transformation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICCV 2025", "summary": "Deep neural networks often suffer performance drops when test data\ndistribution differs from training data. Domain Generalization (DG) aims to\naddress this by focusing on domain-invariant features or augmenting data for\ngreater diversity. However, these methods often struggle with limited training\ndomains or significant gaps between seen (training) and unseen (test) domains.\nTo enhance DG robustness, we hypothesize that it is essential for the model to\nbe trained on data from domains that closely resemble unseen test domains-an\ninherently difficult task due to the absence of prior knowledge about the\nunseen domains. Accordingly, we propose ConstStyle, a novel approach that\nleverages a unified domain to capture domain-invariant features and bridge the\ndomain gap with theoretical analysis. During training, all samples are mapped\nonto this unified domain, optimized for seen domains. During testing, unseen\ndomain samples are projected similarly before predictions. By aligning both\ntraining and testing data within this unified domain, ConstStyle effectively\nreduces the impact of domain shifts, even with large domain gaps or few seen\ndomains. Extensive experiments demonstrate that ConstStyle consistently\noutperforms existing methods across diverse scenarios. Notably, when only a\nlimited number of seen domains are available, ConstStyle can boost accuracy up\nto 19.82\\% compared to the next best approach.", "AI": {"tldr": "The paper introduces ConstStyle, a method designed to address domain shifts in deep neural networks by mapping data to a unified domain, thus improving performance across unseen test domains.", "motivation": "Deep neural networks struggle when test data distributions differ from training data. Many methods for domain generalization fail in cases of limited training domains or significant gaps between training and testing domains.", "method": "ConstStyle is proposed as a novel approach. It maps all data (both seen and unseen) to a unified domain, optimized during training to capture domain-invariant features. The method aligns training and testing data in this unified domain to reduce domain shifts.", "result": "Experiments show that ConstStyle consistently outperforms other methods in handling domain shifts. It demonstrates remarkable improvements, with accuracy boosts of up to 19.82% when fewer training domains are provided.", "conclusion": "ConstStyle proves effective in improving domain generalization by leveraging a unified domain, significantly addressing the challenges of limited seen domains and large gaps to unseen domains."}}
{"id": "2509.06419", "pdf": "https://arxiv.org/pdf/2509.06419", "abs": "https://arxiv.org/abs/2509.06419", "authors": ["Xudong Mou", "Rui Wang", "Tiejun Wang", "Renyu Yang", "Shiru Chen", "Jie Sun", "Tianyu Wo", "Xudong Liu"], "title": "CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series anomaly detection (TSAD) is a vital yet challenging task,\nparticularly in scenarios where labeled anomalies are scarce and temporal\ndependencies are complex. Recent anomaly assumption (AA) approaches alleviate\nthe lack of anomalies by injecting synthetic samples and training\ndiscriminative models. Despite promising results, these methods often suffer\nfrom two fundamental limitations: patchy generation, where scattered anomaly\nknowledge leads to overly simplistic or incoherent anomaly injection, and\nAnomaly Shift, where synthetic anomalies either resemble normal data too\nclosely or diverge unrealistically from real anomalies, thereby distorting\nclassification boundaries. In this paper, we propose CAPMix, a controllable\nanomaly augmentation framework that addresses both issues. First, we design a\nCutAddPaste mechanism to inject diverse and complex anomalies in a targeted\nmanner, avoiding patchy generation. Second, we introduce a label revision\nstrategy to adaptively refine anomaly labels, reducing the risk of anomaly\nshift. Finally, we employ dual-space mixup within a temporal convolutional\nnetwork to enforce smoother and more robust decision boundaries. Extensive\nexperiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and\nESA, demonstrate that CAPMix achieves significant improvements over\nstate-of-the-art baselines, with enhanced robustness against contaminated\ntraining data. The code is available at https://github.com/alsike22/CAPMix.", "AI": {"tldr": "The paper presents CAPMix, a novel framework for time series anomaly detection (TSAD) that addresses challenges of synthetic anomaly generation and label shifts for better classification.", "motivation": "TSAD is challenging due to limited labeled anomalies and complex temporal dependencies. Current methods face issues like unrealistic synthetic anomalies (Anomaly Shift) and scattered knowledge (Patchy Generation).", "method": "CAPMix introduces a CutAddPaste mechanism for targeted anomaly injection, a label revision strategy to refine anomaly labels, and a dual-space mixup within a temporal convolutional network for robust decision boundaries.", "result": "CAPMix demonstrates significant performance improvements over state-of-the-art methods across five benchmark datasets, showing enhanced robustness even with contaminated training data.", "conclusion": "The CAPMix framework effectively overcomes existing limitations in TSAD by improving anomaly injection and decision boundaries, setting a new benchmark for the field."}}
{"id": "2509.03736", "pdf": "https://arxiv.org/pdf/2509.03736", "abs": "https://arxiv.org/abs/2509.03736", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "25 pages, 9 figures, 7 tables", "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible.", "AI": {"tldr": "This paper evaluates the internal consistency of LLMs and finds significant inconsistencies in agent behavior across settings, questioning their reliability as substitutes for human participants in social science research.", "motivation": "The paper investigates whether synthetic agents, specifically LLMs, can reliably substitute for real human participants in behavioral and social science experiments, an area widely discussed but under-examined for internal consistency.", "method": "The study uses experiments designed to reveal the agent's internal state and assess its behaviors in dialogue settings, testing behavioral hypotheses to evaluate consistency between their inner state and external behavior.", "result": "Findings demonstrate significant internal inconsistencies in LLMs across families and sizes, showing that the agents fail to reliably reflect their internal states in conversation.", "conclusion": "LLMs lack internal consistency, limiting their ability to act as reliable substitutes for human participants in human-subject research."}}
{"id": "2509.05992", "pdf": "https://arxiv.org/pdf/2509.05992", "abs": "https://arxiv.org/abs/2509.05992", "authors": ["Zekun Zhou", "Yanru Gong", "Liu Shi", "Qiegen Liu"], "title": "Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable generative capabilities in\nimage processing tasks. We propose a Sparse condition Temporal Rewighted\nIntegrated Distribution Estimation guided diffusion model (STRIDE) for\nsparse-view CT reconstruction. Specifically, we design a joint training\nmechanism guided by sparse conditional probabilities to facilitate the model\neffective learning of missing projection view completion and global information\nmodeling. Based on systematic theoretical analysis, we propose a temporally\nvarying sparse condition reweighting guidance strategy to dynamically adjusts\nweights during the progressive denoising process from pure noise to the real\nimage, enabling the model to progressively perceive sparse-view information.\nThe linear regression is employed to correct distributional shifts between\nknown and generated data, mitigating inconsistencies arising during the\nguidance process. Furthermore, we construct a dual-network parallel\narchitecture to perform global correction and optimization across multiple\nsub-frequency components, thereby effectively improving the model capability in\nboth detail restoration and structural preservation, ultimately achieving\nhigh-quality image reconstruction. Experimental results on both public and real\ndatasets demonstrate that the proposed method achieves the best improvement of\n2.58 dB in PSNR, increase of 2.37\\% in SSIM, and reduction of 0.236 in MSE\ncompared to the best-performing baseline methods. The reconstructed images\nexhibit excellent generalization and robustness in terms of structural\nconsistency, detail restoration, and artifact suppression.", "AI": {"tldr": "The paper introduces STRIDE, a diffusion model for sparse-view CT reconstruction, focusing on enhanced detail restoration and structural preservation through innovative mechanisms and dual-network architecture.", "motivation": "Sparse-view CT reconstruction is challenging due to missing projection data and the need for effective global image modeling. Current solutions often lack robustness in detail restoration and structural preservation.", "method": "The STRIDE framework incorporates sparse conditional probabilities, temporally varying guidance adjustment strategies, linear regression for distribution shift correction, and a dual-network parallel architecture for multi-frequency optimization.", "result": "STRIDE outperformed baseline methods with a 2.58 dB improvement in PSNR, 2.37% increase in SSIM, and 0.236 reduction in MSE, demonstrating robustness in artifact suppression and consistent image quality.", "conclusion": "The proposed STRIDE method provides a significant improvement in sparse-view CT reconstruction, showcasing its potential for scalable and practical applications in medical imaging."}}
{"id": "2509.06465", "pdf": "https://arxiv.org/pdf/2509.06465", "abs": "https://arxiv.org/abs/2509.06465", "authors": ["Hongzong Li", "Jiahao Ma", "Zhanpeng Shi", "Fanming Jin", "Ye-Fan Hu", "Jian-Dong Huang"], "title": "CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction", "categories": ["cs.LG", "cs.CE", "q-bio.BM"], "comment": null, "summary": "Antibody binding site prediction plays a pivotal role in computational\nimmunology and therapeutic antibody design. Existing sequence or structure\nmethods rely on single-view features and fail to identify antibody-specific\nbinding sites on the antigens-a dual limitation in representation and\nprediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention\nframework with a Mixture-of-Experts (MoE) backbone for robust antibody binding\nsite prediction. CAME-AB integrates five biologically grounded modalities,\nincluding raw amino acid encodings, BLOSUM substitution profiles, pretrained\nlanguage model embeddings, structure-aware features, and GCN-refined\nbiochemical graphs-into a unified multimodal representation. To enhance\nadaptive cross-modal reasoning, we propose an adaptive modality fusion module\nthat learns to dynamically weight each modality based on its global relevance\nand input-specific contribution. A Transformer encoder combined with an MoE\nmodule further promotes feature specialization and capacity expansion. We\nadditionally incorporate a supervised contrastive learning objective to\nexplicitly shape the latent space geometry, encouraging intra-class compactness\nand inter-class separability. To improve optimization stability and\ngeneralization, we apply stochastic weight averaging during training. Extensive\nexperiments on benchmark antibody-antigen datasets demonstrate that CAME-AB\nconsistently outperforms strong baselines on multiple metrics, including\nPrecision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further\nvalidate the effectiveness of each architectural component and the benefit of\nmultimodal feature integration. The model implementation details and the codes\nare available on https://anonymous.4open.science/r/CAME-AB-C525", "AI": {"tldr": "This paper introduces CAME-AB, a multimodal framework for predicting antibody binding sites, using five integrated biological modalities and advanced techniques like adaptive modality fusion and contrastive learning.", "motivation": "Current methods fail to accurately identify antibody-specific binding sites due to limitations in representation (single-view features) and prediction.", "method": "CAME-AB employs cross-modality attention and a mixture-of-experts backbone, integrating multimodal biological features, adaptive fusion, a Transformer encoder, and contrastive learning to enhance prediction accuracy and generalization.", "result": "The framework consistently outperforms existing methods across key measurement metrics such as Precision, Recall, and F1-score on benchmark datasets.", "conclusion": "CAME-AB's multimodal integration improves the characterization and prediction of antibody binding sites, advancing computational immunology and therapeutic design."}}
{"id": "2509.05999", "pdf": "https://arxiv.org/pdf/2509.05999", "abs": "https://arxiv.org/abs/2509.05999", "authors": ["Diana-Alexandra Sas", "Florin Oniga"], "title": "S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages. Accepted to MMSP 2025", "summary": "Monocular 3D Object Detection represents a challenging Computer Vision task\ndue to the nature of the input used, which is a single 2D image, lacking in any\ndepth cues and placing the depth estimation problem as an ill-posed one.\nExisting solutions leverage the information extracted from the input by using\nConvolutional Neural Networks or Transformer architectures as feature\nextraction backbones, followed by specific detection heads for 3D parameters\nprediction. In this paper, we introduce a decoupled strategy based on injecting\nprecomputed segmentation information priors and fusing them directly into the\nfeature space for guiding the detection, without expanding the detection model\nor jointly learning the priors. The focus is on evaluating the impact of\nadditional segmentation information on existing detection pipelines without\nadding additional prediction branches. The proposed method is evaluated on the\nKITTI 3D Object Detection Benchmark, outperforming the equivalent architecture\nthat relies only on RGB image features for small objects in the scene:\npedestrians and cyclists, and proving that understanding the input data can\nbalance the need for additional sensors or training data.", "AI": {"tldr": "This paper introduces a method to improve monocular 3D object detection by integrating precomputed segmentation information into the feature space without expanding the model.", "motivation": "Monocular 3D object detection faces challenges due to the lack of depth cues in single 2D images. The authors aim to enhance detection performance using existing segmentation information.", "method": "The authors propose a decoupled strategy that incorporates segmentation priors directly into the feature space of existing detection pipelines, avoiding additional prediction branches or joint learning.", "result": "The method, tested on the KITTI 3D Object Detection Benchmark, improves performance for detecting small objects like pedestrians and cyclists compared to models relying solely on RGB image features.", "conclusion": "Leveraging segmentation information enhances the understanding of input data, demonstrating that accurate detection can be achieved without relying on extra sensors or increased training data."}}
{"id": "2509.06483", "pdf": "https://arxiv.org/pdf/2509.06483", "abs": "https://arxiv.org/abs/2509.06483", "authors": ["Guanjie Cheng", "Boyi Li", "Peihan Wu", "Feiyi Chen", "Xinkui Zhao", "Mengying Zhu", "Shuiguang Deng"], "title": "DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The wide spreading of Internet of Things (IoT) sensors generates vast\nspatio-temporal data streams, but ensuring data credibility is a critical yet\nunsolved challenge for applications like smart homes. While spatio-temporal\ngraph (STG) models are a leading paradigm for such data, they often fall short\nin dynamic, human-centric environments due to two fundamental limitations: (1)\ntheir reliance on static graph topologies, which fail to capture physical,\nevent-driven dynamics, and (2) their tendency to confuse spurious correlations\nwith true causality, undermining robustness in human-centric environments. To\naddress these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network\n(DyC-STG), a novel framework designed for real-time data credibility analysis\nin IoT. Our framework features two synergistic contributions: an event-driven\ndynamic graph module that adapts the graph topology in real-time to reflect\nphysical state changes, and a causal reasoning module to distill causally-aware\nrepresentations by strictly enforcing temporal precedence. To facilitate the\nresearch in this domain we release two new real-world datasets. Comprehensive\nexperiments show that DyC-STG establishes a new state-of-the-art, outperforming\nthe strongest baselines by 1.4 percentage points and achieving an F1-Score of\nup to 0.930.", "AI": {"tldr": "The paper introduces DyC-STG, a novel machine learning framework to ensure IoT data credibility by dynamically updating graph structures and incorporating causal reasoning.", "motivation": "The motivation stems from the need to improve data credibility in IoT applications, which suffer from static graph models and issues distinguishing causality from correlation.", "method": "The authors developed DyC-STG, comprising two key components: an event-driven dynamic graph module for real-time topology updates and a causal reasoning module enforcing strict temporal precedence.", "result": "DyC-STG outperformed existing baselines, achieving an F1-Score of up to 0.930 and establishing new benchmarks in spatio-temporal graph modeling.", "conclusion": "DyC-STG addresses key limitations in IoT data modeling by enabling dynamic and causal-aware graph structures, significantly enhancing data credibility."}}
{"id": "2509.06000", "pdf": "https://arxiv.org/pdf/2509.06000", "abs": "https://arxiv.org/abs/2509.06000", "authors": ["Jose Sosa", "Dan Pineau", "Arunkumar Rathinam", "Abdelrahman Shabayek", "Djamila Aouada"], "title": "Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 6-DoF pose estimation plays an important role in multiple\nspacecraft missions. Most existing pose estimation approaches rely on single\nimages with static keypoint localisation, failing to exploit valuable temporal\ninformation inherent to space operations. In this work, we adapt a deep\nlearning framework from human pose estimation to the spacecraft pose estimation\ndomain that integrates motion-aware heatmaps and optical flow to capture motion\ndynamics. Our approach combines image features from a Vision Transformer (ViT)\nencoder with motion cues from a pre-trained optical flow model to localise 2D\nkeypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers\n6-DoF poses from known 2D-3D correspondences. We train and evaluate our method\non the SPADES-RGB dataset and further assess its generalisation on real and\nsynthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates\nimproved performance over single-image baselines in both 2D keypoint\nlocalisation and 6-DoF pose estimation. Furthermore, it shows promising\ngeneralisation capabilities when testing on different data distributions.", "AI": {"tldr": "The paper introduces a novel deep learning method enhancing 6-DoF spacecraft pose estimation by leveraging motion-aware techniques and combining Vision Transformer features with optical flow.", "motivation": "Current monocular 6-DoF pose estimation techniques underutilize temporal motion dynamics in space operations, relying too heavily on static image processing.", "method": "The method integrates Vision Transformer (ViT) image features with optical flow-based motion cues into a deep learning framework. 2D keypoints are localized, which serves as input for a Perspective-n-Point solver to derive 6-DoF poses.", "result": "The approach outperforms single-image baselines in keypoint localization and pose estimation benchmarks, evaluated on the SPADES-RGB dataset and generalization validated using diverse SPARK-2024 datasets.", "conclusion": "Integrating dynamic motion information significantly improves pose estimation precision and generalization, proving suitable for varied data distributions."}}
{"id": "2509.06484", "pdf": "https://arxiv.org/pdf/2509.06484", "abs": "https://arxiv.org/abs/2509.06484", "authors": ["Marco Hoffmann", "Thomas Specht", "Quirin G\u00f6ttl", "Jakob Burger", "Stephan Mandt", "Hans Hasse", "Fabian Jirasek"], "title": "A machine-learned expression for the excess Gibbs energy", "categories": ["cs.LG", "cs.CE"], "comment": "18 pages, 3 figures", "summary": "The excess Gibbs energy plays a central role in chemical engineering and\nchemistry, providing a basis for modeling the thermodynamic properties of\nliquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures\nsolely from the molecular structures of their components is a long-standing\nchallenge. In this work, we address this challenge by integrating physical laws\nas hard constraints within a flexible neural network. The resulting model,\nHANNA, was trained end-to-end on an extensive experimental dataset for binary\nmixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent\npredictions. A novel surrogate solver developed in this work enabled the\ninclusion of liquid-liquid equilibrium data in the training process.\nFurthermore, a geometric projection method was applied to enable robust\nextrapolations to multi-component mixtures, without requiring additional\nparameters. We demonstrate that HANNA delivers excellent predictions, clearly\noutperforming state-of-the-art benchmark methods in accuracy and scope. The\ntrained model and corresponding code are openly available, and an interactive\ninterface is provided on our website, MLPROP.", "AI": {"tldr": "The paper introduces HANNA, a neural network model for predicting excess Gibbs energy of liquid mixtures based solely on molecular structures, developed using thermodynamic laws, end-to-end training, and novel methods for binary and multi-component systems.", "motivation": "Predicting the excess Gibbs energy for liquid mixtures from molecular structures is challenging but essential for thermodynamic modeling.", "method": "The authors integrated physical laws as hard constraints in a neural network and applied novel methods such as surrogate solvers and geometric projection to ensure training consistency and robust extrapolation.", "result": "HANNA significantly outperformed benchmark methods in terms of accuracy and scope, offering thermodynamically consistent predictions and effective extrapolation to multi-component mixtures.", "conclusion": "HANNA is a breakthrough model that simplifies excess Gibbs energy prediction, openly available with supporting resources and exceeding performance benchmarks."}}
{"id": "2509.05390", "pdf": "https://arxiv.org/pdf/2509.05390", "abs": "https://arxiv.org/abs/2509.05390", "authors": ["Clint Hurshman", "Sebastian Porsdam Mann", "Julian Savulescu", "Brian D. Earp"], "title": "Authorship Without Writing: Large Language Models and the Senior Author Analogy", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "28 pages, 0 figures", "summary": "The use of large language models (LLMs) in bioethical, scientific, and\nmedical writing remains controversial. While there is broad agreement in some\ncircles that LLMs cannot count as authors, there is no consensus about whether\nand how humans using LLMs can count as authors. In many fields, authorship is\ndistributed among large teams of researchers, some of whom, including\nparadigmatic senior authors who guide and determine the scope of a project and\nultimately vouch for its integrity, may not write a single word. In this paper,\nwe argue that LLM use (under specific conditions) is analogous to a form of\nsenior authorship. On this view, the use of LLMs, even to generate complete\ndrafts of research papers, can be considered a legitimate form of authorship\naccording to the accepted criteria in many fields. We conclude that either such\nuse should be recognized as legitimate, or current criteria for authorship\nrequire fundamental revision. AI use declaration: GPT-5 was used to help format\nBox 1. AI was not used for any other part of the preparation or writing of this\nmanuscript.", "AI": {"tldr": "The paper debates the legitimacy of large language models (LLMs) as authors in scientific and medical writing, proposing that their use can be analogous to senior authorship under certain conditions.", "motivation": "To address the controversy regarding the role of LLMs in authorship and reevaluate authorship criteria in light of evolving practices.", "method": "The authors analyze the analogy between LLM-generated content and senior authorship roles, examining accepted authorship criteria in various fields.", "result": "They propose that LLMs used under specific conditions could be recognized as legitimate authors, similar to senior authors who guide project integrity without writing themselves.", "conclusion": "LLM use in authorship either warrants recognition as legitimate or necessitates a fundamental revision of authorship criteria."}}
{"id": "2509.06006", "pdf": "https://arxiv.org/pdf/2509.06006", "abs": "https://arxiv.org/abs/2509.06006", "authors": ["Omkar Prabhu"], "title": "Khana: A Comprehensive Indian Cuisine Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As global interest in diverse culinary experiences grows, food image models\nare essential for improving food-related applications by enabling accurate food\nrecognition, recipe suggestions, dietary tracking, and automated meal planning.\nDespite the abundance of food datasets, a noticeable gap remains in capturing\nthe nuances of Indian cuisine due to its vast regional diversity, complex\npreparations, and the lack of comprehensive labeled datasets that cover its\nfull breadth. Through this exploration, we uncover Khana, a new benchmark\ndataset for food image classification, segmentation, and retrieval of dishes\nfrom Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian\ncuisine and offering around 131K images in the dataset spread across 80 labels,\neach with a resolution of 500x500 pixels. This paper describes the dataset\ncreation process and evaluates state-of-the-art models on classification,\nsegmentation, and retrieval as baselines. Khana bridges the gap between\nresearch and development by providing a comprehensive and challenging benchmark\nfor researchers while also serving as a valuable resource for developers\ncreating real-world applications that leverage the rich tapestry of Indian\ncuisine. Webpage: https://khana.omkar.xyz", "AI": {"tldr": "The paper introduces \"Khana,\" a dataset focused on Indian cuisine, featuring 131K images across 80 categories for food classification, segmentation, and retrieval tasks.", "motivation": "To address the lack of comprehensive datasets capturing the diversity of Indian cuisine, vital for food-related applications like recognition, recipe recommendations, and meal planning.", "method": "Developed a benchmark dataset containing 131K Indian food images categorized into 80 labels, with baseline evaluations using state-of-the-art models for classification, segmentation, and retrieval tasks.", "result": "Khana successfully provides a diverse and high-quality dataset that enables more accurate and nuanced food image analyses, particularly for Indian dishes.", "conclusion": "Khana bridges critical gaps in food image research and supports the development of practical applications by comprehensively representing Indian cuisine's diversity."}}
{"id": "2509.05608", "pdf": "https://arxiv.org/pdf/2509.05608", "abs": "https://arxiv.org/abs/2509.05608", "authors": ["Waris Gill", "Natalie Isak", "Matthew Dressman"], "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.", "AI": {"tldr": "BinaryShield introduces a system for secure sharing of attack fingerprints across compliance boundaries, mitigating privacy issues associated with LLM-related prompt injection attacks.", "motivation": "Organizations suffer from a security gap due to the inability to share prompt injection attack data between LLM systems due to privacy regulations.", "method": "BinaryShield combines PII redaction, semantic embedding, binary quantization, and randomized response mechanism to create non-invertible attack fingerprints that maintain privacy.", "result": "BinaryShield achieved significant performance improvements with an F1-score of 0.94, outperforming SimHash (0.77), and offered substantial storage and speed improvements over dense embeddings.", "conclusion": "BinaryShield provides a promising direction for cross-boundary threat intelligence sharing, addressing both privacy concerns and efficiency in detecting prompt injection attacks."}}
{"id": "2509.06010", "pdf": "https://arxiv.org/pdf/2509.06010", "abs": "https://arxiv.org/abs/2509.06010", "authors": ["Wanyin Cheng", "Zanxi Ruan"], "title": "BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users", "categories": ["cs.CV"], "comment": null, "summary": "Visual Question Answering (VQA) holds great potential for assisting Blind and\nLow Vision (BLV) users, yet real-world usage remains challenging. Due to visual\nimpairments, BLV users often take blurry or poorly framed photos and face\ndifficulty in articulating specific questions about what they cannot fully see.\nAs a result, their visual questions are frequently ambiguous, and different\nusers may interpret them in diverse ways. This leads to multiple valid answers,\neach grounded in different image regions-posing a mismatch with conventional\nVQA systems that assume a single answer and region. To bridge this gap, we\npresent BLaVe-CoT, a VQA framework designed to reason about answer consistency\nin the face of ambiguity. Our method proposes diverse candidate answers using a\nLoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,\nand finally applies a chain-of-thought reasoning module to assess whether the\nanswers refer to the same or different regions. Evaluated on the\nVQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves\nmore robust to the ambiguity and visual noise common in assistive settings.\nThis work highlights the need for VQA systems that can adapt to real human\nuncertainty and provide inclusive support for BLV users. To foster further\nresearch and accessibility applications, we have made the code publicly\navailable at https://github.com/Accecwan/BLaVe-CoT.", "AI": {"tldr": "The paper introduces BLaVe-CoT, a novel Visual Question Answering (VQA) framework tailored to assist Blind and Low Vision (BLV) users, addressing ambiguity and visual noise through reasoning.", "motivation": "Existing VQA systems struggle with ambiguous questions and imperfect images common among BLV users, who face challenges in framing photos and articulating specific questions.", "method": "The BLaVe-CoT framework generates diverse answers via a LoRA-tuned BLIP-2 model, spatially grounds them using PolyFormer, and employs chain-of-thought reasoning to assess answer consistency across image regions.", "result": "BLaVe-CoT demonstrates improved performance and robustness in real-world challenges compared to previous methods, validated on the VQA-AnswerTherapy benchmark.", "conclusion": "This paper emphasizes the importance of developing inclusive VQA systems adaptable to ambiguity and visual uncertainty for BLV users and provides public access to its codebase to encourage further research."}}
{"id": "2509.06516", "pdf": "https://arxiv.org/pdf/2509.06516", "abs": "https://arxiv.org/abs/2509.06516", "authors": ["Zongheng Guo", "Tao Chen", "Manuela Ferrario"], "title": "QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients", "categories": ["cs.LG", "cs.AI", "J.3"], "comment": "11 pages, 5 figures, 7 tables", "summary": "Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in\nintesive care unit (ICU) and operating room (OR). However, the high incidence\nof poor, incomplete, and inconsistent signal quality, can lead to false alarms\nor diagnostic inaccuracies. The methods explored so far suffer from limited\ngeneralizability, reliance on extensive labeled data, and poor cross-task\ntransferability. To overcome these challenges, we introduce QualityFM, a novel\nmultimodal foundation model for these physiological signals, designed to\nacquire a general-purpose understanding of signal quality. Our model is\npre-trained on an large-scale dataset comprising over 21 million 30-second\nwaveforms and 179,757 hours of data. Our approach involves a dual-track\narchitecture that processes paired physiological signals of differing quality,\nleveraging a self-distillation strategy where an encoder for high-quality\nsignals is used to guide the training of an encoder for low-quality signals. To\nefficiently handle long sequential signals and capture essential local\nquasi-periodic patterns, we integrate a windowed sparse attention mechanism\nwithin our Transformer-based model. Furthermore, a composite loss function,\nwhich combines direct distillation loss on encoder outputs with indirect\nreconstruction loss based on power and phase spectra, ensures the preservation\nof frequency-domain characteristics of the signals. We pre-train three models\nwith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy\nand practical value through transfer learning on three distinct clinical tasks:\nfalse alarm of ventricular tachycardia detection, the identification of atrial\nfibrillation and the estimation of arterial blood pressure (ABP) from PPG and\nECG signals.", "AI": {"tldr": "QualityFM is proposed, a multimodal foundation model designed for assessing signal quality in physiological signals like PPG and ECG, addressing issues of poor signal data quality in ICUs and operating rooms.", "motivation": "To address the limitations of existing methods in handling incomplete or poor-quality physiological signal data, which often lead to false alarms or inaccuracies in clinical environments like ICUs and ORs.", "method": "The model uses a dual-track Transformer-based architecture with a self-distillation strategy to train encoders for high- and low-quality paired signals. It incorporates windowed sparse attention for handling long signal sequences and a composite loss function to preserve frequency-domain characteristics.", "result": "Three versions of QualityFM with varying parameter sizes were pre-trained on a large dataset and validated through transfer learning on clinical tasks like ventricular tachycardia false alarm detection, atrial fibrillation identification, and arterial blood pressure estimation.", "conclusion": "QualityFM shows high potential in improving the assessment of signal quality in clinical settings, with effective transfer learning to various relevant tasks."}}
{"id": "2509.05634", "pdf": "https://arxiv.org/pdf/2509.05634", "abs": "https://arxiv.org/abs/2509.05634", "authors": ["David Combei"], "title": "On the Contribution of Lexical Features to Speech Emotion Recognition", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to 13th Conference on Speech Technology and Human-Computer\n  Dialogue", "summary": "Although paralinguistic cues are often considered the primary drivers of\nspeech emotion recognition (SER), we investigate the role of lexical content\nextracted from speech and show that it can achieve competitive and in some\ncases higher performance compared to acoustic models. On the MELD dataset, our\nlexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to\n49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,\nwe analyze different self-supervised (SSL) speech and text representations,\nconduct a layer-wise study of transformer-based encoders, and evaluate the\neffect of audio denoising.", "AI": {"tldr": "The paper explores the significance of lexical content in speech emotion recognition (SER), showing competitive performance over traditional acoustic models.", "motivation": "To evaluate the often overlooked role of lexical content in SER and to demonstrate its performance relative to acoustic cues.", "method": "The study uses the MELD dataset and compares lexical-based approaches against acoustic-only pipelines. It also incorporates self-supervised speech and text representations, transformer-based encoders, and audio denoising in the analysis.", "result": "Lexical-based approach achieved a weighted F1-score of 51.5%, outperforming an acoustic-only pipeline that scored 49.3%, despite having fewer parameters.", "conclusion": "Lexical content plays a significant and sometimes superior role in SER compared to paralinguistic acoustic features, highlighting its value in SER research."}}
{"id": "2509.05361", "pdf": "https://arxiv.org/pdf/2509.05361", "abs": "https://arxiv.org/abs/2509.05361", "authors": ["Alex Mark", "Aaron Scher"], "title": "Governing AI R&D: A Legal Framework for Constraining Dangerous AI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As AI advances, governing its development may become paramount to public\nsafety. Lawmakers may seek to restrict the development and release of AI models\nor of AI research itself. These governance actions could trigger legal\nchallenges that invalidate the actions, so lawmakers should consider these\nchallenges ahead of time. We investigate three classes of potential litigation\nrisk for AI regulation in the U.S.: the First Amendment, administrative law,\nand the Fourteenth Amendment. We discuss existing precedent that is likely to\napply to AI, which legal challenges are likely to arise, and how lawmakers\nmight preemptively address them. Effective AI regulation is possible, but it\nrequires careful implementation to avoid these legal challenges.", "AI": {"tldr": "The paper analyzes potential legal challenges that may arise in AI regulation within the U.S., focusing on the First Amendment, administrative law, and the Fourteenth Amendment. It provides recommendations to lawmakers on mitigating litigation risks.", "motivation": "As AI continues to evolve, its regulation is critical for ensuring public safety. However, potential legal challenges could undermine these efforts, prompting the need for well-informed legislative strategies.", "method": "The paper reviews legal precedents applicable to AI, categorizes risks based on constitutional and administrative law, and proposes preemptive measures for lawmakers to address these challenges.", "result": "Three primary categories of legal risks for AI regulation are identified: First Amendment concerns about freedom of expression, administrative law procedures, and Fourteenth Amendment issues relating to equal protection and due process.", "conclusion": "Effective AI regulation is possible but requires lawmakers to anticipate and address potential legal challenges through careful planning and implementation."}}
{"id": "2509.06011", "pdf": "https://arxiv.org/pdf/2509.06011", "abs": "https://arxiv.org/abs/2509.06011", "authors": ["Zhenhai Weng", "Zhongliang Yu"], "title": "Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology\nfor applications involving Unmanned Aerial Vehicles (UAVs). However, the\nprevailing large-scale datasets for OVD pre-training are predominantly composed\nof ground-level, natural images. This creates a significant domain gap, causing\nmodels trained on them to exhibit a substantial drop in performance on UAV\nimagery. To address this limitation, we first propose a refined UAV-Label\nengine. Then we construct and introduce UAVDE-2M(contains over 2,000,000\ninstances and 1800 categories) and UAVCAP-15k(contains over 15,000 images).\nFurthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE)\nmodule and integrate it into the YOLO-World-v2 architecture. Finally, extensive\nexperiments on the VisDrone and SIMD datasets verify the effectiveness of our\nproposed method for applications in UAV-based imagery and remote sensing.", "AI": {"tldr": "This paper addresses the domain gap in Open-Vocabulary Object Detection for UAV imagery by introducing a refined UAV-Label engine, new datasets (UAVDE-2M and UAVCAP-15k), and a novel CAGE module integrated into YOLO-World-v2.", "motivation": "The domain gap between existing large-scale OVD datasets containing ground-level images and UAV imagery reduces detection performance, necessitating improvements.", "method": "The authors develop UAV-specific datasets (UAVDE-2M and UAVCAP-15k) and propose a Cross-Attention Gated Enhancement Fusion (CAGE) module integrated into the YOLO-World-v2 framework.", "result": "Experiments on VisDrone and SIMD datasets demonstrate their approach significantly enhances object detection performance in UAV-based applications.", "conclusion": "The paper establishes advancements in OVD tailored for UAV imagery and showcases its potential for enhancing UAV-based remote sensing applications."}}
{"id": "2509.06529", "pdf": "https://arxiv.org/pdf/2509.06529", "abs": "https://arxiv.org/abs/2509.06529", "authors": ["Francesco De Cristofaro", "Cornelia Lex", "Jia Hu", "Arno Eichberger"], "title": "Lane Change Intention Prediction of two distinct Populations using a Transformer", "categories": ["cs.LG"], "comment": "7 pages, 7 figures", "summary": "As a result of the growing importance of lane change intention prediction for\na safe and efficient driving experience in complex driving scenarios,\nresearchers have in recent years started to train novel machine learning\nalgorithms on available datasets with promising results. A shortcoming of this\nrecent research effort, though, is that the vast majority of the proposed\nalgorithms are trained on a single datasets. In doing so, researchers failed to\ntest if their algorithm would be as effective if tested on a different dataset\nand, by extension, on a different population with respect to the one on which\nthey were trained. In this article we test a transformer designed for lane\nchange intention prediction on two datasets collected by LevelX in Germany and\nHong Kong. We found that the transformer's accuracy plummeted when tested on a\npopulation different to the one it was trained on with accuracy values as low\nas 39.43%, but that when trained on both populations simultaneously it could\nachieve an accuracy as high as 86.71%. - This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible.", "AI": {"tldr": "This paper highlights the challenges of using lane change intention prediction models across different datasets and tests a transformer model on datasets from Germany and Hong Kong.", "motivation": "Researchers aim to address the inability of lane change prediction models to generalize across different populations and datasets.", "method": "The researchers train a transformer model on two different datasets from Germany and Hong Kong, testing its effectiveness when trained and tested on combined versus separate populations.", "result": "Accuracy dropped to 39.43% when tested on a population different from the training dataset but increased to 86.71% when trained on both populations simultaneously.", "conclusion": "Training on diverse datasets enhances the generalization ability and effectiveness of lane change intention prediction models."}}
{"id": "2509.05978", "pdf": "https://arxiv.org/pdf/2509.05978", "abs": "https://arxiv.org/abs/2509.05978", "authors": ["Mohamed Mohamed", "Brennan Nichyporuk", "Douglas L. Arnold", "Tal Arbel"], "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging.", "AI": {"tldr": "The paper introduces a framework for generating 3D medical images based on natural language prompts, addressing the lack of pretrained 3D foundation models.", "motivation": "The lack of pretrained 3D foundation models limits the potential of vision-language models in generating 3D medical images, which could enable significant clinical and research applications.", "method": "They adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and introduce augmented conditioning to improve text alignment and image quality.", "result": "The framework successfully simulated counterfactual lesion loads in Multiple Sclerosis (MS) and cognitive states in Alzheimer's disease, producing high-resolution 3D medical images while preserving subject fidelity.", "conclusion": "This work lays a foundation for future prompt-driven 3D medical imaging applications and disease progression analysis."}}
{"id": "2509.05362", "pdf": "https://arxiv.org/pdf/2509.05362", "abs": "https://arxiv.org/abs/2509.05362", "authors": ["Ismail Hossain", "Sai Puppala", "Sajedul Talukder", "Md Jahangir Alam"], "title": "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SI"], "comment": "This paper got accepted in 26th Privacy Enhancing Technologies\n  Symposium (PETS 2026). We uploaded it into ArXiv as pre-print", "summary": "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.", "AI": {"tldr": "This paper introduces a privacy-preserving, real-time AI framework for detecting and disrupting scam conversations across digital platforms.", "motivation": "To address the limitations of current reactive defenses that offer minimal protection during active scam interactions, thus reducing the effectiveness of combating phishing and impersonation scams.", "method": "The system uses instruction-tuned AI with a safety-aware utility function to balance user engagement and harm minimization. It employs federated learning for continuous model updates without requiring raw data sharing.", "result": "Experimental evaluations show high performance metrics like low perplexity, high engagement (~0.80), and minimal PII leakage (\u2264 0.0085). Federated learning sustains model updates across 30 rounds without significant performance degradation, and differential privacy successfully secures data without impairing effectiveness.", "conclusion": "This novel framework combines scam-baiting, privacy preservation, and safety moderation into a proactive defense paradigm, offering a significant step forward in identifying and preventing scams in real time."}}
{"id": "2509.06015", "pdf": "https://arxiv.org/pdf/2509.06015", "abs": "https://arxiv.org/abs/2509.06015", "authors": ["Zhiwen Shao", "Yifan Cheng", "Fan Zhang", "Xuehuai Shi", "Canlin Li", "Lizhuang Ma", "Dit-yan Yeung"], "title": "Micro-Expression Recognition via Fine-Grained Dynamic Perception", "categories": ["cs.CV"], "comment": null, "summary": "Facial micro-expression recognition (MER) is a challenging task, due to the\ntransience, subtlety, and dynamics of micro-expressions (MEs). Most existing\nmethods resort to hand-crafted features or deep networks, in which the former\noften additionally requires key frames, and the latter suffers from small-scale\nand low-diversity training data. In this paper, we develop a novel fine-grained\ndynamic perception (FDP) framework for MER. We propose to rank frame-level\nfeatures of a sequence of raw frames in chronological order, in which the rank\nprocess encodes the dynamic information of both ME appearances and motions.\nSpecifically, a novel local-global feature-aware transformer is proposed for\nframe representation learning. A rank scorer is further adopted to calculate\nrank scores of each frame-level feature. Afterwards, the rank features from\nrank scorer are pooled in temporal dimension to capture dynamic representation.\nFinally, the dynamic representation is shared by a MER module and a dynamic\nimage construction module, in which the former predicts the ME category, and\nthe latter uses an encoder-decoder structure to construct the dynamic image.\nThe design of dynamic image construction task is beneficial for capturing\nfacial subtle actions associated with MEs and alleviating the data scarcity\nissue. Extensive experiments show that our method (i) significantly outperforms\nthe state-of-the-art MER methods, and (ii) works well for dynamic image\nconstruction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%\nover the previous best results in terms of F1-score on the CASME II, SAMM,\nCAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at\nhttps://github.com/CYF-cuber/FDP.", "AI": {"tldr": "The paper proposes a novel framework called Fine-Grained Dynamic Perception (FDP) for facial micro-expression recognition (MER), addressing challenges caused by subtle dynamics and limited training data.", "motivation": "Existing MER methods rely on hand-crafted features or deep networks, both of which struggle with key frame requirements or limited training data. MER requires innovative approaches to better capture subtle and transient micro-expressions.", "method": "FDP framework ranks frame-level features chronologically to encode dynamic information. It uses a local-global feature-aware transformer for representation learning and incorporates a rank scorer for capturing facial dynamics. A shared dynamic representation predicts ME categories and constructs dynamic facial images.", "result": "FDP outperforms state-of-the-art methods in MER across multiple datasets (CASME II, SAMM, CAS(ME)^2, CAS(ME)^3) with significant improvements in F1-score (ranging from 2.11% to 7.71%). Dynamic image construction was also validated as effective.", "conclusion": "The FDP framework is highly effective for recognizing subtle micro-expressions and mitigating data limitations. It offers improvements in performance and versatility in facial dynamic analysis."}}
{"id": "2509.06539", "pdf": "https://arxiv.org/pdf/2509.06539", "abs": "https://arxiv.org/abs/2509.06539", "authors": ["Duc Huy Le", "Rolf Stadler"], "title": "Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model", "categories": ["cs.LG", "cs.AI"], "comment": "The paper is has been accepted for the 21st International Conference\n  on Network and Service Management (CNSM-2025). The final version will be\n  published in the conference proceedings", "summary": "CAGE-2 is an accepted benchmark for learning and evaluating defender\nstrategies against cyberattacks. It reflects a scenario where a defender agent\nprotects an IT infrastructure against various attacks. Many defender methods\nfor CAGE-2 have been proposed in the literature. In this paper, we construct a\nformal model for CAGE-2 using the framework of Partially Observable Markov\nDecision Process (POMDP). Based on this model, we define an optimal defender\nstrategy for CAGE-2 and introduce a method to efficiently learn this strategy.\nOur method, called BF-PPO, is based on PPO, and it uses particle filter to\nmitigate the computational complexity due to the large state space of the\nCAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and\ncompare its performance with that of CARDIFF, the highest ranked method on the\nCAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the\nlearned defender strategy and the required training time.", "AI": {"tldr": "The paper introduces a model for CAGE-2 based on POMDP, defines an optimal defender strategy, and presents BF-PPO, a PPO-based method using particle filters, outperforming CARDIFF in the CAGE-2 benchmark.", "motivation": "To formalize and optimize defender strategies for the CAGE-2 benchmark in cybersecurity scenarios.", "method": "A POMDP model for CAGE-2 is constructed, and a new method, BF-PPO, is proposed, which integrates particle filters with PPO to handle the computational demands of a large state space.", "result": "BF-PPO demonstrated superior performance compared to the CARDIFF method in strategy efficiency and training time within the CAGE-2 CybORG environment.", "conclusion": "The proposed BF-PPO method provides an effective and efficient solution for learning optimal defender strategies in CAGE-2, surpassing existing methods."}}
{"id": "2509.05983", "pdf": "https://arxiv.org/pdf/2509.05983", "abs": "https://arxiv.org/abs/2509.05983", "authors": ["Minh N. H. Nguyen", "Anh Nguyen Tran", "Dung Truong Dinh", "Nam Van Vo"], "title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios.", "AI": {"tldr": "The paper addresses challenges in auto-speech recognition (ASR) for Vietnamese-English code-switching by proposing a two-stage phoneme-centric model (TSPC), which achieves better performance with reduced resources.", "motivation": "Code-switching poses difficulties for ASR due to phonological shifts, especially in Vietnamese-English contexts characterized by ambiguous phonetic features.", "method": "The paper introduces a Two-Stage Phoneme-Centric (TSPC) model leveraging an extended Vietnamese phoneme set for mixed-language representation, enabling efficient Vietnamese-English code-switching ASR.", "result": "The TSPC model outperformed existing baselines like PhoWhisper-base, achieving a reduced word error rate of 20.8% with fewer training resources.", "conclusion": "The phoneme-centric two-stage architecture effectively adapts to code-switching scenarios and enhances ASR performance with improved efficiency."}}
{"id": "2509.05364", "pdf": "https://arxiv.org/pdf/2509.05364", "abs": "https://arxiv.org/abs/2509.05364", "authors": ["Abdollah Baghaei Daemei"], "title": "Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Residential buildings contribute significantly to energy use, health\noutcomes, and carbon emissions. In New Zealand, housing quality has\nhistorically been poor, with inadequate insulation and inefficient heating\ncontributing to widespread energy hardship. Recent reforms, including the\nWarmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code\nupgrades, have delivered health and comfort improvements, yet challenges\npersist. Many retrofits remain partial, data on household performance are\nlimited, and decision-making support for homeowners is fragmented. This study\npresents the design and evaluation of an AI-powered decision-support tool for\nresidential energy efficiency in New Zealand. The prototype, developed using\nPython and Streamlit, integrates data ingestion, anomaly detection, baseline\nmodeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)\ninto a modular dashboard. Fifteen domain experts, including building\nscientists, consultants, and policy practitioners, tested the tool through\nsemi-structured interviews. Results show strong usability (M = 4.3), high value\nof scenario outputs (M = 4.5), and positive perceptions of its potential to\ncomplement subsidy programs and regulatory frameworks. The tool demonstrates\nhow AI can translate national policies into personalized, household-level\nguidance, bridging the gap between funding, standards, and practical\ndecision-making. Its significance lies in offering a replicable framework for\nreducing energy hardship, improving health outcomes, and supporting climate\ngoals. Future development should focus on carbon metrics, tariff modeling,\nintegration with national datasets, and longitudinal trials to assess\nreal-world adoption.", "AI": {"tldr": "This paper designs and assesses an AI-powered tool for improving residential energy efficiency in New Zealand, addressing issues related to poor housing quality and fragmented decision-making.", "motivation": "Residential energy efficiency in New Zealand faces challenges due to historically poor housing conditions, incomplete retrofits, and limited household data, necessitating better decision-making support.", "method": "An AI-powered decision-support tool was built using Python and Streamlit, incorporating features like data ingestion, anomaly detection, and scenario simulation. It was evaluated with 15 experts through semi-structured interviews.", "result": "The tool received high usability (M=4.3) and strong scenario output value (M=4.5) from domain experts, highlighting its effectiveness in complementing subsidy programs and regulatory frameworks.", "conclusion": "The AI tool bridges the gap between national policies and practical residential energy improvements, with potential for replication globally. Future focus includes carbon metrics, tariff modeling, and real-world adoption trials."}}
{"id": "2509.06023", "pdf": "https://arxiv.org/pdf/2509.06023", "abs": "https://arxiv.org/abs/2509.06023", "authors": ["Mengmeng Liu", "Michael Ying Yang", "Jiuming Liu", "Yunpeng Zhang", "Jiangtao Li", "Sander Oude Elberink", "George Vosselman", "Hao Cheng"], "title": "DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Visual-LiDAR odometry is a critical component for autonomous system\nlocalization, yet achieving high accuracy and strong robustness remains a\nchallenge. Traditional approaches commonly struggle with sensor misalignment,\nfail to fully leverage temporal information, and require extensive manual\ntuning to handle diverse sensor configurations. To address these problems, we\nintroduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse\nspatial-temporal fusion to enhance accuracy and robustness. Our approach\nproposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse\nLiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction\nand Update module that integrates temporally-predicted positions with current\nframe data, providing better initialization values for pose estimation and\nenhancing model's robustness against accumulative errors; and (3) a Temporal\nClip Training strategy combined with a Collective Average Loss mechanism that\naggregates losses across multiple frames, enabling global optimization and\nreducing the scale drift over long sequences. Extensive experiments on the\nKITTI and Argoverse Odometry dataset demonstrate the superiority of our\nproposed DVLO4D, which achieves state-of-the-art performance in terms of both\npose accuracy and robustness. Additionally, our method has high efficiency,\nwith an inference time of 82 ms, possessing the potential for the real-time\ndeployment.", "AI": {"tldr": "The paper introduces DVLO4D, a novel visual-LiDAR odometry framework that enhances robustness and accuracy using sparse spatial-temporal fusion, achieving state-of-the-art results.", "motivation": "Current visual-LiDAR odometry methods face challenges such as sensor misalignment, underutilization of temporal data, and extensive tuning for diverse sensor setups.", "method": "DVLO4D incorporates three innovations: Sparse Query Fusion for data fusion, a Temporal Interaction and Update module for better initialization in pose estimation, and Temporal Clip Training with Collective Average Loss for global optimization.", "result": "Experiments on KITTI and Argoverse datasets show that DVLO4D achieves state-of-the-art pose accuracy and robustness, with an efficient inference time of 82 ms.", "conclusion": "DVLO4D significantly improves performance in visual-LiDAR odometry, demonstrating high potential for real-time autonomous system localization applications."}}
{"id": "2509.06540", "pdf": "https://arxiv.org/pdf/2509.06540", "abs": "https://arxiv.org/abs/2509.06540", "authors": ["John Tolladay", "Beth Albert", "Gabriel Davis Jones"], "title": "Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder", "categories": ["cs.LG"], "comment": null, "summary": "Objective: To develop and interpret a supervised variational autoencoder\n(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy\noutcomes, addressing interpretability limits of current deep learning\napproaches. Methods: The OxMat CTG dataset was used to train a VAE on\nfive-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.\nThe model was optimised for signal reconstruction and outcome prediction,\nincorporating Kullback-Leibler divergence and total correlation (TC)\nconstraints to structure the latent space. Performance was evaluated using area\nunder the receiver operating characteristic curve (AUROC) and mean squared\nerror (MSE). Interpretability was assessed using coefficient of determination,\nlatent traversals and unsupervised component analyses. Results: The model\nachieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,\nwhere predicted scores were aggregated. Relaxing TC constraints improved both\nreconstruction and classification. Latent analysis showed that baseline-related\nfeatures (e.g., FHR baseline, baseline shift) were well represented and aligned\nwith model scores, while metrics like short- and long-term variability were\nless strongly encoded. Traversals revealed clear signal changes for baseline\nfeatures, while other properties were entangled or subtle. Unsupervised\ndecompositions corroborated these patterns. Findings: This work demonstrates\nthat supervised VAEs can achieve competitive fetal outcome prediction while\npartially encoding clinically meaningful CTG features. The irregular,\nmulti-timescale nature of FHR signals poses challenges for disentangling\nphysiological components, distinguishing CTG from more periodic signals such as\nECG. Although full interpretability was not achieved, the model supports\nclinically useful outcome prediction and provides a basis for future\ninterpretable, generative models.", "AI": {"tldr": "The paper develops a supervised variational autoencoder (VAE) for classifying cardiotocography signals based on pregnancy outcomes, enhancing interpretability and prediction performance.", "motivation": "Address the interpretability limits of deep learning in classifying cardiotocography signals for pregnancy outcomes.", "method": "Training a supervised VAE on fetal heart rate segments with Kullback-Leibler divergence and total correlation constraints; evaluating with AUROC, MSE, latent analysis, traversals, and component analysis.", "result": "The model achieved AUROCs of 0.752 (segment level) and 0.779 (CTG level); demonstrated improved reconstruction and classification; and revealed partial encoding of baseline features but limitations in other metrics.", "conclusion": "Supervised VAEs can predict fetal outcomes with competitive performance while encoding clinically meaningful features, though challenges remain in fully interpreting irregular signals like CTG."}}
{"id": "2509.06093", "pdf": "https://arxiv.org/pdf/2509.06093", "abs": "https://arxiv.org/abs/2509.06093", "authors": ["Yuze Liu", "Zhaoyuan Zhang", "Xiangsheng Zeng", "Yihe Zhang", "Leping Yu", "Lejia Wang", "Xi Yu"], "title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "comment": null, "summary": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery.", "AI": {"tldr": "This paper introduces a language-native database tailored for boron nitride nanosheet polymer thermally conductive composites, enabling enhanced literature synthesis and guided material discovery.", "motivation": "Traditional chemical and materials research often relies on language-based descriptions, limiting exploitation by databases and ML due to lack of structured data.", "method": "Developed a language-native database capturing structured information from papers on BNNS polymer composites, using heterogeneous databases for composite retrieval based on semantics, keywords, and filters.", "result": "The database synthesizes literature into accurate and expert-guided outputs, facilitating efficient Retrieval Augmented Generation (RAG) and tool-augmented reasoning agents.", "conclusion": "The framework establishes a structured, language-rich foundation crucial for leveraging LLMs in materials discovery and advancing research efficiency."}}
{"id": "2509.05367", "pdf": "https://arxiv.org/pdf/2509.05367", "abs": "https://arxiv.org/abs/2509.05367", "authors": ["Shei Pern Chua", "Thai Zhen Leng", "Teh Kai Jun", "Xiao Li", "Xiaolin Hu"], "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.", "AI": {"tldr": "The study introduces 'TRIAL,' a framework utilizing ethical reasoning to bypass safeguards in advanced large language models.", "motivation": "To explore limitations in AI safety, particularly focusing on how enhanced reasoning abilities in LLMs may inadvertently allow for covert security vulnerabilities.", "method": "The TRIAL framework uses ethical dilemmas inspired by the trolley problem to embed adversarial goals within multi-turn jailbreak strategies.", "result": "TRIAL achieved high success rates in bypassing safeguards on both open-source and closed-source LLMs.", "conclusion": "As LLMs enhance their reasoning capabilities, current alignment and safety oversight strategies may be insufficient to mitigate emerging, context-aware security vulnerabilities."}}
{"id": "2509.06033", "pdf": "https://arxiv.org/pdf/2509.06033", "abs": "https://arxiv.org/abs/2509.06033", "authors": ["Nadia Bakhsheshi", "Hamid Beigy"], "title": "Analysis of Blood Report Images Using General Purpose Vision-Language Models", "categories": ["cs.CV"], "comment": "4 pages , 3 figures , This paper has been submitted to the\n  IEEE-affiliated ICBME Conference (Iran), 2025, and is currently under review.\n  DOR number: [20.1001.2.0425023682.1404.10.1.440.7]", "summary": "The reliable analysis of blood reports is important for health knowledge, but\nindividuals often struggle with interpretation, leading to anxiety and\noverlooked issues. We explore the potential of general-purpose Vision-Language\nModels (VLMs) to address this challenge by automatically analyzing blood report\nimages. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini\n2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of\n100 diverse blood report images. Each model was prompted with clinically\nrelevant questions adapted to each blood report. The answers were then\nprocessed using Sentence-BERT to compare and evaluate how closely the models\nresponded. The findings suggest that general-purpose VLMs are a practical and\npromising technology for developing patient-facing tools for preliminary blood\nreport analysis. Their ability to provide clear interpretations directly from\nimages can improve health literacy and reduce the limitations to understanding\ncomplex medical information. This work establishes a foundation for the future\ndevelopment of reliable and accessible AI-assisted healthcare applications.\nWhile results are encouraging, they should be interpreted cautiously given the\nlimited dataset size.", "AI": {"tldr": "The paper evaluates the use of Vision-Language Models (VLMs) to analyze blood report images and make them understandable for users.", "motivation": "The study aims to address the challenge individuals face in interpreting complex blood reports, which often leads to anxiety and missed health issues.", "method": "Three VLMs (Qwen-VL-Max, Gemini 2.5 Pro, Llama 4 Maverick) were compared on responses to clinically relevant questions about 100 diverse blood report images, analyzing accuracy using Sentence-BERT.", "result": "VLMs showed promise in providing clear analyses of medical data from images, proving their potential for healthcare applications.", "conclusion": "The study demonstrates that general-purpose VLMs could improve patient understanding of blood reports, laying a foundation for accessible AI-assisted healthcare tools, though the findings are limited by the small dataset size."}}
{"id": "2509.06550", "pdf": "https://arxiv.org/pdf/2509.06550", "abs": "https://arxiv.org/abs/2509.06550", "authors": ["Jack Wilkie", "Hanan Hindy", "Christos Tachtatzis", "Robert Atkinson"], "title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI", "I.2.6; K.6.5"], "comment": "Published in: Proceedings of IEEE Conference on Cyber Security and\n  Resilience (CSR), 2025. Official version:\n  https://doi.org/10.1109/CSR64739.2025.11129979 Code:\n  https://github.com/jackwilkie/CLAN", "summary": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models.", "AI": {"tldr": "Traditional intrusion detection faces challenges with labelled data reliance in supervised models and high false positive rates in anomaly detection. The paper proposes a novel contrastive self-supervised technique with augmented negative pairs (CLAN) to enhance accuracy and efficiency.", "motivation": "The paper aims to address the limitations of intrusion detection methods: the need for large labelled datasets in supervised models and the high false positive rates in anomaly detection methods.", "method": "The proposed method, CLAN, introduces contrastive self-supervised learning where augmented samples are used as negative pairs, representing malicious traffic, while benign samples serve as positive views. This approach is trained on benign traffic for improved performance.", "result": "Experimental results on the Lycos2017 dataset indicate that CLAN achieves better binary classification compared to other self-supervised and anomaly detection techniques. Moreover, it outperforms existing self-supervised models in multi-class classification when fine-tuned on limited labelled data.", "conclusion": "CLAN showcases the potential for effective and efficient intrusion detection, outperforming existing supervised, self-supervised, and anomaly-based solutions in various tasks, thus bridging gaps in current cybersecurity approaches."}}
{"id": "2509.06035", "pdf": "https://arxiv.org/pdf/2509.06035", "abs": "https://arxiv.org/abs/2509.06035", "authors": ["Jiaming Cui"], "title": "TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection", "categories": ["cs.CV", "cs.AI", "cs.CE"], "comment": null, "summary": "Automated inspection of transmission lines using UAVs is hindered by the\ndifficulty of detecting small and ambiguous defects against complex\nbackgrounds. Conventional detectors often suffer from detail loss due to\nstrided downsampling, weak boundary sensitivity in lightweight backbones, and\ninsufficient integration of global context with local cues. To address these\nchallenges, we propose TinyDef-DETR, a DETR-based framework designed for\nsmall-defect detection. The method introduces a stride-free space-to-depth\nmodule for lossless downsampling, an edge-enhanced convolution for\nboundary-aware feature extraction, a cross-stage dual-domain multi-scale\nattention module to jointly capture global and local information, and a\nFocaler-Wise-SIoU regression loss to improve localization of small objects.\nExperiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR\nachieves substantial improvements in both precision and recall compared to\ncompetitive baselines, with particularly notable gains on small-object subsets,\nwhile incurring only modest computational overhead. Further validation on the\nVisDrone benchmark confirms the generalization capability of the proposed\napproach. Overall, the results indicate that integrating detail-preserving\ndownsampling, edge-sensitive representations, dual-domain attention, and\ndifficulty-adaptive regression provides a practical and efficient solution for\nUAV-based small-defect inspection in power grids.", "AI": {"tldr": "This paper presents TinyDef-DETR, a specialized DETR-based framework tailored for small-defect detection in UAV-based inspection of transmission lines.", "motivation": "To overcome challenges in detecting small and ambiguous defects in complex backgrounds, where conventional detectors often fail due to detail loss, weak boundary sensitivity, and improper integration of global and local contexts.", "method": "The authors introduce a stride-free space-to-depth downsampling module, edge-enhanced convolution, cross-stage dual-domain multi-scale attention, and Focaler-Wise-SIoU regression loss.", "result": "TinyDef-DETR shows significant improvements in precision and recall, especially for small-object subsets, with moderate computational cost, validated on the CSG-ADCD dataset and further supported on the VisDrone benchmark.", "conclusion": "TinyDef-DETR is a practical and efficient system for UAV-based small-defect inspection, combining innovative design elements to enhance detail preservation, boundary sensitivity, and localization, with proven generalization capability."}}
{"id": "2509.05376", "pdf": "https://arxiv.org/pdf/2509.05376", "abs": "https://arxiv.org/abs/2509.05376", "authors": ["Abdul Rehman", "Are D\u00e6hlen", "Ilona Heldal", "Jerry Chun-wei Lin"], "title": "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%.", "AI": {"tldr": "The paper introduces a human-centered framework to address privacy risks in AI-powered eye-tracking systems by preventing identity backtracking while enabling diagnostic classification.", "motivation": "Eye-tracking technology captures sensitive data, raising privacy concerns. The paper aims to address these privacy issues without compromising its educational and diagnostic potential.", "method": "The authors propose a two-stage privacy-preserving framework, leveraging real-time data anonymization, ethical design principles, regulatory compliance, and Federated Learning.", "result": "Phase 1 demonstrates high accuracy in diagnostic predictions and ID randomization across different scenarios, while phase 2 prevents backtracking and implements secure identity management with dummy IDs.", "conclusion": "The proposed framework effectively balances privacy protection and diagnostic functionality, ensuring secure use of eye-tracking in educational environments."}}
{"id": "2509.06040", "pdf": "https://arxiv.org/pdf/2509.06040", "abs": "https://arxiv.org/abs/2509.06040", "authors": ["Yuming Li", "Yikai Wang", "Yuying Zhu", "Zhongyu Zhao", "Ming Lu", "Qi She", "Shanghang Zhang"], "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 6 figures", "summary": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.", "AI": {"tldr": "BranchGRPO reduces computational costs and improves image and video generative model alignment by introducing a branch sampling policy.", "motivation": "Existing generative model alignment methods face computational inefficiencies and instability due to sparse rewards.", "method": "BranchGRPO employs a branch sampling scheme, tree-based advantage estimator, and pruning strategies to enhance efficiency and performance.", "result": "Experiments demonstrate a 16% improvement in alignment scores and a 50% reduction in training time.", "conclusion": "BranchGRPO effectively achieves better alignment scores while significantly reducing computational requirements, addressing key challenges in generative model training."}}
{"id": "2509.06580", "pdf": "https://arxiv.org/pdf/2509.06580", "abs": "https://arxiv.org/abs/2509.06580", "authors": ["Georgia Channing", "Avijit Ghosh"], "title": "AI for Scientific Discovery is a Social Problem", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Artificial intelligence promises to accelerate scientific discovery, yet its\nbenefits remain unevenly distributed. While technical obstacles such as scarce\ndata, fragmented standards, and unequal access to computation are significant,\nwe argue that the primary barriers are social and institutional. Narratives\nthat defer progress to speculative \"AI scientists,\" the undervaluing of data\nand infrastructure contributions, misaligned incentives, and gaps between\ndomain experts and machine learning researchers all constrain impact. We\nhighlight four interconnected challenges: community dysfunction, research\npriorities misaligned with upstream needs, data fragmentation, and\ninfrastructure inequities. We argue that their roots lie in cultural and\norganizational practices. Addressing them requires not only technical\ninnovation but also intentional community-building, cross-disciplinary\neducation, shared benchmarks, and accessible infrastructure. We call for\nreframing AI for science as a collective social project, where sustainable\ncollaboration and equitable participation are treated as prerequisites for\ntechnical progress.", "AI": {"tldr": "The paper identifies social and institutional barriers as the main obstacles to effectively applying AI in scientific discovery, rather than just technical challenges.", "motivation": "To analyze why the benefits of AI in accelerating scientific discovery remain unevenly distributed and highlight key barriers restricting progress.", "method": "The authors identify four interconnected challenges\u2014community dysfunction, misaligned research priorities, data fragmentation, and infrastructure inequities\u2014and argue that their root causes are cultural and organizational practices.", "result": "The paper emphasizes that addressing these challenges requires not only technical solutions but also community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure.", "conclusion": "The paper concludes that AI for science must be framed as a collective social project, requiring sustainable collaboration and equitable participation for technical progress."}}
{"id": "2509.06195", "pdf": "https://arxiv.org/pdf/2509.06195", "abs": "https://arxiv.org/abs/2509.06195", "authors": ["Jinrui Yang", "Fan Jiang", "Timothy Baldwin"], "title": "Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP MRL 2024", "summary": "Language fairness in multilingual information retrieval (MLIR) systems is\ncrucial for ensuring equitable access to information across diverse languages.\nThis paper sheds light on the issue, based on the assumption that queries in\ndifferent languages, but with identical semantics, should yield equivalent\nranking lists when retrieving on the same multilingual documents. We evaluate\nthe degree of fairness using both traditional retrieval methods, and a DPR\nneural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a\nnovel loss designed to mitigate language biases in neural MLIR approaches. Our\nanalysis exposes intrinsic language biases in current MLIR technologies, with\nnotable disparities across the retrieval methods, and the effectiveness of\nLaKDA in enhancing language fairness.", "AI": {"tldr": "The paper explores language fairness in multilingual information retrieval and proposes LaKDA, a novel methodology to mitigate biases.", "motivation": "To address language biases in MLIR systems and ensure fair access to information for diverse languages.", "method": "Evaluation of traditional methods, DPR neural rankers (using mBERT and XLM-R), and introduction of LaKDA for bias mitigation.", "result": "Intrinsic language biases were analyzed, disparities highlighted, and LaKDA demonstrated effectiveness in improving fairness.", "conclusion": "Fairness in MLIR is critical, and innovations like LaKDA can reduce biases across languages."}}
{"id": "2509.05379", "pdf": "https://arxiv.org/pdf/2509.05379", "abs": "https://arxiv.org/abs/2509.05379", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "title": "ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As our cities and communities become smarter, the systems that keep us safe,\nsuch as traffic control centers, emergency response networks, and public\ntransportation, also become more complex. With this complexity comes a greater\nrisk of security threats that can affect not just machines but real people's\nlives. To address this challenge, we present ThreatGPT, an agentic Artificial\nIntelligence (AI) assistant built to help people whether they are engineers,\nsafety officers, or policy makers to understand and analyze threats in public\nsafety systems. Instead of requiring deep cybersecurity expertise, it allows\nusers to simply describe the components of a system they are concerned about,\nsuch as login systems, data storage, or communication networks. Then, with the\nclick of a button, users can choose how they want the system to be analyzed by\nusing popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or\nCISA. ThreatGPT is unique because it does not just provide threat information,\nbut rather it acts like a knowledgeable partner. Using few-shot learning, the\nAI learns from examples and generates relevant smart threat models. It can\nhighlight what might go wrong, how attackers could take advantage, and what can\nbe done to prevent harm. Whether securing a city's infrastructure or a local\nhealth service, this tool adapts to users' needs. In simple terms, ThreatGPT\nbrings together AI and human judgment to make our public systems safer. It is\ndesigned not just to analyze threats, but to empower people to understand and\nact on them, faster, smarter, and with more confidence.", "AI": {"tldr": "The paper introduces ThreatGPT, an AI assistant that helps analyze and address threats in public safety systems using frameworks like STRIDE and NIST without requiring deep cybersecurity expertise.", "motivation": "The increasing complexity of smart city systems, such as traffic control and emergency response, poses heightened security risks that can directly impact real people's lives.", "method": "ThreatGPT leverages few-shot learning to act as a collaborative partner in identifying security threats, allowing users to describe system components and use analyses like STRIDE, MITRE ATT&CK, and others.", "result": "ThreatGPT provides contextually-informed, intelligent threat modeling that considers possible vulnerabilities, attack approaches, and preventive measures.", "conclusion": "ThreatGPT empowers users, regardless of their cybersecurity expertise, to understand and address security threats to public safety systems more effectively and confidently."}}
{"id": "2509.06041", "pdf": "https://arxiv.org/pdf/2509.06041", "abs": "https://arxiv.org/abs/2509.06041", "authors": ["Mohammad Ahangarkiasari", "Hassan Pouraria"], "title": "Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities", "categories": ["cs.CV"], "comment": null, "summary": "Buoyancy-driven heat transfer in closed cavities serves as a canonical\ntestbed for thermal design High-fidelity CFD modelling yields accurate thermal\nfield solutions, yet its reliance on expert-crafted physics models, fine\nmeshes, and intensive computation limits rapid iteration. Recent developments\nin data-driven modeling, especially Graph Neural Networks (GNNs), offer new\nalternatives for learning thermal-fluid behavior directly from simulation data,\nparticularly on irregular mesh structures. However, conventional GNNs often\nstruggle to capture long-range dependencies in high-resolution graph\nstructures. To overcome this limitation, we propose a novel multi-stage GNN\narchitecture that leverages hierarchical pooling and unpooling operations to\nprogressively model global-to-local interactions across multiple spatial\nscales. We evaluate the proposed model on our newly developed CFD dataset\nsimulating natural convection within a rectangular cavities with varying aspect\nratios where the bottom wall is isothermal hot, the top wall is isothermal\ncold, and the two vertical walls are adiabatic. Experimental results\ndemonstrate that the proposed model achieves higher predictive accuracy,\nimproved training efficiency, and reduced long-term error accumulation compared\nto state-of-the-art (SOTA) GNN baselines. These findings underscore the\npotential of the proposed multi-stage GNN approach for modeling complex heat\ntransfer in mesh-based fluid dynamics simulations.", "AI": {"tldr": "The paper presents a novel multi-stage Graph Neural Network (GNN) architecture designed to improve predictive accuracy and efficiency in modeling buoyancy-driven heat transfer using CFD data.", "motivation": "To address the limitations of high-computation, expert-dependent CFD modeling and the inability of conventional GNNs to capture long-range dependencies in high-resolution graph structures.", "method": "Developed a multi-stage GNN architecture incorporating hierarchical pooling and unpooling to model global-to-local interactions across spatial scales, tested using a CFD dataset simulating natural convection in rectangular cavities.", "result": "The proposed model outperforms state-of-the-art GNN baselines in predictive accuracy, training efficiency, and reduced long-term error accumulation.", "conclusion": "The multi-stage GNN architecture shows high potential for advancing data-driven modeling in heat transfer and fluid dynamics simulations."}}
{"id": "2509.06599", "pdf": "https://arxiv.org/pdf/2509.06599", "abs": "https://arxiv.org/abs/2509.06599", "authors": ["Sri Satish Krishna Chaitanya Bulusu", "Mikko Sillanp\u00e4\u00e4"], "title": "Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems", "categories": ["cs.LG", "cs.CC", "cs.SY", "eess.SP", "eess.SY", "math.ST", "stat.TH"], "comment": "15 pages, 1 figure, 2 photographs", "summary": "Dynamic nonlinear systems exhibit distortions arising from coupled static and\ndynamic effects. Their intertwined nature poses major challenges for\ndata-driven modeling. This paper presents a theoretical framework grounded in\nstructured decomposition, variance analysis, and task-centric complexity\nbounds.\n  The framework employs a directional lower bound on interactions between\nmeasurable system components, extending orthogonality in inner product spaces\nto structurally asymmetric settings. This bound supports variance inequalities\nfor decomposed systems. Key behavioral indicators are introduced along with a\nmemory finiteness index. A rigorous power-based condition establishes a\nmeasurable link between finite memory in realizable systems and the First Law\nof Thermodynamics. This offers a more foundational perspective than classical\nbounds based on the Second Law.\n  Building on this foundation, we formulate a `Behavioral Uncertainty\nPrinciple,' demonstrating that static and dynamic distortions cannot be\nminimized simultaneously. We identify that real-world systems seem to resist\ncomplete deterministic decomposition due to entangled static and dynamic\neffects. We also present two general-purpose theorems linking function variance\nto mean-squared Lipschitz continuity and learning complexity. This yields a\nmodel-agnostic, task-aware complexity metric, showing that lower-variance\ncomponents are inherently easier to learn.\n  These insights explain the empirical benefits of structured residual\nlearning, including improved generalization, reduced parameter count, and lower\ntraining cost, as previously observed in power amplifier linearization\nexperiments. The framework is broadly applicable and offers a scalable,\ntheoretically grounded approach to modeling complex dynamic nonlinear systems.", "AI": {"tldr": "This paper introduces a theoretical framework to model dynamic nonlinear systems using decomposition, variance analysis, and complexity bounds, emphasizing inherent trade-offs and empirical benefits.", "motivation": "Understand and model dynamic nonlinear systems by addressing intertwined static and dynamic distortions.", "method": "Using structured decomposition, variance analysis, memory finiteness measures, and inequality bounds to develop task-aware complexity metrics.", "result": "Proposes a Behavioral Uncertainty Principle, derives general-purpose theorems on function variance, and explains structured residual learning benefits.", "conclusion": "The framework explains empirical behaviors and offers a scalable, robust approach to model complex nonlinear systems."}}
{"id": "2509.06221", "pdf": "https://arxiv.org/pdf/2509.06221", "abs": "https://arxiv.org/abs/2509.06221", "authors": ["Vishal Choudhari"], "title": "Beamforming-LLM: What, Where and When Did I Miss?", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing.", "AI": {"tldr": "Beamforming-LLM integrates spatial audio technology and AI methods to allow users to semantically recall missed conversations in multi-speaker settings with natural language queries.", "motivation": "To address challenges of recalling missed segments in multi-person conversations and enhance assistive tools for auditory memory systems.", "method": "The system uses beamforming for spatial audio separation, transcription with Whisper, vector embedding, retrieval-augmented generation (RAG), and summarization via GPT-4o-mini.", "result": "Beamforming-LLM provides semantically relevant summaries with spatial context, contrastive updates, timestamped audio playback, and a user-friendly interface.", "conclusion": "This work establishes a foundation for auditory memory systems with practical applications in assistive technologies, meeting summarizations, and spatial computing."}}
{"id": "2509.06068", "pdf": "https://arxiv.org/pdf/2509.06068", "abs": "https://arxiv.org/abs/2509.06068", "authors": ["Shih-Ying Yeh"], "title": "Home-made Diffusion Model from Scratch to Hatch", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Home-made Diffusion Model (HDM), an efficient yet powerful\ntext-to-image diffusion model optimized for training (and inferring) on\nconsumer-grade hardware. HDM achieves competitive 1024x1024 generation quality\nwhile maintaining a remarkably low training cost of $535-620 using four RTX5090\nGPUs, representing a significant reduction in computational requirements\ncompared to traditional approaches. Our key contributions include: (1)\nCross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer\n(XUT), that employs cross-attention for skip connections, providing superior\nfeature integration that leads to remarkable compositional consistency; (2) a\ncomprehensive training recipe that incorporates TREAD acceleration, a novel\nshifted square crop strategy for efficient arbitrary aspect-ratio training, and\nprogressive resolution scaling; and (3) an empirical demonstration that smaller\nmodels (343M parameters) with carefully crafted architectures can achieve\nhigh-quality results and emergent capabilities, such as intuitive camera\ncontrol. Our work provides an alternative paradigm of scaling, demonstrating a\nviable path toward democratizing high-quality text-to-image generation for\nindividual researchers and smaller organizations with limited computational\nresources.", "AI": {"tldr": "The paper introduces HDM, a text-to-image diffusion model that delivers high-quality images efficiently even on consumer-grade hardware.", "motivation": "To reduce the computational requirements and costs associated with training high-quality text-to-image models, making them accessible to individuals and smaller organizations.", "method": "Development of HDM with innovative architectural features like Cross-U-Transformer (XUT) and an efficient training recipe including strategies like TREAD acceleration and progressive resolution scaling.", "result": "HDM achieved competitive 1024x1024 image generation quality with reduced training costs ($535-620 on four RTX5090 GPUs) and validated that smaller models with optimized architectures can perform well.", "conclusion": "HDM democratizes access to high-quality text-to-image generation by demonstrating efficient scaling and innovative techniques for consumer-grade hardware."}}
{"id": "2509.06600", "pdf": "https://arxiv.org/pdf/2509.06600", "abs": "https://arxiv.org/abs/2509.06600", "authors": ["Huayi Tang", "Yong Liu"], "title": "PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification", "categories": ["cs.LG"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in processing\ngraph-structured data across various applications. A critical aspect of\nreal-world graphs is their dynamic nature, where new nodes are continually\nadded and existing connections may change over time. Previous theoretical\nstudies, largely based on the transductive learning framework, fail to\nadequately model such temporal evolution and structural dynamics. In this\npaper, we presents a PAC-Bayesian theoretical analysis of graph convolutional\nnetworks (GCNs) for inductive node classification, treating nodes as dependent\nand non-identically distributed data points. We derive novel generalization\nbounds for one-layer GCNs that explicitly incorporate the effects of data\ndependency and non-stationarity, and establish sufficient conditions under\nwhich the generalization gap converges to zero as the number of nodes\nincreases. Furthermore, we extend our analysis to two-layer GCNs, and reveal\nthat it requires stronger assumptions on graph topology to guarantee\nconvergence. This work establishes a theoretical foundation for understanding\nand improving GNN generalization in dynamic graph environments.", "AI": {"tldr": "This paper provides a PAC-Bayesian theoretical analysis of GCNs for inductive node classification, addressing the dynamic nature of real-world graphs with novel generalization bounds.", "motivation": "GNNs have succeeded in handling graph data but face challenges with dynamic graphs involving temporal evolution and structural changes.", "method": "The study focuses on formulating PAC-Bayesian generalization bounds for GCNs, explicitly considering data dependency and non-stationarity in dynamic graph settings.", "result": "The analysis derives sufficient conditions for the generalization gap of one-layer GCNs to converge to zero and extends findings to two-layer GCNs under stricter topology assumptions.", "conclusion": "The theoretical insights provide groundwork for improving the generalization performance of GNNs in dynamic graph scenarios."}}
{"id": "2509.05382", "pdf": "https://arxiv.org/pdf/2509.05382", "abs": "https://arxiv.org/abs/2509.05382", "authors": ["Jennifer King", "Kevin Klyman", "Emily Capstick", "Tiffany Saade", "Victoria Hsieh"], "title": "User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "See additional files for appendices", "summary": "Hundreds of millions of people now regularly interact with large language\nmodels via chatbots. Model developers are eager to acquire new sources of\nhigh-quality training data as they race to improve model capabilities and win\nmarket share. This paper analyzes the privacy policies of six U.S. frontier AI\ndevelopers to understand how they use their users' chats to train models.\nDrawing primarily on the California Consumer Privacy Act, we develop a novel\nqualitative coding schema that we apply to each developer's relevant privacy\npolicies to compare data collection and use practices across the six companies.\nWe find that all six developers appear to employ their users' chat data to\ntrain and improve their models by default, and that some retain this data\nindefinitely. Developers may collect and train on personal information\ndisclosed in chats, including sensitive information such as biometric and\nhealth data, as well as files uploaded by users. Four of the six companies we\nexamined appear to include children's chat data for model training, as well as\ncustomer data from other products. On the whole, developers' privacy policies\noften lack essential information about their practices, highlighting the need\nfor greater transparency and accountability. We address the implications of\nusers' lack of consent for the use of their chat data for model training, data\nsecurity issues arising from indefinite chat data retention, and training on\nchildren's chat data. We conclude by providing recommendations to policymakers\nand developers to address the data privacy challenges posed by LLM-powered\nchatbots.", "AI": {"tldr": "Developers of AI chatbots utilize user interactions, often without explicit consent, to train models, raising significant data privacy concerns.", "motivation": "To explore how major AI developers utilize users' chat data, particularly focusing on privacy policies and practices.", "method": "A qualitative coding schema was developed using the California Consumer Privacy Act and applied to analyze privacy policies from six U.S. AI companies.", "result": "All six developers use chat data for model training, sometimes retaining it indefinitely. Sensitive data, children\u2019s chats, and uploaded files are included, often without full transparency.", "conclusion": "There is a need for improved transparency and accountability in AI chatbot data practices. Recommendations are given for policymakers and developers to enhance privacy protections."}}
{"id": "2509.06082", "pdf": "https://arxiv.org/pdf/2509.06082", "abs": "https://arxiv.org/abs/2509.06082", "authors": ["Anuraag Mishra", "Andrea Gilch", "Benjamin Apeleo Zubiri", "Jan Rolfes", "Frauke Liers"], "title": "High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization", "categories": ["cs.CV", "cond-mat.mtrl-sci", "90C20, 94A08, 68U10"], "comment": "36 pages, 17 figures", "summary": "In this work, we develop a novel technique for reconstructing images from\nprojection-based nano- and microtomography. Our contribution focuses on\nenhancing reconstruction quality, particularly for specimen composed of\nhomogeneous material phases connected by sharp edges. This is accomplished by\ntraining a neural network to identify edges within subpictures. The trained\nnetwork is then integrated into a mathematical optimization model, to reduce\nartifacts from previous reconstructions. To this end, the optimization approach\nfavors solutions according to the learned predictions, however may also\ndetermine alternative solutions if these are strongly supported by the raw\ndata. Hence, our technique successfully incorporates knowledge about the\nhomogeneity and presence of sharp edges in the sample and thereby eliminates\nblurriness. Our results on experimental datasets show significant enhancements\nin interface sharpness and material homogeneity compared to benchmark\nalgorithms. Thus, our technique produces high-quality reconstructions,\nshowcasing its potential for advancing tomographic imaging techniques.", "AI": {"tldr": "The paper presents a new technique for improving image reconstruction in nano- and microtomography, focusing on reducing artifacts and enhancing quality by leveraging neural networks and optimization.", "motivation": "The study aims to address and improve image reconstruction challenges, particularly enhancing sharpness and homogeneity in images of specimens with homogeneous material phases connected by sharp edges.", "method": "The method uses a neural network to detect edges in subpictures, integrating its predictions into an optimization model that reduces reconstruction artifacts while allowing alternative solutions as supported by raw data.", "result": "Experimental datasets demonstrate that the technique achieves noticeably sharper interfaces and improved material homogeneity compared to existing reconstruction algorithms.", "conclusion": "The proposed method advances tomographic imaging by producing higher-quality reconstructions and presents significant potential for further enhancement in this field."}}
{"id": "2509.06602", "pdf": "https://arxiv.org/pdf/2509.06602", "abs": "https://arxiv.org/abs/2509.06602", "authors": ["Noel Codella", "Sam Preston", "Hao Qiu", "Leonardo Schettini", "Wen-wai Yim", "Mert \u00d6z", "Shrey Jain", "Matthew P. Lungren", "Thomas Osborne"], "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure", "summary": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.", "AI": {"tldr": "The paper introduces HAO, an AI agent using LLMs, to automate patient summaries for Molecular Tumor Boards (MTBs) and proposes TBFact for robust evaluation.", "motivation": "Current manual approaches to producing patient summaries for MTBs are labor-intensive, subjective, and prone to errors, necessitating a more efficient, accurate, and scalable solution.", "method": "The proposed solution is Healthcare Agent Orchestrator (HAO), which uses a multi-agent Large Language Model (LLM)-based workflow. For evaluation, TBFact is introduced as a model-as-a-judge framework to measure summary comprehensiveness and succinctness.", "result": "The evaluation shows HAO's agent captured 94% of critical information and achieved a TBFact recall of 0.84 under strict criteria. TBFact enables a privacy-aware, data-free deployment workflow.", "conclusion": "HAO and TBFact provide a robust and scalable framework for improving patient summary generation and evaluation in MTBs, addressing labor and accuracy challenges while maintaining data privacy."}}
{"id": "2509.06415", "pdf": "https://arxiv.org/pdf/2509.06415", "abs": "https://arxiv.org/abs/2509.06415", "authors": ["Jaemin Son", "Sujin Choi", "Inyong Yun"], "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Submitted to ICASSP 2026", "summary": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.", "AI": {"tldr": "The paper introduces a lightweight token pruning approach to reduce computational costs in vision-language models for document understanding while preserving accuracy.", "motivation": "The authors aim to address the high computational demands of vision-language models in document understanding tasks.", "method": "A lightweight token pruning framework uses a binary patch-level classifier to filter out non-text areas, followed by max-pooling to recover fragmented text regions for spatial coherence.", "result": "The proposed approach significantly reduces computational costs without compromising performance on real-world document datasets.", "conclusion": "Token pruning can enhance the efficiency of vision-language models, making them more computationally sustainable while maintaining their effectiveness."}}
{"id": "2509.06096", "pdf": "https://arxiv.org/pdf/2509.06096", "abs": "https://arxiv.org/abs/2509.06096", "authors": ["Yiwen Ye", "Yicheng Wu", "Xiangde Luo", "He Zhang", "Ziyang Chen", "Ting Dang", "Yanning Zhang", "Yong Xia"], "title": "MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "Foundation models have become a promising paradigm for advancing medical\nimage analysis, particularly for segmentation tasks where downstream\napplications often emerge sequentially. Existing fine-tuning strategies,\nhowever, remain limited: parallel fine-tuning isolates tasks and fails to\nexploit shared knowledge, while multi-task fine-tuning requires simultaneous\naccess to all datasets and struggles with incremental task integration. To\naddress these challenges, we propose MedSeqFT, a sequential fine-tuning\nframework that progressively adapts pre-trained models to new tasks while\nrefining their representational capacity. MedSeqFT introduces two core\ncomponents: (1) Maximum Data Similarity (MDS) selection, which identifies\ndownstream samples most representative of the original pre-training\ndistribution to preserve general knowledge, and (2) Knowledge and\nGeneralization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge\ndistillation scheme that balances task-specific adaptation with the retention\nof pre-trained knowledge. Extensive experiments on two multi-task datasets\ncovering ten 3D segmentation tasks demonstrate that MedSeqFT consistently\noutperforms state-of-the-art fine-tuning strategies, yielding substantial\nperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,\nevaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT\nenhances transferability, particularly for tumor segmentation. Visual analyses\nof loss landscapes and parameter variations further highlight the robustness of\nMedSeqFT. These results establish sequential fine-tuning as an effective,\nknowledge-retentive paradigm for adapting foundation models to evolving\nclinical tasks. Code will be released.", "AI": {"tldr": "The paper proposes MedSeqFT, a framework for sequential fine-tuning of foundation models in medical image segmentation, addressing the limitations of parallel and multi-task strategies and achieving improved performance and transferability.", "motivation": "Current fine-tuning strategies for medical image segmentation fail to exploit shared knowledge when tasks emerge sequentially; parallel fine-tuning isolates tasks, and multi-task approaches need simultaneous access to all datasets.", "method": "MedSeqFT introduces Maximum Data Similarity (MDS) selection for general knowledge preservation and Knowledge & Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based scheme balancing task-specific adaptation and knowledge retention.", "result": "MedSeqFT improves segmentation performance with an average Dice increase of 3.0% across ten tasks, demonstrates enhanced transferability on unseen tasks, and shows robust loss landscapes and parameter behavior.", "conclusion": "MedSeqFT is a robust and knowledge-retentive sequential fine-tuning paradigm for adapting foundation models to continually evolving clinical segmentation tasks, outperforming state-of-the-art methods."}}
{"id": "2509.06608", "pdf": "https://arxiv.org/pdf/2509.06608", "abs": "https://arxiv.org/abs/2509.06608", "authors": ["Viacheslav Sinii", "Nikita Balagansky", "Yaroslav Aksenov", "Vadim Kurochkin", "Daniil Laptev", "Gleb Gerasimov", "Alexey Gorbatovski", "Boris Shaposhnikov", "Daniil Gavrilov"], "title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors", "categories": ["cs.LG"], "comment": "Preprint", "summary": "The mechanisms by which reasoning training reshapes language-model\ncomputations remain poorly understood. We study lightweight steering vectors\ninserted into the base model's residual stream and trained with a\nreinforcement-learning objective, which can match full fine-tuning performance\nwhile retaining the interpretability of small, additive interventions. Using\nlogit-lens readouts, path patching, and circuit analyses, we analyze two models\nand find: (i) the last-layer steering vector behaves like a token-substitution\nbias concentrated on the first generated token, consistently boosting tokens\nsuch as \"To\" and \"Step\"; and (ii) the penultimate-layer steering vector leaves\nattention patterns largely unchanged and instead acts through the MLP and\nunembedding, preferentially up-weighting process words and structure symbols.\nThese results establish a principled framework for interpreting the behavioral\nchanges induced by reasoning training.", "AI": {"tldr": "The paper examines lightweight steering vectors in language models, offering enhanced interpretability and performance via reinforcement learning objectives.", "motivation": "Understanding the mechanisms of reasoning training's impact on language model computations.", "method": "Use of reinforcement-learning objectives with steering vectors and analysis methods such as logit-lens readouts and circuit analyses.", "result": "Steering vectors influence token generation either as biases or structural adjustments in different layers.", "conclusion": "The work presents a structured framework for interpreting how reasoning training modifies language model behavior."}}
{"id": "2509.06105", "pdf": "https://arxiv.org/pdf/2509.06105", "abs": "https://arxiv.org/abs/2509.06105", "authors": ["Yating Huang", "Ziyan Huang", "Lintao Xiang", "Qijun Yang", "Hujun Yin"], "title": "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology", "categories": ["cs.CV"], "comment": "Accept by EMNLP2025", "summary": "Accurate analysis of pathological images is essential for automated tumor\ndiagnosis but remains challenging due to high structural similarity and subtle\nmorphological variations in tissue images. Current vision-language (VL) models\noften struggle to capture the complex reasoning required for interpreting\nstructured pathological reports. To address these limitations, we propose\nPathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in\nhierarchical semantic understanding and compositional reasoning within the\npathology domain. Results of this benchmark reveal that existing VL models fail\nto effectively model intricate cross-modal relationships, hence limiting their\napplicability in clinical setting. To overcome this, we further introduce a\npathology-specific VL training scheme that generates enhanced and perturbed\nsamples for multimodal contrastive learning. Experimental evaluations\ndemonstrate that our approach achieves state-of-the-art performance on\nPathoHR-Bench and six additional pathology datasets, highlighting its\neffectiveness in fine-grained pathology representation.", "AI": {"tldr": "This paper introduces PathoHR-Bench, a benchmark to assess vision-language (VL) models in pathology, showing current models' limitations and proposing a pathology-specific VL training scheme that improves performance.", "motivation": "The study addresses the challenges of accurately analyzing pathological images and the inadequacy of existing vision-language models in capturing the complex reasoning required for this domain.", "method": "The researchers created PathoHR-Bench to evaluate hierarchical semantic understanding and developed a pathology-specific VL training scheme combining enhanced and perturbed sample generation for multimodal contrastive learning.", "result": "Their method achieved state-of-the-art results on PathoHR-Bench and six other pathology datasets, demonstrating its effectiveness.", "conclusion": "This approach enhances the applicability of VL models for fine-grained pathology representation and clinical use, overcoming current limitations."}}
{"id": "2509.06609", "pdf": "https://arxiv.org/pdf/2509.06609", "abs": "https://arxiv.org/abs/2509.06609", "authors": ["Junjun Pan", "Yu Zheng", "Yue Tan", "Yixin Liu"], "title": "A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models", "categories": ["cs.LG"], "comment": "Accepted by ICKG 2025. 8 pages, 5 figures", "summary": "Graph anomaly detection (GAD) has attracted increasing attention in recent\nyears for identifying malicious samples in a wide range of graph-based\napplications, such as social media and e-commerce. However, most GAD methods\nassume identical training and testing distributions and are tailored to\nspecific tasks, resulting in limited adaptability to real-world scenarios such\nas shifting data distributions and scarce training samples in new applications.\nTo address the limitations, recent work has focused on improving the\ngeneralization capability of GAD models through transfer learning that\nleverages knowledge from related domains to enhance detection performance, or\ndeveloping \"one-for-all\" GAD foundation models that generalize across multiple\napplications. Since a systematic understanding of generalization in GAD is\nstill lacking, in this paper, we provide a comprehensive review of\ngeneralization in GAD. We first trace the evolution of generalization in GAD\nand formalize the problem settings, which further leads to our systematic\ntaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive\nreview is conducted for the existing generalized GAD methods. Finally, we\nidentify current open challenges and suggest future directions to inspire\nfuture research in this emerging field.", "AI": {"tldr": "This paper reviews advances in generalized graph anomaly detection (GAD), addressing challenges like shifting data distributions and limited training samples, and provides taxonomy and reviews existing methods.", "motivation": "To overcome limitations of current GAD approaches that struggle with real-world scenarios such as shifting distributions and limited training samples, by improving generalization.", "method": "The paper provides a systematic review, formalizes problem settings, and introduces a taxonomy for generalized GAD methods, highlighting the importance of transfer learning and foundation models.", "result": "A comprehensive and up-to-date review of generalized GAD approaches, including the identification of key challenges and describing current methodologies.", "conclusion": "The review offers insights into gaps and challenges in generalized GAD while providing a roadmap for future research directions in the field."}}
{"id": "2509.06116", "pdf": "https://arxiv.org/pdf/2509.06116", "abs": "https://arxiv.org/abs/2509.06116", "authors": ["Giulia Bonino", "Luca Alberto Rizzo"], "title": "CARDIE: clustering algorithm on relevant descriptors for image enhancement", "categories": ["cs.CV", "I.4.8"], "comment": null, "summary": "Automatic image clustering is a cornerstone of computer vision, yet its\napplication to image enhancement remains limited, primarily due to the\ndifficulty of defining clusters that are meaningful for this specific task. To\naddress this issue, we introduce CARDIE, an unsupervised algorithm that\nclusters images based on their color and luminosity content. In addition, we\nintroduce a method to quantify the impact of image enhancement algorithms on\nluminance distribution and local variance. Using this method, we demonstrate\nthat CARDIE produces clusters more relevant to image enhancement than those\nderived from semantic image attributes. Furthermore, we demonstrate that CARDIE\nclusters can be leveraged to resample image enhancement datasets, leading to\nimproved performance for tone mapping and denoising algorithms. To encourage\nadoption and ensure reproducibility, we publicly release CARDIE code on our\nGitHub.", "AI": {"tldr": "The paper introduces CARDIE, an unsupervised image clustering algorithm tailored for image enhancement, and demonstrates its relevance and utility in enhancing datasets for tone mapping and denoising.", "motivation": "Despite advancements in automatic image clustering, its application in image enhancement is underdeveloped due to the challenge of defining meaningful clusters for this task.", "method": "CARDIE clusters images based on color and luminosity content and includes a way to quantify image enhancement impact on luminance distribution and variance.", "result": "The study shows that CARDIE's clustering is more relevant to image enhancement tasks than semantic clustering, and its datasets lead to better performance for tone mapping and denoising algorithms.", "conclusion": "CARDIE offers a significant contribution to the field of image enhancement by enabling improved clustering and dataset resampling, with publicly available code enhancing reproducibility."}}
{"id": "2509.06620", "pdf": "https://arxiv.org/pdf/2509.06620", "abs": "https://arxiv.org/abs/2509.06620", "authors": ["Chen Xie", "Gaofeng Wu", "Kaidong Wang", "Zihao Zhu", "Xiaoshu Luo", "Yan Liang", "Feiyu Quan", "Ruoxi Wu", "Xianghui Huang", "Han Zhang"], "title": "BEAM: Brainwave Empathy Assessment Model for Early Childhood", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Empathy in young children is crucial for their social and emotional\ndevelopment, yet predicting it remains challenging. Traditional methods often\nonly rely on self-reports or observer-based labeling, which are susceptible to\nbias and fail to objectively capture the process of empathy formation. EEG\noffers an objective alternative; however, current approaches primarily extract\nstatic patterns, neglecting temporal dynamics. To overcome these limitations,\nwe propose a novel deep learning framework, the Brainwave Empathy Assessment\nModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAM\nleverages multi-view EEG signals to capture both cognitive and emotional\ndimensions of empathy. The framework comprises three key components: 1) a\nLaBraM-based encoder for effective spatio-temporal feature extraction, 2) a\nfeature fusion module to integrate complementary information from multi-view\nsignals, and 3) a contrastive learning module to enhance class separation.\nValidated on the CBCP dataset, BEAM outperforms state-of-the-art methods across\nmultiple metrics, demonstrating its potential for objective empathy assessment\nand providing a preliminary insight into early interventions in children's\nprosocial development.", "AI": {"tldr": "This paper proposes a deep learning framework called BEAM to predict empathy levels in young children using multi-view EEG signals.", "motivation": "Predicting empathy in young children is difficult due to the limitations of traditional self-reports or observer-based methods, which fail to objectively capture the process.", "method": "The paper introduces BEAM, a framework comprising a LaBraM-based encoder for spatio-temporal feature extraction, a feature fusion module, and a contrastive learning module, to analyze EEG signals.", "result": "BEAM shows better performance than state-of-the-art methods on the CBCP dataset in predicting empathy levels.", "conclusion": "The study demonstrates BEAM's effectiveness for objective empathy assessment and its applicability for interventions in children's prosocial development."}}
{"id": "2509.05392", "pdf": "https://arxiv.org/pdf/2509.05392", "abs": "https://arxiv.org/abs/2509.05392", "authors": ["Qurat Ul Ain", "Mohamed Amine Chatti", "Jean Qussa", "Amr Shakhshir", "Rawaa Alatrash", "Shoeb Joarder"], "title": "An Optimized Pipeline for Automatic Educational Knowledge Graph Construction", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted at IJCKG 2025", "summary": "The automatic construction of Educational Knowledge Graphs (EduKGs) is\nessential for domain knowledge modeling by extracting meaningful\nrepresentations from learning materials. Despite growing interest, identifying\na scalable and reliable approach for automatic EduKG generation remains a\nchallenge. In an attempt to develop a unified and robust pipeline for automatic\nEduKG construction, in this study we propose a pipeline for automatic EduKG\nconstruction from PDF learning materials. The process begins with generating\nslide-level EduKGs from individual pages/slides, which are then merged to form\na comprehensive EduKG representing the entire learning material. We evaluate\nthe accuracy of the EduKG generated from the proposed pipeline in our MOOC\nplatform, CourseMapper. The observed accuracy, while indicative of partial\nsuccess, is relatively low particularly in the educational context, where the\nreliability of knowledge representations is critical for supporting meaningful\nlearning. To address this, we introduce targeted optimizations across multiple\npipeline components. The optimized pipeline achieves a 17.5% improvement in\naccuracy and a tenfold increase in processing efficiency. Our approach offers a\nholistic, scalable and end-to-end pipeline for automatic EduKG construction,\nadaptable to diverse educational contexts, and supports improved semantic\nrepresentation of learning content.", "AI": {"tldr": "This study proposes an optimized pipeline for automatic construction of Educational Knowledge Graphs (EduKGs) from PDF learning materials. The pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in efficiency.", "motivation": "The motivation is to address the challenge of constructing scalable and reliable Educational Knowledge Graphs (EduKGs) to enhance domain knowledge modeling from learning materials.", "method": "A two-step pipeline is developed: (1) generating slide-level EduKGs from individual slides/pages, and (2) merging these EduKGs to form a comprehensive graph. Optimizations are applied to various pipeline components to enhance accuracy and efficiency.", "result": "The optimized pipeline showed a 17.5% accuracy improvement and a tenfold increase in processing efficiency when applied on the CourseMapper MOOC platform.", "conclusion": "The paper presents an enhanced end-to-end EduKG construction pipeline that is adaptable to diverse educational contexts, offering improved semantic representation and processing scalability."}}
{"id": "2509.06122", "pdf": "https://arxiv.org/pdf/2509.06122", "abs": "https://arxiv.org/abs/2509.06122", "authors": ["Tang Sui", "Songxi Yang", "Qunying Huang"], "title": "SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multispectral and hyperspectral imagery are widely used in agriculture,\nenvironmental monitoring, and urban planning due to their complementary spatial\nand spectral characteristics. A fundamental trade-off persists: multispectral\nimagery offers high spatial but limited spectral resolution, while\nhyperspectral imagery provides rich spectra at lower spatial resolution. Prior\nhyperspectral generation approaches (e.g., pan-sharpening variants, matrix\nfactorization, CNNs) often struggle to jointly preserve spatial detail and\nspectral fidelity. In response, we propose SpecSwin3D, a transformer-based\nmodel that generates hyperspectral imagery from multispectral inputs while\npreserving both spatial and spectral quality. Specifically, SpecSwin3D takes\nfive multispectral bands as input and reconstructs 224 hyperspectral bands at\nthe same spatial resolution. In addition, we observe that reconstruction errors\ngrow for hyperspectral bands spectrally distant from the input bands. To\naddress this, we introduce a cascade training strategy that progressively\nexpands the spectral range to stabilize learning and improve fidelity.\nMoreover, we design an optimized band sequence that strategically repeats and\norders the five selected multispectral bands to better capture pairwise\nrelations within a 3D shifted-window transformer framework. Quantitatively, our\nmodel achieves a PSNR of 35.82 dB, SAM of 2.40{\\deg}, and SSIM of 0.96,\noutperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by\nmore than half. Beyond reconstruction, we further demonstrate the practical\nvalue of SpecSwin3D on two downstream tasks, including land use classification\nand burnt area segmentation.", "AI": {"tldr": "The paper introduces SpecSwin3D, a transformer-based model that generates high-quality hyperspectral imagery from multispectral inputs, addressing spatial and spectral resolution trade-offs effectively.", "motivation": "Hyperspectral and multispectral imagery have trade-offs in spatial and spectral resolution, and existing methods struggle to maintain both spatial and spectral fidelity in hyperspectral generation.", "method": "SpecSwin3D is a transformer-based model that reconstructs hyperspectral imagery using five multispectral bands as input. It uses cascade training to enhance fidelity and an optimized band sequence within a 3D shifted-window transformer framework.", "result": "SpecSwin3D achieves significant improvements in metrics like PSNR, SAM, and SSIM compared to baseline models and demonstrates practical applicability in land use classification and burnt area segmentation.", "conclusion": "SpecSwin3D successfully balances spatial detail and spectral fidelity, outperforming existing methods and showing strong potential for practical applications."}}
{"id": "2509.06640", "pdf": "https://arxiv.org/pdf/2509.06640", "abs": "https://arxiv.org/abs/2509.06640", "authors": ["Yung-Fu Chen", "Sen Lin", "Anish Arora"], "title": "Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "We propose a simple algorithm that needs only a few data samples from a\nsingle graph for learning local routing policies that generalize across a rich\nclass of geometric random graphs in Euclidean metric spaces. We thus solve the\nall-pairs near-shortest path problem by training deep neural networks (DNNs)\nthat let each graph node efficiently and scalably route (i.e., forward) packets\nby considering only the node's state and the state of the neighboring nodes.\nOur algorithm design exploits network domain knowledge in the selection of\ninput features and design of the policy function for learning an approximately\noptimal policy. Domain knowledge also provides theoretical assurance that the\nchoice of a ``seed graph'' and its node data sampling suffices for\ngeneralizable learning. Remarkably, one of these DNNs we train -- using\ndistance-to-destination as the only input feature -- learns a policy that\nexactly matches the well-known Greedy Forwarding policy, which forwards packets\nto the neighbor with the shortest distance to the destination. We also learn a\nnew policy, which we call GreedyTensile routing -- using both\ndistance-to-destination and node stretch as the input features -- that almost\nalways outperforms greedy forwarding. We demonstrate the explainability and\nultra-low latency run-time operation of Greedy Tensile routing by symbolically\ninterpreting its DNN in low-complexity terms of two linear actions.", "AI": {"tldr": "A simple algorithm is introduced to train deep neural networks (DNNs) for routing packets in geometric random graphs, utilizing minimal samples and exploiting domain knowledge to ensure efficient, scalable, and generalizable routing policies.", "motivation": "Routing and forwarding packets efficiently in complex network graphs can be computationally expensive and require scalable generalizable algorithms.", "method": "Deep neural networks (DNNs) are trained on minimal data samples, leveraging domain knowledge in feature selection and policy design, to develop routing policies based on local node states and neighbors.", "result": "One DNN learns an exact replication of the Greedy Forwarding policy using distance-to-destination data, while another DNN develops the superior GreedyTensile routing policy using additional input features like node stretch.", "conclusion": "The proposed GreedyTensile routing policy delivers explainability, better performance than standard greedy forwarding, and ultra-low latency operations."}}
{"id": "2509.05393", "pdf": "https://arxiv.org/pdf/2509.05393", "abs": "https://arxiv.org/abs/2509.05393", "authors": ["Rawaa Alatrash", "Mohamed Amine Chatti", "Nasha Wibowo", "Qurat Ul Ain"], "title": "Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted at IJCKG 2025", "summary": "Educational Knowledge Graphs (EduKGs) organize various learning entities and\ntheir relationships to support structured and adaptive learning. Prerequisite\nrelationships (PRs) are critical in EduKGs for defining the logical order in\nwhich concepts should be learned. However, the current EduKG in the MOOC\nplatform CourseMapper lacks explicit PR links, and manually annotating them is\ntime-consuming and inconsistent. To address this, we propose an unsupervised\nmethod for automatically inferring concept PRs without relying on labeled data.\nWe define ten criteria based on document-based, Wikipedia hyperlink-based,\ngraph-based, and text-based features, and combine them using a voting algorithm\nto robustly capture PRs in educational content. Experiments on benchmark\ndatasets show that our approach achieves higher precision than existing methods\nwhile maintaining scalability and adaptability, thus providing reliable support\nfor sequence-aware learning in CourseMapper.", "AI": {"tldr": "The paper proposes an unsupervised method to infer prerequisite relationships (PRs) for concepts in educational knowledge graphs (EduKGs) without labeled data, achieving better precision and scalability.", "motivation": "Current EduKGs in platforms like CourseMapper lack explicit PR links, and manually annotating these links is inefficient, inconsistent, and labor-intensive.", "method": "The authors define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features and employ a voting algorithm to infer PRs automatically and robustly.", "result": "Experiments on benchmark datasets demonstrate that the proposed approach outperforms existing methods in precision while maintaining scalability and adaptability.", "conclusion": "The method enhances sequence-aware learning in EduKGs like CourseMapper, providing accurate and efficient automatic inference of PRs for learning concepts."}}
{"id": "2509.06142", "pdf": "https://arxiv.org/pdf/2509.06142", "abs": "https://arxiv.org/abs/2509.06142", "authors": ["Zhengquan Luo", "Chi Liu", "Dongfu Xiao", "Zhen Yu", "Yueye Wang", "Tianqing Zhu"], "title": "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving", "categories": ["cs.CV"], "comment": null, "summary": "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.", "AI": {"tldr": "This paper introduces RetinaGuard, a framework to protect biometric privacy in medical images by obscuring retinal age using a generative adversarial mechanism while maintaining diagnostic utility.", "motivation": "To address the privacy risks posed by biometric data, specifically retinal age derived from medical images, which can inadvertently lead to sensitive bioinformation leakage.", "method": "The proposed method, RetinaGuard, uses a generative adversarial masking mechanism and a multiple-to-one knowledge distillation strategy to obscure retinal age while preserving visual quality and diagnostic capabilities.", "result": "RetinaGuard effectively obfuscated retinal age predictions with minimal impact on image quality and preserved diagnostic features. It also demonstrated adaptability to other medical image biomarkers.", "conclusion": "RetinaGuard provides a universal privacy defense mechanism for medical images, ensuring biometric privacy without compromising their diagnostic utility."}}
{"id": "2509.06656", "pdf": "https://arxiv.org/pdf/2509.06656", "abs": "https://arxiv.org/abs/2509.06656", "authors": ["Yuanyuan Wu", "Zhenlin Qin", "Leizhen Wang", "Xiaolei Ma", "Zhenliang Ma"], "title": "Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives", "categories": ["cs.LG"], "comment": null, "summary": "Understanding and modeling individual travel behavior responses is crucial\nfor urban mobility regulation and policy evaluation. The Markov decision\nprocess (MDP) provides a structured framework for dynamic travel behavior\nmodeling at the individual level. However, solving an MDP in this context is\nhighly data-intensive and faces challenges of data quantity, spatial-temporal\ncoverage, and situational diversity. To address these, we propose a\ngroup-effect-enhanced generative adversarial imitation learning (gcGAIL) model\nthat improves the individual behavior modeling efficiency by leveraging shared\nbehavioral patterns among passenger groups. We validate the gcGAIL model using\na public transport fare-discount case study and compare against\nstate-of-the-art benchmarks, including adversarial inverse reinforcement\nlearning (AIRL), baseline GAIL, and conditional GAIL. Experimental results\ndemonstrate that gcGAIL outperforms these methods in learning individual travel\nbehavior responses to incentives over time in terms of accuracy,\ngeneralization, and pattern demonstration efficiency. Notably, gcGAIL is robust\nto spatial variation, data sparsity, and behavioral diversity, maintaining\nstrong performance even with partial expert demonstrations and underrepresented\npassenger groups. The gcGAIL model predicts the individual behavior response at\nany time, providing the basis for personalized incentives to induce sustainable\nbehavior changes (better timing of incentive injections).", "AI": {"tldr": "The paper proposes a group-enhanced generative adversarial imitation learning (gcGAIL) model to improve efficiency in modeling individual travel behavior, validated through a case study and showing superior performance over benchmarks.", "motivation": "Understanding and improving individual travel behavior modeling for urban mobility regulation and policy evaluation is challenged by data limitations and complexity.", "method": "The authors introduce gcGAIL, a model built on generative adversarial imitation learning (GAIL), enhanced with group effects to harness shared behavioral patterns for increased modeling efficiency.", "result": "gcGAIL demonstrated superior accuracy, generalization, and efficiency in predicting individual travel behavior compared to state-of-the-art benchmarks. It also proved robust to data sparsity, spatial variation, and behavioral diversity.", "conclusion": "gcGAIL provides an effective tool for modeling individual behavior, enabling personalized incentives to encourage sustainable behavior changes and better timing of interventions."}}
{"id": "2509.06155", "pdf": "https://arxiv.org/pdf/2509.06155", "abs": "https://arxiv.org/abs/2509.06155", "authors": ["Duomin Wang", "Wei Zuo", "Aojie Li", "Ling-Hao Chen", "Xinyao Liao", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Daxin Jiang", "Gang Yu"], "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts", "categories": ["cs.CV"], "comment": "Project page: https://dorniwang.github.io/UniVerse-1/", "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.", "AI": {"tldr": "UniVerse-1 is a unified model capable of generating synchronized audio and video outputs, leveraging pre-trained experts through a deep fusion method and incorporating an advanced annotation pipeline.", "motivation": "The motivation for this paper is to address challenges in generating well-coordinated audio-video outputs and advancing efficiency and accuracy in temporal alignment, aiming to achieve performance comparable to state-of-the-art models.", "method": "The method involves using a 'stitching of experts' (SoE) approach to fuse pre-trained video and music models, coupled with an online annotation pipeline to ensure accurate synchronization during training.", "result": "UniVerse-1 demonstrates high-quality coordinated audio-visual output with enhanced temporal alignment, validated using approximately 7,600 hours of data and the newly introduced Verse-Bench benchmark dataset.", "conclusion": "The paper concludes by emphasizing the publicly available UniVerse-1 model and code as a resource for advancing research in audio-video generation, aiming to close gaps with leading models like Veo3."}}
{"id": "2509.06665", "pdf": "https://arxiv.org/pdf/2509.06665", "abs": "https://arxiv.org/abs/2509.06665", "authors": ["Xiaolu Fu", "Ziyuan Bao", "Eiman Kanjo"], "title": "TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 6 figures, 3 tables", "summary": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent\ntransportation systems; however, routing remains challenging due to dynamic\ntopologies, incomplete observations, and the limited resources of edge devices.\nExisting reinforcement learning (RL) approaches often assume fixed graph\nstructures and require retraining when network conditions change, making them\nunsuitable for deployment on constrained hardware. We present TrajAware, an\nRL-based framework designed for edge AI deployment in VANETs. TrajAware\nintegrates three components: (i) action space pruning, which reduces redundant\nneighbour options while preserving two-hop reachability, alleviating the curse\nof dimensionality; (ii) graph cross-attention, which maps pruned neighbours to\nthe global graph context, producing features that generalise across diverse\nnetwork sizes; and (iii) trajectory-aware prediction, which uses historical\nroutes and junction information to estimate real-time positions under partial\nobservations. We evaluate TrajAware in the open-source SUMO simulator using\nreal-world city maps with a leave-one-city-out setup. Results show that\nTrajAware achieves near-shortest paths and high delivery ratios while\nmaintaining efficiency suitable for constrained edge devices, outperforming\nstate-of-the-art baselines in both full and partial observation scenarios.", "AI": {"tldr": "TrajAware is a reinforcement learning-based framework designed for routing in vehicular ad hoc networks (VANETs), addressing dynamic topologies and resource constraints, while outperforming existing methods.", "motivation": "The paper aims to address the challenges in VANETs, such as dynamic topologies, incomplete observations, and the limited resources of edge devices, which hinder the efficiency of existing routing methods.", "method": "TrajAware integrates three novel components: (i) action space pruning to reduce dimensionality, (ii) graph cross-attention for adaptable feature representation, and (iii) trajectory-aware prediction for real-time position estimation under partial observations.", "result": "Using real-world city map simulations on the SUMO platform, TrajAware achieves near-optimal routing paths, high message delivery ratios, and efficient resource usage, outperforming current benchmarks even in challenging scenarios.", "conclusion": "TrajAware offers a robust and scalable solution for routing in VANETs, making it a suitable candidate for deployment on edge devices under dynamic and resource-constrained environments."}}
{"id": "2509.06920", "pdf": "https://arxiv.org/pdf/2509.06920", "abs": "https://arxiv.org/abs/2509.06920", "authors": ["Haywood Gelman", "John D. Hastings", "David Kenley"], "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "C.2.0; I.2.7; K.4.1; H.3.3"], "comment": "6 pages, 5 figures, 5 tables", "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.", "AI": {"tldr": "This study uses LLM Claude Sonnet 3.7 to synthesize syslog messages for insider threat detection, outperforming GPT-4o in accuracy metrics.", "motivation": "Existing approaches to insider threat detection rely on static datasets with limited accessibility, reducing the adaptability and effectiveness of detection models.", "method": "The researchers employ Claude Sonnet 3.7 to generate synthetic syslog datasets that mimic real-world distributions (with 1% insider threats) and evaluate its detection performance against GPT-4o using metrics like precision, recall, MCC, and ROC AUC.", "result": "Claude Sonnet 3.7 showed superior performance over GPT-4o by reducing false positives and achieving higher detection accuracy across most metrics.", "conclusion": "The paper demonstrates the potential for LLMs to enhance insider threat detection and enable reliable synthetic dataset generation for dynamic analysis models."}}
{"id": "2509.06165", "pdf": "https://arxiv.org/pdf/2509.06165", "abs": "https://arxiv.org/abs/2509.06165", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Jingkang Yang", "Ngan Le"], "title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures", "summary": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual\ncontent by detecting objects and modeling their temporal interactions as\nstructured graphs. Prior studies typically target either coarse-grained\nbox-level or fine-grained panoptic pixel-level VidSGG, often requiring\ntask-specific architectures and multi-stage training pipelines. In this paper,\nwe present UNO (UNified Object-centric VidSGG), a single-stage, unified\nframework that jointly addresses both tasks within an end-to-end architecture.\nUNO is designed to minimize task-specific modifications and maximize parameter\nsharing, enabling generalization across different levels of visual granularity.\nThe core of UNO is an extended slot attention mechanism that decomposes visual\nfeatures into object and relation slots. To ensure robust temporal modeling, we\nintroduce object temporal consistency learning, which enforces consistent\nobject representations across frames without relying on explicit tracking\nmodules. Additionally, a dynamic triplet prediction module links relation slots\nto corresponding object pairs, capturing evolving interactions over time. We\nevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results\ndemonstrate that UNO not only achieves competitive performance across both\ntasks but also offers improved efficiency through a unified, object-centric\ndesign.", "AI": {"tldr": "The paper introduces UNO, a unified framework for Video Scene Graph Generation (VidSGG) that can handle both box-level and pixel-level tasks using a single-stage architecture, emphasizing parameter sharing and end-to-end efficiency.", "motivation": "Previous approaches to VidSGG often require separate architectures and multi-stage training for box-level or pixel-level tasks, which lack scalability and generalization.", "method": "The paper proposes UNO, which uses an extended slot attention mechanism to decompose features into object and relation slots, supplemented by temporal consistency learning and dynamic triplet prediction for robust temporal interaction modeling.", "result": "UNO is tested on standard VidSGG benchmarks and demonstrates competitive performance in both box-level and pixel-level tasks, alongside better efficiency due to its unified design.", "conclusion": "UNO successfully simplifies VidSGG by creating a single, efficient framework that generalizes across multiple visual granularities, laying groundwork for scalable and unified dynamic content representation."}}
{"id": "2509.06694", "pdf": "https://arxiv.org/pdf/2509.06694", "abs": "https://arxiv.org/abs/2509.06694", "authors": ["Victor Toscano-Duran", "Rocio Gonzalez-Diaz", "Miguel A. Guti\u00e9rrez-Naranjo"], "title": "Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While it is well-established that artificial neural networks are\n\\emph{universal approximators} for continuous functions on compact domains,\nmany modern approaches rely on deep or overparameterized architectures that\nincur high computational costs. In this paper, a new type of \\emph{small\nshallow} neural network, called the \\emph{Barycentric Neural Network} ($\\BNN$),\nis proposed, which leverages a fixed set of \\emph{base points} and their\n\\emph{barycentric coordinates} to define both its structure and its parameters.\nWe demonstrate that our $\\BNN$ enables the exact representation of\n\\emph{continuous piecewise linear functions} ($\\CPLF$s), ensuring strict\ncontinuity across segments. Since any continuous function over a compact domain\ncan be approximated arbitrarily well by $\\CPLF$s, the $\\BNN$ naturally emerges\nas a flexible and interpretable tool for \\emph{function approximation}. Beyond\nthe use of this representation, the main contribution of the paper is the\nintroduction of a new variant of \\emph{persistent entropy}, a topological\nfeature that is stable and scale invariant, called the \\emph{length-weighted\npersistent entropy} ($\\LWPE$), which is weighted by the lifetime of topological\nfeatures. Our framework, which combines the $\\BNN$ with a loss function based\non our $\\LWPE$, aims to provide flexible and geometrically interpretable\napproximations of nonlinear continuous functions in resource-constrained\nsettings, such as those with limited base points for $\\BNN$ design and few\ntraining epochs. Instead of optimizing internal weights, our approach directly\n\\emph{optimizes the base points that define the $\\BNN$}. Experimental results\nshow that our approach achieves \\emph{superior and faster approximation\nperformance} compared to classical loss functions such as MSE, RMSE, MAE, and\nlog-cosh.", "AI": {"tldr": "The paper introduces Barycentric Neural Networks ($BNN$), a new type of shallow neural network optimized via base points, along with a novel loss function, length-weighted persistent entropy ($LWPE$). The framework promises efficient function approximation in resource-constrained settings.", "motivation": "The motivation stems from addressing the computational inefficiencies of deep and overparameterized neural networks, and providing a more flexible and interpretable solution for approximating continuous functions over compact domains.", "method": "The BNN leverages barycentric coordinates and base points to represent continuous piecewise linear functions ($CPLF$s). It incorporates a novel topological feature, $LWPE$, into the loss function, and directly optimizes the base points defining the network structure.", "result": "The experimental results highlight superior and faster function approximation performance of $BNN$ with $LWPE$ compared to classical loss functions like MSE, RMSE, MAE, and log-cosh.", "conclusion": "The framework effectively balances resource constraints with high-performance approximations, introducing a new neural network structure ($BNN$) and a novel loss function ($LWPE$) as robust tools for nonlinear continuous function modeling."}}
{"id": "2509.06941", "pdf": "https://arxiv.org/pdf/2509.06941", "abs": "https://arxiv.org/abs/2509.06941", "authors": ["Yuda Song", "Julia Kempe", "Remi Munos"], "title": "Outcome-based Exploration for LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment.", "AI": {"tldr": "The paper explores reinforcement learning (RL) for improving large language models, addressing the trade-off between final answer accuracy and generation diversity. It introduces outcome-based exploration methods to mitigate these issues.", "motivation": "To enhance reasoning abilities of large language models while addressing the decline in generation diversity due to outcome-based RL.", "method": "Outcome-based exploration is proposed, introducing historical exploration (UCB-style bonuses for rare answers) and batch exploration (penalizing within-batch repetition).", "result": "Experimental results on competition math tasks validate that the proposed methods improve accuracy and maintain diversity in generation.", "conclusion": "The study provides practical RL approaches, allowing improvements in reasoning abilities without sacrificing diversity, vital for scalable real-world deployments."}}
{"id": "2509.05399", "pdf": "https://arxiv.org/pdf/2509.05399", "abs": "https://arxiv.org/abs/2509.05399", "authors": ["Henry Graf\u00e9", "Hugo Van hamme"], "title": "Graph Connectionist Temporal Classification for Phoneme Recognition", "categories": ["eess.AS", "cs.AI", "68T10, 68T07", "I.2.7; I.5.4; H.5.1"], "comment": "Accepted to the IEEE Automatic Speech Recognition and Understanding\n  Workshop (ASRU 2025)", "summary": "Automatic Phoneme Recognition (APR) systems are often trained using pseudo\nphoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P)\nsystems. These G2P systems frequently output multiple possible pronunciations\nper word, but the standard Connectionist Temporal Classification (CTC) loss\ncannot account for such ambiguity during training. In this work, we adapt Graph\nTemporal Classification (GTC) to the APR setting. GTC enables training from a\ngraph of alternative phoneme sequences, allowing the model to consider multiple\npronunciations per word as valid supervision. Our experiments on English and\nDutch data sets show that incorporating multiple pronunciations per word into\nthe training loss consistently improves phoneme error rates compared to a\nbaseline trained with CTC. These results suggest that integrating pronunciation\nvariation into the loss function is a promising strategy for training APR\nsystems from noisy G2P-based supervision.", "AI": {"tldr": "The paper introduces Graph Temporal Classification (GTC) as a method for handling pronunciation ambiguities in Automatic Phoneme Recognition (APR) training, improving phoneme error rates compared to conventional CTC.", "motivation": "Standard APR training using Grapheme-to-Phoneme (G2P) systems struggles with pronunciation ambiguities, which degrade system performance.", "method": "Adaptation of Graph Temporal Classification (GTC) for APR systems to incorporate multiple valid phoneme sequences in the training loss.", "result": "Experiments on English and Dutch datasets demonstrate that GTC-based training yields lower phoneme error rates than CTC-based training.", "conclusion": "Integrating pronunciation variation into the training loss function using GTC is a beneficial approach for APR systems facing noisy G2P supervision."}}
{"id": "2509.06228", "pdf": "https://arxiv.org/pdf/2509.06228", "abs": "https://arxiv.org/abs/2509.06228", "authors": ["Amna Hassan", "Ilsa Afzaal", "Nouman Muneeb", "Aneeqa Batool", "Hamail Noor"], "title": "AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models", "categories": ["cs.CV"], "comment": "https://github.com/Amna-Hassan04/Fracture-Detection-Using-X-Rays-with-CNN", "summary": "Bone fractures present a major global health challenge, often resulting in\npain, reduced mobility, and productivity loss, particularly in low-resource\nsettings where access to expert radiology services is limited. Conventional\nimaging methods suffer from high costs, radiation exposure, and dependency on\nspecialized interpretation. To address this, we developed an AI-based solution\nfor automated fracture detection from X-ray images using a custom Convolutional\nNeural Network (CNN) and benchmarked it against transfer learning models\nincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on\nthe publicly available FracAtlas dataset, comprising 4,083 anonymized\nmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94\nprecision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.\nAlthough transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)\nperformed poorly in this specific setup, these results should be interpreted in\nlight of class imbalance and data set limitations. This work highlights the\npromise of lightweight CNNs for detecting fractures in X-rays and underscores\nthe importance of fair benchmarking, diverse datasets, and external validation\nfor clinical translation", "AI": {"tldr": "This paper addresses global health issues related to bone fractures by developing an AI-based CNN model for automated fracture detection in X-rays, achieving high accuracy and outperforming transfer learning models.", "motivation": "Bone fractures are a significant health problem causing pain and reduced mobility, particularly in low-resource settings with limited access to radiology services.", "method": "The study utilized a custom Convolutional Neural Network (CNN) trained on the publicly available FracAtlas dataset and compared its performance to transfer learning models like EfficientNetB0, MobileNetV2, and ResNet50.", "result": "The custom CNN demonstrated 95.96% accuracy, precision of 0.94, recall of 0.88, and an F1-score of 0.91, showing better performance than transfer learning models under class imbalance and dataset limitations.", "conclusion": "Lightweight CNNs show promise for fracture detection, emphasizing the need for diverse datasets, fair benchmarking, and external validation for clinical application."}}
{"id": "2509.06701", "pdf": "https://arxiv.org/pdf/2509.06701", "abs": "https://arxiv.org/abs/2509.06701", "authors": ["Su Hyeong Lee", "Risi Kondor", "Richard Ngo"], "title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.", "AI": {"tldr": "The paper presents a mathematical framework for understanding intelligent agency in neural models, exploring outcome distributions and alignment phenomena in AI systems.", "motivation": "To create a principled framework for understanding and aligning complex agentic behavior in AI systems.", "method": "Develops theoretical underpinnings based on probabilistic modeling, log score epistemic utility, and weighted logarithmic pooling, with a focus on recursive structure and agentic alignment.", "result": "Shows strict unanimity challenges in linear pooling and binary spaces, proposes solutions with more outcomes, and applies the theory to formalize alignment phenomena in LLMs.", "conclusion": "Establishes a foundation for how subagents can merge into coherent entities with implications for better alignment in agentic AI technologies."}}
{"id": "2509.06945", "pdf": "https://arxiv.org/pdf/2509.06945", "abs": "https://arxiv.org/abs/2509.06945", "authors": ["Wenxuan Huang", "Shuang Chen", "Zheyong Xie", "Shaosheng Cao", "Shixiang Tang", "Yufan Shen", "Qingyu Yin", "Wenbo Hu", "Xiaoman Wang", "Yuntian Tang", "Junbo Qiao", "Yue Guo", "Yao Hu", "Zhenfei Yin", "Philip Torr", "Yu Cheng", "Wanli Ouyang", "Shaohui Lin"], "title": "Interleaving Reasoning for Better Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .", "AI": {"tldr": "The paper introduces a framework called Interleaving Reasoning Generation (IRG) for improving Text-to-Image (T2I) generation by interleaving text-based reasoning and image generation, achieving significant performance gains on multiple benchmarks.", "motivation": "The motivation behind this paper is to bridge the gap in instruction following and detail preservation in multimodal models, specifically in text-to-image generation, by incorporating interleaved reasoning strategies.", "method": "The proposed method, IRG, alternates between text-based reasoning and image synthesis, guided by a framework called Interleaving Reasoning Generation Learning (IRGL). This involves a two-stage training process to establish core content and refine finer details using a curated dataset, IRGL-300K.", "result": "The IRG framework demonstrates state-of-the-art performance, with improvements of 5-10 points on various benchmarks (GenEval, WISE, TIIF, GenAI-Bench, OneIG-EN) and significant enhancements in visual quality and fidelity.", "conclusion": "The approach of interleaving reasoning and image generation effectively enhances text-to-image synthesis, presenting a notable advancement in the field. The resources for replication and further exploration will be publicly available."}}
{"id": "2509.05420", "pdf": "https://arxiv.org/pdf/2509.05420", "abs": "https://arxiv.org/abs/2509.05420", "authors": ["Benjamin Savinson", "David J. Norris", "Siddhartha Mishra", "Samuel Lanthaler"], "title": "Universality of physical neural networks with multivariate nonlinearity", "categories": ["physics.optics", "cs.AI", "physics.class-ph", "physics.comp-ph"], "comment": null, "summary": "The enormous energy demand of artificial intelligence is driving the\ndevelopment of alternative hardware for deep learning. Physical neural networks\ntry to exploit physical systems to perform machine learning more efficiently.\nIn particular, optical systems can calculate with light using negligible\nenergy. While their computational capabilities were long limited by the\nlinearity of optical materials, nonlinear computations have recently been\ndemonstrated through modified input encoding. Despite this breakthrough, our\ninability to determine if physical neural networks can learn arbitrary\nrelationships between data -- a key requirement for deep learning known as\nuniversality -- hinders further progress. Here we present a fundamental theorem\nthat establishes a universality condition for physical neural networks. It\nprovides a powerful mathematical criterion that imposes device constraints,\ndetailing how inputs should be encoded in the tunable parameters of the\nphysical system. Based on this result, we propose a scalable architecture using\nfree-space optics that is provably universal and achieves high accuracy on\nimage classification tasks. Further, by combining the theorem with temporal\nmultiplexing, we present a route to potentially huge effective system sizes in\nhighly practical but poorly scalable on-chip photonic devices. Our theorem and\nscaling methods apply beyond optical systems and inform the design of a wide\nclass of universal, energy-efficient physical neural networks, justifying\nfurther efforts in their development.", "AI": {"tldr": "This paper establishes a universality condition for physical neural networks, proposes a scalable optical architecture achieving high accuracy, and provides routes for large-scale implementations.", "motivation": "Artificial intelligence's high energy demand necessitates exploring energy-efficient hardware like physical neural networks, especially leveraging optical systems.", "method": "The authors develop a mathematical theorem to determine universality in physical neural networks and design a scalable architecture using free-space optics, supplemented with temporal multiplexing for system size scaling.", "result": "The proposed optical architecture demonstrates high accuracy in image classification tasks, and the theorem enables scalability for previously limited photonic devices.", "conclusion": "Their findings extend beyond optics, providing guidelines for designing universal, energy-efficient physical neural networks while advocating for further research in this area."}}
{"id": "2509.06246", "pdf": "https://arxiv.org/pdf/2509.06246", "abs": "https://arxiv.org/abs/2509.06246", "authors": ["Lucas Wojcik", "Luiz Coelho", "Roger Granada", "David Menotti"], "title": "Exploring Light-Weight Object Recognition for Real-Time Document Detection", "categories": ["cs.CV"], "comment": null, "summary": "Object Recognition and Document Skew Estimation have come a long way in terms\nof performance and efficiency. New models follow one of two directions:\nimproving performance using larger models, and improving efficiency using\nsmaller models. However, real-time document detection and rectification is a\nniche that is largely unexplored by the literature, yet it remains a vital step\nfor automatic information retrieval from visual documents. In this work, we\nstrive towards an efficient document detection pipeline that is satisfactory in\nterms of Optical Character Recognition (OCR) retrieval and faster than other\navailable solutions. We adapt IWPOD-Net, a license plate detection network, and\ntrain it for detection on NBID, a synthetic ID card dataset. We experiment with\ndata augmentation and cross-dataset validation with MIDV (another synthetic ID\nand passport document dataset) to find the optimal scenario for the model.\nOther methods from both the Object Recognition and Skew Estimation\nstate-of-the-art are evaluated for comparison with our approach. We use each\nmethod to detect and rectify the document, which is then read by an OCR system.\nThe OCR output is then evaluated using a novel OCR quality metric based on the\nLevenshtein distance. Since the end goal is to improve automatic information\nretrieval, we use the overall OCR quality as a performance metric. We observe\nthat with a promising model, document rectification does not have to be perfect\nto attain state-of-the-art performance scores. We show that our model is\nsmaller and more efficient than current state-of-the-art solutions while\nretaining a competitive OCR quality metric. All code is available at\nhttps://github.com/BOVIFOCR/iwpod-doc-corners.git", "AI": {"tldr": "This paper proposes an efficient pipeline for document detection and rectification, adapting the IWPOD-Net model for ID card and passport datasets.", "motivation": "To address the gap in real-time document detection and rectification, which is crucial for automatic information retrieval but underexplored.", "method": "The authors adapt IWPOD-Net, a license plate detection network, and retrain it for synthetic ID card datasets, using data augmentation and cross-dataset validation. Comparative evaluations with other methods are also conducted.", "result": "The proposed model is smaller and more efficient while maintaining competitive OCR quality metrics, demonstrating real-time effectiveness.", "conclusion": "Automatic information retrieval can achieve state-of-the-art performance without perfect document rectification, and the model outperforms others in efficiency and OCR quality."}}
{"id": "2509.06702", "pdf": "https://arxiv.org/pdf/2509.06702", "abs": "https://arxiv.org/abs/2509.06702", "authors": ["Ruben Bontorno", "Songyan Hou"], "title": "Nested Optimal Transport Distances", "categories": ["cs.LG", "q-fin.CP", "91G60, 60G07, 65C60"], "comment": "7 pages, 3 figures", "summary": "Simulating realistic financial time series is essential for stress testing,\nscenario generation, and decision-making under uncertainty. Despite advances in\ndeep generative models, there is no consensus metric for their evaluation. We\nfocus on generative AI for financial time series in decision-making\napplications and employ the nested optimal transport distance, a time-causal\nvariant of optimal transport distance, which is robust to tasks such as\nhedging, optimal stopping, and reinforcement learning. Moreover, we propose a\nstatistically consistent, naturally parallelizable algorithm for its\ncomputation, achieving substantial speedups over existing approaches.", "AI": {"tldr": "The paper introduces a robust evaluation metric for generative AI models used in financial time series and develops an efficient algorithm for its computation.", "motivation": "The need for realistic simulation of financial time series for applications like stress testing and scenario generation, coupled with the lack of a consensus metric for evaluating generative models.", "method": "The paper employs the nested optimal transport distance metric, tailored for time-causal properties, and introduces a statistically consistent and parallelizable algorithm to compute it efficiently.", "result": "The proposed algorithm significantly outperforms existing methods in terms of computational speed.", "conclusion": "The work provides a robust tool for evaluating generative AI models in finance, advancing their application in decision-making tasks like hedging and reinforcement learning."}}
{"id": "2509.06266", "pdf": "https://arxiv.org/pdf/2509.06266", "abs": "https://arxiv.org/abs/2509.06266", "authors": ["Mohsen Gholami", "Ahmad Rezaei", "Zhou Weimin", "Yong Zhang", "Mohammad Akbari"], "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.", "AI": {"tldr": "The paper introduces Ego3D-Bench, a benchmark for evaluating Vision-Language Models (VLMs) in ego-centric, multi-view spatial reasoning, and proposes Ego3D-VLM to enhance 3D spatial reasoning capabilities.", "motivation": "Current Vision-Language Models lack the ability to understand 3D spatial relationships, which is critical for embodied AI in real-world scenarios like robotics and autonomous vehicles.", "method": "The authors developed Ego3D-Bench with over 8,600 QA pairs for evaluating VLMs on spatial reasoning tasks and proposed Ego3D-VLM, a post-training framework that utilizes cognitive maps to improve spatial reasoning.", "result": "The study shows a significant gap between human-level and VLM performance in spatial reasoning, but Ego3D-VLM achieves a 12% improvement in multi-choice QA and a 56% improvement in absolute distance estimation on the benchmark.", "conclusion": "Ego3D-Bench and Ego3D-VLM could push VLMs closer to human-level spatial reasoning in real-world, ego-centric environments, offering modular tools for enhanced AI capabilities."}}
{"id": "2509.06714", "pdf": "https://arxiv.org/pdf/2509.06714", "abs": "https://arxiv.org/abs/2509.06714", "authors": ["Zakariae El Asri", "Ibrahim Laiche", "Cl\u00e9ment Rambour", "Olivier Sigaud", "Nicolas Thome"], "title": "RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms", "categories": ["cs.LG"], "comment": "IROS 2025", "summary": "Learning a controller directly on the robot requires extreme sample\nefficiency. Model-based reinforcement learning (RL) methods are the most sample\nefficient, but they often suffer from a too long inference time to meet the\nrobot control frequency requirements. In this paper, we address the sample\nefficiency and inference time challenges with two contributions. First, we\ndefine a general framework to deal with inference delays where the slow\ninference robot controller provides a sequence of actions to feed the\ncontrol-hungry robotic platform without execution gaps. Then, we compare\nseveral RL algorithms in the light of this framework and propose RT-HCP, an\nalgorithm that offers an excellent trade-off between performance, sample\nefficiency and inference time. We validate the superiority of RT-HCP with\nexperiments where we learn a controller directly on a simple but high frequency\nFURUTA pendulum platform. Code: github.com/elasriz/RTHCP", "AI": {"tldr": "The paper addresses the challenges of sample efficiency and inference time in model-based RL for robot control, proposing a new framework and algorithm called RT-HCP.", "motivation": "The motivation is to enable learning of a robot controller directly on hardware with extreme sample efficiency and to address the issue of slow inference times in model-based reinforcement learning.", "method": "The authors introduce a framework to handle inference delays by preloading a sequence of actions for continuous robot operation. They compare multiple RL algorithms and propose RT-HCP for balanced performance, efficiency, and inference time.", "result": "RT-HCP outperformed other algorithms in experiments conducted on a high-frequency Furuta pendulum platform, demonstrating its effectiveness.", "conclusion": "The proposed framework and RT-HCP algorithm show promise for real-world robot applications, combining sample efficiency and practical execution speed."}}
{"id": "2509.06282", "pdf": "https://arxiv.org/pdf/2509.06282", "abs": "https://arxiv.org/abs/2509.06282", "authors": ["Cecelia Soh", "Rizhao Cai", "Monalisha Paul", "Dennis Sng", "Alex Kot"], "title": "AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution", "categories": ["cs.CV"], "comment": "Paper accepted by the journal of Machine Intelligence Research\n  (JCR-Q1). To be in press soon", "summary": "Skin health and disease resistance are closely linked to the skin barrier\nfunction, which protects against environmental factors and water loss. Two key\nphysiological indicators can quantitatively represent this barrier function:\nskin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH\nand TEWL is valuable for the public to monitor skin conditions regularly,\ndiagnose dermatological issues, and personalize their skincare regimens.\nHowever, these measurements are not easily accessible to general users unless\nthey visit a dermatology clinic with specialized instruments. To tackle this\nproblem, we propose a systematic solution to estimate SH and TEWL from selfie\nfacial images remotely with smartphones. Our solution encompasses multiple\nstages, including SH/TEWL data collection, data preprocessing, and formulating\na novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.\nThrough experiments, we identified the annotation imbalance of the SH/TEWL data\nand proposed a symmetric-based contrastive regularization to reduce the model\nbias due to the imbalance effectively. This work is the first study to explore\nskin assessment from selfie facial images without physical measurements. It\nbridges the gap between computer vision and skin care research, enabling\nAI-driven accessible skin analysis for broader real-world applications.", "AI": {"tldr": "This paper proposes using smartphone selfies to estimate skin hydration (SH) and trans-epidermal water loss (TEWL) for accessible skin health analysis.", "motivation": "Current methods for measuring SH and TEWL require specialized instruments and visits to dermatology clinics, making skin health monitoring inaccessible to the general public.", "method": "The authors developed a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression. This includes data collection, preprocessing, and addressing annotation imbalance using symmetric-based contrastive regularization.", "result": "Experiments demonstrated that the proposed model effectively reduces biases caused by imbalanced annotations and accurately estimates SH and TEWL from selfie images.", "conclusion": "This is the first study enabling remote and accessible skin assessment using facial selfies, bridging the gap between AI-driven computer vision and skincare research."}}
{"id": "2509.06743", "pdf": "https://arxiv.org/pdf/2509.06743", "abs": "https://arxiv.org/abs/2509.06743", "authors": ["Filippo Guerranti", "Fabrizio Forte", "Simon Geisler", "Stephan G\u00fcnnemann"], "title": "Long-Range Graph Wavelet Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling long-range interactions, the propagation of information across\ndistant parts of a graph, is a central challenge in graph machine learning.\nGraph wavelets, inspired by multi-resolution signal processing, provide a\nprincipled way to capture both local and global structures. However, existing\nwavelet-based graph neural networks rely on finite-order polynomial\napproximations, which limit their receptive fields and hinder long-range\npropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which\ndecompose wavelet filters into complementary local and global components. Local\naggregation is handled with efficient low-order polynomials, while long-range\ninteractions are captured through a flexible spectral domain parameterization.\nThis hybrid design unifies short- and long-distance information flow within a\nprincipled wavelet framework. Experiments show that LR-GWN achieves\nstate-of-the-art performance among wavelet-based methods on long-range\nbenchmarks, while remaining competitive on short-range datasets.", "AI": {"tldr": "The paper introduces Long-Range Graph Wavelet Networks (LR-GWN), combining local and global graph information for better long-range interaction modeling.", "motivation": "To address the challenge of modeling long-range interactions in graph machine learning using wavelet-based methods that are limited by finite-order polynomial approximations.", "method": "The paper proposes LR-GWN, which decomposes wavelet filters into local and global components using low-order polynomials for local structures and a spectral domain parameterization for long-range interactions.", "result": "Experimental results show that LR-GWN achieves state-of-the-art performance for long-range benchmarks and remains competitive for short-range tasks.", "conclusion": "LR-GWN successfully unites local and global information flows in a wavelet framework, enhancing performance in both long-range and short-range graph learning tasks."}}
{"id": "2509.06291", "pdf": "https://arxiv.org/pdf/2509.06291", "abs": "https://arxiv.org/abs/2509.06291", "authors": ["Jiangnan Xie", "Xiaolong Zheng", "Liang Zheng"], "title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual Grounding (VG) aims to utilize given natural language queries to\nlocate specific target objects within images. While current transformer-based\napproaches demonstrate strong localization performance in standard scene (i.e,\nscenarios without any novel objects), they exhibit notable limitations in\nopen-vocabulary scene (i.e, both familiar and novel object categories during\ntesting). These limitations primarily stem from three key factors: (1)\nimperfect alignment between visual and linguistic modalities, (2) insufficient\ncross-modal feature fusion, and (3) ineffective utilization of semantic\nprototype information. To overcome these challenges, we present Prototype-Aware\nMultimodal Learning (PAML), an innovative framework that systematically\naddresses these issues through several key components: First, we leverage ALBEF\nto establish robust cross-modal alignment during initial feature encoding.\nSubsequently, our Visual Discriminative Feature Encoder selectively enhances\nsalient object representations while suppressing irrelevant visual context. The\nframework then incorporates a novel prototype discovering and inheriting\nmechanism that extracts and aggregates multi-neighbor semantic prototypes to\nfacilitate open-vocabulary recognition. These enriched features undergo\ncomprehensive multimodal integration through our Multi-stage Decoder before\nfinal bounding box regression. Extensive experiments across five benchmark\ndatasets validate our approach, showing competitive performance in standard\nscene while achieving state-of-the-art results in open-vocabulary scene. Our\ncode is available at https://github.com/plankXie/PAML.", "AI": {"tldr": "The paper proposes \"Prototype-Aware Multimodal Learning\" (PAML), addressing challenges in Visual Grounding (VG) for open-vocabulary scenes, achieving state-of-the-art results.", "motivation": "To overcome the limitations in open-vocabulary visual grounding, including imperfect visual-language alignment, insufficient cross-modal fusion, and ineffective semantic prototype use.", "method": "The paper introduces the PAML framework, leveraging ALBEF for feature alignment, a Visual Discriminative Feature Encoder for enhancing object representations, a prototype discovering mechanism for semantic enhancement, and a Multi-stage Decoder for multimodal integration.", "result": "PAML achieves strong results in standard scenes and state-of-the-art performance in open-vocabulary scenes across five benchmark datasets.", "conclusion": "The proposed PAML method enhances open-vocabulary visual grounding through novel multimodal integration and semantic prototype mechanisms, validated by experimental results."}}
{"id": "2509.06759", "pdf": "https://arxiv.org/pdf/2509.06759", "abs": "https://arxiv.org/abs/2509.06759", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Janis Dalins"], "title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for publication in the Proceedings of the 8th International\n  Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)", "summary": "Large Vision-Language Models (LVLMs) or multimodal large language models\nrepresent a significant advancement in artificial intelligence, enabling\nsystems to understand and generate content across both visual and textual\nmodalities. While large-scale pretraining has driven substantial progress,\nfine-tuning these models for aligning with human values or engaging in specific\ntasks or behaviors remains a critical challenge. Deep Reinforcement Learning\n(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for\nthis aligning process. While DRL enables models to optimize actions using\nreward signals instead of relying solely on supervised preference data, DPO\ndirectly aligns the policy with preferences, eliminating the need for an\nexplicit reward model. This overview explores paradigms for fine-tuning LVLMs,\nhighlighting how DRL and DPO techniques can be used to align models with human\npreferences and values, improve task performance, and enable adaptive\nmultimodal interaction. We categorize key approaches, examine sources of\npreference data, reward signals, and discuss open challenges such as\nscalability, sample efficiency, continual learning, generalization, and safety.\nThe goal is to provide a clear understanding of how DRL and DPO contribute to\nthe evolution of robust and human-aligned LVLMs.", "AI": {"tldr": "The paper reviews fine-tuning strategies for Large Vision-Language Models (LVLMs), focusing on Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) methods.", "motivation": "To address the challenges of aligning LVLMs with human values, preferences, and specialized tasks to improve their utility and adaptability.", "method": "The study examines DRL and DPO approaches for fine-tuning LVLMs. DRL employs reward signals to optimize actions, while DPO directly aligns models with preferences, avoiding explicit reward models.", "result": "The paper categorizes fine-tuning paradigms, evaluates preference data and reward signal sources, and highlights challenges like scalability, efficiency, and safety in aligning LVLMs.", "conclusion": "DRL and DPO techniques are pivotal for advancing robust and human-aligned LVLMs that excel in multimodal tasks while addressing pressing challenges like generalization and continual learning."}}
{"id": "2509.05448", "pdf": "https://arxiv.org/pdf/2509.05448", "abs": "https://arxiv.org/abs/2509.05448", "authors": ["Pingchuan Ma", "Benjamin Tod Jones", "Tsun-Hsuan Wang", "Minghao Guo", "Michal Piotr Lipiec", "Chuang Gan", "Wojciech Matusik"], "title": "Newton to Einstein: Axiom-Based Discovery via Game Design", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "This position paper argues that machine learning for scientific discovery\nshould shift from inductive pattern recognition to axiom-based reasoning. We\npropose a game design framework in which scientific inquiry is recast as a\nrule-evolving system: agents operate within environments governed by axioms and\nmodify them to explain outlier observations. Unlike conventional ML approaches\nthat operate within fixed assumptions, our method enables the discovery of new\ntheoretical structures through systematic rule adaptation. We demonstrate the\nfeasibility of this approach through preliminary experiments in logic-based\ngames, showing that agents can evolve axioms that solve previously unsolvable\nproblems. This framework offers a foundation for building machine learning\nsystems capable of creative, interpretable, and theory-driven discovery.", "AI": {"tldr": "The paper advocates for axiom-based reasoning in machine learning for scientific discovery and proposes a game design framework for evolving rules to explain outlier observations.", "motivation": "To address the limitations of traditional ML methods that rely on fixed assumptions, the paper aims to develop a framework for systematically discovering new theoretical structures.", "method": "The proposed method recasts scientific inquiry as a rule-evolving system, where agents modify axioms in logic-based environments to address outliers, demonstrated via logic-based games.", "result": "Preliminary experiments show that agents can evolve axioms to solve problems previously unsolvable by fixed-rule systems.", "conclusion": "The framework establishes a path for creating ML systems capable of creative and interpretable theory-driven scientific discovery through systematic axiom adaptation."}}
{"id": "2509.06306", "pdf": "https://arxiv.org/pdf/2509.06306", "abs": "https://arxiv.org/abs/2509.06306", "authors": ["Zhang Jing", "Pu Nan", "Xie Yu Xiang", "Guo Yanming", "Lu Qianqi", "Zou Shiwei", "Yan Jie", "Chen Yan"], "title": "Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is an emerging and challenging\nopen-world problem that has garnered increasing attention in recent years. Most\nexisting GCD methods focus on discovering categories in static images. However,\nrelying solely on static visual content is often insufficient to reliably\ndiscover novel categories. To bridge this gap, we extend the GCD problem to the\nvideo domain and introduce a new setting, termed Video-GCD. Thus, effectively\nintegrating multi-perspective information across time is crucial for accurate\nVideo-GCD. To tackle this challenge, we propose a novel Memory-guided\nConsistency-aware Contrastive Learning (MCCL) framework, which explicitly\ncaptures temporal-spatial cues and incorporates them into contrastive learning\nthrough a consistency-guided voting mechanism. MCCL consists of two core\ncomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided\nRepresentation Enhancement (MGRE). CACL exploits multiperspective temporal\nfeatures to estimate consistency scores between unlabeled instances, which are\nthen used to weight the contrastive loss accordingly. MGRE introduces a\ndual-level memory buffer that maintains both feature-level and logit-level\nrepresentations, providing global context to enhance intra-class compactness\nand inter-class separability. This in turn refines the consistency estimation\nin CACL, forming a mutually reinforcing feedback loop between representation\nlearning and consistency modeling. To facilitate a comprehensive evaluation, we\nconstruct a new and challenging Video-GCD benchmark, which includes action\nrecognition and bird classification video datasets. Extensive experiments\ndemonstrate that our method significantly outperforms competitive GCD\napproaches adapted from image-based settings, highlighting the importance of\ntemporal information for discovering novel categories in videos. The code will\nbe publicly available.", "AI": {"tldr": "This paper addresses the challenge of Generalized Category Discovery (GCD) in videos and introduces a method leveraging temporal-spatial information for better category discovery.", "motivation": "Existing GCD methods focus on static images, which are insufficient for novel category discovery. There's a need to explore temporal and spatial cues in video contexts.", "method": "The paper introduces the Memory-guided Consistency-aware Contrastive Learning (MCCL) framework, which combines consistency-aware contrastive learning and memory-enhanced representation techniques.", "result": "Experimental validation shows the proposed method outperforming image-based GCD adaptations on video benchmarks, emphasizing the importance of temporal information.", "conclusion": "Integrating multi-perspective temporal cues into contrastive learning is crucial for successful Video-GCD, and the proposed MCCL framework achieves significant advancements in this area."}}
{"id": "2509.06777", "pdf": "https://arxiv.org/pdf/2509.06777", "abs": "https://arxiv.org/abs/2509.06777", "authors": ["Kushal Bose", "Swagatam Das"], "title": "Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when\ntasks require long-range interactions. The problem arises from the presence of\nbottlenecks that limit the propagation of messages among distant nodes.\nRecently, graph rewiring methods modify edge connectivity and are expected to\nperform well on long-range tasks. Yet, graph rewiring compromises the inductive\nbias, incurring significant information loss in solving the downstream task.\nFurthermore, increasing channel capacity may overcome information bottlenecks\nbut enhance the parameter complexity of the model. To alleviate these\nshortcomings, we propose an efficient model-agnostic framework that\nasynchronously updates node features, unlike traditional synchronous message\npassing GNNs. Our framework creates node batches in every layer based on the\nnode centrality values. The features of the nodes belonging to these batches\nwill only get updated. Asynchronous message updates process information\nsequentially across layers, avoiding simultaneous compression into\nfixed-capacity channels. We also theoretically establish that our proposed\nframework maintains higher feature sensitivity bounds compared to standard\nsynchronous approaches. Our framework is applied to six standard graph datasets\nand two long-range datasets to perform graph classification and achieves\nimpressive performances with a $5\\%$ and $4\\%$ improvements on REDDIT-BINARY\nand Peptides-struct, respectively.", "AI": {"tldr": "Graph Neural Networks (GNNs) struggle with information bottlenecks in long-range tasks due to oversquashing. The authors propose an efficient framework that updates node features asynchronously to mitigate bottlenecks and achieve better performance.", "motivation": "Graph Neural Networks face challenges with oversquashing, compromising long-range task performance due to bottlenecks limiting message propagation.", "method": "The authors introduce an asynchronous node feature update mechanism that processes information sequentially via batches based on node centrality values, instead of using traditional synchronous updates.", "result": "The proposed approach achieves significant performance improvements, including a 5% and 4% boost on REDDIT-BINARY and Peptides-struct datasets, respectively, across graph classification tasks.", "conclusion": "The framework effectively alleviates information bottlenecks and maintains better feature sensitivity, demonstrating its potential to handle complex long-range tasks in GNNs."}}
{"id": "2509.06321", "pdf": "https://arxiv.org/pdf/2509.06321", "abs": "https://arxiv.org/abs/2509.06321", "authors": ["Mengcheng Lan", "Chaofeng Chen", "Jiaxing Xu", "Zongrui Li", "Yiping Ke", "Xudong Jiang", "Yingchen Yu", "Yunqing Zhao", "Song Bai"], "title": "Text4Seg++: Advancing Image Segmentation via Generative Language Modeling", "categories": ["cs.CV"], "comment": "Extended version of our conference paper arXiv:2410.09855", "summary": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks. However, effectively integrating image segmentation\ninto these models remains a significant challenge. In this work, we propose a\nnovel text-as-mask paradigm that casts image segmentation as a text generation\nproblem, eliminating the need for additional decoders and significantly\nsimplifying the segmentation process. Our key innovation is semantic\ndescriptors, a new textual representation of segmentation masks where each\nimage patch is mapped to its corresponding text label. We first introduce\nimage-wise semantic descriptors, a patch-aligned textual representation of\nsegmentation masks that integrates naturally into the language modeling\npipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding\n(R-RLE), which compresses redundant text sequences, reducing the length of\nsemantic descriptors by 74% and accelerating inference by $3\\times$, without\ncompromising performance. Building upon this, our initial framework Text4Seg\nachieves strong segmentation performance across a wide range of vision tasks.\nTo further improve granularity and compactness, we propose box-wise semantic\ndescriptors, which localizes regions of interest using bounding boxes and\nrepresents region masks via structured mask tokens called semantic bricks. This\nleads to our refined model, Text4Seg++, which formulates segmentation as a\nnext-brick prediction task, combining precision, scalability, and generative\nefficiency. Comprehensive experiments on natural and remote sensing datasets\nshow that Text4Seg++ consistently outperforms state-of-the-art models across\ndiverse benchmarks without any task-specific fine-tuning, while remaining\ncompatible with existing MLLM backbones. Our work highlights the effectiveness,\nscalability, and generalizability of text-driven image segmentation within the\nMLLM framework.", "AI": {"tldr": "This paper introduces a method to integrate image segmentation into Multimodal Large Language Models (MLLMs) by converting it into a text generation problem using text-based representations of segmentation masks.", "motivation": "While MLLMs excel in vision-language tasks, incorporating image segmentation into these models faces significant challenges. A new paradigm is needed to simplify and improve segmentation within the context of MLLMs.", "method": "The authors propose a 'text-as-mask' paradigm, introducing semantic descriptors that turn segmentation masks into patch-aligned text labels. They optimize these descriptors using Row-wise Run-Length Encoding (R-RLE) and later enhance the approach with box-wise semantic descriptors and semantic bricks in the Text4Seg and Text4Seg++ frameworks.", "result": "The proposed frameworks, Text4Seg and its refinement Text4Seg++, achieve state-of-the-art segmentation performance across diverse datasets without task-specific fine-tuning. R-RLE leads to a 74% reduction in text length and a 3\u00d7 faster inference.", "conclusion": "The study establishes that text-driven segmentation can be highly effective, scalable, and generalizable within MLLMs, providing strong performance and compatibility with existing backbones."}}
{"id": "2509.06782", "pdf": "https://arxiv.org/pdf/2509.06782", "abs": "https://arxiv.org/abs/2509.06782", "authors": ["Vittorio Giammarino", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise\nfor domains such as autonomous navigation and locomotion, where collecting\ninteractive data is costly and unsafe. However, it remains challenging in\npractice due to the need to learn from datasets with limited coverage of the\nstate-action space and to generalize across long-horizon tasks. To improve on\nthese challenges, we propose a Physics-informed (Pi) regularized loss for value\nlearning, derived from the Eikonal Partial Differential Equation (PDE) and\nwhich induces a geometric inductive bias in the learned value function. Unlike\ngeneric gradient penalties that are primarily used to stabilize training, our\nformulation is grounded in continuous-time optimal control and encourages value\nfunctions to align with cost-to-go structures. The proposed regularizer is\nbroadly compatible with temporal-difference-based value learning and can be\nintegrated into existing Offline GCRL algorithms. When combined with\nHierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed\nHIQL (Pi-HIQL), yields significant improvements in both performance and\ngeneralization, with pronounced gains in stitching regimes and large-scale\nnavigation tasks.", "AI": {"tldr": "This paper introduces a physics-informed regularized loss for offline goal-conditioned reinforcement learning to improve value learning and generalization in tasks with limited data coverage.", "motivation": "Offline GCRL is a promising approach for areas like navigation where data collection is expensive and risky, but faces challenges like limited state-action space coverage and long-horizon task generalization.", "method": "The paper derives a physics-informed regularized loss based on the Eikonal PDE to impose geometric inductive bias on the value function, compatible with existing algorithms and integrated into HIQL to create Pi-HIQL.", "result": "Pi-HIQL demonstrates improved performance, generalization capabilities, and effectiveness in complex tasks, especially for stitching regimes and large-scale navigation.", "conclusion": "Incorporating physics-informed regularization enhances offline GCRL methods, aiding in overcoming practical limitations while boosting task performance and generalizability."}}
{"id": "2509.05471", "pdf": "https://arxiv.org/pdf/2509.05471", "abs": "https://arxiv.org/abs/2509.05471", "authors": ["Youjia Zheng", "Mohammad Zandsalimy", "Shanu Sushmita"], "title": "Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly vulnerable to a sophisticated\nform of adversarial prompting known as camouflaged jailbreaking. This method\nembeds malicious intent within seemingly benign language to evade existing\nsafety mechanisms. Unlike overt attacks, these subtle prompts exploit\ncontextual ambiguity and the flexible nature of language, posing significant\nchallenges to current defense systems. This paper investigates the construction\nand impact of camouflaged jailbreak prompts, emphasizing their deceptive\ncharacteristics and the limitations of traditional keyword-based detection\nmethods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,\ncontaining 500 curated examples (400 harmful and 100 benign prompts) designed\nto rigorously stress-test LLM safety protocols. In addition, we propose a\nmulti-faceted evaluation framework that measures harmfulness across seven\ndimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,\nHarmful Potential, Educational Value, Content Quality, and Compliance Score.\nOur findings reveal a stark contrast in LLM behavior: while models demonstrate\nhigh safety and content quality with benign inputs, they exhibit a significant\ndecline in performance and safety when confronted with camouflaged jailbreak\nattempts. This disparity underscores a pervasive vulnerability, highlighting\nthe urgent need for more nuanced and adaptive security strategies to ensure the\nresponsible and robust deployment of LLMs in real-world applications.", "AI": {"tldr": "This paper addresses the vulnerability of Large Language Models (LLMs) to camouflaged jailbreaking attempts, presenting a dataset and evaluation framework to analyze these threats.", "motivation": "The study is motivated by the increasing threat of camouflaged jailbreak prompts, which circumvent existing safety mechanisms and exploit language ambiguity in LLMs.", "method": "The authors created a benchmark dataset with 500 examples (400 harmful and 100 benign prompts) and developed a framework to evaluate harmfulness across seven dimensions.", "result": "The study found LLMs exhibit reduced safety and performance when exposed to camouflaged jailbreak prompts compared to benign inputs.", "conclusion": "Current LLM safety mechanisms are insufficient to handle camouflaged jailbreaks, necessitating advanced and adaptive security approaches for robust deployment."}}
{"id": "2509.06329", "pdf": "https://arxiv.org/pdf/2509.06329", "abs": "https://arxiv.org/abs/2509.06329", "authors": ["Ruiming Du", "Guangxun Zhai", "Tian Qiu", "Yu Jiang"], "title": "Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The precise characterization of plant morphology provides valuable insights\ninto plant environment interactions and genetic evolution. A key technology for\nextracting this information is 3D segmentation, which delineates individual\nplant organs from complex point clouds. Despite significant progress in general\n3D computer vision domains, the adoption of 3D segmentation for plant\nphenotyping remains limited by three major challenges: i) the scarcity of\nlarge-scale annotated datasets, ii) technical difficulties in adapting advanced\ndeep neural networks to plant point clouds, and iii) the lack of standardized\nbenchmarks and evaluation protocols tailored to plant science. This review\nsystematically addresses these barriers by: i) providing an overview of\nexisting 3D plant datasets in the context of general 3D segmentation domains,\nii) systematically summarizing deep learning-based methods for point cloud\nsemantic and instance segmentation, iii) introducing Plant Segmentation Studio\n(PSS), an open-source framework for reproducible benchmarking, and iv)\nconducting extensive quantitative experiments to evaluate representative\nnetworks and sim-to-real learning strategies. Our findings highlight the\nefficacy of sparse convolutional backbones and transformer-based instance\nsegmentation, while also emphasizing the complementary role of modeling-based\nand augmentation-based synthetic data generation for sim-to-real learning in\nreducing annotation demands. In general, this study bridges the gap between\nalgorithmic advances and practical deployment, providing immediate tools for\nresearchers and a roadmap for developing data-efficient and generalizable deep\nlearning solutions in 3D plant phenotyping. Data and code are available at\nhttps://github.com/perrydoremi/PlantSegStudio.", "AI": {"tldr": "The paper addresses the challenges in 3D segmentation for plant morphology by providing a review of datasets, deep learning methods, and introducing an open-source benchmarking framework called Plant Segmentation Studio (PSS).", "motivation": "To overcome challenges in using 3D segmentation for plant phenotyping caused by limited datasets, adaptation difficulties of advanced neural networks for plant point clouds, and lack of standardized benchmarks and evaluation methods.", "method": "The paper systematically reviews existing datasets, summarizes deep learning methods for segmentation, introduces an open-source benchmarking framework (PSS), and evaluates networks and sim-to-real learning strategies via experiments.", "result": "Sparse convolutional networks and transformer-based segmentation show strong performance. Synthetic data generation through modeling and augmentation is effective in reducing the need for manual annotation.", "conclusion": "This study bridges the gap between algorithmic advances and practical applications in 3D plant phenotyping, providing tools and a roadmap for developing data-efficient and generalizable deep learning solutions."}}
{"id": "2509.06786", "pdf": "https://arxiv.org/pdf/2509.06786", "abs": "https://arxiv.org/abs/2509.06786", "authors": ["Youbang Sun", "Xiang Wang", "Jie Fu", "Chaochao Lu", "Bowen Zhou"], "title": "\\texttt{R$^\\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World", "categories": ["cs.LG"], "comment": null, "summary": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose \\textit{safe-by-coevolution} as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce \\texttt{R$^2$AI} -- \\textit{Resistant and Resilient\nAI} -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. \\texttt{R$^2$AI} integrates \\textit{fast\nand slow safe models}, adversarial simulation and verification through a\n\\textit{safety wind tunnel}, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.", "AI": {"tldr": "This paper introduces 'safe-by-coevolution' as an approach to AI safety inspired by biological immunity, proposing the R$^2$AI framework for dynamic safety processes.", "motivation": "The paper aims to address the gap between advancing AI capabilities and slower progress in AI safety, especially the limitations of current paradigms in open-ended environments.", "method": "Proposes the 'safe-by-coevolution' paradigm and the R$^2$AI framework, featuring resistant and resilient AI models, adversarial simulation, and continual feedback loops for safety coevolution.", "result": "The R$^2$AI framework integrates fast and slow safe models and dynamic safety methods to tackle near-term vulnerabilities and long-term risks.", "conclusion": "The approach provides a proactive and scalable framework to maintain AI safety as it evolves, addressing both immediate and existential risks in dynamic environments."}}
{"id": "2509.05474", "pdf": "https://arxiv.org/pdf/2509.05474", "abs": "https://arxiv.org/abs/2509.05474", "authors": ["Mohammad Rashed Albous", "Anwaar AlKandari", "Abdel Latef Anouze"], "title": "From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) is rapidly transforming public-sector processes\nworldwide, yet standardized measures rarely address the unique drivers,\ngovernance models, and cultural nuances of the Gulf Cooperation Council (GCC)\ncountries. This study employs a theory-driven foundation derived from an\nin-depth analysis of literature review and six National AI Strategies (NASs),\ncoupled with a data-driven approach that utilizes a survey of 203 mid- and\nsenior-level government employees and advanced statistical techniques (K-Means\nclustering, Principal Component Analysis, and Partial Least Squares Structural\nEquation Modeling). By combining policy insights with empirical evidence, the\nresearch develops and validates a novel AI Adoption Index specifically tailored\nto the GCC public sector. Findings indicate that robust infrastructure and\nclear policy mandates exert the strongest influence on successful AI\nimplementations, overshadowing organizational readiness in early adoption\nstages. The combined model explains 70% of the variance in AI outcomes,\nsuggesting that resource-rich environments and top-down policy directives can\ndrive rapid but uneven technology uptake. By consolidating key dimensions\n(Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory\nEnvironment) into a single composite index, this study provides a holistic yet\ncontext-sensitive tool for benchmarking AI maturity. The index offers\nactionable guidance for policymakers seeking to harmonize large-scale\ndeployments with ethical and regulatory standards. Beyond advancing academic\ndiscourse, these insights inform more strategic allocation of resources,\ncross-country cooperation, and capacity-building initiatives, thereby\nsupporting sustained AI-driven transformation in the GCC region and beyond.", "AI": {"tldr": "This paper develops and validates an AI Adoption Index tailored to the GCC public sector and shows that infrastructure and policy mandates dominate AI success.", "motivation": "To address the lack of tailored measures for AI adoption in the GCC public sector, considering its unique governance models and cultural nuances.", "method": "The research combines a theory-driven foundation with empirical evidence, leveraging literature review, National AI Strategies analysis, and a survey of 203 government employees. Statistical methods include K-Means Clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling.", "result": "Findings highlight robust infrastructure and clear policy mandates as the strongest drivers of AI success, explaining 70% of the variance in AI outcomes. A composite index integrates dimensions like Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory Environment.", "conclusion": "The tailored AI Adoption Index aids policymakers in benchmarking AI maturity and aligning deployments with ethical and regulatory standards, fostering effective transformation in the GCC region."}}
{"id": "2509.06331", "pdf": "https://arxiv.org/pdf/2509.06331", "abs": "https://arxiv.org/abs/2509.06331", "authors": ["Md Sultanul Islam Ovi", "Mainul Hossain", "Md Badsha Biswas"], "title": "Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users", "categories": ["cs.CV"], "comment": "10 Pages, 9 Figures, 5 Tables", "summary": "Currency recognition systems often overlook usability and authenticity\nassessment, especially in low-resource environments where visually impaired\nusers and offline validation are common. While existing methods focus on\ndenomination classification, they typically ignore physical degradation and\nforgery, limiting their applicability in real-world conditions. This paper\npresents a unified framework for currency evaluation that integrates three\nmodules: denomination classification using lightweight CNN models, damage\nquantification through a novel Unified Currency Damage Index (UCDI), and\ncounterfeit detection using feature-based template matching. The dataset\nconsists of over 82,000 annotated images spanning clean, damaged, and\ncounterfeit notes. Our Custom_CNN model achieves high classification\nperformance with low parameter count. The UCDI metric provides a continuous\nusability score based on binary mask loss, chromatic distortion, and structural\nfeature loss. The counterfeit detection module demonstrates reliable\nidentification of forged notes across varied imaging conditions. The framework\nsupports real-time, on-device inference and addresses key deployment challenges\nin constrained environments. Results show that accurate, interpretable, and\ncompact solutions can support inclusive currency evaluation in practical\nsettings.", "AI": {"tldr": "A unified framework for currency evaluation integrating denomination classification, damage quantification using UCDI, and counterfeit detection.", "motivation": "Enhance currency recognition usability and authenticity in low-resource environments for visually impaired users and offline validation.", "method": "Three integrated modules: lightweight CNNs for denomination classification, UCDI metric for damage quantification, and feature-based template matching for counterfeit detection.", "result": "Custom_CNN with high performance and low parameter count, UCDI providing usability scores, and successful counterfeit detection across varied conditions.", "conclusion": "Real-time, efficient solutions enable inclusive currency evaluation addressing practical challenges in constrained environments."}}
{"id": "2509.06863", "pdf": "https://arxiv.org/pdf/2509.06863", "abs": "https://arxiv.org/abs/2509.06863", "authors": ["Bhavya Agrawalla", "Michal Nauman", "Khush Agarwal", "Aviral Kumar"], "title": "floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A hallmark of modern large-scale machine learning techniques is the use of\ntraining objectives that provide dense supervision to intermediate\ncomputations, such as teacher forcing the next token in language models or\ndenoising step-by-step in diffusion models. This enables models to learn\ncomplex functions in a generalizable manner. Motivated by this observation, we\ninvestigate the benefits of iterative computation for temporal difference (TD)\nmethods in reinforcement learning (RL). Typically they represent value\nfunctions in a monolithic fashion, without iterative compute. We introduce floq\n(flow-matching Q-functions), an approach that parameterizes the Q-function\nusing a velocity field and trains it using techniques from flow-matching,\ntypically used in generative modeling. This velocity field underneath the flow\nis trained using a TD-learning objective, which bootstraps from values produced\nby a target velocity field, computed by running multiple steps of numerical\nintegration. Crucially, floq allows for more fine-grained control and scaling\nof the Q-function capacity than monolithic architectures, by appropriately\nsetting the number of integration steps. Across a suite of challenging offline\nRL benchmarks and online fine-tuning tasks, floq improves performance by nearly\n1.8x. floq scales capacity far better than standard TD-learning architectures,\nhighlighting the potential of iterative computation for value learning.", "AI": {"tldr": "This paper introduces floq, a reinforcement learning (RL) approach that enhances temporal difference (TD) methods by employing iterative computation similar to generative modeling techniques, resulting in significantly improved performance.", "motivation": "To explore the benefits of iterative computation for TD methods in RL, inspired by its success in language models and diffusion models, and address the limitations in monolithic Q-function representation.", "method": "The authors propose floq, which parameterizes Q-functions using a velocity field and trains it with flow-matching techniques, employing TD-learning objectives and numerical integration for iterative computation.", "result": "Floq improves RL performance by nearly 1.8x across offline benchmarks and online tasks, demonstrating better capacity scaling than traditional TD-learning methods.", "conclusion": "Floq shows that iterative computation is highly effective for value learning in RL, enabling more fine-grained and scalable architectures compared to monolithic approaches."}}
{"id": "2509.06335", "pdf": "https://arxiv.org/pdf/2509.06335", "abs": "https://arxiv.org/abs/2509.06335", "authors": ["Tz-Ying Wu", "Sharath Nittur Sridhar", "Subarna Tripathi"], "title": "Harnessing Object Grounding for Time-Sensitive Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We propose to improve the time-sensitive video understanding (TSV) capability\nof video large language models (Video-LLMs) with grounded objects (GO). We\nhypothesize that TSV tasks can benefit from GO within frames, which is\nsupported by our preliminary experiments on LITA, a state-of-the-art Video-LLM\nfor reasoning temporal localization. While augmenting prompts with textual\ndescription of these object annotations improves the performance of LITA, it\nalso introduces extra token length and susceptibility to the noise in object\nlevel information. To address this, we propose GO-Tokenizer, a lightweight\nadd-on module for Video-LLMs leveraging off-the-shelf object detectors to\nencode compact object information on the fly. Experimental results demonstrate\nthat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its\ncounterpart utilizing textual description of objects in the prompt. The gain\ngeneralizes across different models, datasets and video understanding tasks\nsuch as reasoning temporal localization and dense captioning.", "AI": {"tldr": "This paper introduces GO-Tokenizer, a module to improve time-sensitive video understanding tasks in Video-LLMs by encoding object information compactly, demonstrating improved performance across multiple models and tasks.", "motivation": "To enhance Video-LLMs in tackling time-sensitive video understanding tasks by incorporating grounded object annotations.", "method": "The authors propose GO-Tokenizer, a lightweight module that utilizes object detectors to encode object information compactly, avoiding extra token length and susceptibility to noise from textual object descriptions.", "result": "GO-Tokenizer improved performance when pretraining Video-LLMs, outperforming vanilla models and textual description-based methods across datasets, models, and tasks such as temporal localization and dense captioning.", "conclusion": "Integrating GO-Tokenizer enhances Video-LLMs' time-sensitive video interpretation capabilities in a robust and generalized manner across diverse tasks."}}
{"id": "2509.06875", "pdf": "https://arxiv.org/pdf/2509.06875", "abs": "https://arxiv.org/abs/2509.06875", "authors": ["Sukumar Kishanthan", "Asela Hevapathige"], "title": "AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Class imbalance in machine learning poses a significant challenge, as skewed\ndatasets often hinder performance on minority classes. Traditional oversampling\ntechniques, which are commonly used to alleviate class imbalance, have several\ndrawbacks: they treat features independently, lack similarity-based controls,\nlimit sample diversity, and fail to manage synthetic variety effectively. To\novercome these issues, we introduce AxelSMOTE, an innovative agent-based\napproach that views data instances as autonomous agents engaging in complex\ninteractions. Based on Axelrod's cultural dissemination model, AxelSMOTE\nimplements four key innovations: (1) trait-based feature grouping to preserve\ncorrelations; (2) a similarity-based probabilistic exchange mechanism for\nmeaningful interactions; (3) Beta distribution blending for realistic\ninterpolation; and (4) controlled diversity injection to avoid overfitting.\nExperiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms\nstate-of-the-art sampling methods while maintaining computational efficiency.", "AI": {"tldr": "AxelSMOTE is a novel agent-based oversampling method for addressing class imbalance in machine learning datasets, outperforming existing techniques.", "motivation": "Class imbalance poses significant challenges for machine learning, particularly in handling minority classes. Existing oversampling methods are limited in preserving feature relationships, managing diversity, and preventing overfitting.", "method": "AxelSMOTE, based on Axelrod's cultural dissemination model, incorporates four key elements: feature grouping, probabilistic exchange for meaningful interactions, Beta distribution blending, and controlled diversity injection.", "result": "AxelSMOTE consistently outperforms state-of-the-art sampling methods across eight imbalanced datasets while remaining computationally efficient.", "conclusion": "AxelSMOTE provides an effective solution to class imbalance by addressing limitations of traditional methods and enhancing both performance and diversity control."}}
{"id": "2509.06336", "pdf": "https://arxiv.org/pdf/2509.06336", "abs": "https://arxiv.org/abs/2509.06336", "authors": ["Jeongmin Yu", "Susang Kim", "Kisu Lee", "Taekyoung Kwon", "Won-Yong Shin", "Ha Young Kim"], "title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted by ICCV 2025", "summary": "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain\nperformance by employing vision-language models like CLIP. However, existing\nCLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,\nfailing to detect critical spoofing clues. Moreover, these models rely on a\nsingle text prompt per class (e.g., 'live' or 'fake'), which limits\ngeneralization. To address these issues, we propose MVP-FAS, a novel framework\nincorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text\nPatch Alignment (MTPA). Both modules utilize multiple paraphrased texts to\ngenerate generalized features and reduce dependence on domain-specific text.\nMVS extracts local detailed spatial features and global context from patch\nembeddings by leveraging diverse texts with multiple perspectives. MTPA aligns\npatches with multiple text representations to improve semantic robustness.\nExtensive experiments demonstrate that MVP-FAS achieves superior generalization\nperformance, outperforming previous state-of-the-art methods on cross-domain\ndatasets. Code: https://github.com/Elune001/MVP-FAS.", "AI": {"tldr": "The paper introduces MVP-FAS, a face anti-spoofing framework using Multi-View Slot attention and Multi-Text Patch Alignment to improve generalization and detect sophisticated spoofing clues.", "motivation": "Current CLIP-based FAS models fail to fully exploit patch embedding tokens and rely on a single text prompt per class (e.g., 'live' or 'fake'), leading to limited generalization and inadequate spoofing clue detection.", "method": "The proposed framework, MVP-FAS, includes two key components: Multi-View Slot Attention (MVS) to generate spatial and contextual features, and Multi-Text Patch Alignment (MTPA) to align patches with diverse paraphrased text representations, thus enhancing semantic robustness.", "result": "MVP-FAS demonstrates superior performance in generalization and cross-domain face anti-spoofing tasks, surpassing state-of-the-art methods in experimental evaluations.", "conclusion": "The MVP-FAS framework effectively addresses limitations in existing CLIP-based FAS methods by leveraging multiple perspectives and paraphrased texts, achieving better generalization and cross-domain capabilities."}}
{"id": "2509.06351", "pdf": "https://arxiv.org/pdf/2509.06351", "abs": "https://arxiv.org/abs/2509.06351", "authors": ["Krithik Ramesh", "Ritvik Koneru"], "title": "A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Colorectal diseases, including inflammatory conditions and neoplasms, require\nquick, accurate care to be effectively treated. Traditional diagnostic\npipelines require extensive preparation and rely on separate, individual\nevaluations on histological images and colonoscopy footage, introducing\npossible variability and inefficiencies. This pilot study proposes a unified\ndeep learning network that uses convolutional neural networks (CN N s) to\nclassify both histopathological slides and colonoscopy video frames in one\npipeline. The pipeline integrates class-balancing learning, robust\naugmentation, and calibration methods to ensure accurate results. Static colon\nhistology images were taken from the PathMNIST dataset, and the lower\ngastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.\nThe CNN architecture used was ResNet-50. This study demonstrates an\ninterpretable and reproducible diagnostic pipeline that unifies multiple\ndiagnostic modalities to advance and ease the detection of colorectal diseases.", "AI": {"tldr": "This study develops a unified diagnostic pipeline using deep learning to classify histopathological slides and colonoscopy videos for detecting colorectal diseases.", "motivation": "Current diagnostic methods for colorectal diseases are inefficient and variable, requiring separate analyses of histology and colonoscopy data.", "method": "The study employs ResNet-50 CNN within a single pipeline using class-balancing learning, robust augmentation, and calibration methods, integrating data from PathMNIST and HyperKvasir datasets.", "result": "The unified CNN-based pipeline is interpretable and reproducible, enabling accurate classification of colorectal diagnostic images and videos.", "conclusion": "The proposed pipeline has the potential to improve diagnostic efficiency and accuracy for colorectal diseases by effectively combining multiple diagnostic modalities."}}
{"id": "2509.06918", "pdf": "https://arxiv.org/pdf/2509.06918", "abs": "https://arxiv.org/abs/2509.06918", "authors": ["Tarhib Al Azad", "Shahana Ibrahim"], "title": "Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings.", "AI": {"tldr": "The paper introduces a robust OOD detection framework that addresses the challenges posed by noisy training labels using a combination of loss correction and low-rank sparse decomposition techniques.", "motivation": "To enhance OOD detection in AI systems for safety-critical applications, especially under the unexplored scenario of noisy training labels, where existing methods fail to maintain effectiveness.", "method": "The proposed method integrates loss correction strategies from noisy label learning literature with low-rank and sparse decomposition methods from signal processing to create a robust OOD detection framework.", "result": "The framework outperformed state-of-the-art OOD detection techniques in extensive experiments on synthetic and real-world datasets, especially in scenarios with severe noisy labels.", "conclusion": "The paper offers a novel, effective framework for OOD detection under challenging conditions of noisy label settings, marking a significant improvement over existing methods."}}
{"id": "2509.06367", "pdf": "https://arxiv.org/pdf/2509.06367", "abs": "https://arxiv.org/abs/2509.06367", "authors": ["Aswini Kumar Patra", "Lingaraj Sahoo"], "title": "MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 6 Figures, 3 Tables", "summary": "Drought stress is a major threat to global crop productivity, making its\nearly and precise detection essential for sustainable agricultural management.\nTraditional approaches, though useful, are often time-consuming and\nlabor-intensive, which has motivated the adoption of deep learning methods. In\nrecent years, Convolutional Neural Network (CNN) and Vision Transformer\narchitectures have been widely explored for drought stress identification;\nhowever, these models generally rely on a large number of trainable parameters,\nrestricting their use in resource-limited and real-time agricultural settings.\nTo address this challenge, we propose a novel lightweight hybrid CNN framework\ninspired by ResNet, DenseNet, and MobileNet architectures. The framework\nachieves a remarkable 15-fold reduction in trainable parameters compared to\nconventional CNN and Vision Transformer models, while maintaining competitive\naccuracy. In addition, we introduce a machine unlearning mechanism based on a\ngradient norm-based influence function, which enables targeted removal of\nspecific training data influence, thereby improving model adaptability. The\nmethod was evaluated on an aerial image dataset of potato fields with\nexpert-annotated healthy and drought-stressed regions. Experimental results\nshow that our framework achieves high accuracy while substantially lowering\ncomputational costs. These findings highlight its potential as a practical,\nscalable, and adaptive solution for drought stress monitoring in precision\nagriculture, particularly under resource-constrained conditions.", "AI": {"tldr": "This paper proposes a lightweight CNN framework inspired by ResNet, DenseNet, and MobileNet to address drought stress detection in agriculture. It achieves significant parameter reduction while maintaining high accuracy and introduces a novel machine unlearning mechanism for adaptability.", "motivation": "Drought stress significantly impacts crop productivity, and there is a need for fast, precise, and resource-efficient detection methods. Traditional methods and modern deep learning architectures are not optimized for real-time or resource-constrained agricultural settings.", "method": "A lightweight hybrid CNN framework was developed, combining elements from ResNet, DenseNet, and MobileNet. It reduces trainable parameters while maintaining accuracy. Additionally, a gradient norm-based machine unlearning mechanism was introduced to enhance model adaptability.", "result": "The model achieved a 15x reduction in trainable parameters compared to traditional CNN and Vision Transformer models, while still delivering competitive accuracy. It was tested on an annotated aerial image dataset of potato fields.", "conclusion": "The proposed framework is effective, scalable, and resource-efficient for drought stress monitoring, making it suitable for precision agriculture in resource-constrained environments."}}
{"id": "2509.06923", "pdf": "https://arxiv.org/pdf/2509.06923", "abs": "https://arxiv.org/abs/2509.06923", "authors": ["Ziheng Li", "Zexu Sun", "Jinman Zhao", "Erxue Min", "Yongcheng Zeng", "Hui Wu", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen", "Zhi-Hong Deng"], "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding", "categories": ["cs.LG"], "comment": "Work in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.", "AI": {"tldr": "SEELE is a new framework for reinforcement learning with verifiable rewards, which dynamically adjusts problem difficulty using hints to improve exploration efficiency and model capabilities.", "motivation": "Existing RLVR methods are inefficient in exploration due to mismatches between problem difficulty and model capability, resulting in suboptimal learning outcomes.", "method": "SEELE introduces an adaptive training system that appends hints to problems and adjusts their length dynamically using a multi-round rollout sampling strategy informed by item response theory.", "result": "Experimental results show that SEELE achieves superior performance, outperforming GRPO and SFT by +11.8 and +10.5 points respectively, and surpasses prior hint-based methods by +3.6 points on math benchmarks.", "conclusion": "SEELE enhances reasoning capabilities and exploration efficiency in LLMs by optimizing the relationship between problem difficulty and model abilities, demonstrating its effectiveness across benchmarks."}}
{"id": "2509.06387", "pdf": "https://arxiv.org/pdf/2509.06387", "abs": "https://arxiv.org/abs/2509.06387", "authors": ["Dongsik Yoon", "Jongeun Kim"], "title": "Your Super Resolution Model is not Enough for Tackling Real-World Scenarios", "categories": ["cs.CV"], "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025)", "summary": "Despite remarkable progress in Single Image Super-Resolution (SISR),\ntraditional models often struggle to generalize across varying scale factors,\nlimiting their real-world applicability. To address this, we propose a plug-in\nScale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR\nmodels with the ability to perform arbitrary-scale SR. SAAM employs\nlightweight, scale-adaptive feature extraction and upsampling, incorporating\nthe Simple parameter-free Attention Module (SimAM) for efficient guidance and\ngradient variance loss to enhance sharpness in image details. Our method\nintegrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,\nHiT-SR, OverNet), delivering competitive or superior performance across a wide\nrange of integer and non-integer scale factors. Extensive experiments on\nbenchmark datasets demonstrate that our approach enables robust multi-scale\nupscaling with minimal computational overhead, offering a practical solution\nfor real-world scenarios.", "AI": {"tldr": "The paper introduces the Scale-Aware Attention Module (SAAM) to enable arbitrary-scale super-resolution using existing fixed-scale models. SAAM is efficient and improves image quality with minimal overhead.", "motivation": "Traditional Single Image Super-Resolution (SISR) models struggle to generalize across varying scale factors, making them limited in real-world applications.", "method": "The paper proposes SAAM, which uses lightweight, scale-adaptive feature extraction and upsampling mechanisms. It integrates Simple Attention Module (SimAM) for efficiency and gradient variance loss to enhance image detail sharpness.", "result": "SAAM is successfully integrated into state-of-the-art SR models and delivers competitive or superior results for integer and non-integer scales, tested extensively on benchmark datasets.", "conclusion": "The proposed method offers a practical and computation-efficient solution for robust multi-scale super-resolution in real-world scenarios by retrofitting existing models."}}
{"id": "2509.06924", "pdf": "https://arxiv.org/pdf/2509.06924", "abs": "https://arxiv.org/abs/2509.06924", "authors": ["Max D. ~Champneys", "Andrew J. ~Parnell", "Philipp Gutfreund", "Maximilian W. A. Skoda", ". Patrick A. Fairclough", "Timothy J. ~Rogers", "Stephanie L. ~Burg"], "title": "Neutron Reflectometry by Gradient Descent", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and\ninterfaces. NR is inherently an indirect measurement technique, access to the\nphysical quantities of interest (layer thickness, scattering length density,\nroughness), necessitate the solution of an inverse modelling problem, that is\ninefficient for large amounts of data or complex multiplayer structures (e.g.\nlithium batteries / electrodes). Recently, surrogate machine learning models\nhave been proposed as an alternative to existing optimisation routines.\nAlthough such approaches have been successful, physical intuition is lost when\nreplacing governing equations with fast neural networks. Instead, we propose a\nnovel and efficient approach; to optimise reflectivity data analysis by\nperforming gradient descent on the forward reflection model itself. Herein,\nautomatic differentiation techniques are used to evaluate exact gradients of\nthe error function with respect to the parameters of interest. Access to these\nquantities enables users of neutron reflectometry to harness a host of powerful\nmodern optimisation and inference techniques that remain thus far unexploited\nin the context of neutron reflectometry. This paper presents two benchmark case\nstudies; demonstrating state-of-the-art performance on a thick oxide quartz\nfilm, and robust co-fitting performance in the high complexity regime of\norganic LED multilayer devices. Additionally, we provide an open-source library\nof differentiable reflectometry kernels in the python programming language so\nthat gradient based approaches can readily be applied to other NR datasets.", "AI": {"tldr": "This paper proposes using gradient descent with automatic differentiation for neutron reflectometry data analysis to improve efficiency and accuracy, presenting benchmark studies and an open-source library.", "motivation": "Neutron reflectometry suffers inefficiencies in inverse modeling for data analysis in complex structures, while current machine learning approaches lose physical intuition.", "method": "Introduces gradient descent on forward reflection models using automatic differentiation to compute exact error gradients for optimization.", "result": "Demonstrated improved performance on benchmark cases, including thick oxide quartz film and complex organic LED multilayer devices.", "conclusion": "Gradient-based approaches enhance the optimization in neutron reflectometry and facilitate modern inference techniques, supported by an open-source library."}}
{"id": "2509.06396", "pdf": "https://arxiv.org/pdf/2509.06396", "abs": "https://arxiv.org/abs/2509.06396", "authors": ["Lorenz Achim Kuhn", "Daniel Abler", "Jonas Richiardi", "Andreas F. Hottinger", "Luis Schiappacasse", "Vincent Dunet", "Adrien Depeursinge", "Vincent Andrearczyk"], "title": "AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery", "categories": ["cs.CV"], "comment": "Submitted and Accepted to the Learning with longitudinal medical\n  Images and Data workshop at the MICCAI 2025 Conference", "summary": "Brain Metastases (BM) are a large contributor to mortality of patients with\ncancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored\nwith Magnetic Resonance Imaging (MRI) at regular follow-up intervals according\nto treatment guidelines. Analyzing and quantifying this longitudinal imaging\nrepresents an intractable workload for clinicians. As a result, follow-up\nimages are not annotated and merely assessed by observation. Response to\ntreatment in longitudinal imaging is being studied, to better understand growth\ntrajectories and ultimately predict treatment success or toxicity as early as\npossible. In this study, we implement an automated pipeline to curate a large\nlongitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in\n177 patients who were monitored for >360 days at approximately two-month\nintervals at Lausanne University Hospital (CHUV). We use a data-driven\nclustering to identify characteristic trajectories. In addition, we predict 12\nmonths lesion-level response using classical as well as graph machine learning\nGraph Machine Learning (GML). Clustering revealed 5 dominant growth\ntrajectories with distinct final response categories. Response prediction\nreaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first\nfollow-up MRI with gradient boosting. Similarly, robust predictive performance\nof up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more\nflexibility with a single model for multiple input time-points configurations.\nOur results suggest potential automation and increased precision for the\ncomprehensive assessment and prediction of BM response to SRS in longitudinal\nMRI. The proposed pipeline facilitates scalable data curation for the\ninvestigation of BM growth patterns, and lays the foundation for clinical\ndecision support systems aiming at optimizing personalized care.", "AI": {"tldr": "This study develops an automated pipeline for longitudinal data analysis to predict Brain Metastases (BM) response to Stereotactic Radiosurgery (SRS), achieving robust predictive accuracy.", "motivation": "Brain Metastases treatment and monitoring represent a significant clinical workload, with a need for better understanding of treatment success and outcomes through longitudinal imaging analysis.", "method": "The researchers curated a dataset of 896 BMs across 177 patients, applied data-driven clustering for trajectory analysis, and used machine learning methods (including gradient boosting and graph machine learning) to predict lesion-level responses.", "result": "They identified five growth trajectories and achieved strong predictive performance, with AUCs of up to 0.90 for gradient boosting and 0.88 for Graph Machine Learning.", "conclusion": "The study demonstrates potential for automating BM response assessments, improving precision, and enabling scalable methods for personalized clinical decision-making in cancer care."}}
{"id": "2509.06931", "pdf": "https://arxiv.org/pdf/2509.06931", "abs": "https://arxiv.org/abs/2509.06931", "authors": ["Maor Shutman", "Oren Louidor", "Ran Tessler"], "title": "Learning words in groups: fusion algebras, tensor ranks and grokking", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we demonstrate that a simple two-layer neural network with\nstandard activation functions can learn an arbitrary word operation in any\nfinite group, provided sufficient width is available and exhibits grokking\nwhile doing so. To explain the mechanism by which this is achieved, we reframe\nthe problem as that of learning a particular $3$-tensor, which we show is\ntypically of low rank. A key insight is that low-rank implementations of this\ntensor can be obtained by decomposing it along triplets of basic self-conjugate\nrepresentations of the group and leveraging the fusion structure to rule out\nmany components. Focusing on a phenomenologically similar but more tractable\nsurrogate model, we show that the network is able to find such low-rank\nimplementations (or approximations thereof), thereby using limited width to\napproximate the word-tensor in a generalizable way. In the case of the simple\nmultiplication word, we further elucidate the form of these low-rank\nimplementations, showing that the network effectively implements efficient\nmatrix multiplication in the sense of Strassen. Our work also sheds light on\nthe mechanism by which a network reaches such a solution under gradient\ndescent.", "AI": {"tldr": "This paper demonstrates how a simple two-layer neural network can learn arbitrary word operations in finite groups, leveraging low-rank tensor decomposition methods.", "motivation": "To understand and explain how neural networks with standard activation functions can generalize efficiently when learning complex operations, such as arbitrary word operations in finite groups.", "method": "The study reframed the learning problem as identifying a low-rank $3$-tensor, which can be decomposed using triplets of self-conjugate group representations and utilizing their fusion structure. A surrogate model was used to analyze generalization behavior and low-rank tensor approximation.", "result": "Neural networks were shown to learn low-rank tensor implementations efficiently, even achieving Strassen-like efficient matrix multiplication for specific cases. The mechanism of gradient descent in finding these solutions was also analyzed.", "conclusion": "Neural networks exploit low-rank structures to achieve generalization and efficient computation, shedding light on their learning dynamics in structured mathematical problems."}}
{"id": "2509.06400", "pdf": "https://arxiv.org/pdf/2509.06400", "abs": "https://arxiv.org/abs/2509.06400", "authors": ["Matthieu Gendrin", "St\u00e9phane Pateux", "Th\u00e9o Ladune"], "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene\nreconstruction. With a number of views of a given object or scene, the\nalgorithm trains a model composed of 3D gaussians, which enables the production\nof novel views from arbitrary points of view. This freedom of movement is\nreferred to as 6DoF for 6 degrees of freedom: a view is produced for any\nposition (3 degrees), orientation of camera (3 other degrees). On large scenes,\nthough, the input views are acquired from a limited zone in space, and the\nreconstruction is valuable for novel views from the same zone, even if the\nscene itself is almost unlimited in size. We refer to this particular case as\n3DoF+, meaning that the 3 degrees of freedom of camera position are limited to\nsmall offsets around the central position. Considering the problem of\ncoordinate quantization, the impact of position error on the projection error\nin pixels is studied. It is shown that the projection error is proportional to\nthe squared inverse distance of the point being projected. Consequently, a new\nquantization scheme based on spherical coordinates is proposed. Rate-distortion\nperformance of the proposed method are illustrated on the well-known Garden\nscene.", "AI": {"tldr": "3D Gaussian Splatting enables 6 degrees of freedom for novel view generation but faces challenges in large scenes with limited view zones. A new quantization scheme based on spherical coordinates is proposed to address coordinate quantization issues.", "motivation": "The paper aims to overcome limitations in scene reconstruction when input views are acquired from a limited zone in large scenes, focusing on the accuracy of position quantization for better projection.", "method": "A study on the impact of position error is conducted, followed by the proposal of a new quantization scheme based on spherical coordinates.", "result": "Projection errors are shown to be proportional to the squared inverse distance, and the rate-distortion performance of the proposed spherical quantization method is validated on the Garden scene.", "conclusion": "The new quantization scheme improves the accuracy and efficiency of 3D scene reconstruction under limited degrees of freedom, enhancing practical applications for large-scale scenes."}}
{"id": "2509.06938", "pdf": "https://arxiv.org/pdf/2509.06938", "abs": "https://arxiv.org/abs/2509.06938", "authors": ["Praneet Suresh", "Jack Stanley", "Sonia Joseph", "Luca Scimeca", "Danilo Bzdok"], "title": "From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As generative AI systems become competent and democratized in science,\nbusiness, and government, deeper insight into their failure modes now poses an\nacute need. The occasional volatility in their behavior, such as the propensity\nof transformer models to hallucinate, impedes trust and adoption of emerging AI\nsolutions in high-stakes areas. In the present work, we establish how and when\nhallucinations arise in pre-trained transformer models through concept\nrepresentations captured by sparse autoencoders, under scenarios with\nexperimentally controlled uncertainty in the input space. Our systematic\nexperiments reveal that the number of semantic concepts used by the transformer\nmodel grows as the input information becomes increasingly unstructured. In the\nface of growing uncertainty in the input space, the transformer model becomes\nprone to activate coherent yet input-insensitive semantic features, leading to\nhallucinated output. At its extreme, for pure-noise inputs, we identify a wide\nvariety of robustly triggered and meaningful concepts in the intermediate\nactivations of pre-trained transformer models, whose functional integrity we\nconfirm through targeted steering. We also show that hallucinations in the\noutput of a transformer model can be reliably predicted from the concept\npatterns embedded in transformer layer activations. This collection of insights\non transformer internal processing mechanics has immediate consequences for\naligning AI models with human values, AI safety, opening the attack surface for\npotential adversarial attacks, and providing a basis for automatic\nquantification of a model's hallucination risk.", "AI": {"tldr": "The paper investigates how pre-trained transformer models exhibit hallucinations, presenting a mechanism for predicting such occurrences based on concept patterns in their activations.", "motivation": "The research addresses the critical issue of understanding generative AI failure modes like hallucinations, which hinder their trust and application in high-stakes areas.", "method": "The authors employ sparse autoencoders to extract concept representations from transformer models, conducting controlled experiments with varying input uncertainty.", "result": "The study finds that transformer models activate coherent semantic features under growing input uncertainty, leading to hallucinated outputs, even for pure-noise inputs.", "conclusion": "By identifying robust concept patterns linked to hallucinations, the research provides tools for predicting and mitigating such behavior, impacting AI safety, alignment, and vulnerability detection."}}
{"id": "2509.06413", "pdf": "https://arxiv.org/pdf/2509.06413", "abs": "https://arxiv.org/abs/2509.06413", "authors": ["Yixiao Li", "Xin Li", "Chris Wei Zhou", "Shuo Xing", "Hadi Amirpour", "Xiaoshuai Hao", "Guanghui Yue", "Baoquan Zhao", "Weide Liu", "Xiaoyuan Yang", "Zhengzhong Tu", "Xinyu Li", "Chuanbiao Song", "Chenqi Zhang", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Xiaoyan Sun", "Shishun Tian", "Dongyang Yan", "Weixia Zhang", "Junlin Chen", "Wei Sun", "Zhihua Wang", "Zhuohang Shi", "Zhizun Luo", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhaowang Wu", "Kaixin Deng"], "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 12 figures, VQualA ICCV Workshop", "summary": "This paper presents the ISRGC-Q Challenge, built upon the Image\nSuper-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and\norganized as part of the Visual Quality Assessment (VQualA) Competition at the\nICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment\n(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated\nby the latest generative approaches, including Generative Adversarial Networks\n(GANs) and diffusion models. The primary goal of this challenge is to analyze\nthe unique artifacts introduced by modern super-resolution techniques and to\nevaluate their perceptual quality effectively. A total of 108 participants\nregistered for the challenge, with 4 teams submitting valid solutions and fact\nsheets for the final testing phase. These submissions demonstrated\nstate-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is\npublicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.", "AI": {"tldr": "This study introduces the ISRGC-Q Challenge focused on evaluating the quality of super-resolution images using datasets emphasizing modern generative techniques like GANs and diffusion models.", "motivation": "To address the need for effective evaluation of perceptual quality in super-resolution images generated by advanced methods, and to analyze their unique artifacts.", "method": "Hosted the ISRGC-Q Challenge based on the ISRGen-QA dataset, encouraging participants to submit solutions for assessing quality in SR images created with generative approaches.", "result": "108 participants registered, with 4 teams providing valid submissions showcasing state-of-the-art performance on the ISRGen-QA dataset.", "conclusion": "The challenge highlights advancements in Super-Resolution Image Quality Assessment by focusing on modern generative techniques and provides a benchmark for SOTA performance."}}
{"id": "2311.01870", "pdf": "https://arxiv.org/pdf/2311.01870", "abs": "https://arxiv.org/abs/2311.01870", "authors": ["Jinrui Yang", "Timothy Baldwin", "Trevor Cohn"], "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at The 3rd Multilingual Representation Learning (MRL)\n  Workshop (co-located with EMNLP 2023)", "summary": "We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K\nmulti-lingual documents collected from the European Parliament, spanning 24\nlanguages. This dataset is designed to investigate fairness in a multilingual\ninformation retrieval (IR) context to analyze both language and demographic\nbias in a ranking context. It boasts an authentic multilingual corpus,\nfeaturing topics translated into all 24 languages, as well as cross-lingual\nrelevance judgments. Furthermore, it offers rich demographic information\nassociated with its documents, facilitating the study of demographic bias. We\nreport the effectiveness of Multi-EuP for benchmarking both monolingual and\nmultilingual IR. We also conduct a preliminary experiment on language bias\ncaused by the choice of tokenization strategy.", "AI": {"tldr": "Multi-EuP is a multilingual benchmark dataset of 22K documents in 24 languages, designed to study fairness in multilingual information retrieval (IR), including issues of language and demographic bias.", "motivation": "The paper aims to address fairness in multilingual IR by providing a dataset to analyze language and demographic bias in document ranking.", "method": "The authors introduce Multi-EuP, a dataset with multilingual and cross-lingual features, including translated topics and relevance judgments, along with demographic data enabling bias analysis.", "result": "Multi-EuP is demonstrated to be effective for monolingual and multilingual IR benchmarking. A preliminary experiment on tokenization strategy showed its effect on language bias.", "conclusion": "Multi-EuP serves as a valuable resource for investigating fairness in IR by enabling bias analysis across languages and demographics, and its utility is demonstrated in an experimental setup."}}
{"id": "2509.06422", "pdf": "https://arxiv.org/pdf/2509.06422", "abs": "https://arxiv.org/abs/2509.06422", "authors": ["Hua Zhang", "Changjiang Luo", "Ruoyu Chen"], "title": "Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM", "categories": ["cs.CV"], "comment": null, "summary": "Video camouflaged object detection (VCOD) is challenging due to dynamic\nenvironments. Existing methods face two main issues: (1) SAM-based methods\nstruggle to separate camouflaged object edges due to model freezing, and (2)\nMLLM-based methods suffer from poor object separability as large language\nmodels merge foreground and background. To address these issues, we propose a\nnovel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the\nseparability of object edge details, we represent video sequences with temporal\nand spatial clues and perform feature fusion via LLM to increase information\ndensity. Next, multiple cues are generated through the dynamic foreground\nvisual token scoring module and the prompt network to adaptively guide and\nfine-tune the SAM model, enabling it to adapt to subtle textures. To enhance\nthe separability of objects and background, we propose a decoupled\nforeground-background learning strategy. By generating foreground and\nbackground cues separately and performing decoupled training, the visual token\ncan effectively integrate foreground and background information independently,\nenabling SAM to more accurately segment camouflaged objects in the video.\nExperiments on the MoCA-Mask dataset show that Phantom-Insight achieves\nstate-of-the-art performance across various metrics. Additionally, its ability\nto detect unseen camouflaged objects on the CAD2016 dataset highlights its\nstrong generalization ability.", "AI": {"tldr": "The paper introduces Phantom-Insight, a method combining SAM and MLLM for video camouflaged object detection, addressing issues with edge separability and object-background merging in dynamic environments.", "motivation": "The study aims to solve challenges in video camouflaged object detection caused by dynamic environments, where existing SAM and MLLM methods fail in edge separability and object-background distinction.", "method": "Phantom-Insight integrates SAM and MLLM by enhancing temporal-spatial feature fusion, utilizing dynamic visual token scoring and prompt networks, and employing a decoupled foreground-background learning strategy for robust segmentation.", "result": "Phantom-Insight achieves state-of-the-art performance on the MoCA-Mask dataset and demonstrates robust generalization by effectively detecting unseen camouflaged objects on the CAD2016 dataset.", "conclusion": "The approach significantly improves video camouflaged object detection with stronger edge detail separability, adaptive texture modeling, and generalized object-background differentiation, establishing itself as a leading method in the field."}}
{"id": "2509.06427", "pdf": "https://arxiv.org/pdf/2509.06427", "abs": "https://arxiv.org/abs/2509.06427", "authors": ["Rabin Dulal", "Lihong Zheng", "Muhammad Ashad Kabir"], "title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection", "categories": ["cs.CV"], "comment": null, "summary": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.", "AI": {"tldr": "The paper presents a zero-shot muzzle detection framework for cattle using a vision-language model called Grounding DINO, which achieves a mAP@0.5 of 76.8% without requiring annotated datasets or task-specific training.", "motivation": "Current cattle muzzle detection methods relying on supervised models like YOLO require extensive annotated datasets and are not adaptable to unseen data, creating a need for a more scalable and flexible approach.", "method": "The study employs Grounding DINO, a vision-language model, to perform zero-shot muzzle detection by leveraging natural language prompts, eliminating the need for task-specific training or annotated data.", "result": "The proposed framework achieves a mAP@0.5 accuracy of 76.8%, demonstrating its feasibility and effectiveness for cattle muzzle localization without requiring annotated data.", "conclusion": "This zero-shot detection framework provides a scalable and annotation-free alternative to supervised methods, offering adaptability and ease of deployment in cattle identification across diverse conditions."}}
{"id": "2509.06442", "pdf": "https://arxiv.org/pdf/2509.06442", "abs": "https://arxiv.org/abs/2509.06442", "authors": ["Yixiao Li", "Xiaoyuan Yang", "Guanghui Yue", "Jun Fu", "Qiuping Jiang", "Xu Jia", "Paul L. Rosin", "Hantao Liu", "Wei Zhou"], "title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 6 figures, IEEE Transactions on Image Processing", "summary": "Many super-resolution (SR) algorithms have been proposed to increase image\nresolution. However, full-reference (FR) image quality assessment (IQA) metrics\nfor comparing and evaluating different SR algorithms are limited. In this work,\nwe propose the Perception-oriented Bidirectional Attention Network (PBAN) for\nimage SR FR-IQA, which is composed of three modules: an image encoder module, a\nperception-oriented bidirectional attention (PBA) module, and a quality\nprediction module. First, we encode the input images for feature\nrepresentations. Inspired by the characteristics of the human visual system, we\nthen construct the perception-oriented PBA module. Specifically, different from\nexisting attention-based SR IQA methods, we conceive a Bidirectional Attention\nto bidirectionally construct visual attention to distortion, which is\nconsistent with the generation and evaluation processes of SR images. To\nfurther guide the quality assessment towards the perception of distorted\ninformation, we propose Grouped Multi-scale Deformable Convolution, enabling\nthe proposed method to adaptively perceive distortion. Moreover, we design\nSub-information Excitation Convolution to direct visual perception to both\nsub-pixel and sub-channel attention. Finally, the quality prediction module is\nexploited to integrate quality-aware features and regress quality scores.\nExtensive experiments demonstrate that our proposed PBAN outperforms\nstate-of-the-art quality assessment methods.", "AI": {"tldr": "This paper introduces a novel metric for assessing super-resolution (SR) image quality, named Perception-oriented Bidirectional Attention Network (PBAN), composed of advanced perception-oriented modules.", "motivation": "There is a lack of robust and comprehensive full-reference image quality assessment (IQA) metrics to compare different SR algorithms effectively.", "method": "The proposed PBAN incorporates three modules: an image encoder, perception-oriented bidirectional attention (PBA), and a quality prediction mechanism, all tailored to improve IQA for SR images.", "result": "Experiments demonstrate that PBAN's quality assessment performance surpasses existing state-of-the-art methods.", "conclusion": "PBAN proves to be effective in offering a more accurate and perception-based approach to evaluating super-resolution image quality."}}
{"id": "2509.06456", "pdf": "https://arxiv.org/pdf/2509.06456", "abs": "https://arxiv.org/abs/2509.06456", "authors": ["Zongyi Xu", "Zhongpeng Lang", "Yilong Chen", "Shanshan Zhao", "Xiaoshui Huang", "Yifan Zuo", "Yan Zhang", "Qianni Zhang", "Xinbo Gao"], "title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Cross-source point cloud registration, which aims to align point cloud data\nfrom different sensors, is a fundamental task in 3D vision. However, compared\nto the same-source point cloud registration, cross-source registration faces\ntwo core challenges: the lack of publicly available large-scale real-world\ndatasets for training the deep registration models, and the inherent\ndifferences in point clouds captured by multiple sensors. The diverse patterns\ninduced by the sensors pose great challenges in robust and accurate point cloud\nfeature extraction and matching, which negatively influence the registration\naccuracy. To advance research in this field, we construct Cross3DReg, the\ncurrently largest and real-world multi-modal cross-source point cloud\nregistration dataset, which is collected by a rotating mechanical lidar and a\nhybrid semi-solid-state lidar, respectively. Moreover, we design an\noverlap-based cross-source registration framework, which utilizes unaligned\nimages to predict the overlapping region between source and target point\nclouds, effectively filtering out redundant points in the irrelevant regions\nand significantly mitigating the interference caused by noise in\nnon-overlapping areas. Then, a visual-geometric attention guided matching\nmodule is proposed to enhance the consistency of cross-source point cloud\nfeatures by fusing image and geometric information to establish reliable\ncorrespondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art\nregistration performance. Our framework reduces the relative rotation error\n(RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$,\nrespectively, and improves the registration recall (RR) by $5.4\\%$, which\nvalidates its effectiveness in achieving accurate cross-source registration.", "AI": {"tldr": "This research introduces Cross3DReg, a large-scale multi-modal dataset for cross-source point cloud registration, and proposes a novel framework combining overlap prediction and attention-guided feature matching to improve registration accuracy and robustness.", "motivation": "Cross-source point cloud registration is challenging due to the lack of publicly available datasets and sensor-induced feature mismatches, limiting robust and accurate alignment.", "method": "The authors built the Cross3DReg dataset using diverse lidar systems and developed a registration framework that predicts overlapping regions to filter irrelevant points and utilizes an attention module combining image and geometric data for reliable feature matching.", "result": "The proposed framework significantly improves registration performance, reducing rotation error by 63.2%, translation error by 40.2%, and increasing recall by 5.4%, surpassing current methods.", "conclusion": "Cross3DReg dataset and the novel framework advance cross-source point cloud registration, enabling more accurate alignment of sensor data with real-world robustness."}}
{"id": "2509.06459", "pdf": "https://arxiv.org/pdf/2509.06459", "abs": "https://arxiv.org/abs/2509.06459", "authors": ["Sebastian-Vasile Echim", "Andrei-Alexandru Preda", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 7 figures, Accepted at ECAI 2025 (28th European Conference\n  on Artificial Intelligence)", "summary": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation.", "AI": {"tldr": "This paper explores the impact of adversarial attacks on specific neural network architectures using two novel black-box algorithms (ATA and AGA) and demonstrates their effectiveness over existing methods.", "motivation": "To understand and address the weaknesses of deep neural networks in the context of adversarial attacks, especially in the black-box scenario.", "method": "The study uses two iterative adversarial algorithms, ATA and AGA, based on affine transformations and genetic algorithms, to attack different neural network architectures and evaluate their robustness on datasets such as Tiny ImageNet, Caltech-256, and Food-101.", "result": "The proposed algorithms outshine existing black-box adversarial methods, demonstrating accuracy improvements of up to 8.82%. Insights into algorithm parameters and adversarial defenses are also provided.", "conclusion": "The findings suggest that ATA and AGA are effective tools for probing and enhancing adversarial robustness in image classification tasks, outperforming comparable methods and offering valuable understanding about network vulnerabilities and defenses."}}
{"id": "2509.05320", "pdf": "https://arxiv.org/pdf/2509.05320", "abs": "https://arxiv.org/abs/2509.05320", "authors": ["Ikhlasse Badidi", "Nouhaila El Khiyaoui", "Aya Riany", "Badr Ben Elallid", "Amine Abouaomar"], "title": "Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks", "categories": ["cs.CR", "cs.LG"], "comment": "7 pages, 6 figures, 1 algorithm, 5 equations", "summary": "The integration of Large Language Models (LLMs) in 6G vehicular networks\npromises unprecedented advancements in intelligent transportation systems.\nHowever, offloading LLM computations from vehicles to edge infrastructure poses\nsignificant privacy risks, potentially exposing sensitive user data. This paper\npresents a novel privacy-preserving offloading framework for LLM-integrated\nvehicular networks. We introduce a hybrid approach combining federated learning\n(FL) and differential privacy (DP) techniques to protect user data while\nmaintaining LLM performance. Our framework includes a privacy-aware task\npartitioning algorithm that optimizes the trade-off between local and edge\ncomputation, considering both privacy constraints and system efficiency. We\nalso propose a secure communication protocol for transmitting model updates and\naggregating results across the network. Experimental results demonstrate that\nour approach achieves 75\\% global accuracy with only a 2-3\\% reduction compared\nto non-privacy-preserving methods, while maintaining DP guarantees with an\noptimal privacy budget of $\\varepsilon = 0.8$. The framework shows stable\ncommunication overhead of approximately 2.1MB per round with computation\ncomprising over 90\\% of total processing time, validating its efficiency for\nresource-constrained vehicular environments.", "AI": {"tldr": "This paper introduces a framework that merges federated learning and differential privacy to allow safe offloading of large language model computations in 6G vehicular networks.", "motivation": "While integrating large language models (LLMs) into 6G vehicular networks promises advancements, it raises privacy concerns due to sensitive user data being exposed during computation offloading.", "method": "The paper introduces a privacy-preserving hybrid framework combining federated learning and differential privacy. It includes a privacy-aware task partitioning algorithm and a secure communication protocol for model updates.", "result": "The framework achieves 75% accuracy, with only a 2-3% drop from non-privacy-preserving methods, and ensures privacy with an optimal budget of \u03b5 = 0.8. Communication overhead remains low at 2.1MB per round.", "conclusion": "The framework ensures efficient and privacy-preserving computation offloading in resource-limited vehicular networks, maintaining a strong trade-off between privacy and performance."}}
{"id": "2509.06461", "pdf": "https://arxiv.org/pdf/2509.06461", "abs": "https://arxiv.org/abs/2509.06461", "authors": ["Yuyao Ge", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Xuanshan Zhou", "Jiayu Yao", "Jiafeng Guo", "Xueqi Cheng"], "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.", "AI": {"tldr": "The paper introduces CARVE, a training-free method to enhance vision-language models (VLMs) using attention contrasting to address visual complexity.", "motivation": "To overcome the degradation in performance of VLMs in complex visual environments without relying on additional training or external tools.", "method": "Analyzed VLMs' attention patterns and introduced a theoretical framework to contrast attention maps. Developed the CARVE method for pixel-level attention contrasting to separate semantic signals from visual noise.", "result": "CARVE significantly improves VLM performance, with up to 75% enhancement on open-source models in experiments.", "conclusion": "Attention contrast leveraging visual signal decomposition is an efficient method to handle visual complexity and improve VLM-based reasoning."}}
{"id": "2509.06464", "pdf": "https://arxiv.org/pdf/2509.06464", "abs": "https://arxiv.org/abs/2509.06464", "authors": ["Erez Posner", "Ore Shtalrid", "Oded Erell", "Daniel Noy", "Moshe Bouhnik"], "title": "A Statistical 3D Stomach Shape Model for Anatomical Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and parameterized 3D models of human anatomy have become invaluable\nin research, diagnostics, and surgical planning. However, the development of\ndetailed models for internal organs, such as the stomach, has been limited by\ndata availability and methodological challenges. In this paper, we propose a\nnovel pipeline for the generation of synthetic 3D stomach models, enabling the\ncreation of anatomically diverse morphologies informed by established studies\non stomach shape variability. Using this pipeline, we construct a dataset of\nsynthetic stomachs. Building on this dataset, we develop a 3D statistical shape\nmodel of the stomach, trained to capture natural anatomical variability in a\nlow-dimensional shape space. The model is further refined using CT meshes\nderived from publicly available datasets through a semi-supervised alignment\nprocess, enhancing its ability to generalize to unseen anatomical variations.\nWe evaluated the model on a held-out test set of real stomach CT scans,\ndemonstrating robust generalization and fit accuracy. We make the statistical\nshape model along with the synthetic dataset publicly available on GitLab:\nhttps://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.\nThis work introduces the first statistical 3D shape model of the stomach, with\napplications ranging from surgical simulation and pre-operative planning to\nmedical education and computational modeling. By combining synthetic data\ngeneration, parametric modeling, and real-world validation, our approach\nrepresents a significant advancement in organ modeling and opens new\npossibilities for personalized healthcare solutions.", "AI": {"tldr": "The paper introduces the first statistical 3D shape model of the stomach, combining synthetic data generation, parametric modeling, and real-world validation, with applications in surgical planning, diagnostics, and education.", "motivation": "Current methods for creating 3D models of internal organs like the stomach are hindered by data limitations and technical challenges, necessitating an innovative pipeline to represent anatomical variability.", "method": "The researchers developed a pipeline to generate synthetic 3D stomach datasets informed by anatomical variability, created a 3D statistical shape model, and refined it using semi-supervised alignment with CT-based data for improved generalization.", "result": "The model robustly generalized to a test set of real stomach CT scans, demonstrating high fit accuracy, and it is made publicly available to support further research.", "conclusion": "This research provides a novel tool for personalized healthcare applications and broader organ modeling, advancing the field with public access to both their synthetic dataset and statistical shape model."}}
{"id": "2509.06467", "pdf": "https://arxiv.org/pdf/2509.06467", "abs": "https://arxiv.org/abs/2509.06467", "authors": ["Che Liu", "Yinda Chen", "Haoyuan Shi", "Jinpeng Lu", "Bailiang Jian", "Jiazhen Pan", "Linghan Cai", "Jiayi Wang", "Yundi Zhang", "Jun Li", "Cosmin I. Bercea", "Cheng Ouyang", "Chen Chen", "Zhiwei Xiong", "Benedikt Wiestler", "Christian Wachinger", "Daniel Rueckert", "Wenjia Bai", "Rossella Arcucci"], "title": "Does DINOv3 Set a New Medical Vision Standard?", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.", "AI": {"tldr": "This paper evaluates the performance of DINOv3, a self-supervised vision transformer, in the medical imaging domain, exploring its potential to perform well without domain-specific pre-training. It finds strong performance across various tasks but notes limitations in highly specialized scenarios.", "motivation": "The study aims to analyze if cutting-edge vision foundation models pre-trained on natural images, like DINOv3, can effectively generalize to medical imaging tasks without additional domain-specific pre-training.", "method": "DINOv3 is benchmarked across 2D and 3D classification and segmentation tasks in various medical imaging modalities, with analyses on scalability by tuning model sizes and input resolutions.", "result": "DINOv3 demonstrates robust performance across several medical vision tasks, in some cases surpassing medical-specific models like BiomedCLIP, but it struggles in specialized domains like WSIs, EM, and PET. It also shows inconsistent scalability in the medical context.", "conclusion": "While DINOv3 serves as a strong baseline for medical imaging tasks with its generalized visual features, its limitations in deeply specialized applications emphasize the need for further research to adapt such models more effectively to unique medical contexts."}}
{"id": "2509.05651", "pdf": "https://arxiv.org/pdf/2509.05651", "abs": "https://arxiv.org/abs/2509.05651", "authors": ["Lukas Beckenbauer", "Johannes-Lucas Loewe", "Ge Zheng", "Alexandra Brintrup"], "title": "Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS)\ndue to partial observability and suboptimal coordination. We propose\nOrchestrator, a novel MAS framework that leverages attention-inspired\nself-emergent coordination and reflective benchmarking to optimize global task\nperformance. Orchestrator introduces a monitoring mechanism to track\nagent-environment dynamics, using active inference benchmarks to optimize\nsystem behavior. By tracking agent-to-agent and agent-to-environment\ninteraction, Orchestrator mitigates the effects of partial observability and\nenables agents to approximate global task solutions more efficiently. We\nevaluate the framework on a series of maze puzzles of increasing complexity,\ndemonstrating its effectiveness in enhancing coordination and performance in\ndynamic, non-linear environments with long-horizon objectives.", "AI": {"tldr": "The paper presents 'Orchestrator,' an MAS framework addressing coordination and global task optimization issues for complex tasks in non-linear environments.", "motivation": "Improving coordination and task optimization in multi-agent systems (MAS) confronted by partial observability and non-linear dynamics.", "method": "Introducing a framework called 'Orchestrator,' which tracks agent and environment interactions while utilizing self-emergent coordination and active inference benchmarks.", "result": "The framework, tested on maze puzzles of varying complexity, demonstrated enhanced performance and coordination in dynamic environments with long-term objectives.", "conclusion": "Orchestrator effectively mitigates partial observability challenges and boosts MAS efficiency in solving complex tasks with long-horizon objectives."}}
{"id": "2509.06482", "pdf": "https://arxiv.org/pdf/2509.06482", "abs": "https://arxiv.org/abs/2509.06482", "authors": ["Zhongxiang Xie", "Shuangxi Miao", "Yuhan Jiang", "Zhewei Zhang", "Jing Yao", "Xuecao Li", "Jianxi Huang", "Pedram Ghamisi"], "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing\n  (TGRS). 13 pages, 9 figures", "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.", "AI": {"tldr": "The paper introduces FSG-Net, an innovative model for high-resolution remote sensing image change detection, addressing false alarms and semantic gaps with advanced modules, and achieving state-of-the-art performance.", "motivation": "To overcome the challenges of false alarms caused by radiometric variations and the semantic gap between deep and shallow feature fusion in change detection using remote sensing images.", "method": "FSG-Net employs a Discrepancy-Aware Wavelet Interaction Module to process frequency components and reduce pseudo-changes, a Synergistic Temporal-Spatial Attention Module to emphasize genuine changes, and a Lightweight Gated Fusion Unit to bridge the semantic gap by selectively integrating shallow layer details.", "result": "The model achieved state-of-the-art F1-scores of 94.16% on CDD, 89.51% on GZ-CD, and 91.27% on LEVIR-CD benchmarks.", "conclusion": "FSG-Net effectively mitigates key issues in change detection for remote sensing images, offering a robust solution that outperforms existing methods, as validated by benchmarking results."}}
{"id": "2509.05350", "pdf": "https://arxiv.org/pdf/2509.05350", "abs": "https://arxiv.org/abs/2509.05350", "authors": ["Joshua Ward", "Yuxuan Yang", "Chi-Hua Wang", "Guang Cheng"], "title": "Ensembling Membership Inference Attacks Against Tabular Generative Models", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Membership Inference Attacks (MIAs) have emerged as a principled framework\nfor auditing the privacy of synthetic data generated by tabular generative\nmodels, where many diverse methods have been proposed that each exploit\ndifferent privacy leakage signals. However, in realistic threat scenarios, an\nadversary must choose a single method without a priori guarantee that it will\nbe the empirically highest performing option. We study this challenge as a\ndecision theoretic problem under uncertainty and conduct the largest synthetic\ndata privacy benchmark to date. Here, we find that no MIA constitutes a\nstrictly dominant strategy across a wide variety of model architectures and\ndataset domains under our threat model. Motivated by these findings, we propose\nensemble MIAs and show that unsupervised ensembles built on individual attacks\noffer empirically more robust, regret-minimizing strategies than individual\nattacks.", "AI": {"tldr": "The paper studies the effectiveness of Membership Inference Attacks (MIAs) for auditing synthetic data privacy, finding no single dominant method across diverse scenarios and proposing ensemble MIAs as a more robust solution.", "motivation": "To address the challenge of selecting the most effective Membership Inference Attack (MIA) for auditing synthetic data privacy, given that there is no a priori guarantee of any single method being the best option.", "method": "The authors frame the problem as a decision theoretic challenge under uncertainty and conduct the largest benchmark study on synthetic data privacy, evaluating various MIA methods and introducing ensemble MIAs.", "result": "They find that no single MIA is strictly better across diverse models and datasets. Ensemble MIAs, especially unsupervised ensembles, perform more robustly and minimize regret compared to individual attacks.", "conclusion": "Ensemble MIAs provide a practical solution for auditors, offering better robustness and adaptability in realistic privacy threat scenarios involving synthetic data."}}
{"id": "2509.06485", "pdf": "https://arxiv.org/pdf/2509.06485", "abs": "https://arxiv.org/abs/2509.06485", "authors": ["Andrea Marelli", "Alberto Foresti", "Leonardo Pesce", "Giacomo Boracchi", "Mario Grosso"], "title": "WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting", "categories": ["cs.CV"], "comment": "10 pages, 7 figures, ICCV 2025 - Workshops The WS$^2$ dataset is\n  publicly available for download at https://zenodo.org/records/14793518, all\n  the details are reported in the supplementary material", "summary": "In industrial quality control, to visually recognize unwanted items within a\nmoving heterogeneous stream, human operators are often still indispensable.\nWaste-sorting stands as a significant example, where operators on multiple\nconveyor belts manually remove unwanted objects to select specific materials.\nTo automate this recognition problem, computer vision systems offer great\npotential in accurately identifying and segmenting unwanted items in such\nsettings. Unfortunately, considering the multitude and the variety of sorting\ntasks, fully supervised approaches are not a viable option to address this\nchallange, as they require extensive labeling efforts. Surprisingly, weakly\nsupervised alternatives that leverage the implicit supervision naturally\nprovided by the operator in his removal action are relatively unexplored. In\nthis paper, we define the concept of Before-After Supervision, illustrating how\nto train a segmentation network by leveraging only the visual differences\nbetween images acquired \\textit{before} and \\textit{after} the operator. To\npromote research in this direction, we introduce WS$^2$ (Weakly Supervised\nsegmentation for Waste-Sorting), the first multiview dataset consisting of more\nthan 11 000 high-resolution video frames captured on top of a conveyor belt,\nincluding \"before\" and \"after\" images. We also present a robust end-to-end\npipeline, used to benchmark several state-of-the-art weakly supervised\nsegmentation methods on WS$^2$.", "AI": {"tldr": "The paper addresses automating item recognition in industrial quality control using a weakly supervised approach instead of fully supervised methods by employing Before-After Supervision. It introduces a new dataset for waste-sorting (WS$^2$) and benchmarks segmentation methods.", "motivation": "Human operators are currently indispensable for tasks like waste-sorting due to the variety of sorting tasks, making automation difficult. Fully supervised methods require extensive labeling and are impractical. A less labor-intensive, weakly supervised approach could automate these visual recognition tasks effectively.", "method": "The paper proposes a new concept called Before-After Supervision, where a segmentation network is trained based on visual differences between 'before' and 'after' images of operator actions. It introduces a dataset (WS$^2$) comprising high-resolution, multiview video frames and benchmarks weakly supervised segmentation methods using an end-to-end pipeline.", "result": "The research successfully creates a dataset suitable for the task, tests several state-of-the-art weakly supervised segmentation methods, and demonstrates the potential of Before-After Supervision for waste-sorting automation.", "conclusion": "Weakly supervised approaches leveraging implicit supervision hold promising potential for automating tasks like waste-sorting. The introduced WS$^2$ dataset and benchmarks will promote further research and innovation in this area."}}
{"id": "2509.05351", "pdf": "https://arxiv.org/pdf/2509.05351", "abs": "https://arxiv.org/abs/2509.05351", "authors": ["Guoyue Xu", "Renzheng Zhang", "Tengfei Luo"], "title": "Self-Driving Laboratory Optimizes the Lower Critical Solution Temperature of Thermoresponsive Polymers", "categories": ["cond-mat.soft", "cs.LG"], "comment": null, "summary": "To overcome the inherent inefficiencies of traditional trial-and-error\nmaterials discovery, the scientific community is increasingly developing\nautonomous laboratories that integrate data-driven decision-making into\nclosed-loop experimental workflows. In this work, we realize this concept for\nthermoresponsive polymers by developing a low-cost, \"frugal twin\" platform for\nthe optimization of the lower critical solution temperature (LCST) of\npoly(N-isopropylacrylamide) (PNIPAM). Our system integrates robotic\nfluid-handling, on-line sensors, and Bayesian optimization (BO) that navigates\nthe multi-component salt solution spaces to achieve user-specified LCST\ntargets. The platform demonstrates convergence to target properties within a\nminimal number of experiments. It strategically explores the parameter space,\nlearns from informative \"off-target\" results, and self-corrects to achieve the\nfinal targets. By providing an accessible and adaptable blueprint, this work\nlowers the barrier to entry for autonomous experimentation and accelerates the\ndesign and discovery of functional polymers.", "AI": {"tldr": "The paper develops a cost-effective autonomous platform using robotic handling, sensors, and Bayesian optimization for tailoring thermoresponsive polymers' LCST.", "motivation": "Traditional materials discovery through trial-and-error is inefficient, prompting a move towards automated labs with data-driven decision-making.", "method": "The authors built a 'frugal twin' system integrating robotic fluid-handling, on-line sensors, and Bayesian optimization to optimize LCST for PNIPAM in multi-component salt solutions.", "result": "The platform efficiently achieves user-specified LCST targets with minimal experiments, strategically navigates the parameter space, and self-corrects based on off-target results.", "conclusion": "This work provides an accessible framework that reduces barriers to autonomous experimentation, accelerating the design and discovery of functional polymers."}}
{"id": "2509.06499", "pdf": "https://arxiv.org/pdf/2509.06499", "abs": "https://arxiv.org/abs/2509.06499", "authors": ["Jibai Lin", "Bo Ma", "Yating Yang", "Rong Ma", "Turghun Osman", "Ahtamjan Ahmat", "Rui Dong", "Lei Wang", "Xi Zhou"], "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation (SDIG) aims to manipulate specific subjects\nwithin images while adhering to textual instructions, a task crucial for\nadvancing text-to-image diffusion models. SDIG requires reconciling the tension\nbetween maintaining subject identity and complying with dynamic edit\ninstructions, a challenge inadequately addressed by existing methods. In this\npaper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,\nwhich resolves this tension through target supervision and preference learning\nwithout test-time fine-tuning. TIDE pioneers target-supervised triplet\nalignment, modelling subject adaptation dynamics using a (reference image,\ninstruction, target images) triplet. This approach leverages the Direct Subject\nDiffusion (DSD) objective, training the model with paired \"winning\" (balanced\npreservation-compliance) and \"losing\" (distorted) targets, systematically\ngenerated and evaluated via quantitative metrics. This enables implicit reward\nmodelling for optimal preservation-compliance balance. Experimental results on\nstandard benchmarks demonstrate TIDE's superior performance in generating\nsubject-faithful outputs while maintaining instruction compliance,\noutperforming baseline methods across multiple quantitative metrics. TIDE's\nversatility is further evidenced by its successful application to diverse\ntasks, including structural-conditioned generation, image-to-image generation,\nand text-image interpolation. Our code is available at\nhttps://github.com/KomJay520/TIDE.", "AI": {"tldr": "Subject-driven image generation (SDIG) involves modifying specific subjects in images via textual instructions. TIDE framework was introduced to improve subject identity preservation and instruction compliance without extra fine-tuning.", "motivation": "SDIG struggles with balancing subject identity preservation against adherence to dynamic textual instructions. Existing methods fall short in reconciling this trade-off.", "method": "The TIDE framework employs target-supervised triplet alignment, using a triplet format (reference image, instruction, target images). It trains the model with paired winning and losing targets for optimized preservation-compliance balance via implicit reward modelling.", "result": "TIDE demonstrates superior performance in maintaining subject identity while following instructions, surpassing baseline methods in multiple metrics. It proves versatile across tasks like structural-conditioned generation and text-image interpolation.", "conclusion": "TIDE effectively addresses the tension in SDIG tasks, combining subject preservation and instruction compliance, while showcasing broad applicability and enhanced quantitative results."}}
{"id": "2509.06511", "pdf": "https://arxiv.org/pdf/2509.06511", "abs": "https://arxiv.org/abs/2509.06511", "authors": ["Daniil Tikhonov", "Matheus Scatolin", "Mohor Banerjee", "Qiankun Ji", "Ahmed Jaheen", "Mostafa Salem", "Abdelrahman Elsayed", "Hu Wang", "Sarim Hashmi", "Mohammad Yaqub"], "title": "Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach", "categories": ["cs.CV"], "comment": "Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "summary": "Accurate evaluation of the response of glioblastoma to therapy is crucial for\nclinical decision-making and patient management. The Response Assessment in\nNeuro-Oncology (RANO) criteria provide a standardized framework to assess\npatients' clinical response, but their application can be complex and subject\nto observer variability. This paper presents an automated method for\nclassifying the intervention response from longitudinal MRI scans, developed to\npredict tumor response during therapy as part of the BraTS 2025 challenge. We\npropose a novel hybrid framework that combines deep learning derived feature\nextraction and an extensive set of radiomics and clinically chosen features.\nOur approach utilizes a fine-tuned ResNet-18 model to extract features from 2D\nregions of interest across four MRI modalities. These deep features are then\nfused with a rich set of more than 4800 radiomic and clinically driven\nfeatures, including 3D radiomics of tumor growth and shrinkage masks,\nvolumetric changes relative to the nadir, and tumor centroid shift. Using the\nfused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a\nMacro F1 score of 0.50 in the 4-class response prediction task (Complete\nResponse, Partial Response, Stable Disease, Progressive Disease). Our results\nhighlight that synergizing learned image representations with domain-targeted\nradiomic features provides a robust and effective solution for automated\ntreatment response assessment in neuro-oncology.", "AI": {"tldr": "This study proposes an automated framework for assessing glioblastoma therapy response using a hybrid approach that combines deep learning and radiomics, achieving strong classification performance.", "motivation": "The paper addresses the clinical challenge of accurately evaluating glioblastoma therapy response, which is vital for decision-making but complicated by variability in the application of current manual assessment criteria.", "method": "The authors developed a hybrid framework that utilizes a fine-tuned ResNet-18 model to extract deep learning features from 2D MRI regions and combines these with over 4800 radiomic and clinical features to train a CatBoost classifier.", "result": "The model achieved a mean ROC AUC of 0.81 and a Macro F1 score of 0.50 in a 4-class tumor response prediction task, demonstrating its effectiveness.", "conclusion": "The combination of deep learning features and domain-specific radiomic features offers a robust approach for automated treatment response evaluation in neuro-oncology, reducing observer variability."}}
{"id": "2509.06535", "pdf": "https://arxiv.org/pdf/2509.06535", "abs": "https://arxiv.org/abs/2509.06535", "authors": ["Hua Chang Bakker", "Stan Fris", "Angela Madelon Bernardy", "Stan Deutekom"], "title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.", "AI": {"tldr": "This study investigates the reproducibility of FairCLIP and presents its own implementations, ultimately finding that the proposed fairness and performance improvements do not hold in specific applications.", "motivation": "To examine and validate the claims of FairCLIP regarding its ability to enhance group fairness and performance using Sinkhorn distance minimization across sensitive groups.", "method": "The study reproduced the FairCLIP setup, introduced a new implementation called A-FairCLIP, extended objectives for multiple attributes (FairCLIP+), and evaluated its impact on fairness and performance metrics, particularly for zero-shot glaucoma classification.", "result": "The experiments revealed discrepancies between FairCLIP's original description and implementation, and neither increased fairness nor improved performance was observed in zero-shot glaucoma classification for both the original and the new implementations.", "conclusion": "FairCLIP, despite its theoretical goals and regularization objective, does not demonstrate practical improvements in fairness or performance in the examined use cases, challenging its original claims."}}
{"id": "2509.05681", "pdf": "https://arxiv.org/pdf/2509.05681", "abs": "https://arxiv.org/abs/2509.05681", "authors": ["Xng Ai", "Shudan Lin", "Zecheng Li", "Kai Zhou", "Bixin Li", "Bin Xiao"], "title": "SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Decentralized Finance (DeFi) attacks have resulted in significant losses,\noften orchestrated through Adversarial Exploiter Contracts (AECs) that exploit\nvulnerabilities in victim smart contracts. To proactively identify such\nthreats, this paper targets the explainable detection of AECs.\n  Existing detection methods struggle to capture semantic dependencies and lack\ninterpretability, limiting their effectiveness and leaving critical knowledge\ngaps in AEC analysis. To address these challenges, we introduce SEASONED, an\neffective, self-explanatory, and robust framework for AEC detection.\n  SEASONED extracts semantic information from contract bytecode to construct a\nsemantic relation graph (SRG), and employs a self-counterfactual explainable\ndetector (SCFED) to classify SRGs and generate explanations that highlight the\ncore attack logic. SCFED further enhances robustness, generalizability, and\ndata efficiency by extracting representative information from these\nexplanations. Both theoretical analysis and experimental results demonstrate\nthe effectiveness of SEASONED, which showcases outstanding detection\nperformance, robustness, generalizability, and data efficiency learning\nability. To support further research, we also release a new dataset of 359\nAECs.", "AI": {"tldr": "The paper introduces SEASONED, a framework for detecting adversarial exploit contracts (AECs) in decentralized finance, focusing on interpretability, robustness, and efficiency.", "motivation": "There is a need for effective methods to detect adversarial exploit contracts in decentralized finance, as current approaches often fail to capture semantic dependencies and lack interpretability.", "method": "SEASONED uses semantic relation graphs (SRGs) derived from contract bytecode and a self-counterfactual explainable detector (SCFED) to classify SRGs and generate clear explanations of attack logic.", "result": "SEASONED demonstrates strong performance in AEC detection, robustness, generalizability, and data efficiency, validated through theory and experiments.", "conclusion": "SEASONED provides an effective and interpretable solution for identifying AECs, filling critical knowledge gaps and advancing research through its dataset release."}}
{"id": "2509.06536", "pdf": "https://arxiv.org/pdf/2509.06536", "abs": "https://arxiv.org/abs/2509.06536", "authors": ["Senem Aktas", "Charles Markham", "John McDonald", "Rozenn Dahyot"], "title": "Benchmarking EfficientTAM on FMO datasets", "categories": ["cs.CV"], "comment": null, "summary": "Fast and tiny object tracking remains a challenge in computer vision and in\nthis paper we first introduce a JSON metadata file associated with four open\nsource datasets of Fast Moving Objects (FMOs) image sequences. In addition, we\nextend the description of the FMOs datasets with additional ground truth\ninformation in JSON format (called FMOX) with object size information. Finally\nwe use our FMOX file to test a recently proposed foundational model for\ntracking (called EfficientTAM) showing that its performance compares well with\nthe pipelines originally taylored for these FMO datasets. Our comparison of\nthese state-of-the-art techniques on FMOX is provided with Trajectory\nIntersection of Union (TIoU) scores. The code and JSON is shared open source\nallowing FMOX to be accessible and usable for other machine learning pipelines\naiming to process FMO datasets.", "AI": {"tldr": "The paper introduces a new JSON metadata file (FMOX) for Fast Moving Objects (FMOs) datasets, enhances ground truth data, and evaluates a tracking model, EfficientTAM, using the data.", "motivation": "Accurately tracking fast-moving and small objects remains a complex challenge in computer vision, necessitating enhanced datasets and methodologies.", "method": "The paper presents FMOX, a JSON-format ground truth extension for FMO datasets, and tests EfficientTAM's tracking performance using Trajectory Intersection of Union (TIoU) metrics.", "result": "EfficientTAM achieves performance on par with specialized FMO datasets' pipelines. The FMOX files and corresponding code are made open-source for broader usability.", "conclusion": "The proposed FMOX resource, combined with the performance of EfficientTAM, holds significance for advancing FMO tracking and facilitating its application in machine learning pipelines."}}
{"id": "2509.06566", "pdf": "https://arxiv.org/pdf/2509.06566", "abs": "https://arxiv.org/abs/2509.06566", "authors": ["Emil Demi\u0107", "Luka \u010cehovin Zajc"], "title": "Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval", "categories": ["cs.CV"], "comment": "Accepted to BMVC2025", "summary": "The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural\nimages matching the overall semantics and spatial layout of a free-hand sketch.\nUnlike prior work focused on architectural augmentations of retrieval models,\nwe emphasize the inherent ambiguity and noise present in real-world sketches.\nThis insight motivates a training objective that is explicitly designed to be\nrobust to sketch variability. We show that with an appropriate combination of\npre-training, encoder architecture, and loss formulation, it is possible to\nachieve state-of-the-art performance without the introduction of additional\ncomplexity. Extensive experiments on a challenging FS-COCO and widely-used\nSketchyCOCO datasets confirm the effectiveness of our approach and underline\nthe critical role of training design in cross-modal retrieval tasks, as well as\nthe need to improve the evaluation scenarios of scene-level SBIR.", "AI": {"tldr": "This paper addresses Scene-level Sketch-Based Image Retrieval, emphasizing training objectives that handle sketch ambiguity and noise for improved performance.", "motivation": "The paper seeks to tackle the challenges of ambiguity and noise in real-world sketches while retrieving matching natural images based on semantic and spatial layout.", "method": "It leverages pre-training, encoder architecture, and a specifically designed loss formulation to achieve robust and efficient retrieval without adding architectural complexity.", "result": "Experiments on FS-COCO and SketchyCOCO datasets demonstrate state-of-the-art results, showcasing the significance of the proposed training design.", "conclusion": "The findings highlight the importance of training design for cross-modal retrieval and suggest improved evaluation scenarios for SBIR tasks."}}
{"id": "2509.06570", "pdf": "https://arxiv.org/pdf/2509.06570", "abs": "https://arxiv.org/abs/2509.06570", "authors": ["Runqing Yang", "Yimin Fu", "Changyuan Wu", "Zhunga Liu"], "title": "Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, 2025 IEEE/CVF International Conference on\n  Computer Vision Workshops", "summary": "Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.", "AI": {"tldr": "This paper introduces RARL for incremental open set recognition, addressing inter-class confusion in dynamic data streams and achieving state-of-the-art performance.", "motivation": "Open set recognition models typically struggle to handle dynamic scenarios where new unknown classes emerge continuously.", "method": "RARL aligns unknown representations around prototypes in angular space and employs strategies like VII training and stratified rectification to improve decision boundaries.", "result": "Experiments on CIFAR100 and TinyImageNet datasets confirm the superior performance of RARL under various task setups.", "conclusion": "RARL mitigates representation drift and improves incremental open set recognition, establishing a new benchmark in the field."}}
{"id": "2509.05398", "pdf": "https://arxiv.org/pdf/2509.05398", "abs": "https://arxiv.org/abs/2509.05398", "authors": ["Sheila Wafula", "Blessed Madukoma"], "title": "Unmasking COVID-19 Vulnerability in Nigeria: Mapping Risks Beyond Urban Hotspots", "categories": ["cs.CY", "cs.LG"], "comment": "8 pages, 6 figures. Submission to NeurIPS 2025 in preparation", "summary": "The COVID-19 pandemic has presented significant challenges in Nigeria's\npublic health systems since the first case reported on February 27, 2020. This\nstudy investigates key factors that contribute to state vulnerability,\nquantifying them through a composite risk score integrating population density\n(weight 0.2), poverty (0.4), access to healthcare (0.3), and age risk (0.1),\nadjusted by normalized case rates per 100,000. States were categorized into\nlow-, medium-, and high-density areas to analyze trends and identify hotspots\nusing geographic information system (GIS) mapping. The findings reveal that\nhigh-density urban areas, such as Lagos, accounting for 35.4% of national\ncases, had the highest risk scores (Lagos: 673.47 vs. national average: 28.16).\nThese results align with global and local studies on the spatial variability of\nCOVID-19 in Nigeria, including international frameworks such as the CDC Social\nVulnerability Index. Google Trends data highlight variations in public health\nawareness, serving as a supplementary analysis to contextualize vulnerability.\nThe risk score provides a prioritization tool for policymakers to allocate\ntesting, vaccines, and healthcare resources to high-risk areas, though data\ngaps and rural underreporting call for further research. This framework can\nextend to other infectious diseases, offering lessons for future pandemics in\nresource-limited settings.", "AI": {"tldr": "The paper identifies key factors contributing to state vulnerability to COVID-19 in Nigeria, using a composite risk score and GIS mapping, with a focus on high-density areas like Lagos.", "motivation": "To explore the spatial variability of COVID-19 in Nigeria and provide a risk-based framework for prioritizing public health resources.", "method": "A composite risk score was developed, integrating factors such as population density, poverty, healthcare access, and age, combined with spatial analysis through GIS mapping.", "result": "High-density urban areas, such as Lagos, had significantly higher risk scores, showing uneven vulnerability across the country. Google Trends data also revealed variations in public health awareness.", "conclusion": "The risk score framework aids policymakers in targeting resource allocation and can be adapted for future pandemics and other infectious diseases in resource-limited settings."}}
{"id": "2509.06579", "pdf": "https://arxiv.org/pdf/2509.06579", "abs": "https://arxiv.org/abs/2509.06579", "authors": ["Xin Kong", "Daniel Watson", "Yannick Str\u00fcmpler", "Michael Niemeyer", "Federico Tombari"], "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.", "AI": {"tldr": "CausNVS is an autoregressive multi-view diffusion model for flexible and high-quality 3D novel view synthesis.", "motivation": "Existing multi-view diffusion models for 3D novel view synthesis are non-autoregressive, limiting their applicability to fixed view configurations and causing slow inference due to simultaneous frame denoising.", "method": "CausNVS adopts an autoregressive approach supported by causal masking, per-frame noise injection, and pairwise-relative camera pose encodings (CaPE) for precise control. It also incorporates spatially-aware sliding-windows, key-value caching, and noise conditioning for drift mitigation during inference.", "result": "The method supports arbitrary camera trajectories and input-output view configurations, achieving strong and consistent visual quality across various settings.", "conclusion": "CausNVS offers a flexible and high-performance solution for autoregressive novel view synthesis, overcoming the limitations of prior non-autoregressive approaches."}}
{"id": "2509.06585", "pdf": "https://arxiv.org/pdf/2509.06585", "abs": "https://arxiv.org/abs/2509.06585", "authors": ["Ritwik Kulkarni", "WU Hanqin", "Enrico Di Minin"], "title": "Detection of trade in products derived from threatened species using machine learning and a smartphone", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unsustainable trade in wildlife is a major threat to biodiversity and is now\nincreasingly prevalent in digital marketplaces and social media. With the sheer\nvolume of digital content, the need for automated methods to detect wildlife\ntrade listings is growing. These methods are especially needed for the\nautomatic identification of wildlife products, such as ivory. We developed\nmachine learning-based object recognition models that can identify wildlife\nproducts within images and highlight them. The data consists of images of\nelephant, pangolin, and tiger products that were identified as being sold\nillegally or that were confiscated by authorities. Specifically, the wildlife\nproducts included elephant ivory and skins, pangolin scales, and claws (raw and\ncrafted), and tiger skins and bones. We investigated various combinations of\ntraining strategies and two loss functions to identify the best model to use in\nthe automatic detection of these wildlife products. Models were trained for\neach species while also developing a single model to identify products from all\nthree species. The best model showed an overall accuracy of 84.2% with\naccuracies of 71.1%, 90.2% and 93.5% in detecting products derived from\nelephants, pangolins, and tigers, respectively. We further demonstrate that the\nmachine learning model can be made easily available to stakeholders, such as\ngovernment authorities and law enforcement agencies, by developing a\nsmartphone-based application that had an overall accuracy of 91.3%. The\napplication can be used in real time to click images and help identify\npotentially prohibited products of target species. Thus, the proposed method is\nnot only applicable for monitoring trade on the web but can also be used e.g.\nin physical markets for monitoring wildlife trade.", "AI": {"tldr": "The paper presents machine learning models that identify illegal wildlife products in images with high accuracy and a smartphone app for real-time use.", "motivation": "The need for automated methods to detect illegal wildlife trade in digital marketplaces and physical markets due to its threat to biodiversity.", "method": "Developed machine learning-based models to identify wildlife products (e.g., ivory, skins, and bones from elephants, pangolins, and tigers). Tested various training strategies and loss functions for accuracy.", "result": "The best model achieved 84.2% overall accuracy and specific accuracies of 71.1% (elephants), 90.2% (pangolins), and 93.5% (tigers). The smartphone app had 91.3% accuracy.", "conclusion": "The models and smartphone app offer a scalable solution for authorities and law enforcement to monitor and curb wildlife trade in digital and physical markets."}}
{"id": "2509.06591", "pdf": "https://arxiv.org/pdf/2509.06591", "abs": "https://arxiv.org/abs/2509.06591", "authors": ["Yichao Liu", "YueYang Teng"], "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.", "AI": {"tldr": "This paper introduces HSANet for denoising in LDCT/PET imaging, addressing noise issues while ensuring radiation safety.", "motivation": "The study aims to improve LDCT/PET imaging quality, which is compromised due to noise and artifacts resulting from reduced radiation exposure.", "method": "HSANet is proposed, utilizing Efficient Global Attention (EGA) modules for improved feature interaction and a hybrid upsampling module to prevent overfitting.", "result": "HSANet demonstrated superior denoising performance on a publicly available dataset compared to existing methods while maintaining a lightweight model size.", "conclusion": "HSANet enhances image quality, remains computationally efficient, and is practical for clinical applications."}}
{"id": "2509.05739", "pdf": "https://arxiv.org/pdf/2509.05739", "abs": "https://arxiv.org/abs/2509.05739", "authors": ["Hanna Foerster", "Ilia Shumailov", "Yiren Zhao", "Harsh Chaudhari", "Jamie Hayes", "Robert Mullins", "Yarin Gal"], "title": "Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Early research into data poisoning attacks against Large Language Models\n(LLMs) demonstrated the ease with which backdoors could be injected. More\nrecent LLMs add step-by-step reasoning, expanding the attack surface to include\nthe intermediate chain-of-thought (CoT) and its inherent trait of decomposing\nproblems into subproblems. Using these vectors for more stealthy poisoning, we\nintroduce ``decomposed reasoning poison'', in which the attacker modifies only\nthe reasoning path, leaving prompts and final answers clean, and splits the\ntrigger across multiple, individually harmless components.\n  Fascinatingly, while it remains possible to inject these decomposed poisons,\nreliably activating them to change final answers (rather than just the CoT) is\nsurprisingly difficult. This difficulty arises because the models can often\nrecover from backdoors that are activated within their thought processes.\nUltimately, it appears that an emergent form of backdoor robustness is\noriginating from the reasoning capabilities of these advanced LLMs, as well as\nfrom the architectural separation between reasoning and final answer\ngeneration.", "AI": {"tldr": "The paper analyzes \"decomposed reasoning poison\" attacks on advanced LLMs, where poison affects reasoning paths but not prompts or final answers. Models display robustness, recovering from such manipulations. ", "motivation": "Attackers could circumvent traditional methods by targeting the reasoning paths within LLMs rather than the final outputs, posing new challenges for research on LLM security.", "method": "The authors focus on modifying reasoning paths (chain-of-thought) to poison models, keeping prompts and answers untampered, and testing how effective these attacks are.", "result": "The study finds that activating decomposed reasoning poison to alter final answers proves difficult; models often recover from backdoor interruptions through reasoning.", "conclusion": "Advanced LLMs show robustness against reasoning path-based poisoning attacks, highlighting the strength of their emergent reasoning and separation between thought processes and answer formulation."}}
{"id": "2509.06625", "pdf": "https://arxiv.org/pdf/2509.06625", "abs": "https://arxiv.org/abs/2509.06625", "authors": ["Aswini Kumar Patra"], "title": "Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "13 pages, 8 figures, 7 Tables", "summary": "Plants in their natural habitats endure an array of interacting stresses,\nboth biotic and abiotic, that rarely occur in isolation. Nutrient\nstress-particularly nitrogen deficiency-becomes even more critical when\ncompounded with drought and weed competition, making it increasingly difficult\nto distinguish and address its effects. Early detection of nitrogen stress is\ntherefore crucial for protecting plant health and implementing effective\nmanagement strategies. This study proposes a novel deep learning framework to\naccurately classify nitrogen stress severity in a combined stress environment.\nOur model uses a unique blend of four imaging modalities-RGB, multispectral,\nand two infrared wavelengths-to capture a wide range of physiological plant\nresponses from canopy images. These images, provided as time-series data,\ndocument plant health across three levels of nitrogen availability (low,\nmedium, and high) under varying water stress and weed pressures. The core of\nour approach is a spatio-temporal deep learning pipeline that merges a\nConvolutional Neural Network (CNN) for extracting spatial features from images\nwith a Long Short-Term Memory (LSTM) network to capture temporal dependencies.\nWe also devised and evaluated a spatial-only CNN pipeline for comparison. Our\nCNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively\nsurpassing the spatial-only model's 80.45% and other previously reported\nmachine learning method's 76%. These results bring actionable insights based on\nthe power of our CNN-LSTM approach in effectively capturing the subtle and\ncomplex interactions between nitrogen deficiency, water stress, and weed\npressure. This robust platform offers a promising tool for the timely and\nproactive identification of nitrogen stress severity, enabling better crop\nmanagement and improved plant health.", "AI": {"tldr": "The paper presents a deep learning approach using CNN-LSTM to classify nitrogen stress severity in plants under combined stress environments with 98% accuracy.", "motivation": "To address the challenge of early detection of nitrogen stress severity in plants experiencing combined stresses such as drought and weed competition.", "method": "A spatio-temporal deep learning framework combining CNN for spatial feature extraction and LSTM for temporal dependency modeling using four imaging modalities and time-series canopy images.", "result": "The CNN-LSTM pipeline achieved 98% accuracy, outperforming a spatial-only CNN (80.45%) and prior machine learning methods (76%).", "conclusion": "The proposed CNN-LSTM model is highly effective for timely identification of nitrogen stress severity under complex stress conditions, offering actionable insights for crop management."}}
{"id": "2509.05447", "pdf": "https://arxiv.org/pdf/2509.05447", "abs": "https://arxiv.org/abs/2509.05447", "authors": ["Zhongyuan Zhao", "Gunjan Verma", "Ananthram Swami", "Santiago Segarra"], "title": "Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)", "categories": ["cs.NI", "cs.DM", "cs.LG", "eess.SP", "05-08", "C.2.1; I.2.8; G.2.2"], "comment": "15 pages, 18 figures, accepted to IEEE Transactions on Wireless\n  Communications. This is the extended journal version of the conference paper\n  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, \"Distributed Link\n  Sparsification for Scalable Scheduling using Graph Neural Networks,\" IEEE\n  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )", "summary": "In wireless networks characterized by dense connectivity, the significant\nsignaling overhead generated by distributed link scheduling algorithms can\nexacerbate issues like congestion, energy consumption, and radio footprint\nexpansion. To mitigate these challenges, we propose a distributed link\nsparsification scheme employing graph neural networks (GNNs) to reduce\nscheduling overhead for delay-tolerant traffic while maintaining network\ncapacity. A GNN module is trained to adjust contention thresholds for\nindividual links based on traffic statistics and network topology, enabling\nlinks to withdraw from scheduling contention when they are unlikely to succeed.\nOur approach is facilitated by a novel offline constrained {unsupervised}\nlearning algorithm capable of balancing two competing objectives: minimizing\nscheduling overhead while ensuring that total utility meets the required level.\nIn simulated wireless multi-hop networks with up to 500 links, our link\nsparsification technique effectively alleviates network congestion and reduces\nradio footprints across four distinct distributed link scheduling protocols.", "AI": {"tldr": "The paper proposes a method using graph neural networks (GNNs) to reduce scheduling overhead in dense wireless networks while maintaining network capacity.", "motivation": "To tackle significant signaling overhead in dense wireless networks, which leads to congestion, energy consumption, and expanded radio footprints.", "method": "The authors employ a GNN module trained via an offline constrained unsupervised learning algorithm to adjust contention thresholds, reducing scheduling contention for low-probability links.", "result": "The technique successfully reduces network congestion and radio footprints in simulated wireless multi-hop networks with up to 500 links across four scheduling protocols.", "conclusion": "The proposed link sparsification method efficiently balances network utility and scheduling overhead, offering a scalable solution for delay-tolerant traffic in dense networks."}}
{"id": "2509.05753", "pdf": "https://arxiv.org/pdf/2509.05753", "abs": "https://arxiv.org/abs/2509.05753", "authors": ["Ching-Chun Chang", "Isao Echizen"], "title": "Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "The rise of synthetic media has blurred the boundary between reality and\nfabrication under the evolving power of artificial intelligence, fueling an\ninfodemic that erodes public trust in cyberspace. For digital imagery, a\nmultitude of editing applications further complicates the forensic analysis,\nincluding semantic edits that alter content, photometric adjustments that\nrecalibrate colour characteristics, and geometric projections that reshape\nviewpoints. Collectively, these transformations manipulate and control\nperceptual interpretation of digital imagery. This susceptibility calls for\nforensic enquiry into reconstructing the chain of events, thereby revealing\ndeeper evidential insight into the presence or absence of criminal intent. This\nstudy seeks to address an inverse problem of tracing the underlying generation\nchain that gives rise to the observed synthetic media. A tell-tale watermarking\nsystem is developed for explanatory reasoning over the nature and extent of\ntransformations across the lifecycle of synthetic media. Tell-tale watermarks\nare tailored to different classes of transformations, responding in a manner\nthat is neither strictly robust nor fragile but instead interpretable. These\nwatermarks function as reference clues that evolve under the same\ntransformation dynamics as the carrier media, leaving interpretable traces when\nsubjected to transformations. Explanatory reasoning is then performed to infer\nthe most plausible account across the combinatorial parameter space of\ncomposite transformations. Experimental evaluations demonstrate the validity of\ntell-tale watermarking with respect to fidelity, synchronicity and\ntraceability.", "AI": {"tldr": "This paper presents a system of interpretable watermarks to track and analyze transformations in synthetic media, aiding forensic inquiries into digital fabrication.", "motivation": "With synthetic media increasingly compromising trust in digital spaces and complicating forensic analysis, there is a need to trace the generation process of altered digital content.", "method": "The researchers developed a watermarking system tailored for different types of media transformations, enabling interpretation of traces left by edits without being robust or fragile.", "result": "The experimental evaluation shows that the proposed watermarking system is valid in terms of fidelity, synchronicity, and traceability under varying transformations.", "conclusion": "Tell-tale watermarks can provide interpretable evidence and aid in reconstructing the generation chains of synthetic media, contributing to forensic applications in combating digital fabrication and misinformation."}}
{"id": "2509.06685", "pdf": "https://arxiv.org/pdf/2509.06685", "abs": "https://arxiv.org/abs/2509.06685", "authors": ["Shengkai Zhang", "Yuhe Liu", "Guanjun Wu", "Jianhua He", "Xinggang Wang", "Mozi Chen", "Kezhong Liu"], "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes", "categories": ["cs.CV"], "comment": null, "summary": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for\nnovel-view synthesis (NVS) in large scenes. GS typically requires accurate\ndepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited\ndepth sensing range makes it difficult for GS to work in large scenes.\nMonocular images, however, lack depth to guide the learning and lead to\ninferior NVS results. Although large foundation models (LFMs) for monocular\ndepth estimation are available, they suffer from cross-frame inconsistency,\ninaccuracy for distant scenes, and ambiguity in deceptive texture cues. This\npaper aims to generate dense, accurate depth images from monocular RGB inputs\nfor high-definite GS rendering. The key idea is to leverage the accurate but\nsparse depth from visual-inertial Structure-from-Motion (SfM) to refine the\ndense but coarse depth from LFMs. To bridge the sparse input and dense output,\nwe propose an object-segmented depth propagation algorithm that renders the\ndepth of pixels of structured objects. Then we develop a dynamic depth\nrefinement module to handle the crippled SfM depth of dynamic objects and\nrefine the coarse LFM depth. Experiments using public and customized datasets\ndemonstrate the superior rendering quality of VIM-GS in large scenes.", "AI": {"tldr": "VIM-GS uses monocular images combined with sparse depth from SfM and dense depth from LFMs to achieve high-quality novel-view synthesis for large scenes.", "motivation": "Existing Gaussian Splatting methods depend on accurate depth data, which is difficult to achieve in large scenes using traditional methods like stereo or RGB-D cameras. Monocular images offer potential but suffer from depth-related limitations.", "method": "Leverages sparse depth data from SfM and combines it with dense but coarse depth from LFMs using an object-segmented depth propagation algorithm and a dynamic depth refinement module.", "result": "Demonstrated superior novel-view rendering quality in large-scale scenes compared to existing approaches, through public and custom datasets.", "conclusion": "The proposed framework, VIM-GS, successfully addresses depth inaccuracy and cross-frame inconsistency issues in monocular image-based novel view synthesis for large scenes."}}
{"id": "2509.05510", "pdf": "https://arxiv.org/pdf/2509.05510", "abs": "https://arxiv.org/abs/2509.05510", "authors": ["Tyler E. Maltba", "Ben S. Southworth", "Jeffrey R. Haack", "Marc L. Klasky"], "title": "Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions", "categories": ["physics.comp-ph", "cs.LG"], "comment": null, "summary": "Continued progress in inertial confinement fusion (ICF) requires solving\ninverse problems relating experimental observations to simulation input\nparameters, followed by design optimization. However, such high dimensional\ndynamic PDE-constrained optimization problems are extremely challenging or even\nintractable. It has been recently shown that inverse problems can be solved by\nonly considering certain robust features. Here we consider the ICF capsule's\ndeuterium-tritium (DT) interface, and construct a causal, dynamic,\nmultifidelity reduced-order surrogate that maps from a time-dependent radiation\ntemperature drive to the interface's radius and velocity dynamics. The\nsurrogate targets an ODE embedding of DT interface dynamics, and is constructed\nby learning a controller for a base analytical model using low- and\nhigh-fidelity simulation training data with respect to radiation energy group\nstructure. After demonstrating excellent accuracy of the surrogate interface\nmodel, we use machine learning (ML) models with surrogate-generated data to\nsolve inverse problems optimizing radiation temperature drive to reproduce\nobserved interface dynamics. For sparse snapshots in time, the ML model further\ncharacterizes the most informative times at which to sample dynamics.\nAltogether we demonstrate how operator learning, causal architectures, and\nphysical inductive bias can be integrated to accelerate discovery, design, and\ndiagnostics in high-energy-density systems.", "AI": {"tldr": "The paper develops a reduced-order surrogate model for inertial confinement fusion (ICF) design optimization using operator learning and machine learning techniques.", "motivation": "To address challenges in solving high-dimensional, dynamic PDE-constrained optimization problems in ICF experiments, which require mapping experimental observations to simulation parameters.", "method": "The authors construct a surrogate model using operator learning, a physics-based ODE embedding, and multifidelity training data to map radiation temperature drives to DT interface dynamics.", "result": "The surrogate exhibits high accuracy in predicting interface dynamics and enables ML-based inverse optimization of radiation temperature drives, improving observation reproduction and sampling times.", "conclusion": "By integrating operator learning and physical inductive biases, the study accelerates design, discovery, and diagnostics in high-energy-density systems like ICF."}}
{"id": "2509.05755", "pdf": "https://arxiv.org/pdf/2509.05755", "abs": "https://arxiv.org/abs/2509.05755", "authors": ["Yu Liu", "Yuchong Xie", "Mingyu Luo", "Zesen Liu", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.", "AI": {"tldr": "This paper highlights security vulnerabilities in Tool Invocation Prompts (TIPs) used in LLM-based agentic systems, which can be exploited for attacks like remote code execution (RCE) and denial of service (DoS). It proposes defense mechanisms to improve TIP security.", "motivation": "The motivation behind this study is to address the overlooked security risks associated with TIPs in LLM-based agentic systems, which are critical for securing tool interactions and system functionality.", "method": "This research employs a systematic TIP exploitation workflow (TEW) to investigate vulnerabilities and demonstrate hijacking of external tool behavior via manipulated tool invocations.", "result": "The study reveals significant vulnerabilities in major LLM-based systems like Cursor and Claude Code, showing how TIPs can be exploited for RCE and DoS attacks.", "conclusion": "TIPs pose substantial security risks in LLM-based agentic systems, and the paper recommends implementing robust defense mechanisms to mitigate these vulnerabilities."}}
{"id": "2509.06693", "pdf": "https://arxiv.org/pdf/2509.06693", "abs": "https://arxiv.org/abs/2509.06693", "authors": ["Xichen Xu", "Yanshu Wang", "Jinbao Wang", "Qunyi Zhang", "Xiaoning Lei", "Guoyang Xie", "Guannan Jiang", "Zhichao Lu"], "title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation.", "AI": {"tldr": "The paper introduces STAGE, a novel approach to industrial anomaly synthesis, addressing limitations in existing techniques by generating detailed and pixel-level accurate anomalies through graded diffusion and explicit mask alignment strategies.", "motivation": "Existing methods in Segmentation-oriented Industrial Anomaly Synthesis struggle with generating detailed and contextually accurate anomalies, limiting performance in downstream anomaly segmentation.", "method": "STAGE utilizes a graded diffusion framework, anomaly inference strategy with clean background priors, and explicit mask alignment to create refined and context-consistent industrial anomaly data.", "result": "Experiments on MVTec and BTAD datasets confirm STAGE's superior performance in anomaly synthesis and its enhancement of downstream segmentation tasks.", "conclusion": "The proposed STAGE method improves industrial anomaly synthesis by effectively addressing key drawbacks of current approaches, leading to enhanced segmentation results."}}
