{"id": "2506.04245", "pdf": "https://arxiv.org/pdf/2506.04245", "abs": "https://arxiv.org/abs/2506.04245", "authors": ["Guangchen Lan", "Huseyin A. Inan", "Sahar Abdelnabi", "Janardhan Kulkarni", "Lukas Wutschitz", "Reza Shokri", "Christopher G. Brinton", "Robert Sim"], "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only $\\sim700$\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.", "AI": {"tldr": "The paper explores how autonomous agents can ensure contextual integrity (CI) by reasoning about their operating context, focusing on minimizing inappropriate information disclosure.", "motivation": "The paper aims to address the challenge of ensuring appropriate information disclosure by autonomous agents, which is critical for user trust and contextual integrity in decision-making.", "method": "The authors test reasoning-based approaches using LLMs and build an RL framework, coupled with a synthetic dataset, to teach agents how to disclose information appropriately in different contexts.", "result": "The method significantly improves performance in reducing inappropriate information disclosure while maintaining effectiveness. This progress translates well to external CI benchmarks like PrivacyLens.", "conclusion": "Reasoning about context is crucial for enhancing contextual integrity in autonomous agents, and the proposed RL approach effectively achieves this goal."}}
{"id": "2506.04251", "pdf": "https://arxiv.org/pdf/2506.04251", "abs": "https://arxiv.org/abs/2506.04251", "authors": ["Zhengyang Li"], "title": "Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.", "AI": {"tldr": "This paper introduces the LLM-MARL framework to enhance multi-agent reinforcement learning using large language models, showing increased performance and generalization in simulated games.", "motivation": "To bridge the gap between language modeling and policy learning, aiming to improve coordination and communication in multi-agent systems.", "method": "The framework includes modular components\u2014Coordinator, Communicator, and Memory\u2014for subgoal generation, symbolic messaging, and episodic recall, combining PPO training with language-conditioned losses and LLM query gating.", "result": "LLM-MARL demonstrates improved win rates, coordination scores, and zero-shot generalization across tested environments, outperforming baseline methods like MAPPO and QMIX.", "conclusion": "This framework highlights the potential of LLMs to enhance multi-agent interaction, offering advancements in AI systems for games, training, and human-AI collaboration."}}
{"id": "2506.04368", "pdf": "https://arxiv.org/pdf/2506.04368", "abs": "https://arxiv.org/abs/2506.04368", "authors": ["Aayush Gupta", "Gopal Pandurangan"], "title": "Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks", "categories": ["cs.DC"], "comment": null, "summary": "We address a fundamental problem in Peer-to-Peer (P2P) networks, namely,\nconstructing and maintaining dynamic P2P overlay network topologies with\nessential properties such as connectivity, low diameter, and high expansion,\nthat are resilient to continuous high churn and the presence of a large number\nof malicious (Byzantine) nodes. Our main goal is to construct and maintain a\nsparse (bounded degree) expander topology despite high churn and a large number\nof Byzantine nodes. Such an expander topology has logarithmic diameter, high\nexpansion, and is robust to churn and the presence of a large number of bad\nnodes, and facilitates efficient and robust algorithms for fundamental problems\nin distributed computing, such as agreement, broadcasting, routing, etc.\n  Our main contribution is a randomized, fully-distributed dynamic P2P protocol\nthat works with only local initial knowledge and guarantees, with a high\nprobability, the maintenance of a constant degree graph with high expansion\neven under continuous churn and in the presence of a large number of Byzantine\nnodes. Our protocol can tolerate up to $o(n/poly\\log(n))$ Byzantine nodes\n(where $n$ is the stable network size). Our protocol is efficient, lightweight,\nand scalable, and it incurs only $O(poly\\log(n))$ overhead for topology\nmaintenance: only polylogarithmic (in $n$) bits need to be processed and sent\nby each honest node per round, and any honest node's computation cost per round\nis also polylogarithmic.\n  Our protocol can be used as a building block for solving fundamental\ndistributed computing problems in highly dynamic networks, such as Byzantine\nagreement and Byzantine leader election, and enables fast and scalable\nalgorithms for these problems.", "AI": {"tldr": "The paper proposes a fully-distributed P2P protocol to maintain resilient, sparse expander network topologies under high churn and Byzantine nodes with minimal overhead.", "motivation": "To solve the problem of constructing and maintaining robust P2P overlay networks that are resilient to dynamic churn and adversarial Byzantine nodes, aiding efficient distributed computing.", "method": "A randomized dynamic P2P protocol that ensures expansion properties of the network graph with bounded degree, using only local knowledge and with minimal communication and processing overhead.", "result": "The proposed protocol guarantees, with high probability, a constant degree graph with logarithmic diameter and high expansion, tolerating up to $o(n/poly\\log(n))$ Byzantine nodes.", "conclusion": "The protocol ensures scalability, resilience, and efficiency, offering a robust building block for fundamental distributed computing problems like Byzantine agreement, even in hostile network environments."}}
{"id": "2506.04252", "pdf": "https://arxiv.org/pdf/2506.04252", "abs": "https://arxiv.org/abs/2506.04252", "authors": ["Yang Zhao", "Chengxiao Dai", "Dusit Niyato", "Chuan Fu Tan", "Keyi Xiang", "Yueyang Wang", "Zhiquan Yeo", "Daren Tan Zong Loong", "Jonathan Low Zhaozhi", "Eugene H. Z. HO"], "title": "A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) hold promise for sustainable manufacturing, but\noften hallucinate industrial codes and emission factors, undermining regulatory\nand investment decisions. We introduce CircuGraphRAG, a retrieval-augmented\ngeneration (RAG) framework that grounds LLMs outputs in a domain-specific\nknowledge graph for the circular economy. This graph connects 117,380\nindustrial and waste entities with classification codes and GWP100 emission\ndata, enabling structured multi-hop reasoning. Natural language queries are\ntranslated into SPARQL and verified subgraphs are retrieved to ensure accuracy\nand traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG\nachieves superior performance in single-hop and multi-hop question answering,\nwith ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also\nimproves efficiency, halving the response time and reducing token usage by 16%\nin representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready\nsupport for circular economy planning, advancing reliable, low-carbon resource\ndecision making.", "AI": {"tldr": "CircuGraphRAG enhances LLM accuracy by anchoring outputs in a knowledge graph for circular economy, improving question answering and efficiency.", "motivation": "LLMs often hallucinate data critical to sustainable manufacturing decisions, thus needing a grounded and reliable mechanism.", "method": "A Retrieval-Augmented Generation (RAG) framework leveraging a domain-specific knowledge graph, translating natural language queries into SPARQL and retrieving verified subgraphs.", "result": "CircuGraphRAG outperforms baselines in question answering (ROUGE-L F1 up to 1.0), halves response time, and reduces token usage by 16%.", "conclusion": "CircuGraphRAG provides reliable, traceable support for circular economy decisions, aiding sustainable manufacturing."}}
{"id": "2506.04456", "pdf": "https://arxiv.org/pdf/2506.04456", "abs": "https://arxiv.org/abs/2506.04456", "authors": ["Ke Ma", "Junfei Xie"], "title": "Knowledge-Guided Attention-Inspired Learning for Task Offloading in Vehicle Edge Computing", "categories": ["cs.DC", "cs.AR"], "comment": null, "summary": "Vehicle edge computing (VEC) brings abundant computing resources close to\nvehicles by deploying them at roadside units (RSUs) or base stations, thereby\nenabling diverse computation-intensive and delay sensitive applications.\nExisting task offloading strategies are often computationally expensive to\nexecute or generate suboptimal solutions. In this paper, we propose a novel\nlearning-based approach, Knowledge-guided Attention-inspired Task Offloading\n(KATO), designed to efficiently offload tasks from moving vehicles to nearby\nRSUs. KATO integrates an attention-inspired encoder-decoder model for selecting\na subset of RSUs that can reduce overall task processing time, along with an\nefficient iterative algorithm for computing optimal task allocation among the\nselected RSUs. Simulation results demonstrate that KATO achieves optimal or\nnear-optimal performance with significantly lower computational overhead and\ngeneralizes well across networks of varying sizes and configurations.", "AI": {"tldr": "This paper introduces KATO, an efficient learning-based task offloading framework for Vehicle Edge Computing (VEC), using attention-guided models and iterative algorithms.", "motivation": "Task offloading in Vehicle Edge Computing is complicated due to the need for fast, computation-efficient solutions to meet delay-sensitive application demands.", "method": "KATO employs an attention-inspired encoder-decoder model to select RSUs and an iterative algorithm to allocate the tasks optimally among them.", "result": "Simulations reveal that KATO delivers optimal or near-optimal performance with reduced computational costs, adapting well to diverse network sizes and setups.", "conclusion": "KATO streamlines task offloading in VEC with a robust and efficient approach that minimizes overhead and achieves reliable performance across various scenarios."}}
{"id": "2506.04238", "pdf": "https://arxiv.org/pdf/2506.04238", "abs": "https://arxiv.org/abs/2506.04238", "authors": ["Shriyank Somvanshi", "Md Monzurul Islam", "Syed Aaqib Javed", "Gaurab Chhetri", "Kazi Sifatul Islam", "Tausif Islam Chowdhury", "Sazzad Bin Bashar Polock", "Anandi Dutta", "Subasish Das"], "title": "A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,\nswarm behavior, foraging, and plant growth to solve complex, nonlinear,\nhigh-dimensional optimization problems. This survey categorizes BIAs into eight\ngroups: evolutionary, swarm intelligence, physics-inspired, ecosystem and\nplant-based, predator-prey, neural-inspired, human-inspired, and hybrid\napproaches, and reviews their core principles, strengths, and limitations. We\nillustrate the usage of these algorithms in machine learning, engineering\ndesign, bioinformatics, and intelligent systems, and highlight recent advances\nin hybridization, parameter tuning, and adaptive strategies. Finally, we\nidentify open challenges such as scalability, convergence, reliability, and\ninterpretability to suggest directions for future research. This work aims to\nserve as a foundational resource for both researchers and practitioners\ninterested in understanding the current landscape and future directions of\nbio-inspired computing.", "AI": {"tldr": "The paper categorizes Bio-inspired Algorithms (BIAs) into eight groups and reviews their applications, strengths, limitations, and future challenges.", "motivation": "The aim is to provide researchers and practitioners with an in-depth understanding of BIAs and guide future research efforts.", "method": "The survey organizes BIAs into eight categories and examines their principles, applications, advancements, and challenges.", "result": "The paper summarizes key developments in hybridization, parameter tuning, adaptive strategies, and the application of BIAs in diverse fields.", "conclusion": "The review highlights open challenges in BIAs such as scalability, reliability, and interpretability, serving as a roadmap for ongoing research."}}
{"id": "2506.04263", "pdf": "https://arxiv.org/pdf/2506.04263", "abs": "https://arxiv.org/abs/2506.04263", "authors": ["Alan Mitkiy", "James Smith", "Hana Satou", "Hiroshi Tanaka", "Emily Johnson", "F Monkey"], "title": "Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial training is among the most effective strategies for defending\ndeep neural networks against adversarial examples. A key limitation of existing\nadversarial training approaches lies in their reliance on a fixed perturbation\nbudget, which fails to account for instance-specific robustness\ncharacteristics. While prior works such as IAAT and MMA introduce\ninstance-level adaptations, they often rely on heuristic or static\napproximations of data robustness. In this paper, we propose Dynamic Epsilon\nScheduling (DES), a novel framework that adaptively adjusts the adversarial\nperturbation budget per instance and per training iteration. DES integrates\nthree key factors: (1) the distance to the decision boundary approximated via\ngradient-based proxies, (2) prediction confidence derived from softmax entropy,\nand (3) model uncertainty estimated via Monte Carlo dropout. By combining these\ncues into a unified scheduling strategy, DES tailors the perturbation budget\ndynamically to guide more effective adversarial learning. Experimental results\non CIFAR-10 and CIFAR-100 show that our method consistently improves both\nadversarial robustness and standard accuracy compared to fixed-epsilon\nbaselines and prior adaptive methods. Moreover, we provide theoretical insights\ninto the stability and convergence of our scheduling policy. This work opens a\nnew avenue for instance-aware, data-driven adversarial training methods.", "AI": {"tldr": "This paper introduces Dynamic Epsilon Scheduling (DES), a method to improve adversarial robustness by dynamically adapting the perturbation budget during adversarial training.", "motivation": "Existing adversarial training methods rely on a fixed perturbation budget, which does not consider the unique robustness characteristics of individual instances.", "method": "The proposed DES framework uses a dynamic scheduling strategy based on three factors: (1) distance to the decision boundary via gradient-based proxies, (2) prediction confidence through softmax entropy, and (3) model uncertainty via Monte Carlo dropout.", "result": "The method outperforms fixed-epsilon baselines and prior adaptive approaches in both adversarial robustness and standard accuracy on CIFAR-10 and CIFAR-100 datasets.", "conclusion": "DES offers a novel, instance-aware, and data-driven approach to adversarial training, combining multiple cues into an effective adaptive strategy."}}
{"id": "2506.04344", "pdf": "https://arxiv.org/pdf/2506.04344", "abs": "https://arxiv.org/abs/2506.04344", "authors": ["Caojin Zhang", "Qiang Zhang", "Ke Li", "Sai Vidyaranya Nuthalapati", "Benyu Zhang", "Jason Liu", "Serena Li", "Lizhu Zhang", "Xiangjun Fan"], "title": "GEM: Empowering LLM for both Embedding Generation and Language Understanding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large decoder-only language models (LLMs) have achieved remarkable success in\ngeneration and reasoning tasks, where they generate text responses given\ninstructions. However, many applications, e.g., retrieval augmented generation\n(RAG), still rely on separate embedding models to generate text embeddings,\nwhich can complicate the system and introduce discrepancies in understanding of\nthe query between the embedding model and LLMs. To address this limitation, we\npropose a simple self-supervised approach, Generative Embedding large language\nModel (GEM), that enables any large decoder-only LLM to generate high-quality\ntext embeddings while maintaining its original text generation and reasoning\ncapabilities. Our method inserts new special token(s) into a text body, and\ngenerates summarization embedding of the text by manipulating the attention\nmask. This method could be easily integrated into post-training or fine tuning\nstages of any existing LLMs. We demonstrate the effectiveness of our approach\nby applying it to two popular LLM families, ranging from 1B to 8B parameters,\nand evaluating the transformed models on both text embedding benchmarks (MTEB)\nand NLP benchmarks (MMLU). The results show that our proposed method\nsignificantly improves the original LLMs on MTEB while having a minimal impact\non MMLU. Our strong results indicate that our approach can empower LLMs with\nstate-of-the-art text embedding capabilities while maintaining their original\nNLP performance", "AI": {"tldr": "The paper introduces GEM, a method enabling decoder-only LLMs to generate high-quality text embeddings without compromising their generation and reasoning capabilities.", "motivation": "Current applications like RAG require separate embedding models, leading to complications and discrepancies between embedding models and LLM understanding.", "method": "GEM inserts special tokens into text and manipulates the attention mask during post-training or fine-tuning stages of decoder-only LLMs.", "result": "Testing GEM on LLMs ranging from 1B to 8B parameters shows significant improvements in text embedding benchmarks (MTEB) with minimal impact on NLP benchmarks (MMLU).", "conclusion": "GEM effectively empowers LLMs to achieve state-of-the-art text embedding qualities while retaining their original NLP functionalities."}}
{"id": "2506.04544", "pdf": "https://arxiv.org/pdf/2506.04544", "abs": "https://arxiv.org/abs/2506.04544", "authors": ["Charles Hong", "Brendan Roberts", "Huijae An", "Alex Um", "Advay Ratan", "Yakun Sophia Shao"], "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.", "AI": {"tldr": "The paper introduces hdl2v, a dataset translating other hardware description languages to Verilog, which enhances Verilog generation by large language models by up to 63%.", "motivation": "To address the scarcity of human-written Verilog code for training large language models in hardware code generation.", "method": "The authors created the hdl2v dataset by translating VHDL, Chisel, and PyMTL3 to Verilog and tested its impact on LLM performance.", "result": "The dataset improved a 32-billion-parameter model's Verilog generation accuracy by 23% and enhanced fine-tuning approaches by 63%.", "conclusion": "Hdl2v enriches Verilog code resources and paves the way for better hardware code generation using LLMs, while identifying areas for further dataset improvements."}}
{"id": "2506.04418", "pdf": "https://arxiv.org/pdf/2506.04418", "abs": "https://arxiv.org/abs/2506.04418", "authors": ["Noor Nashid", "Daniel Ding", "Keheliya Gallaba", "Ahmed E. Hassan", "Ali Mesbah"], "title": "Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges", "categories": ["cs.SE"], "comment": null, "summary": "Multi-hunk bugs, where fixes span disjoint regions of code, are common in\npractice, yet remain underrepresented in automated repair. Existing techniques\nand benchmarks pre-dominantly target single-hunk scenarios, overlooking the\nadded complexity of coordinating semantically related changes across the\ncodebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches\nderived from 372 real-world defects. We propose hunk divergence, a metric that\nquantifies the variation among edits in a patch by capturing lexical,\nstructural, and file-level differences, while incorporating the number of hunks\ninvolved. We further define spatial proximity, a classification that models how\nhunks are spatially distributed across the program hierarchy. Our empirical\nstudy spanning six LLMs reveals that model success rates decline with increased\ndivergence and spatial dispersion. Notably, when using the LLM alone, no model\nsucceeds in the most dispersed Fragment class. These findings highlight a\ncritical gap in LLM capabilities and motivate divergence-aware repair\nstrategies.", "AI": {"tldr": "Examines challenges of fixing multi-hunk bugs using large language models (LLMs) and introduces metrics to evaluate their complexity.", "motivation": "Multi-hunk bugs, involving fixes across disjoint code regions, are underrepresented in automated repair but are prevalent in real-world scenarios.", "method": "Analyzed a dataset of multi-hunk patches derived from 372 real-world defects. Proposed metrics such as hunk divergence and spatial proximity to evaluate patch complexity. Empirically tested six LLMs to assess their proficiency.", "result": "Model success rates decline as the divergence increases and spatial dispersion rises. None of the tested LLMs succeeded in fixing the most dispersed multi-hunk cases.", "conclusion": "Existing LLMs struggle with multi-hunk bugs, and divergence-aware strategies in automated repair are needed to address this gap."}}
{"id": "2506.04308", "pdf": "https://arxiv.org/pdf/2506.04308", "abs": "https://arxiv.org/abs/2506.04308", "authors": ["Enshen Zhou", "Jingkun An", "Cheng Chi", "Yi Han", "Shanyu Rong", "Chi Zhang", "Pengwei Wang", "Zhongyuan Wang", "Tiejun Huang", "Lu Sheng", "Shanghang Zhang"], "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://zhoues.github.io/RoboRefer/", "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.", "AI": {"tldr": "RoboRefer is a 3D-aware vision-language model (VLM) designed for precise spatial reasoning and understanding in robots, achieving state-of-the-art accuracy through innovative training methods and datasets.", "motivation": "To address limitations of existing models in spatial reasoning and dynamic location understanding for embodied robots in 3D environments.", "method": "The model integrates a depth encoder with supervised fine-tuning (SFT) and uses reinforcement fine-tuning (RFT) with metric-sensitive reward functions. It is trained using a new large-scale dataset, RefSpatial, and evaluated with a custom benchmark, RefSpatial-Bench.", "result": "RoboRefer achieves an 89.6% success rate in spatial understanding and outperforms existing methods by a significant margin in multi-step spatial reasoning tasks, particularly on RefSpatial-Bench.", "conclusion": "RoboRefer demonstrates superior spatial reasoning abilities and versatility for robotic tasks, making it suitable for real-world dynamic and complex environments."}}
{"id": "2506.04237", "pdf": "https://arxiv.org/pdf/2506.04237", "abs": "https://arxiv.org/abs/2506.04237", "authors": ["Sanchit Sinha", "Aidong Zhang"], "title": "A Comprehensive Survey on the Risks and Limitations of Concept-based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept-based Models are a class of inherently explainable networks that\nimprove upon standard Deep Neural Networks by providing a rationale behind\ntheir predictions using human-understandable `concepts'. With these models\nbeing highly successful in critical applications like medical diagnosis and\nfinancial risk prediction, there is a natural push toward their wider adoption\nin sensitive domains to instill greater trust among diverse stakeholders.\nHowever, recent research has uncovered significant limitations in the structure\nof such networks, their training procedure, underlying assumptions, and their\nsusceptibility to adversarial vulnerabilities. In particular, issues such as\nconcept leakage, entangled representations, and limited robustness to\nperturbations pose challenges to their reliability and generalization.\nAdditionally, the effectiveness of human interventions in these models remains\nan open question, raising concerns about their real-world applicability. In\nthis paper, we provide a comprehensive survey on the risks and limitations\nassociated with Concept-based Models. In particular, we focus on aggregating\ncommonly encountered challenges and the architecture choices mitigating these\nchallenges for Supervised and Unsupervised paradigms. We also examine recent\nadvances in improving their reliability and discuss open problems and promising\navenues of future research in this domain.", "AI": {"tldr": "This paper offers a survey on the limitations and risks of Concept-based Models, focusing on challenges like concept leakage and model robustness while discussing solutions and future directions.", "motivation": "Concept-based Models are needed to enhance trust in sensitive applications such as medical diagnosis and financial risk assessment but face critical limitations and challenges.", "method": "The paper conducts a comprehensive survey, aggregating challenges and proposed solutions for both Supervised and Unsupervised paradigms of Concept-based Models.", "result": "It identifies key risks (e.g., concept leakage, entangled representations, low robustness) and discusses architectural strategies to mitigate these issues.", "conclusion": "While Concept-based Models hold great promise for explainability, addressing their limitations and exploring future research directions are crucial for their wider adoption and success."}}
{"id": "2506.04247", "pdf": "https://arxiv.org/pdf/2506.04247", "abs": "https://arxiv.org/abs/2506.04247", "authors": ["Gage K. R. Hooper"], "title": "The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model", "categories": ["q-bio.NC", "cs.AI", "cs.NE", "92B20, 37N25, 68T05", "I.2.6; I.5.1; I.6.3"], "comment": "31 pages, 16 figures", "summary": "While many neural networks focus on layers to process information, the GAIN\nmodel uses a grid-based structure to improve biological plausibility and the\ndynamics of the model. The grid structure helps neurons to interact with their\nclosest neighbors and improve their connections with one another, which is seen\nin biological neurons. While also being implemented with the Izhikevich model\nthis approach allows for a computationally efficient and biologically accurate\nsimulation that can aid in the development of neural networks, large scale\nsimulations, and the development in the neuroscience field. This adaptation of\nthe Izhikevich model can improve the dynamics and accuracy of the model,\nallowing for its uses to be specialized but efficient.", "AI": {"tldr": "The paper introduces the GAIN model, which employs a grid-based structure using the Izhikevich model to achieve computational efficiency and biological plausibility.", "motivation": "The motivation is to enhance the biological plausibility and efficiency of neural network simulations by improving the interaction dynamics among neurons based on principles seen in biological systems.", "method": "The GAIN model introduces a grid-structured neural network architecture combined with the Izhikevich model for computational simulations.", "result": "The model achieves better biological accuracy, improved neuron dynamics, and efficient simulation capabilities.", "conclusion": "The grid-based adaptation of the Izhikevich model offers specialized but efficient applications in neural networks, large-scale simulations, and neuroscience development."}}
{"id": "2506.04480", "pdf": "https://arxiv.org/pdf/2506.04480", "abs": "https://arxiv.org/abs/2506.04480", "authors": ["Nina Vesseron", "Elsa Cazelles", "Alice Le Brigant", "Thierry Klein"], "title": "On the Wasserstein Geodesic Principal Component Analysis of probability measures", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "This paper focuses on Geodesic Principal Component Analysis (GPCA) on a\ncollection of probability distributions using the Otto-Wasserstein geometry.\nThe goal is to identify geodesic curves in the space of probability measures\nthat best capture the modes of variation of the underlying dataset. We first\naddress the case of a collection of Gaussian distributions, and show how to\nlift the computations in the space of invertible linear maps. For the more\ngeneral setting of absolutely continuous probability measures, we leverage a\nnovel approach to parameterizing geodesics in Wasserstein space with neural\nnetworks. Finally, we compare to classical tangent PCA through various examples\nand provide illustrations on real-world datasets.", "AI": {"tldr": "This paper proposes a method to identify geodesic curves in the space of probability distributions using the Otto-Wasserstein geometry, focusing on Gaussian distributions and absolutely continuous probability measures, with comparisons to classical techniques.", "motivation": "To better understand the modes of variation in datasets of probability distributions by leveraging advanced geometric and computational techniques.", "method": "Utilize Otto-Wasserstein geometry to define geodesic principal component analysis, employing neural networks for parameterization in general cases, and applying operations in the space of invertible linear maps for Gaussian distributions.", "result": "The proposed method was demonstrated through comparisons with tangent PCA, showcasing its utility across examples and real-world datasets.", "conclusion": "The paper demonstrates the effectiveness and potential of GPCA in Otto-Wasserstein geometry for capturing dataset variations, showing promise in both theoretical frameworks and practical applications."}}
{"id": "2506.04891", "pdf": "https://arxiv.org/pdf/2506.04891", "abs": "https://arxiv.org/abs/2506.04891", "authors": ["Viacheslav Kuzmin", "Basil Kyriacou", "Mateusz Papierz", "Mo Kordzanganeh", "Alexey Melnikov"], "title": "TQml Simulator: Optimized Simulation of Quantum Machine Learning", "categories": ["quant-ph", "cs.ET", "cs.LG", "cs.PF"], "comment": "25 pages, 13 figures, 1 table", "summary": "Hardware-efficient circuits employed in Quantum Machine Learning are\ntypically composed of alternating layers of uniformly applied gates. High-speed\nnumerical simulators for such circuits are crucial for advancing research in\nthis field. In this work, we numerically benchmark universal and gate-specific\ntechniques for simulating the action of layers of gates on quantum state\nvectors, aiming to accelerate the overall simulation of Quantum Machine\nLearning algorithms. Our analysis shows that the optimal simulation method for\na given layer of gates depends on the number of qubits involved, and that a\ntailored combination of techniques can yield substantial performance gains in\nthe forward and backward passes for a given circuit. Building on these\ninsights, we developed a numerical simulator, named TQml Simulator, that\nemploys the most efficient simulation method for each layer in a given circuit.\nWe evaluated TQml Simulator on circuits constructed from standard gate sets,\nsuch as rotations and CNOTs, as well as on native gates from IonQ and IBM\nquantum processing units. In most cases, our simulator outperforms equivalent\nPennylane's default.qubit simulator by approximately 2- to 100-fold, depending\non the circuit, the number of qubits, the batch size of the input data, and the\nhardware used.", "AI": {"tldr": "The paper introduces TQml Simulator, a high-speed numerical simulator for efficient simulation of quantum machine learning circuits, outperforming Pennylane's default.qubit simulator in many cases.", "motivation": "The paper aims to address the need for faster numerical simulators tailored for hardware-efficient quantum machine learning circuits to expedite research and performance in the field.", "method": "The study benchmarks various gate-specific and universal simulation techniques, identifies optimal simulation methods for different quantum layers, and implements these insights to develop the TQml Simulator.", "result": "TQml Simulator significantly enhances simulation speeds, achieving up to 100-fold improvements compared to Pennylane\u2019s simulator depending on circuit complexity, qubit counts, input batch sizes, and hardware used.", "conclusion": "A tailored combination of simulation techniques offers substantial performance benefits for quantum machine learning simulations, enabling faster and more scalable solutions."}}
{"id": "2506.04253", "pdf": "https://arxiv.org/pdf/2506.04253", "abs": "https://arxiv.org/abs/2506.04253", "authors": ["Tapio Pitk\u00e4ranta", "Leena Pitk\u00e4ranta"], "title": "HADA: Human-AI Agent Decision Alignment Architecture", "categories": ["cs.AI", "cs.HC", "cs.AI, cs.SE, cs.MA, cs.CL, cs.LG"], "comment": "18 pages, 4 figures", "summary": "We present HADA (Human-AI Agent Decision Alignment), a protocol- and\nframework agnostic reference architecture that keeps both large language model\n(LLM) agents and legacy algorithms aligned with organizational targets and\nvalues. HADA wraps any algorithm or LLM in role-specific stakeholder agents --\nbusiness, data-science, audit, ethics, and customer -- each exposing\nconversational APIs so that technical and non-technical actors can query,\nsteer, audit, or contest every decision across strategic, tactical, and\nreal-time horizons. Alignment objectives, KPIs, and value constraints are\nexpressed in natural language and are continuously propagated, logged, and\nversioned while thousands of heterogeneous agents run on different\norchestration stacks. A cloud-native proof of concept packages a production\ncredit-scoring model (getLoanDecision) and deploys it on\nDocker/Kubernetes/Python; five scripted retail-bank scenarios show how target\nchanges, parameter tweaks, explanation requests, and ethics triggers flow end\nto end through the architecture. Evaluation followed the Design-Science\nResearch Methodology. Walkthrough observation and log inspection demonstrated\ncomplete coverage of six predefined objectives: every role could invoke\nconversational control, trace KPIs and value constraints, detect and mitigate\nZIP-code bias, and reproduce full decision lineage, independent of the\nunderlying LLM or agent library. Contributions: (1) an open-source HADA\narchitecture, (2) a mid-range design theory for human-AI alignment in\nmulti-agent systems, and (3) empirical evidence that framework-agnostic,\nprotocol-compliant stakeholder agents improve accuracy, transparency, and\nethical compliance in real-world decision pipelines.", "AI": {"tldr": "HADA introduces a framework to ensure alignment of AI and legacy systems with organizational values by enabling conversational APIs and stakeholder-specific agents.", "motivation": "The paper aims to address challenges in aligning AI and legacy algorithms with organizational values, ensuring ethical and transparent decisions.", "method": "HADA uses stakeholder-specific agents, conversational APIs, and natural language expression of objectives, deployed on Docker/Kubernetes/Python for real-world testing.", "result": "A proof of concept demonstrated alignment across multiple use cases with enhanced control, transparency, and bias mitigation.", "conclusion": "HADA effectively aligns decision pipelines with ethical and organizational standards, improving transparency and compliance."}}
{"id": "2506.04507", "pdf": "https://arxiv.org/pdf/2506.04507", "abs": "https://arxiv.org/abs/2506.04507", "authors": ["Narangerelt Batsoyol", "Jonathan Guiang", "Diego Davila", "Aashay Arora", "Philip Chang", "Frank W\u00fcrthwein", "Steven Swanson"], "title": "SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing", "categories": ["cs.DC"], "comment": "27TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY & NUCLEAR\n  PHYSICS - 2024", "summary": "Data analysis in high-energy physics (HEP) begins with data reduction, where\nvast datasets are filtered to extract relevant events. At the Large Hadron\nCollider (LHC), this process is bottlenecked by slow data transfers between\nstorage and compute nodes. To address this, we introduce SkimROOT, a near-data\nfiltering system leveraging Data Processing Units (DPUs) to accelerate LHC data\nanalysis. By performing filtering directly on storage servers and returning\nonly the relevant data, SkimROOT minimizes data movement and reduces processing\ndelays. Our prototype demonstrates significant efficiency gains, achieving a\n44.3$\\times$ performance improvement, paving the way for faster physics\ndiscoveries.", "AI": {"tldr": "The abstract introduces SkimROOT, a system leveraging Data Processing Units (DPUs) to filter large datasets directly on storage servers, significantly speeding up data analysis in high-energy physics (HEP) at the LHC by minimizing data movement.", "motivation": "The bottleneck in HEP data analysis at the LHC arises from slow data transfers between storage and compute nodes during the data reduction phase, which delays physics discoveries.", "method": "The authors developed SkimROOT, a near-data filtering system that uses DPUs to process data directly on storage servers, reducing irrelevant data movement and minimizing delays.", "result": "The prototype of SkimROOT demonstrated a 44.3x performance improvement in data analysis efficiency.", "conclusion": "SkimROOT accelerates HEP data analysis by addressing data transfer bottlenecks, enabling faster physics discoveries, and providing a more efficient data filtering solution."}}
{"id": "2506.04328", "pdf": "https://arxiv.org/pdf/2506.04328", "abs": "https://arxiv.org/abs/2506.04328", "authors": ["Akira SaiToh", "Arezoo Modiri", "Amit Sawant", "Robabeh Rahimi"], "title": "Quantum-Inspired Genetic Optimization for Patient Scheduling in Radiation Oncology", "categories": ["cs.NE", "physics.med-ph", "68W50", "I.6.3; J.3"], "comment": "15 pages, 11 figures, 10 tables", "summary": "Among the genetic algorithms generally used for optimization problems in the\nrecent decades, quantum-inspired variants are known for fast and high-fitness\nconvergence and small resource requirement. Here the application to the patient\nscheduling problem in proton therapy is reported. Quantum chromosomes are\ntailored to possess the superposed data of patient IDs and gantry statuses.\nSelection and repair strategies are also elaborated for reliable convergence to\na clinically feasible schedule although the employed model is not complex.\nClear advantage in population size is shown over the classical counterpart in\nour numerical results for both a medium-size test case and a large-size\npractical problem instance. It is, however, observed that program run time is\nrather long for the large-size practical case, which is due to the limitation\nof classical emulation and demands the forthcoming true quantum computation.\nOur results also revalidate the stability of the conventional classical genetic\nalgorithm.", "AI": {"tldr": "The paper discusses using quantum-inspired genetic algorithms (QGA) for patient scheduling in proton therapy, highlighting advantages in convergence quality and resource efficiency but noting limitations in runtime for larger cases.", "motivation": "Quantum-inspired genetic algorithms hold the potential to improve optimization problems but require validation in real-world applications like patient scheduling for proton therapy.", "method": "The authors used QGAs with quantum chromosomes for patient IDs and gantry statuses. They introduced selection and repair strategies to ensure clinically usable scheduling.", "result": "QGA demonstrated clear advantages in population size and convergence quality over classical methods in both medium-size and large-size test cases, but runtime was long for large problems due to classical emulation.", "conclusion": "QGA offers promising improvements in scheduling optimization, but practical implementation awaits advancements in quantum computing. Classical genetic algorithms remain stable and reliable for now."}}
{"id": "2506.04277", "pdf": "https://arxiv.org/pdf/2506.04277", "abs": "https://arxiv.org/abs/2506.04277", "authors": ["Yi Lu", "Jiawang Cao", "Yongliang Wu", "Bozheng Li", "Licheng Tang", "Yangguang Ji", "Chong Wu", "Jay Wu", "Wenbo Zhu"], "title": "RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as ACL 2025 Main", "summary": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nreasoning capability while lack explicit mechanisms for visual grounding and\nsegmentation, creating a gap between cognitive reasoning and visual perception.\nTo bridge this gap, we introduce Reasoning Segmentation via Visual Prompting\n(RSVP), a novel framework that unifies multi-step multimodal reasoning with\ngrounded visual understanding. RSVP is a two-stage structuralized framework\nthat integrates reasoning-driven localization with segmentation refinement. In\nthe reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to\nhelp MLLMs understand queries and infer targets, generating interpretable\nregion proposals that enhance visual grounding. In segmentation stage, RSVP\nrefines these proposals with a Vision-Language Segmentation Module (VLSM),\nseamlessly integrates textual and visual cues to produce precise segmentation\nmasks. By explicitly modelling the interaction between multimodal reasoning and\nsegmentation, RSVP introduces a new paradigm for interpretable reasoning\nsegmentation. It exploits MLLMs' inherent localization capabilities, enabling\nthe models to not only reason about objects but also generate structured visual\nrepresentations. Our extensive experiments demonstrate that RSVP achieves\nstate-of-the-art performance, surpasses state-of-the-art methods by up to +6.5\ngIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under\nzero-shot settings. These results validate RSVP as an effective and scalable\nframework for integrating cognitive reasoning with structured visual\nunderstanding.", "AI": {"tldr": "This paper introduces RSVP, a framework that bridges the gap between cognitive reasoning and visual perception, enabling better visual grounding and segmentation in multi-modal large language models.", "motivation": "To address the lack of visual grounding and segmentation capabilities in MLLMs, creating a gap in their cognitive reasoning and visual perception.", "method": "RSVP uses a two-stage framework: reasoning-driven localization through multimodal chain-of-thought prompts, followed by segmentation refinement using a Vision-Language Segmentation Module for precise results.", "result": "RSVP achieves state-of-the-art performance with improvements of up to +6.5 gIoU, +9.2 cIoU on ReasonSeg, and 49.7 mAP in SegInW under zero-shot conditions.", "conclusion": "RSVP effectively combines multimodal reasoning and segmentation, enabling interpretable and structured visual understanding, showcasing its scalability and impact."}}
{"id": "2506.04364", "pdf": "https://arxiv.org/pdf/2506.04364", "abs": "https://arxiv.org/abs/2506.04364", "authors": ["Zheng-Xin Yong", "Vineel Pratap", "Michael Auli", "Jean Maillard"], "title": "Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "To build an automatic speech recognition (ASR) system that can serve everyone\nin the world, the ASR needs to be robust to a wide range of accents including\nunseen accents. We systematically study how three different variables in\ntraining data -- the number of speakers, the audio duration per each individual\nspeaker, and the diversity of accents -- affect ASR robustness towards unseen\naccents in a low-resource training regime. We observe that for a fixed number\nof ASR training hours, it is more beneficial to increase the number of speakers\n(which means each speaker contributes less) than the number of hours\ncontributed per speaker. We also observe that more speakers enables ASR\nperformance gains from scaling number of hours. Surprisingly, we observe\nminimal benefits to prioritizing speakers with different accents when the\nnumber of speakers is controlled. Our work suggests that practitioners should\nprioritize increasing the speaker count in ASR training data composition for\nnew languages.", "AI": {"tldr": "This study explores optimizing automatic speech recognition (ASR) systems for unseen accents by analyzing the impact of speaker count, audio duration per speaker, and accent diversity in low-resource scenarios.", "motivation": "To create ASR systems that effectively serve global populations, it is necessary to ensure robustness to a variety of accents, including those not seen during training.", "method": "The research systematically evaluates the relationships between speaker count, per-speaker audio duration, and accent diversity, analyzing their effects on ASR robustness under low-resource conditions.", "result": "Increasing the number of speakers in training data rather than prioritizing longer durations per speaker or accent diversity leads to improved ASR performance for unseen accents in low-resource regimes.", "conclusion": "Practitioners should focus on maximizing the number of unique speakers in ASR training data, as this leads to better robustness against unseen accents."}}
{"id": "2506.04438", "pdf": "https://arxiv.org/pdf/2506.04438", "abs": "https://arxiv.org/abs/2506.04438", "authors": ["Katerina Goseva-Popstojanova", "Denny Hood", "Johann Schumann", "Noble Nkwocha"], "title": "On the Practices of Autonomous Systems Development: Survey-based Empirical Findings", "categories": ["cs.SE"], "comment": "25 pages", "summary": "Autonomous systems have gained an important role in many industry domains and\nare beginning to change everyday life. However, due to dynamically emerging\napplications and often proprietary constraints, there is a lack of information\nabout the practice of developing autonomous systems. This paper presents the\nfirst part of the longitudinal study focused on establishing\nstate-of-the-practice, identifying and quantifying the challenges and benefits,\nidentifying the processes and standards used, and exploring verification and\nvalidation (V&V) practices used for the development of autonomous systems. The\nresults presented in this paper are based on data about software systems that\nhave autonomous functionality and may employ model-based software engineering\n(MBSwE) and reuse. These data were collected using an anonymous online survey\nthat was administered in 2019 and were provided by experts with experience in\ndevelopment of autonomous systems and /or the use of MBSwE. Our current work is\nfocused on repeating the survey to collect more recent data and discover how\nthe development of autonomous systems has evolved over time.", "AI": {"tldr": "This paper initiates a longitudinal study, examining the state-of-practice, challenges, benefits, standards, and V&V used in autonomous systems development through an expert-provided survey from 2019.", "motivation": "Autonomous systems are impacting industries and daily life, but information about their development practices is scarce due to emerging applications and proprietary constraints.", "method": "The study leverages data from a 2019 anonymous online survey answered by experts in autonomous systems development and MBSwE use.", "result": "Findings report on the current practices, challenges, benefits, standards, and verification and validation methods in autonomous systems development.", "conclusion": "Results serve as a benchmark for understanding autonomous system development practices and plans to track evolution by repeating the survey for updated insights."}}
{"id": "2506.04359", "pdf": "https://arxiv.org/pdf/2506.04359", "abs": "https://arxiv.org/abs/2506.04359", "authors": ["Alexander Korovko", "Dmitry Slepichev", "Alexander Efitorov", "Aigul Dzhumamuratova", "Viktor Kuznetsov", "Hesam Rabeti", "Joydeep Biswas"], "title": "cuVSLAM: CUDA accelerated visual odometry", "categories": ["cs.RO", "cs.AI", "cs.SE"], "comment": null, "summary": "Accurate and robust pose estimation is a key requirement for any autonomous\nrobot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous\nlocalization and mapping, which can operate with a variety of visual-inertial\nsensor suites, including multiple RGB and depth cameras, and inertial\nmeasurement units. cuVSLAM supports operation with as few as one RGB camera to\nas many as 32 cameras, in arbitrary geometric configurations, thus supporting a\nwide range of robotic setups. cuVSLAM is specifically optimized using CUDA to\ndeploy in real-time applications with minimal computational overhead on\nedge-computing devices such as the NVIDIA Jetson. We present the design and\nimplementation of cuVSLAM, example use cases, and empirical results on several\nstate-of-the-art benchmarks demonstrating the best-in-class performance of\ncuVSLAM.", "AI": {"tldr": "cuVSLAM is an advanced visual SLAM system optimized for real-time performance on edge devices, supporting diverse camera and sensor configurations.", "motivation": "The paper aims to address the challenge of accurate and robust pose estimation, a critical requirement for autonomous robotics.", "method": "The authors introduce cuVSLAM, which uses CUDA for real-time optimization and supports flexible visual-inertial sensor combinations, suitable for edge devices.", "result": "cuVSLAM demonstrates superior performance on state-of-the-art benchmarks, showing both versatility and high accuracy.", "conclusion": "cuVSLAM proves to be a robust and scalable SLAM solution optimized for robotic applications, particularly on computationally constrained edge devices."}}
{"id": "2506.04241", "pdf": "https://arxiv.org/pdf/2506.04241", "abs": "https://arxiv.org/abs/2506.04241", "authors": ["Konstantin Kirchheim", "Frank Ortmeier"], "title": "Improving Out-of-Distribution Detection with Markov Logic Networks", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for ensuring the reliability\nof deep learning models operating in open-world scenarios. Current OOD\ndetectors mainly rely on statistical models to identify unusual patterns in the\nlatent representations of a deep neural network. This work proposes to augment\nexisting OOD detectors with probabilistic reasoning, utilizing Markov logic\nnetworks (MLNs). MLNs connect first-order logic with probabilistic reasoning to\nassign probabilities to inputs based on weighted logical constraints defined\nover human-understandable concepts, which offers improved explainability.\nThrough extensive experiments on multiple datasets, we demonstrate that MLNs\ncan significantly enhance the performance of a wide range of existing OOD\ndetectors while maintaining computational efficiency. Furthermore, we introduce\na simple algorithm for learning logical constraints for OOD detection from a\ndataset and showcase its effectiveness.", "AI": {"tldr": "This paper proposes leveraging probabilistic reasoning with Markov logic networks (MLNs) to enhance existing OOD detectors, improving performance and explainability.", "motivation": "Ensuring the reliability of deep models in open-world scenarios through improved OOD detection, as current methods primarily rely on statistical models.", "method": "Integrates Markov logic networks (MLNs) with OOD detectors, introduces a way to learn logical constraints for detection from datasets, and uses logical reasoning to assign probabilities.", "result": "Demonstrated an improvement in performance across various OOD detectors while maintaining efficiency, confirmed through experiments on multiple datasets.", "conclusion": "Probabilistic reasoning with MLNs enhances both accuracy and explainability of OOD detection systems, validating the proposed approach and algorithm."}}
{"id": "2506.04549", "pdf": "https://arxiv.org/pdf/2506.04549", "abs": "https://arxiv.org/abs/2506.04549", "authors": ["Vardhan Palod", "Pranav Mahajan", "Veeky Baths", "Boris S. Gutkin"], "title": "Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning", "categories": ["q-bio.NC"], "comment": "Accepted in the Proceedings of Cognitive Computational Neuroscience\n  (CCN) 2025", "summary": "Despite a strong desire to quit, individuals with long-term substance use\ndisorder (SUD) often struggle to resist drug use, even when aware of its\nharmful consequences. This disconnect between knowledge and compulsive behavior\nreflects a fundamental cognitive-behavioral conflict in addiction.\nNeurobiologically, differential cue-induced activity within striatal\nsubregions, along with dopamine-mediated connectivity from the ventral to the\ndorsal striatum, contributes to compulsive drug-seeking. However, the\nfunctional mechanism linking these findings to behavioral conflict remains\nunclear. Another hallmark of addiction is temporal discounting: individuals\nwith drug dependence exhibit steeper discount rates than non-users. Assuming\nthe ventral-dorsal striatal organization reflects a gradient from cognitive to\nmotor representations, addiction can be modeled within a hierarchical\nreinforcement learning (HRL) framework. However, integrating discounting into\nbiologically grounded HRL remains an open challenge. In this work, we build on\na model showing how action choices reinforced with drug rewards become\ninsensitive to the negative consequences that follow. We address the\nintegration of discounting by ensuring natural reward values converge across\nall levels in the HRL hierarchy, while drug rewards diverge due to their\ndopaminergic effects. Our results show that high discounting amplifies\ndrug-seeking across the hierarchy, linking faster discounting with increased\naddiction severity and impulsivity. We demonstrate alignment with empirical\nfindings on temporal discounting and propose testable predictions, establishing\naddiction as a disorder of hierarchical decision-making.", "AI": {"tldr": "Individuals with long-term substance use disorder struggle with cognitive and behavioral conflicts, influenced by neurobiological mechanisms and temporal discounting. This paper models addiction using hierarchical reinforcement learning (HRL) and integrates discounting effects, linking them to addiction severity and impulsivity.", "motivation": "To understand the cognitive-behavioral conflicts in addiction, caused by striatal activity and temporal discounting, and to provide an integrative framework explaining addiction as hierarchical decision-making dysfunction.", "method": "The study builds on a hierarchical reinforcement learning (HRL) model, modifying it to integrate temporal discounting by aligning natural reward values and accounting for the dopaminergic effects of drug rewards.", "result": "The results show that high discounting elevates drug-seeking behavior within the HRL hierarchy, correlating faster discounting rates with greater addiction severity and impulsivity, and align with empirical findings.", "conclusion": "Temporal discounting, modeled within a HRL framework, contributes to compulsive drug-seeking, supporting the view of addiction as a disorder of hierarchical decision-making with biologically grounded predictions."}}
{"id": "2506.04626", "pdf": "https://arxiv.org/pdf/2506.04626", "abs": "https://arxiv.org/abs/2506.04626", "authors": ["Haochen Zhang", "Zhong Zheng", "Lingzhou Xue"], "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning", "categories": ["stat.ML", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2502.02859", "summary": "Motivated by real-world settings where data collection and policy deployment\n-- whether for a single agent or across multiple agents -- are costly, we study\nthe problem of on-policy single-agent reinforcement learning (RL) and federated\nRL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to\nreach near-optimal regret) and policy switching or communication costs. In\nparallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$\nstates and $A$ actions, existing methods either require superlinear burn-in\ncosts in $S$ and $A$ or fail to achieve logarithmic switching or communication\ncosts. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost\nand FedQ-EarlySettled-LowCost -- that are the first in the literature to\nsimultaneously achieve: (i) the best near-optimal regret among all known\nmodel-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly\nwith $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent\nRL or communication cost for FRL. Additionally, we establish gap-dependent\ntheoretical guarantees for both regret and switching/communication costs,\nimproving or matching the best-known gap-dependent bounds.", "AI": {"tldr": "The paper introduces two algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, addressing reinforcement learning challenges in reducing burn-in, policy switching, and communication costs while maintaining high performance.", "motivation": "Optimizing reinforcement learning in scenarios with expensive data collection and policy deployment, focusing on balancing regret performance with operational cost limitations.", "method": "Two model-free RL algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, are proposed. These achieve linear burnout costs and logarithmic switching or communication costs while maintaining near-optimal regret.", "result": "The proposed algorithms outperform existing techniques by achieving linear burn-in costs, logarithmic policy switching or communication costs, and effective gap-dependent guarantees.", "conclusion": "The research solves key inefficiencies in on-policy single-agent and federated RL by introducing algorithms that set new benchmarks in cost-efficiency and regret optimality."}}
{"id": "2506.04640", "pdf": "https://arxiv.org/pdf/2506.04640", "abs": "https://arxiv.org/abs/2506.04640", "authors": ["Jon Altonaga Puente", "Enrico Mezzetti", "Irune Agirre Troncoso", "Jaume Abella Ferrer", "Francisco J. Cazorla Almeida"], "title": "ROSGuard: A Bandwidth Regulation Mechanism for ROS2-based Applications", "categories": ["cs.AR", "B.8.1"], "comment": "13 pages, 16 figures, submitted to RTSS2025", "summary": "Multicore timing interference, arising when multiple requests contend for the\nsame shared hardware resources, is a primary concern for timing verification\nand validation of time-critical applications. Bandwidth control and regulation\napproaches have been proposed in the literature as an effective method to\nmonitor and limit the impact of timing interference at run time. These\napproaches seek for fine-grained control of the bandwidth consumption (at the\nmicrosecond level) to meet stringent timing requirements on embedded critical\nsystems. Such granularity and configurations, while effective, can become an\nentry barrier for the application of bandwidth control to a wide class of\nproductized, modular ROS2 applications. This is so because those applications\nhave less stringent timing requirements but would still benefit from bandwidth\nregulation, though under less restrictive, and therefore more portable,\ngranularity and configurations.\n  In this work, we provide ROSGuard, a highly-portable, modular implementation\nof a timing interference monitoring and control mechanism that builds on the\nabstractions available on top of a generic and portable Linux-based software\nstack with the Robotic Operating System 2 (ROS2) layer, a widespreadedly\nadopted middleware for a wide class of industrial applications, far beyond the\nrobotic domain. We deploy ROSGuard on an NVIDIA AGX Orin platform as a\nrepresentative target for functionally rich distributed AI-based applications\nand a set of synthetic and real-world benchmarks. We apply an effective\nbandwidth regulation scheme on ROS2-based applications and achieve comparable\neffectiveness to specialized, finer-grained state-of-the-art solutions.", "AI": {"tldr": "The paper addresses multicore timing interference in shared hardware resources and introduces ROSGuard, a modular implementation for timing interference monitoring and control for ROS2 applications, achieving effectiveness comparable to state-of-the-art solutions.", "motivation": "Timing interference poses challenges for meeting timing requirements in embedded critical systems, and the existing fine-grained bandwidth control methods are not portable or suitable for less stringent but still beneficial applications like modular ROS2 systems.", "method": "The authors developed ROSGuard, a timing interference monitoring and control mechanism, leveraging generic Linux-based software and the ROS2 middleware. It was implemented and tested on the NVIDIA AGX Orin platform.", "result": "ROSGGuard applied an effective bandwidth regulation scheme on ROS2-based applications, demonstrating performance comparable to fine-grained state-of-the-art methods.", "conclusion": "ROSGGuard offers a portable and modular solution for timing interference control in ROS2 applications, making it practical for wider industrial adoption and diverse applications beyond robotics."}}
{"id": "2506.04902", "pdf": "https://arxiv.org/pdf/2506.04902", "abs": "https://arxiv.org/abs/2506.04902", "authors": ["Preethika Pradeep", "Eyhab Al-Masri"], "title": "Energy-Optimized Scheduling for AIoT Workloads Using TOPSIS", "categories": ["cs.DC", "cs.PF", "cs.SY", "eess.SY"], "comment": null, "summary": "AIoT workloads demand energy-efficient orchestration across cloud-edge\ninfrastructures, but Kubernetes' default scheduler lacks multi-criteria\noptimization for heterogeneous environments. This paper presents GreenPod, a\nTOPSIS-based scheduler optimizing pod placement based on execution time, energy\nconsumption, processing core, memory availability, and resource balance. Tested\non a heterogeneous Google Kubernetes cluster, GreenPod improves energy\nefficiency by up to 39.1% over the default Kubernetes (K8s) scheduler,\nparticularly with energy-centric weighting schemes. Medium complexity workloads\nshowed the highest energy savings, despite slight scheduling latency. GreenPod\neffectively balances sustainability and performance for AIoT applications.", "AI": {"tldr": "The paper introduces GreenPod, a TOPSIS-based Kubernetes scheduler that improves energy efficiency by up to 39.1% for AIoT workloads in heterogeneous cloud-edge setups.", "motivation": "Default Kubernetes scheduler lacks multi-criteria optimization to efficiently handle AIoT workloads across heterogeneous cloud-edge environments.", "method": "Proposes GreenPod, a scheduler based on the TOPSIS decision-making method, optimizing pod placement with multiple metrics such as energy use, execution time, and resource balance.", "result": "GreenPod outperforms the default Kubernetes scheduler, demonstrating up to 39.1% energy efficiency improvement, especially with energy-focused weighting schemes.", "conclusion": "GreenPod effectively balances sustainability and performance, making it suitable for energy-conscious AIoT applications in heterogeneous cloud-edge infrastructures."}}
{"id": "2506.04287", "pdf": "https://arxiv.org/pdf/2506.04287", "abs": "https://arxiv.org/abs/2506.04287", "authors": ["Yongjin Yang", "Sinjae Kang", "Juyong Lee", "Dongjun Lee", "Se-Young Yun", "Kimin Lee"], "title": "Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint, under review", "summary": "Training large language model (LLM) agents to acquire necessary skills and\nperform diverse tasks within an environment is gaining interest as a means to\nenable open-endedness. However, creating the training dataset for their skill\nacquisition faces several challenges. Manual trajectory collection requires\nsignificant human effort. Another approach, where LLMs directly propose tasks\nto learn, is often invalid, as the LLMs lack knowledge of which tasks are\nactually feasible. Moreover, the generated data may not provide a meaningful\nlearning signal, as agents often already perform well on the proposed tasks. To\naddress this, we propose a novel automatic skill discovery framework EXIF for\nLLM-powered agents, designed to improve the feasibility of generated target\nbehaviors while accounting for the agents' capabilities. Our method adopts an\nexploration-first strategy by employing an exploration agent (Alice) to train\nthe target agent (Bob) to learn essential skills in the environment.\nSpecifically, Alice first interacts with the environment to retrospectively\ngenerate a feasible, environment-grounded skill dataset, which is then used to\ntrain Bob. Crucially, we incorporate an iterative feedback loop, where Alice\nevaluates Bob's performance to identify areas for improvement. This feedback\nthen guides Alice's next round of exploration, forming a closed-loop data\ngeneration process. Experiments on Webshop and Crafter demonstrate EXIF's\nability to effectively discover meaningful skills and iteratively expand the\ncapabilities of the trained agent without any human intervention, achieving\nsubstantial performance improvements. Interestingly, we observe that setting\nAlice to the same model as Bob also notably improves performance, demonstrating\nEXIF's potential for building a self-evolving system.", "AI": {"tldr": "The EXIF framework enables automatic skill discovery for agents powered by large language models (LLMs), utilizing exploration agents to generate grounded datasets and an iterative feedback loop, eliminating human intervention and achieving significant performance improvements.", "motivation": "Training Large Language Model (LLM) agents in diverse environments to acquire skills without manual effort presents challenges such as infeasible task proposals and minimal learning signals from generated data.", "method": "The paper proposes EXIF, which uses an exploration agent (Alice) to interact with the environment and produce a skill dataset for another agent (Bob). It incorporates an iterative feedback loop where Alice evaluates Bob's performance and refines exploration for better data generation.", "result": "EXIF demonstrates effective skill discovery and capability expansion in two tested environments, Webshop and Crafter, without human intervention, achieving notable performance improvements.", "conclusion": "EXIF validates the potential for automating skill discovery and evolving LLM-powered agents' capabilities in open-ended environments, highlighting the system's capacity for self-improvement."}}
{"id": "2506.04667", "pdf": "https://arxiv.org/pdf/2506.04667", "abs": "https://arxiv.org/abs/2506.04667", "authors": ["Osayamen Jonathan Aimuyo", "Byungsoo Oh", "Rachee Singh"], "title": "FlashDMoE: Fast Distributed MoE in a Single Kernel", "categories": ["cs.DC", "cs.AR", "cs.LG"], "comment": "In submission. See code at https://github.com/osayamenja/Aristos", "summary": "The computational sparsity of Mixture-of-Experts (MoE) models enables\nsub-linear growth in compute cost as model size increases, offering a scalable\npath to training massive neural networks. However, existing implementations\nsuffer from \\emph{low GPU utilization}, \\emph{significant latency overhead},\nand a fundamental \\emph{inability to leverage task locality}, primarily due to\nCPU-managed scheduling, host-initiated communication, and frequent kernel\nlaunches. To overcome these limitations, we develop FlashDMoE, a fully\nGPU-resident MoE operator that fuses expert computation and inter-GPU\ncommunication into a \\emph{single persistent GPU kernel}. FlashDMoE enables\nfine-grained pipelining of dispatch, compute, and combine phases, eliminating\nlaunch overheads and reducing idle gaps. Its device-initiated communication\nprotocol introduces \\emph{payload-efficient} data transfers, significantly\nshrinking buffer sizes in sparsely activated MoE layers. When evaluated on a\nsingle 8-H100 GPU node with MoE models having up to 128 experts and 16K token\nsequences, FlashDMoE achieves up to \\textbf{6}x lower latency, \\textbf{5,7}x\nhigher throughput, \\textbf{4}x better weak scaling efficiency, and \\textbf{9}x\nhigher GPU utilization compared to state-of-the-art baselines, despite using\nFP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU\nkernel-hardware co-design is key to unlocking the performance ceiling of\nlarge-scale distributed ML workloads.", "AI": {"tldr": "FlashDMoE overcomes inefficiencies in Mixture-of-Experts models by developing a GPU-resident operator that boosts performance across latency, throughput, scaling efficiency, and utilization in distributed ML workloads.", "motivation": "Existing MoE implementations struggle with computational inefficiencies such as low GPU utilization, latency overhead, and lack of locality, limiting their scalability for larger neural networks.", "method": "FlashDMoE employs a GPU-resident operator, integrating expert computation and inter-GPU communication into a single GPU kernel. It uses fine-grained pipelining and device-initiated communication protocols for efficient data transfer.", "result": "On a setup with 8-H100 GPU nodes, FlashDMoE achieves up to 6x lower latency, 5.7x higher throughput, 4x better scaling efficiency, and 9x higher GPU utilization compared to top baselines, operating on FP32 precision.", "conclusion": "FlashDMoE highlights the importance of tailored GPU kernel-hardware co-design for overcoming bottlenecks in large-scale neural networks, paving the way for more efficient distributed ML workloads."}}
{"id": "2506.04698", "pdf": "https://arxiv.org/pdf/2506.04698", "abs": "https://arxiv.org/abs/2506.04698", "authors": ["Hugo Alcaraz-Herrera", "Michail-Antisthenis Tsompanas", "Igor Balaz", "Andrew Adamatzky"], "title": "NEAT and HyperNEAT based Design for Soft Actuator Controllers", "categories": ["cs.NE"], "comment": null, "summary": "Since soft robotics are composed of compliant materials, they perform better\nthan conventional rigid robotics in specific fields, such as medical\napplications. However, the field of soft robotics is fairly new, and the design\nprocess of their morphology and their controller strategies has not yet been\nthoroughly studied. Consequently, here, an automated design method for the\ncontroller of soft actuators based on Neuroevolution is proposed. Specifically,\nthe suggested techniques employ Neuroevolution of Augmenting Topologies (NEAT)\nand Hypercube-based NEAT (HyperNEAT) to generate the synchronization profile of\nthe components of a simulated soft actuator by employing Compositional Pattern\nProducing Networks (CPPNs). As a baseline methodology, a Standard Genetic\nAlgorithm (SGA) was used. Moreover, to test the robustness of the proposed\nmethodologies, both high- and low-performing morphologies of soft actuators\nwere utilized as testbeds. Moreover, the use of an affluent and a more limited\nset of activation functions for the Neuroevolution targets was tested\nthroughout the experiments. The results support the hypothesis that\nNeuroevolution based methodologies are more appropriate for designing\ncontrollers that align with both types of morphologies. In specific, NEAT\nperformed better for all different scenarios tested and produced more\nsimplistic networks that are easier to implement in real life applications.", "AI": {"tldr": "This paper explores Neuroevolution methods, specifically NEAT and HyperNEAT, for developing controllers of soft actuators, and finds NEAT to perform better in robustness and simplicity.", "motivation": "Soft robotics excel in applications requiring compliance, such as medical tasks, but the design of their controllers and morphology is understudied.", "method": "The authors proposed automated methods for designing soft actuator controllers using Neuroevolution techniques (NEAT and HyperNEAT), employing CPPNs and comparing them with a Standard Genetic Algorithm.", "result": "The experimental results showed that NEAT outperformed other methods across different scenarios, producing simpler and practical network designs.", "conclusion": "Neuroevolution methods, particularly NEAT, are more effective for designing controllers for soft robotics, offering robust and implementable solutions."}}
{"id": "2506.04280", "pdf": "https://arxiv.org/pdf/2506.04280", "abs": "https://arxiv.org/abs/2506.04280", "authors": ["Ziming Cheng", "Binrui Xu", "Lisheng Gong", "Zuhe Song", "Tianshuo Zhou", "Shiqi Zhong", "Siyu Ren", "Mingxiang Chen", "Xiangchao Meng", "Yuxin Zhang", "Yanlin Li", "Lei Ren", "Wei Chen", "Zhiyuan Huang", "Mingjie Zhan", "Xiaojie Wang", "Fangxiang Feng"], "title": "Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark", "categories": ["cs.CV", "cs.AI", "68T50", "I.2.7"], "comment": "18 pages", "summary": "With enhanced capabilities and widespread applications, Multimodal Large\nLanguage Models (MLLMs) are increasingly required to process and reason over\nmultiple images simultaneously. However, existing MLLM benchmarks focus either\non single-image visual reasoning or on multi-image understanding tasks with\nonly final-answer evaluation, leaving the reasoning capabilities of MLLMs over\nmulti-image inputs largely underexplored. To address this gap, we introduce the\n$\\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first\nbenchmark designed to evaluate structured visual reasoning across multiple\nimages. MMRB comprises $\\textbf{92 sub-tasks}$ covering spatial, temporal, and\nsemantic reasoning, with multi-solution, CoT-style annotations generated by\nGPT-4o and refined by human experts. A derivative subset is designed to\nevaluate multimodal reward models in multi-image scenarios. To support fast and\nscalable evaluation, we propose a sentence-level matching framework using\nopen-source LLMs. Extensive baseline experiments on $\\textbf{40 MLLMs}$,\nincluding 9 reasoning-specific models and 8 reward models, demonstrate that\nopen-source MLLMs still lag significantly behind commercial MLLMs in\nmulti-image reasoning tasks. Furthermore, current multimodal reward models are\nnearly incapable of handling multi-image reward ranking tasks.", "AI": {"tldr": "The paper introduces the Multimodal Multi-image Reasoning Benchmark (MMRB), the first benchmark to evaluate structured reasoning over multi-image inputs across 92 sub-tasks, revealing significant performance gaps in open-source MLLMs and reward models.", "motivation": "Existing benchmarks focus on either single-image reasoning or give only final-answer evaluation for multi-image tasks, making the reasoning capabilities of MLLMs over multi-image inputs underexplored.", "method": "The authors created MMRB, a benchmark encompassing 92 sub-tasks for spatial, temporal, and semantic multi-image reasoning, employing multi-solution Chain-of-Thought (CoT) annotations generated by GPT-4o and refined by humans. They also proposed a sentence-level matching framework using open-source LLMs for scalable evaluations.", "result": "Baseline experiments on 40 MLLMs indicate significant underperformance of open-source MLLMs compared to commercial ones, and revealed a nearly complete inability of current multimodal reward models to tackle multi-image reward ranking tasks.", "conclusion": "MMRB highlights the limitations of current MLLMs and reward models in multi-image reasoning and provides a structured benchmark to facilitate further improvements in the field."}}
{"id": "2506.04373", "pdf": "https://arxiv.org/pdf/2506.04373", "abs": "https://arxiv.org/abs/2506.04373", "authors": ["Matthieu Tehenan", "Vikram Natarajan", "Jonathan Michala", "Milton Lin", "Juri Opitz"], "title": "Mechanistic Decomposition of Sentence Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sentence embeddings are central to modern NLP and AI systems, yet little is\nknown about their internal structure. While we can compare these embeddings\nusing measures such as cosine similarity, the contributing features are not\nhuman-interpretable, and the content of an embedding seems untraceable, as it\nis masked by complex neural transformations and a final pooling operation that\ncombines individual token embeddings. To alleviate this issue, we propose a new\nmethod to mechanistically decompose sentence embeddings into interpretable\ncomponents, by using dictionary learning on token-level representations. We\nanalyze how pooling compresses these features into sentence representations,\nand assess the latent features that reside in a sentence embedding. This\nbridges token-level mechanistic interpretability with sentence-level analysis,\nmaking for more transparent and controllable representations. In our studies,\nwe obtain several interesting insights into the inner workings of sentence\nembedding spaces, for instance, that many semantic and syntactic aspects are\nlinearly encoded in the embeddings.", "AI": {"tldr": "The paper introduces a method for decomposing sentence embeddings to make their internal structure more interpretable using dictionary learning on token-level representations.", "motivation": "The authors aim to address the lack of interpretability in the internal structure of sentence embeddings, which are widely used in modern NLP and AI systems.", "method": "The proposed method involves using dictionary learning on token-level representations to mechanistically decompose sentence embeddings into interpretable components.", "result": "The study reveals that many semantic and syntactic aspects are linearly encoded in sentence embeddings, enhancing their interpretability.", "conclusion": "The work bridges the gap between token-level and sentence-level analysis, enabling more transparent and controllable representations in NLP systems."}}
{"id": "2506.04464", "pdf": "https://arxiv.org/pdf/2506.04464", "abs": "https://arxiv.org/abs/2506.04464", "authors": ["Oussama Ben Sghaier", "Rosalia Tufano", "Gabriele Bavota", "Houari Sahraoui"], "title": "Leveraging Reward Models for Guiding Code Review Comment Generation", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Code review is a crucial component of modern software development, involving\nthe evaluation of code quality, providing feedback on potential issues, and\nrefining the code to address identified problems. Despite these benefits, code\nreview can be rather time consuming, and influenced by subjectivity and human\nfactors. For these reasons, techniques to (partially) automate the code review\nprocess have been proposed in the literature. Among those, the ones exploiting\ndeep learning (DL) are able to tackle the generative aspect of code review, by\ncommenting on a given code as a human reviewer would do (i.e., comment\ngeneration task) or by automatically implementing code changes required to\naddress a reviewer's comment (i.e., code refinement task). In this paper, we\nintroduce CoRAL, a deep learning framework automating review comment generation\nby exploiting reinforcement learning with a reward mechanism considering both\nthe semantics of the generated comments as well as their usefulness as input\nfor other models automating the code refinement task. The core idea is that if\nthe DL model generates comments that are semantically similar to the expected\nones or can be successfully implemented by a second model specialized in code\nrefinement, these comments are likely to be meaningful and useful, thus\ndeserving a high reward in the reinforcement learning framework. We present\nboth quantitative and qualitative comparisons between the comments generated by\nCoRAL and those produced by the latest baseline techniques, highlighting the\neffectiveness and superiority of our approach.", "AI": {"tldr": "This paper introduces CoRAL, a deep learning framework for automating code review comment generation using reinforcement learning, aiming to enhance comment quality and usefulness.", "motivation": "The inefficiencies and subjectivity in traditional code review processes, combined with the time-consuming nature, motivate the need for automated techniques that can mimic human reviewers effectively.", "method": "CoRAL leverages a reinforcement learning framework with a reward mechanism focused on generating semantically meaningful and practically useful comments for code review tasks.", "result": "Quantitative and qualitative evaluations demonstrate that CoRAL-produced comments outperform previous baseline methods in terms of semantic similarity and practical implementation quality.", "conclusion": "CoRAL proves to be a superior tool for enhancing the efficiency and effectiveness of code review by automating meaningful comment generation and supporting follow-up code refinement tasks."}}
{"id": "2506.04362", "pdf": "https://arxiv.org/pdf/2506.04362", "abs": "https://arxiv.org/abs/2506.04362", "authors": ["Zihao Dong", "Alan Papalia", "Leonard Jung", "Alenna Spiro", "Philip R. Osteen", "Christa S. Robison", "Michael Everett"], "title": "Learning Smooth State-Dependent Traversability from Dense Point Clouds", "categories": ["cs.RO", "cs.CV"], "comment": "16 pages, 13 figures", "summary": "A key open challenge in off-road autonomy is that the traversability of\nterrain often depends on the vehicle's state. In particular, some obstacles are\nonly traversable from some orientations. However, learning this interaction by\nencoding the angle of approach as a model input demands a large and diverse\ntraining dataset and is computationally inefficient during planning due to\nrepeated model inference. To address these challenges, we present SPARTA, a\nmethod for estimating approach angle conditioned traversability from point\nclouds. Specifically, we impose geometric structure into our network by\noutputting a smooth analytical function over the 1-Sphere that predicts risk\ndistribution for any angle of approach with minimal overhead and can be reused\nfor subsequent queries. The function is composed of Fourier basis functions,\nwhich has important advantages for generalization due to their periodic nature\nand smoothness. We demonstrate SPARTA both in a high-fidelity simulation\nplatform, where our model achieves a 91\\% success rate crossing a 40m boulder\nfield (compared to 73\\% for the baseline), and on hardware, illustrating the\ngeneralization ability of the model to real-world settings.", "AI": {"tldr": "SPARTA is a method proposed to estimate traversability conditioned on approach angles using point clouds, addressing the challenges of state-dependent terrain traversability in off-road autonomy.", "motivation": "To address the dependency of terrain traversability on vehicle states, specifically approach angles, which complicates learning and computational efficiency.", "method": "SPARTA uses Fourier basis functions to model smooth analytical traversability functions over approach angles from point clouds, imposing geometric structure for efficient planning and generalization.", "result": "The method achieves a 91% success rate crossing a challenging boulder field in simulation and demonstrates effective generalization on hardware in real-world settings.", "conclusion": "SPARTA effectively balances computational efficiency and learning for state-conditioned terrain traversability, showing strong performance and generalization potential."}}
{"id": "2506.04243", "pdf": "https://arxiv.org/pdf/2506.04243", "abs": "https://arxiv.org/abs/2506.04243", "authors": ["Warayut Dokduea", "Weerachart Tangchirapat", "Sompote Youwai"], "title": "Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a novel Triple Attention Transformer Architecture for\npredicting time-dependent concrete creep, addressing fundamental limitations in\ncurrent approaches that treat time as merely an input parameter rather than\nmodeling the sequential nature of deformation development. By transforming\nconcrete creep prediction into an autoregressive sequence modeling task similar\nto language processing, our architecture leverages the transformer's\nself-attention mechanisms to capture long-range dependencies in historical\ncreep patterns. The model implements a triple-stream attention framework\nincorporating temporal attention for sequential progression, feature attention\nfor material property interactions, and batch attention for inter-sample\nrelationships. Evaluated on experimental datasets with standardized daily\nmeasurements spanning 160 days, the architecture achieves exceptional\nperformance with mean absolute percentage error of 1.63% and R2 values of 0.999\nacross all datasets, substantially outperforming traditional empirical models\nand existing machine learning approaches. Ablation studies confirm the critical\nrole of attention mechanisms, with attention pooling contributing most\nsignificantly to model performance. SHAP analysis reveals Young's modulus as\nthe primary predictive feature, followed by density and compressive strength,\nproviding interpretability essential for engineering applications. A deployed\nweb-based interface facilitates practical implementation, enabling real-time\npredictions using standard laboratory parameters. This work establishes the\nviability of applying transformer architectures to materials science problems,\ndemonstrating the potential for data-driven approaches to revolutionize\nstructural behavior prediction and engineering design practices.", "AI": {"tldr": "This paper introduces a Triple Attention Transformer Architecture for autoregressively predicting time-dependent concrete creep using time as a sequential parameter, achieving high accuracy and outperforming existing models.", "motivation": "Current approaches to concrete creep prediction inadequately model time as a mere input parameter instead of considering the sequential nature of deformation development.", "method": "The paper uses a Triple Attention Transformer framework incorporating temporal, feature, and batch attention for autoregressive sequence modeling of historical creep patterns.", "result": "The presented model achieves a mean absolute percentage error of 1.63% and R2 values of 0.999 on experimental datasets spanning 160 days, surpassing traditional empirical models and other machine learning methods.", "conclusion": "The research establishes transformers as effective tools for materials science, enabling accurate predictions and interpretability, and provides practical implementation via a web-based interface."}}
{"id": "2506.04875", "pdf": "https://arxiv.org/pdf/2506.04875", "abs": "https://arxiv.org/abs/2506.04875", "authors": ["Alexander Gorsky"], "title": "Consciousness via MIPT?", "categories": ["q-bio.NC"], "comment": "24 pages", "summary": "The measurement-induced phase transition (MIPT) is a recently formulated\nphenomenon in out-of-equilibrium systems. The competition between unitary\nevolutions and measurement-induced non-unitaries leads to the transition\nbetween the entangled and disentangled phases at some critical measurement\nrate. We conjecture that self-organized MIPT plays a key role in the generative\nmodel of cognitive networks and the formation of the state of consciousness in\nthe \"newborn-adult\" transition. To this aim, we formulate the probe-target\npicture for the brain and suggest that MIPT interpreted as learnability\ntransition takes place in the mental part of the target where the sites in the\ncognitive networks of semantic memory are concepts. Comparison with the\nsynchronization phase transitions in the probe is made.", "AI": {"tldr": "This paper links measurement-induced phase transitions (MIPT) in physical systems to cognitive processes, suggesting a role in consciousness emergence during developmental transitions.", "motivation": "The study aims to explore the parallels between physical phenomena like MIPT and cognitive systems, hypothesizing their relevance to the emergence of consciousness in developmental stages.", "method": "The authors propose a probe-target model for the brain, interpreting sites in semantic memory as concepts within cognitive networks affected by MIPT dynamics.", "result": "The paper theorizes that cognitive networks experience MIPT as a learnability transition, contributing to the organization of consciousness, validated by analogy to physical systems.", "conclusion": "Self-organized MIPT and its cognitive counterpart are hypothesized as foundational in consciousness and cognitive development, warranting further exploration."}}
{"id": "2506.04813", "pdf": "https://arxiv.org/pdf/2506.04813", "abs": "https://arxiv.org/abs/2506.04813", "authors": ["S\u00e9bastien Da Veiga"], "title": "Distributional encoding for Gaussian process regression with qualitative inputs", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Gaussian Process (GP) regression is a popular and sample-efficient approach\nfor many engineering applications, where observations are expensive to acquire,\nand is also a central ingredient of Bayesian optimization (BO), a highly\nprevailing method for the optimization of black-box functions. However, when\nall or some input variables are categorical, building a predictive and\ncomputationally efficient GP remains challenging. Starting from the naive\ntarget encoding idea, where the original categorical values are replaced with\nthe mean of the target variable for that category, we propose a generalization\nbased on distributional encoding (DE) which makes use of all samples of the\ntarget variable for a category. To handle this type of encoding inside the GP,\nwe build upon recent results on characteristic kernels for probability\ndistributions, based on the maximum mean discrepancy and the Wasserstein\ndistance. We also discuss several extensions for classification, multi-task\nlearning and incorporation or auxiliary information. Our approach is validated\nempirically, and we demonstrate state-of-the-art predictive performance on a\nvariety of synthetic and real-world datasets. DE is naturally complementary to\nrecent advances in BO over discrete and mixed-spaces.", "AI": {"tldr": "The paper addresses challenges in handling categorical variables in Gaussian Process (GP) regression by introducing a distributional encoding (DE) approach, achieving state-of-the-art performance on various datasets.", "motivation": "The motivation is to improve GP regression and Bayesian optimization (BO) methods when categorical input variables are involved, as existing approaches struggle with predictive and computational efficiency in this context.", "method": "The paper generalizes naive target encoding to distributional encoding (DE), which leverages all target variable samples for a category. It incorporates this encoding within GP using characteristic kernels based on maximum mean discrepancy and Wasserstein distance.", "result": "The proposed method achieved state-of-the-art predictive performance on synthetic and real-world datasets and extended applicability to classification, multi-task learning, and the inclusion of auxiliary information.", "conclusion": "The distributional encoding (DE) approach is an effective, generalizable solution for handling categorical variables in GP, complementing recent advances in Bayesian optimization for discrete and mixed-spaces."}}
{"id": "2506.05007", "pdf": "https://arxiv.org/pdf/2506.05007", "abs": "https://arxiv.org/abs/2506.05007", "authors": ["Rui Zhang", "Yuanbo Wen", "Shuyao Cheng", "Di Huang", "Shaohui Peng", "Jiaming Guo", "Pengwei Jin", "Jiacheng Zhao", "Tianrui Ma", "Yaoyu Zhu", "Yifan Hao", "Yongwei Zhao", "Shengwen Liang", "Ying Wang", "Xing Hu", "Zidong Du", "Huimin Cui", "Ling Li", "Qi Guo", "Yunji Chen"], "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.", "AI": {"tldr": "The paper introduces QiMeng, a fully automated system for processor chip design using domain-specific AI models.", "motivation": "Rapid advancements in technology have revealed limitations in conventional chip design methods, such as fabrication constraints, resource demands, and ecosystem diversity.", "method": "QiMeng is structured into three hierarchical layers: a domain-specific Large Processor Chip Model (LPCM), Hardware and Software Design Agents, and top-layer applications.", "result": "Several components of QiMeng are completed and successfully applied in chip design tasks, demonstrating efficiency and advantages.", "conclusion": "The proposed QiMeng system provides a promising solution for fully automated processor chip design, though further integration and iterative development are needed."}}
{"id": "2506.04374", "pdf": "https://arxiv.org/pdf/2506.04374", "abs": "https://arxiv.org/abs/2506.04374", "authors": ["Jack David Carson", "Amir Reisizadeh"], "title": "A Statistical Physics of Language Model Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Transformer LMs show emergent reasoning that resists mechanistic\nunderstanding. We offer a statistical physics framework for continuous-time\nchain-of-thought reasoning dynamics. We model sentence-level hidden state\ntrajectories as a stochastic dynamical system on a lower-dimensional manifold.\nThis drift-diffusion system uses latent regime switching to capture diverse\nreasoning phases, including misaligned states or failures. Empirical\ntrajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing\nvariance capture and feasibility) explains ~50% variance. We find four latent\nreasoning regimes. An SLDS model is formulated and validated to capture these\nfeatures. The framework enables low-cost reasoning simulation, offering tools\nto study and predict critical transitions like misaligned states or other LM\nfailures.", "AI": {"tldr": "The paper introduces a statistical physics approach to model transformer language models' reasoning dynamics using stochastic systems on a lower-dimensional manifold.", "motivation": "Understanding the emergent reasoning behavior in transformer language models, which lacks a clear mechanistic explanation.", "method": "The reasoning dynamics are modeled as drift-diffusion systems using latent regime switching and a stochastic linear dynamical system (SLDS). Empirical data from 8 models and 7 benchmarks validate this approach.", "result": "The study identifies four latent reasoning regimes and shows that a rank-40 projection explains approximately 50% of the variance, enabling analysis of transitions like LM failures.", "conclusion": "The proposed framework provides tools to simulate reasoning transitions and study critical states in language models at low computational cost."}}
{"id": "2506.04833", "pdf": "https://arxiv.org/pdf/2506.04833", "abs": "https://arxiv.org/abs/2506.04833", "authors": ["Jincheng Guan", "Jun Zhang"], "title": "Distributed system perspective on Backscatter systems", "categories": ["cs.DC"], "comment": null, "summary": "Backscatter system is a system based on backscatter communication technology,\nwhich is a low cost, low power consumption and easy to deploy communication\ntechnology. At present, the backscatter technology is mainly applied to RFID\ntags and the Internet of Things and other fields. With the rapid development of\nthe Internet of Things, the application of backscatter systems is increasing.\nMoreover, the backscatter system is essentially a distributed system, but\nexisting research rarely conducts studies and analyses from a distributed\nperspective. This paper conducts a study on the backscattering system from the\nperspective of distributed systems, comprehensively reviewing the basic\nprinciples of the backscattering system, and analyzing the distributed system\narchitectures of different backscattering systems. Then, it introduces the\napplication scenarios, research status and challenges of the backscattering\nsystem, and finally discusses the future research directions of the\nbackscattering system, hoping to provide references for future research.", "AI": {"tldr": "This paper studies the distributed perspective of backscatter communication systems, emphasizing their principles, architectures, applications, challenges, and future directions.", "motivation": "To explore backscatter communication systems as distributed systems, which are currently understudied from this perspective.", "method": "Comprehensive review and analysis of backscatter system principles, architectures, applications, research status, and challenges.", "result": "Provides a detailed understanding of distributed system architectures and application scenarios for backscatter systems.", "conclusion": "The study offers insights and references on the status, challenges, and future directions of backscatter systems for researchers."}}
{"id": "2506.04236", "pdf": "https://arxiv.org/pdf/2506.04236", "abs": "https://arxiv.org/abs/2506.04236", "authors": ["Botao Amber Hu", "Helena Rong"], "title": "Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs", "categories": ["cs.MA", "cs.AI", "cs.HC", "cs.NE"], "comment": "Submitted to ALIFE 2025", "summary": "In Artificial Life (ALife) research, replicating Open-Ended Evolution\n(OEE)-the continuous emergence of novelty observed in biological life-has\ntraditionally been pursued within isolated closed system simulations, such as\nTierra and Avida, which have typically plateaued after an initial burst of\nnovelty, failing to achieve sustained OEE. Scholars suggest that OEE requires\nan \"open\" system that continually exchanges information or energy with its\nenvironment. A recent technological innovation in decentralized physical\ninfrastructure networks (DePIN) providing permissionless computational\nsubstrates enables deploying large language model (LLM)-based AI agents on\nblockchains integrated with Trusted Execution Environments (TEEs). This enables\non-chain agents to operate autonomously \"in the wild,\" achieving\nself-sovereignty without human oversight. These agents can control their own\nsocial media accounts and cryptocurrency wallets, allowing them to interact\ndirectly with blockchain-based financial networks and broader human social\nmedia. Building on this new paradigm of on-chain agents, Spore.fun is a recent\nreal-world AI evolution experiment that enables autonomous breeding and\nevolution of new on-chain agents. This paper presents a detailed case study of\nSpore.fun, examining agent behaviors and their evolutionary trajectories\nthrough digital ethology. We aim to spark discussion about whether \"open\" ALife\nsystems \"in-the-wild,\" based on permissionless computational substrates and\ndriven by economic incentives to interact with their environment, could finally\nachieve the long-sought goal of OEE.", "AI": {"tldr": "The paper explores using autonomous on-chain AI agents within an open system to achieve continuous Open-Ended Evolution (OEE), using Spore.fun as a case study.", "motivation": "Traditional ALife research has struggled to achieve sustained Open-Ended Evolution (OEE), as isolated closed systems plateau in novelty generation. The paper aims to address this limitation by investigating novel \"open\" systems enabled by decentralized infrastructure.", "method": "The study examines Spore.fun, a platform deploying autonomous AI agents that evolve in the wild. These agents leverage blockchain-enabled permissionless computation and Trusted Execution Environments (TEEs), interacting with financial and social environments dynamically.", "result": "The case study documents the behaviors and evolutionary pathways of on-chain agents in the Spore.fun ecosystem, showing the potential for novel agent interactions and adaptations.", "conclusion": "The paper highlights the potential for \"open\" ALife systems driven by economic and environmental interactions to achieve sustained OEE. It invites further exploration of digital agents evolving in real-world conditions."}}
{"id": "2506.04351", "pdf": "https://arxiv.org/pdf/2506.04351", "abs": "https://arxiv.org/abs/2506.04351", "authors": ["Maksym Ivashechkin", "Oscar Mendez", "Richard Bowden"], "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D human generation is an important problem with a wide range of applications\nin computer vision and graphics. Despite recent progress in generative AI such\nas diffusion models or rendering methods like Neural Radiance Fields or\nGaussian Splatting, controlling the generation of accurate 3D humans from text\nprompts remains an open challenge. Current methods struggle with fine detail,\naccurate rendering of hands and faces, human realism, and controlability over\nappearance. The lack of diversity, realism, and annotation in human image data\nalso remains a challenge, hindering the development of a foundational 3D human\nmodel. We present a weakly supervised pipeline that tries to address these\nchallenges. In the first step, we generate a photorealistic human image dataset\nwith controllable attributes such as appearance, race, gender, etc using a\nstate-of-the-art image diffusion model. Next, we propose an efficient mapping\napproach from image features to 3D point clouds using a transformer-based\narchitecture. Finally, we close the loop by training a point-cloud diffusion\nmodel that is conditioned on the same text prompts used to generate the\noriginal samples. We demonstrate orders-of-magnitude speed-ups in 3D human\ngeneration compared to the state-of-the-art approaches, along with\nsignificantly improved text-prompt alignment, realism, and rendering quality.\nWe will make the code and dataset available.", "AI": {"tldr": "The paper proposes a new weakly supervised pipeline for generating 3D human models from text prompts, addressing issues like realism and controlability.", "motivation": "To overcome challenges in generating realistic and controllable 3D humans from text prompts using generative AI techniques.", "method": "The approach involves generating photorealistic human image datasets, mapping image features to 3D point clouds using transformers, and training a point-cloud diffusion model conditioned on text prompts.", "result": "The method achieves significant improvements in 3D human generation speed, text-prompt alignment, and rendering quality compared to the state-of-the-art.", "conclusion": "The proposed system effectively addresses key challenges in 3D human generation, showing potential for broad application while providing open access to the code and dataset."}}
{"id": "2506.04381", "pdf": "https://arxiv.org/pdf/2506.04381", "abs": "https://arxiv.org/abs/2506.04381", "authors": ["Neeraj Agrawal", "Saurabh Kumar", "Priyanka Bhatt", "Tanishka Agarwal"], "title": "Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy", "categories": ["cs.CL", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2203.03825 by other authors", "summary": "Hierarchical Text Classification (HTC) has recently gained traction given the\nability to handle complex label hierarchy. This has found applications in\ndomains like E- commerce, customer care and medicine industry among other\nreal-world applications. Existing HTC models either encode label hierarchy\nseparately and mix it with text encoding or guide the label hierarchy structure\nin the text encoder. Both approaches capture different characteristics of label\nhierarchy and are complementary to each other. In this paper, we propose a\nHierarchical Text Classification using Contrastive Learning Informed Path\nguided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation\nand text informed path guided hierarchy representation using contrastive\nlearning. During the training of HTC-CLIP, we learn two different sets of class\nprobabilities distributions and during inference, we use the pooled output of\nboth probabilities for each class to get the best of both representations. Our\nresults show that the two previous approaches can be effectively combined into\none architecture to achieve improved performance. Tests on two public benchmark\ndatasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP\nover the existing state-of-the-art models.", "AI": {"tldr": "The paper introduces HTC-CLIP, a novel model that effectively combines two existing approaches for hierarchical text classification using contrastive learning, achieving improved performance on benchmark datasets.", "motivation": "Hierarchical text classification models are essential for applications requiring understanding of complex label hierarchies, such as e-commerce and medical domains. However, existing models capture different complementary facets of label hierarchy separately, necessitating a unified approach.", "method": "The proposed HTC-CLIP model uses contrastive learning to create hierarchy-aware text and path-guided hierarchy representations. It trains two sets of class probability distributions and combines their pooled outputs during inference to leverage the strengths of both representations.", "result": "The HTC-CLIP model demonstrated improved performance, with a 0.99-2.37% increase in Macro F1 scores over state-of-the-art models when tested on two public benchmark datasets.", "conclusion": "HTC-CLIP successfully merges complementary hierarchy-aware approaches, achieving superior accuracy and reinforcing the value of combining text and path-guided representations in hierarchical text classification tasks."}}
{"id": "2506.04509", "pdf": "https://arxiv.org/pdf/2506.04509", "abs": "https://arxiv.org/abs/2506.04509", "authors": ["Kishan Kumar Ganguly", "Tim Menzies"], "title": "BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets", "categories": ["cs.SE"], "comment": null, "summary": "Traditional multi-objective optimization in software engineering (SE) can be\nslow and complex. This paper introduces the BINGO effect: a novel phenomenon\nwhere SE data surprisingly collapses into a tiny fraction of possible solution\n\"buckets\" (e.g., only 100 used from 4,096 expected).\n  We show the BINGO effect's prevalence across 39 optimization in SE problems.\nExploiting this, we optimize 10,000 times faster than state-of-the-art methods,\nwith comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate\nthat simple stochastic selection can match complex optimizers like DEHB. This\nwork explains why simple methods succeed in SE-real data occupies a small\ncorner of possibilities-and guides when to apply them, challenging the need for\nCPU-heavy optimization.\n  Our data and code are public at GitHub (see anon-artifacts/bingo).", "AI": {"tldr": "This paper introduces the BINGO effect, showing that software engineering data collapses into a small number of solution buckets, enabling simpler and faster optimization methods.", "motivation": "Optimization in software engineering is often slow and complex, necessitating exploration of faster alternatives.", "method": "The study identifies the BINGO effect by analyzing 39 SE problems, resulting in development of LITE and LINE algorithms for simple stochastic selection.", "result": "With the BINGO effect, optimization becomes 10,000 times faster while maintaining comparable effectiveness to state-of-the-art methods.", "conclusion": "The research challenges the necessity of CPU-heavy optimization, showing that simple methods can perform effectively due to the concentrated nature of SE data."}}
{"id": "2506.04484", "pdf": "https://arxiv.org/pdf/2506.04484", "abs": "https://arxiv.org/abs/2506.04484", "authors": ["William Ward", "Sarah Etter", "Tyler Ingebrand", "Christian Ellis", "Adam J. Thorpe", "Ufuk Topcu"], "title": "Online Adaptation of Terrain-Aware Dynamics for Planning in Unstructured Environments", "categories": ["cs.RO"], "comment": "Accepted to RSS-ROAR 2025", "summary": "Autonomous mobile robots operating in remote, unstructured environments must\nadapt to new, unpredictable terrains that can change rapidly during operation.\nIn such scenarios, a critical challenge becomes estimating the robot's dynamics\non changing terrain in order to enable reliable, accurate navigation and\nplanning. We present a novel online adaptation approach for terrain-aware\ndynamics modeling and planning using function encoders. Our approach\nefficiently adapts to new terrains at runtime using limited online data without\nretraining or fine-tuning. By learning a set of neural network basis functions\nthat span the robot dynamics on diverse terrains, we enable rapid online\nadaptation to new, unseen terrains and environments as a simple least-squares\ncalculation. We demonstrate our approach for terrain adaptation in a\nUnity-based robotics simulator and show that the downstream controller has\nbetter empirical performance due to higher accuracy of the learned model. This\nleads to fewer collisions with obstacles while navigating in cluttered\nenvironments as compared to a neural ODE baseline.", "AI": {"tldr": "The paper proposes a method for mobile robots to adapt their dynamics models to unpredictable terrains rapidly using limited online data, without retraining.", "motivation": "Autonomous robots need the ability to navigate rapidly changing terrains in unpredictable environments reliably.", "method": "The authors use neural network basis functions to span robot dynamics across terrains, enabling runtime adaptation through simple calculations.", "result": "In simulations, the approach improves navigation accuracy, reduces collisions, and outperforms a neural ODE baseline.", "conclusion": "This technique enhances rapid terrain adaptation for autonomous robots, leading to better motion planning and obstacle avoidance in unstructured environments."}}
{"id": "2506.04250", "pdf": "https://arxiv.org/pdf/2506.04250", "abs": "https://arxiv.org/abs/2506.04250", "authors": ["Shaona Ghosh", "Amrita Bhattacharjee", "Yftah Ziser", "Christopher Parisien"], "title": "SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2410.01174", "summary": "Fine-tuning large language models (LLMs) to adapt to evolving safety policies\nis costly and impractical. Mechanistic interpretability enables inference-time\ncontrol through latent activation steering, yet its potential for precise,\ncustomizable safety adjustments remains largely untapped. This paper\ninvestigates an approach called SafeSteer for guiding the outputs of LLMs by:\n(i) leveraging category-specific steering vectors for more precise control,\n(ii) employing a simple, gradient-free unsupervised method to enhance safety\nsteering while preserving text quality, topic relevance, and without explicit\nrefusal, and (iii) accomplishing this without a hard requirement of contrastive\npairwise safe data. We also highlight that our method, being simple and\neffective, aligns with recent studies suggesting that simple techniques often\noutperform more complex ones in activation steering. We showcase the\neffectiveness of our approach across various LLMs, datasets, and risk\ncategories, demonstrating its ability to provide precise control, prevent\nblanket refusals, and guide models toward generating safe content while\nmaintaining topic relevance.", "AI": {"tldr": "The paper introduces SafeSteer, a technique for safety steering in LLMs using category-specific steering vectors and a gradient-free unsupervised method, showcasing its efficacy without hard pairwise safe data.", "motivation": "Fine-tuning LLMs for updated safety policies is expensive and impractical, necessitating a more efficient safety control approach.", "method": "The paper employs category-specific steering vectors combined with a simple, unsupervised gradient-free method to adjust LLM outputs for safety, avoiding the need for explicit safe data pairs.", "result": "SafeSteer demonstrates precise safety control across multiple LLMs, datasets, and risk categories, preserving text quality and topic relevance without blanket refusals.", "conclusion": "The SafeSteer method is a simple, effective, and customizable approach for safety adjustments in LLMs, aligning with modern preferences for simplicity in latent activation steering techniques."}}
{"id": "2506.04906", "pdf": "https://arxiv.org/pdf/2506.04906", "abs": "https://arxiv.org/abs/2506.04906", "authors": ["Lisa Schmors", "Dominic Gonschorek", "Jan Niklas B\u00f6hm", "Yongrong Qiu", "Na Zhou", "Dmitry Kobak", "Andreas Tolias", "Fabian Sinz", "Jacob Reimer", "Katrin Franke", "Sebastian Damrich", "Philipp Berens"], "title": "TRACE: Contrastive learning for multi-trial time-series data in neuroscience", "categories": ["q-bio.NC"], "comment": null, "summary": "Modern neural recording techniques such as two-photon imaging allow to\nacquire vast time-series datasets with responses of hundreds or thousands of\nneurons. Contrastive learning is a powerful self-supervised framework for\nlearning representations of complex datasets. Existing applications for neural\ntime series rely on generic data augmentations and do not exploit the\nmulti-trial data structure inherent in many neural datasets. Here we present\nTRACE, a new contrastive learning framework that averages across different\nsubsets of trials to generate positive pairs. TRACE allows to directly learn a\ntwo-dimensional embedding, combining ideas from contrastive learning and\nneighbor embeddings. We show that TRACE outperforms other methods, resolving\nfine response differences in simulated data. Further, using in vivo recordings,\nwe show that the representations learned by TRACE capture both biologically\nrelevant continuous variation, cell-type-related cluster structure, and can\nassist data quality control.", "AI": {"tldr": "TRACE is a novel contrastive learning framework tailored for neural time-series data. It leverages multi-trial data to effectively learn embeddings for analyzing complex neural responses.", "motivation": "To address the challenge of analyzing vast and complex neural time-series datasets acquired using modern neural recording techniques, which are often underutilized due to reliance on generic data augmentations.", "method": "TRACE uses multi-trial averaging to produce positive pairs for contrastive learning, integrating contrastive learning with neighbor embedding, enabling two-dimensional embeddings to capture fine response variances.", "result": "TRACE outperforms existing methods in simulations by resolving fine differences in responses and provides biologically meaningful embeddings in in vivo studies, aiding cell-type clustering and quality control.", "conclusion": "TRACE successfully leverages the unique structure of neural datasets for more effective representations, demonstrating its utility in both synthetic and real-world neural data analyses."}}
{"id": "2506.04945", "pdf": "https://arxiv.org/pdf/2506.04945", "abs": "https://arxiv.org/abs/2506.04945", "authors": ["Armin Keki\u0107", "Sergio Hernan Garrido Mejia", "Bernhard Sch\u00f6lkopf"], "title": "Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models", "categories": ["stat.ML", "cs.LG"], "comment": "To be published at the International Conference on Machine Learning\n  (ICML) 2025", "summary": "Estimating causal effects of joint interventions on multiple variables is\ncrucial in many domains, but obtaining data from such simultaneous\ninterventions can be challenging. Our study explores how to learn joint\ninterventional effects using only observational data and single-variable\ninterventions. We present an identifiability result for this problem, showing\nthat for a class of nonlinear additive outcome mechanisms, joint effects can be\ninferred without access to joint interventional data. We propose a practical\nestimator that decomposes the causal effect into confounded and unconfounded\ncontributions for each intervention variable. Experiments on synthetic data\ndemonstrate that our method achieves performance comparable to models trained\ndirectly on joint interventional data, outperforming a purely observational\nestimator.", "AI": {"tldr": "This paper studies learning joint interventional effects from observational and single-variable intervention data, proposing a decomposition-based estimator that achieves promising results.", "motivation": "Estimating causal effects of multiple simultaneous interventions is important, but acquiring the necessary joint interventional data is often difficult or impractical.", "method": "The authors establish an identifiability result for nonlinear additive outcome mechanisms and propose a practical estimator that separates causal effects into confounded and unconfounded contributions for each variable.", "result": "The proposed method performs comparably to models trained on joint interventional data and outperforms a purely observational estimator in synthetic data experiments.", "conclusion": "Joint interventional effects can be effectively inferred without direct joint interventional data, expanding the capability of causal estimation in challenging data scenarios."}}
{"id": "2506.04301", "pdf": "https://arxiv.org/pdf/2506.04301", "abs": "https://arxiv.org/abs/2506.04301", "authors": ["Jiin Kim", "Byeongjun Shin", "Jinha Chung", "Minsoo Rhu"], "title": "The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Large-language-model (LLM)-based AI agents have recently showcased impressive\nversatility by employing dynamic reasoning, an adaptive, multi-step process\nthat coordinates with external tools. This shift from static, single-turn\ninference to agentic, multi-turn workflows broadens task generalization and\nbehavioral flexibility, but it also introduces serious concerns about\nsystem-level cost, efficiency, and sustainability. This paper presents the\nfirst comprehensive system-level analysis of AI agents, quantifying their\nresource usage, latency behavior, energy consumption, and datacenter-wide power\nconsumption demands across diverse agent designs and test-time scaling\nstrategies. We further characterize how AI agent design choices, such as\nfew-shot prompting, reflection depth, and parallel reasoning, impact\naccuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy\nwith increased compute, they suffer from rapidly diminishing returns, widening\nlatency variance, and unsustainable infrastructure costs. Through detailed\nevaluation of representative agents, we highlight the profound computational\ndemands introduced by AI agent workflows, uncovering a looming sustainability\ncrisis. These results call for a paradigm shift in agent design toward\ncompute-efficient reasoning, balancing performance with deployability under\nreal-world constraints.", "AI": {"tldr": "The paper conducts a thorough analysis of large-language-model-based AI agents, focusing on their resource usage, efficiency, and sustainability challenges.", "motivation": "To address the growing concerns regarding the efficiency, cost, and sustainability of AI agents that use dynamic, multi-turn reasoning workflows.", "method": "The authors perform a system-level analysis to quantify resource usage, latency, energy consumption, and power demands in datacenters, exploring factors like prompting, reasoning depth, and parallel processes.", "result": "AI agents exhibit accuracy improvements with higher compute usage but face diminishing returns, high latency variance, and unsustainable infrastructure demands.", "conclusion": "A paradigm shift toward compute-efficient reasoning in AI agent design is necessary to ensure balance between performance and long-term deployability."}}
{"id": "2506.04410", "pdf": "https://arxiv.org/pdf/2506.04410", "abs": "https://arxiv.org/abs/2506.04410", "authors": ["Peter Jansen", "Samiah Hassan", "Ruoyao Wang"], "title": "Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CL"], "comment": "8 pages", "summary": "Contemporary approaches to assisted scientific discovery use language models\nto automatically generate large numbers of potential hypothesis to test, while\nalso automatically generating code-based experiments to test those hypotheses.\nWhile hypotheses can be comparatively inexpensive to generate, automated\nexperiments can be costly, particularly when run at scale (i.e. thousands of\nexperiments). Developing the capacity to filter hypotheses based on their\nfeasibility would allow discovery systems to run at scale, while increasing\ntheir likelihood of making significant discoveries. In this work we introduce\nMatter-of-Fact, a challenge dataset for determining the feasibility of\nhypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from\nscientific articles spanning four high-impact contemporary materials science\ntopics, including superconductors, semiconductors, batteries, and aerospace\nmaterials, while including qualitative and quantitative claims from\ntheoretical, experimental, and code/simulation results. We show that strong\nbaselines that include retrieval augmented generation over scientific\nliterature and code generation fail to exceed 72% performance on this task\n(chance performance is 50%), while domain-expert verification suggests nearly\nall are solvable -- highlighting both the difficulty of this task for current\nmodels, and the potential to accelerate scientific discovery by making\nnear-term progress.", "AI": {"tldr": "The paper introduces Matter-of-Fact, a benchmark dataset for evaluating the feasibility of scientific hypotheses.", "motivation": "To enhance automated scientific discovery systems by filtering hypotheses based on feasibility, reducing the cost of running large-scale experiments and increasing the likelihood of significant discoveries.", "method": "The authors created Matter-of-Fact, a dataset containing 8.4k claims (qualitative and quantitative) from materials science literature across four domains, and evaluated current models like retrieval-augmented generation and code generation on their feasibility-determination performance.", "result": "Current strong baselines achieve up to 72% performance on the task, indicating significant room for improvement in hypothesis feasibility evaluation.", "conclusion": "The difficulty of the task for existing models demonstrates the need for advancements, and progress in this area could offer substantial improvements in the efficiency of scientific discovery."}}
{"id": "2506.04873", "pdf": "https://arxiv.org/pdf/2506.04873", "abs": "https://arxiv.org/abs/2506.04873", "authors": ["Tonghuan Xiao", "Jiecheng Zhou"], "title": "A distributed system perspective on Backscatter systems: A review", "categories": ["cs.DC"], "comment": "9 pages, 9 figures", "summary": "This review investigates the pivotal role of distributed architectures and\nintelligent resource allocation in enabling robust and scalable wireless\nsystems, with a particular emphasis on backscatter communication, indoor\nlocalization, battery-free networks, and Simultaneous Wireless Information and\nPower Transfer (SWIPT).", "AI": {"tldr": "The paper reviews distributed architectures and resource allocation for wireless systems, focusing on backscatter communication, indoor localization, battery-free networks, and SWIPT.", "motivation": "To address challenges in creating robust and scalable wireless systems through intelligent design and architectures.", "method": "The study conducts a review of distributed architectures and resource allocation strategies, emphasizing specific wireless communication technologies.", "result": "The review identifies how distributed architectures enable improvements in scalability, robustness, and efficiency in specific wireless systems.", "conclusion": "Distributed architectures and intelligent resource allocation are critical for advancing robust, scalable, and efficient wireless systems in the studied domains."}}
{"id": "2506.04353", "pdf": "https://arxiv.org/pdf/2506.04353", "abs": "https://arxiv.org/abs/2506.04353", "authors": ["Ankit Pal", "Jung-Oh Lee", "Xiaoman Zhang", "Malaikannan Sankarasubbu", "Seunghyeon Roh", "Won Jung Kim", "Meesun Lee", "Pranav Rajpurkar"], "title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "We present ReXVQA, the largest and most comprehensive benchmark for visual\nquestion answering (VQA) in chest radiology, comprising approximately 696,000\nquestions paired with 160,000 chest X-rays studies across training, validation,\nand test sets. Unlike prior efforts that rely heavily on template based\nqueries, ReXVQA introduces a diverse and clinically authentic task suite\nreflecting five core radiological reasoning skills: presence assessment,\nlocation analysis, negation detection, differential diagnosis, and geometric\nreasoning. We evaluate eight state-of-the-art multimodal large language models,\nincluding MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The\nbest-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge\nthe gap between AI performance and clinical expertise, we conducted a\ncomprehensive human reader study involving 3 radiology residents on 200\nrandomly sampled cases. Our evaluation demonstrates that MedGemma achieved\nsuperior performance (83.84% accuracy) compared to human readers (best\nradiology resident: 77.27%), representing a significant milestone where AI\nperformance exceeds expert human evaluation on chest X-ray interpretation. The\nreader study reveals distinct performance patterns between AI models and human\nexperts, with strong inter-reader agreement among radiologists while showing\nmore variable agreement patterns between human readers and AI models. ReXVQA\nestablishes a new standard for evaluating generalist radiological AI systems,\noffering public leaderboards, fine-grained evaluation splits, structured\nexplanations, and category-level breakdowns. This benchmark lays the foundation\nfor next-generation AI systems capable of mimicking expert-level clinical\nreasoning beyond narrow pathology classification. Our dataset will be\nopen-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA", "AI": {"tldr": "ReXVQA is a new benchmark for visual question answering in chest radiology, featuring a large dataset and focusing on five key reasoning skills. AI models surpassed radiology residents in accuracy on this task.", "motivation": "The paper aims to address the limitations of existing VQA benchmarks in radiology, which often rely on template-based queries, by introducing a more clinically realistic and comprehensive dataset and evaluation framework.", "method": "The study introduces ReXVQA, comprising 696,000 questions and 160,000 chest X-ray studies, and evaluates eight state-of-the-art multimodal AI models against a human reader study involving radiology residents.", "result": "The best-performing AI model, MedGemma, achieved 83.24% overall accuracy, surpassing the best radiology resident's performance of 77.27%.", "conclusion": "ReXVQA sets a new standard for evaluating radiological AI, bridging the gap between AI and clinical reasoning while highlighting distinct performance patterns between AI and human experts."}}
{"id": "2506.04385", "pdf": "https://arxiv.org/pdf/2506.04385", "abs": "https://arxiv.org/abs/2506.04385", "authors": ["Kurt Micallef", "Claudia Borg"], "title": "MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings Camera-Ready", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious Natural Language Processing (NLP) tasks, largely due to their\ngeneralisability and ability to perform tasks without additional training.\nHowever, their effectiveness for low-resource languages remains limited. In\nthis study, we evaluate the performance of 55 publicly available LLMs on\nMaltese, a low-resource language, using a newly introduced benchmark covering\n11 discriminative and generative tasks. Our experiments highlight that many\nmodels perform poorly, particularly on generative tasks, and that smaller\nfine-tuned models often perform better across all tasks. From our\nmultidimensional analysis, we investigate various factors impacting\nperformance. We conclude that prior exposure to Maltese during pre-training and\ninstruction-tuning emerges as the most important factor. We also examine the\ntrade-offs between fine-tuning and prompting, highlighting that while\nfine-tuning requires a higher initial cost, it yields better performance and\nlower inference costs. Through this work, we aim to highlight the need for more\ninclusive language technologies and recommend that researchers working with\nlow-resource languages consider more \"traditional\" language modelling\napproaches.", "AI": {"tldr": "This paper evaluates 55 LLMs on Maltese, a low-resource language, using a benchmark covering 11 tasks. It finds smaller fine-tuned models outperform larger ones, emphasizing the importance of prior exposure to Maltese and fine-tuning.", "motivation": "The effectiveness of LLMs for low-resource languages like Maltese is limited, and there is a need to assess their performance to identify gaps and potential solutions.", "method": "55 publicly available LLMs were tested against a benchmark designed for Maltese that includes 11 discriminative and generative tasks. A multidimensional performance analysis was conducted.", "result": "Most models performed poorly, especially on generative tasks. Smaller fine-tuned models often outperformed larger ones. Performance was strongly linked to prior exposure of Maltese during pre-training and instruction tuning.", "conclusion": "Fine-tuning improves performance despite higher costs, and prior exposure to Maltese is crucial. Researchers are encouraged to adopt more traditional approaches for low-resource language modelling and push for inclusive language technology."}}
{"id": "2506.04569", "pdf": "https://arxiv.org/pdf/2506.04569", "abs": "https://arxiv.org/abs/2506.04569", "authors": ["Wenwei Gu", "Renyi Zhong", "Guangba Yu", "Xinying Sun", "Jinyang Liu", "Yintong Huo", "Zhuangbin Chen", "Jianping Zhang", "Jiazhen Gu", "Yongqiang Yang", "Michael R. Lyu"], "title": "KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems", "categories": ["cs.SE"], "comment": null, "summary": "To ensure the reliability of cloud systems, their performance is monitored\nusing KPIs (key performance indicators). When issues arise, root cause\nlocalization identifies KPIs responsible for service degradation, aiding in\nquick diagnosis and resolution. Traditional methods rely on similarity\ncalculations, which can be ineffective in complex, interdependent cloud\nenvironments. While deep learning-based approaches model these dependencies\nbetter, they often face challenges such as high computational demands and lack\nof interpretability.\n  To address these issues, KPIRoot is proposed as an efficient method combining\nsimilarity and causality analysis. It uses symbolic aggregate approximation for\ncompact KPI representation, improving analysis efficiency. However, deployment\nin Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses\nsome performance anomalies, and 2) SAX representation fails to capture\nintricate variation trends. KPIRoot+ addresses these limitations, outperforming\neight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by\n34.7%. We also share our experience deploying KPIRoot in a large-scale cloud\nprovider's production environment.", "AI": {"tldr": "The paper addresses challenges in root cause localization for cloud system performance issues by introducing KPIRoot+ as a solution with enhanced efficiency and accuracy.", "motivation": "Traditional methods for root cause localization in cloud systems struggle in complex environments, and deep learning approaches face limitations like high computational cost and lack of interpretability.", "method": "KPIRoot+ combines similarity and causality analysis, uses symbolic aggregate approximation for efficiency, and improves upon KPIRoot with better anomaly detection and trend capture.", "result": "KPIRoot+ outperforms eight state-of-the-art baselines in efficiency and accuracy, achieving up to 35.7% improvement while reducing time cost by 34.7%.", "conclusion": "KPIRoot+ is a robust and scalable solution for root cause localization in cloud systems, offering significant performance gains and practical usability in large-scale production."}}
{"id": "2506.04505", "pdf": "https://arxiv.org/pdf/2506.04505", "abs": "https://arxiv.org/abs/2506.04505", "authors": ["Nikita Oskolkov", "Huzhenyu Zhang", "Dmitry Makarov", "Dmitry Yudin", "Aleksandr Panov"], "title": "SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 11 figures", "summary": "The 3D scene graph models spatial relationships between objects, enabling the\nagent to efficiently navigate in a partially observable environment and predict\nthe location of the target object.This paper proposes an original framework\nnamed SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for\nmapless reinforcement learning-based robot navigation with learnable\nrepresentation of open-vocabulary 3D scene graph. To accelerate and stabilize\nthe training of reinforcement learning-based algorithms, the framework also\nemploys imitation learning and curriculum learning. The first one enables the\nagent to learn from demonstrations, while the second one structures the\ntraining process by gradually increasing task complexity from simple to more\nadvanced scenarios. Numerical experiments conducted in the Isaac Sim\nenvironment showed that using a 3D scene graph for reinforcement learning\nsignificantly increased the success rate in difficult navigation cases. The\ncode is open-sourced and available at: https://github.com/Xisonik/Aloha\\_graph.", "AI": {"tldr": "This paper introduces SGN-CIRL, a framework leveraging 3D scene graphs for mapless reinforcement learning-based robot navigation, enhancing performance with imitation and curriculum learning.", "motivation": "To address challenges in robot navigation in partially observable environments by using 3D scene graph-based reinforcement learning for efficient target localization.", "method": "The SGN-CIRL framework integrates 3D scene graphs, imitation learning (learning from demonstrations), and curriculum learning (gradually increasing task complexity) for mapless navigation.", "result": "Implementing 3D scene graphs in reinforcement learning improved the success rate in difficult navigation cases, as validated in Isaac Sim environment tests.", "conclusion": "The proposed SGN-CIRL framework provides improved navigation efficiency, demonstrating the utility of 3D scene graphs and enhanced training approaches in robot reinforcement learning."}}
{"id": "2506.04254", "pdf": "https://arxiv.org/pdf/2506.04254", "abs": "https://arxiv.org/abs/2506.04254", "authors": ["Nicolas Caron", "Christophe Guyeux", "Hassan Noura", "Benjamin Aynes"], "title": "Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 7 figures, 3 tables, submitted to ECAI2025", "summary": "Forest fire prediction involves estimating the likelihood of fire ignition or\nrelated risk levels in a specific area over a defined time period. With climate\nchange intensifying fire behavior and frequency, accurate prediction has become\none of the most pressing challenges in Artificial Intelligence (AI).\nTraditionally, fire ignition is approached as a binary classification task in\nthe literature. However, this formulation oversimplifies the problem,\nespecially from the perspective of end-users such as firefighters. In general,\nas is the case in France, firefighting units are organized by department, each\nwith its terrain, climate conditions, and historical experience with fire\nevents. Consequently, fire risk should be modeled in a way that is sensitive to\nlocal conditions and does not assume uniform risk across all regions. This\npaper proposes a new approach that tailors fire risk assessment to departmental\ncontexts, offering more actionable and region-specific predictions for\noperational use. With this, we present the first national-scale AI benchmark\nfor metropolitan France using state-of-the-art AI models on a relatively\nunexplored dataset. Finally, we offer a summary of important future works that\nshould be taken into account. Supplementary materials are available on GitHub.", "AI": {"tldr": "This paper proposes a locally-tailored AI approach to predict forest fire risks in metropolitan France, moving beyond traditional binary prediction models.", "motivation": "To address the limitations in traditional binary classification models for fire risks, which don't account for local variations and operational needs of firefighting units.", "method": "The study utilizes a new AI benchmark tailored for metropolitan France, applied to a relatively unexplored dataset, and employs state-of-the-art AI models.", "result": "The methodology provides more region-specific, actionable fire risk predictions tailored to departmental contexts within metropolitan France.", "conclusion": "The proposed approach demonstrates the feasibility and importance of localized fire prediction models and underscores the need for regionally-sensitive risk assessments in future research."}}
{"id": "2506.04943", "pdf": "https://arxiv.org/pdf/2506.04943", "abs": "https://arxiv.org/abs/2506.04943", "authors": ["Shujun Zhou", "Guozhang Chen"], "title": "The Hippocampal Place Field Gradient: An Eigenmode Theory Linking Grid Cell Projections to Multiscale Learning", "categories": ["q-bio.NC"], "comment": null, "summary": "The hippocampus encodes space through a striking gradient of place field\nsizes along its dorsal-ventral axis, yet the principles generating this\ncontinuous gradient from discrete grid cell inputs remain debated. We propose a\nunified theoretical framework establishing that hippocampal place fields arise\nnaturally as linear projections of grid cell population activity, interpretable\nas eigenmodes. Critically, we demonstrate that a frequency-dependent decay of\nthese grid-to-place connection weights naturally transforms inputs from\ndiscrete grid modules into a continuous spectrum of place field sizes. This\nmultiscale organization is functionally significant: we reveal it shapes the\ninductive bias of the population code, balancing a fundamental trade-off\nbetween precision and generalization. Mathematical analysis and simulations\ndemonstrate an optimal place field size for few-shot learning, which scales\nwith environment structure. Our results offer a principled explanation for the\nplace field gradient and generate testable predictions, bridging anatomical\nconnectivity with adaptive learning in both biological and artificial\nintelligence.", "AI": {"tldr": "Place fields in the hippocampus originate from grid cell activity through linear projections, creating a spectrum of field sizes. Mathematical analysis and simulations link this organization to optimal learning.", "motivation": "The paper aims to understand the continuous gradient of place field sizes in the hippocampus and connect this phenomenon to its anatomical connectivity and adaptive learning.", "method": "The authors propose a theoretical framework, employing mathematical analysis and simulations to study the transformation of grid cell inputs into place field sizes.", "result": "The findings demonstrate that grid-to-place connection weights decay by frequency, producing a spectrum of place field sizes significant for balancing precision and generalization in learning.", "conclusion": "The study provides an explanation for the hippocampal place field gradient and suggests a relationship with learning dynamics, which is extendable to artificial intelligence models."}}
{"id": "2506.05120", "pdf": "https://arxiv.org/pdf/2506.05120", "abs": "https://arxiv.org/abs/2506.05120", "authors": ["Konstantin G\u00f6bler", "Tobias Windisch", "Mathias Drton"], "title": "Nonlinear Causal Discovery for Grouped Data", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "9 pages, 5 figures, to be published at UAI'25", "summary": "Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.", "AI": {"tldr": "The paper expands nonlinear additive noise models to infer causal relationships in multivariate data, addressing real-world scenarios such as neuroscience and industrial settings.", "motivation": "Traditional causal inference focuses on scalar variables, but many real-world applications involve groups of variables.", "method": "The authors propose a two-step method: (1) infer causal order among random vectors, (2) conduct model selection for graph identification.", "result": "The approach shows strong simulation performance and is tested on assembly line data with partially known causal ordering.", "conclusion": "The method effectively handles causal relationships in multivariate settings, with potential applications across diverse domains."}}
{"id": "2506.04427", "pdf": "https://arxiv.org/pdf/2506.04427", "abs": "https://arxiv.org/abs/2506.04427", "authors": ["Xixi Wang", "Miguel Costa", "Jordanka Kovaceva", "Shuai Wang", "Francisco C. Pereira"], "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches this graph\nto construct interpretable reasoning chains, aided by pruning and sub-path\nmerging strategies to enhance efficiency and coherence. Experiments on both\nstandard benchmarks and a realistic, large-scale dataset demonstrate the\neffectiveness of our approach. To our knowledge, this is the first multi-table\nQA system applied to truly complex industrial tabular data.", "AI": {"tldr": "This paper proposes a graph-based method to improve multi-table question answering (QA) by encoding schema links and join paths, addressing the challenges in complex tabular datasets.", "motivation": "The motivation is to overcome the limitations of existing multi-table QA methods that fail to handle the complexity and diversity of columns in real-world tabular data.", "method": "The authors developed a graph-based framework that incorporates relational knowledge to explicitly encode schema links and join paths. It includes techniques like reasoning chain construction, pruning, and sub-path merging to enhance performance.", "result": "Experiments on standard benchmarks and a large-scale industrial dataset show that the proposed method is effective for multi-table QA in complex scenarios.", "conclusion": "This paper presents the first multi-table QA framework designed for highly complex industrial tabular data, offering improved reasoning and interpretability over prior approaches."}}
{"id": "2506.04523", "pdf": "https://arxiv.org/pdf/2506.04523", "abs": "https://arxiv.org/abs/2506.04523", "authors": ["Cliff B. Abbott", "Mark Elo", "Dmytro A. Bozhko"], "title": "Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing", "categories": ["cs.LG", "cond-mat.mes-hall", "cs.ET", "cs.NE", "physics.comp-ph"], "comment": "7 pages, 8 figures, submitted to IEEE Transactions on Neural Netowrks\n  and Learning Systems", "summary": "We introduce Perturbative Gradient Training (PGT), a novel training paradigm\nthat overcomes a critical limitation of physical reservoir computing: the\ninability to perform backpropagation due to the black-box nature of physical\nreservoirs. Drawing inspiration from perturbation theory in physics, PGT uses\nrandom perturbations in the network's parameter space to approximate gradient\nupdates using only forward passes. We demonstrate the feasibility of this\napproach on both simulated neural network architectures, including a dense\nnetwork and a transformer model with a reservoir layer, and on experimental\nhardware using a magnonic auto-oscillation ring as the physical reservoir. Our\nresults show that PGT can achieve performance comparable to that of standard\nbackpropagation methods in cases where backpropagation is impractical or\nimpossible. PGT represents a promising step toward integrating physical\nreservoirs into deeper neural network architectures and achieving significant\nenergy efficiency gains in AI training.", "AI": {"tldr": "The paper presents Perturbative Gradient Training (PGT), a method to perform effective gradient updates using random parameter perturbations, overcoming the inability to backpropagate in physical reservoirs.", "motivation": "Physical reservoirs have shown promise in AI systems but are limited due to the inability to perform backpropagation. The authors seek to address this bottleneck and enable their integration into deeper architectures.", "method": "PGT leverages random perturbations of parameters in physical reservoirs and utilizes forward pass computations to approximate gradient updates without relying on traditional backpropagation methods.", "result": "Experimental evaluation on simulated architectures and physical hardware demonstrated that PGT delivers comparable performance to standard backpropagation while enabling energy-efficient AI training.", "conclusion": "PGT opens new avenues for incorporating physical reservoirs into complex neural networks with enhanced energy efficiency, addressing limitations of standard backpropagation systems."}}
{"id": "2506.04363", "pdf": "https://arxiv.org/pdf/2506.04363", "abs": "https://arxiv.org/abs/2506.04363", "authors": ["Delong Chen", "Willy Chung", "Yejin Bang", "Ziwei Ji", "Pascale Fung"], "title": "WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning", "categories": ["cs.CV"], "comment": null, "summary": "Humans are known to have an internal \"world model\" that enables us to carry\nout action planning based on world states. AI agents need to have such a world\nmodel for action planning as well. It is not clear how current AI models,\nespecially generative models, are able to learn such world models and carry out\nprocedural planning in diverse environments. We introduce WorldPrediction, a\nvideo-based benchmark for evaluating world modeling and procedural planning\ncapabilities of different AI models. In contrast to prior benchmarks that focus\nprimarily on low-level world modeling and robotic motion planning,\nWorldPrediction is the first benchmark that emphasizes actions with temporal\nand semantic abstraction. Given initial and final world states, the task is to\ndistinguish the proper action (WorldPrediction-WM) or the properly ordered\nsequence of actions (WorldPrediction-PP) from a set of counterfactual\ndistractors. This discriminative task setup enable us to evaluate different\ntypes of world models and planners and realize a thorough comparison across\ndifferent hypothesis. The benchmark represents states and actions using visual\nobservations. In order to prevent models from exploiting low-level continuity\ncues in background scenes, we provide \"action equivalents\" - identical actions\nobserved in different contexts - as candidates for selection. This benchmark is\ngrounded in a formal framework of partially observable semi-MDP, ensuring\nbetter reliability and robustness of the evaluation. We conduct extensive human\nfiltering and validation on our benchmark and show that current frontier models\nbarely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP\nwhereas humans are able to solve both tasks perfectly.", "AI": {"tldr": "WorldPrediction is a video-based benchmark aimed at evaluating AI's ability for world modeling and procedural planning. It poses tasks requiring semantic and temporal action reasoning, showing current AI models fall short compared to humans.", "motivation": "The paper aims to address the gap in evaluating AI's ability to model and plan actions in diverse environments, particularly focusing beyond low-level robotic motion to semantic and temporal abstraction.", "method": "The study introduces the WorldPrediction benchmark composed of two tasks: selecting appropriate actions based on visual observations (WorldPrediction-WM) and determining proper sequences of actions (WorldPrediction-PP). It includes human filtering and validation to ensure robustness.", "result": "Current AI models achieve only 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP, while humans perform flawlessly, highlighting the inadequacy of present AI approaches in these tasks.", "conclusion": "AI models need significant advancements to approach human-level world modeling and procedural planning capabilities, emphasizing the importance of generalized and abstract reasoning benchmarks like WorldPrediction."}}
{"id": "2506.04389", "pdf": "https://arxiv.org/pdf/2506.04389", "abs": "https://arxiv.org/abs/2506.04389", "authors": ["Saurabh Kumar", "Sourav Bansal", "Neeraj Agrawal", "Priyanka Bhatt"], "title": "Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Customer care is an essential pillar of the e-commerce shopping experience\nwith companies spending millions of dollars each year, employing automation and\nhuman agents, across geographies (like US, Canada, Mexico, Chile), channels\n(like Chat, Interactive Voice Response (IVR)), and languages (like English,\nSpanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on\nannotated data have shown good performance in downstream tasks relevant to\nCustomer Care. However, model performance is largely subject to the\navailability of sufficient annotated domain-specific data. Cross-domain\navailability of data remains a bottleneck, thus building an intent classifier\nthat generalizes across domains (defined by channel, geography, and language)\nwith only a few annotations, is of great practical value. In this paper, we\npropose an embedder-cum-classifier model architecture which extends\nstate-of-the-art domain-specific models to other domains with only a few\nlabeled samples. We adopt a supervised fine-tuning approach with isotropic\nregularizers to train a domain-specific sentence embedder and a multilingual\nknowledge distillation strategy to generalize this embedder across multiple\ndomains. The trained embedder, further augmented with a simple linear\nclassifier can be deployed for new domains. Experiments on Canada and Mexico\ne-commerce Customer Care dataset with few-shot intent detection show an\nincrease in accuracy by 20-23% against the existing state-of-the-art\npre-trained models.", "AI": {"tldr": "The paper introduces a novel embedder-cum-classifier model that generalizes intent classification across multiple channels, geographies, and languages with minimal labeled data, achieving significant performance improvements.", "motivation": "The motivation is to address the challenge of building intent classifiers for customer care that generalize across multiple domains (channel, geography, language) despite limited annotated data, essential for enhancing e-commerce customer experiences.", "method": "The authors propose a supervised fine-tuning approach using isotropic regularizers to train a domain-specific sentence embedder, coupled with multilingual knowledge distillation to generalize across domains. This embedder is then paired with a linear classifier for deployment.", "result": "The new method achieves a 20-23% accuracy improvement over existing state-of-the-art pre-trained models for few-shot intent detection on datasets from Canada and Mexico e-commerce customer care.", "conclusion": "The proposed architecture effectively enables domain generalization of intent classifiers with limited annotations, showing significant practical value in cross-domain scenarios for e-commerce customer care."}}
{"id": "2506.04639", "pdf": "https://arxiv.org/pdf/2506.04639", "abs": "https://arxiv.org/abs/2506.04639", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development", "categories": ["cs.SE"], "comment": "A short version of the paper will be appeared in the proceedings of\n  the IEEE Computers, Software, and Applications Conference (COMPSAC 2025) on\n  July 8-11, 2025", "summary": "This paper introduces QuanUML, an extension of the Unified Modeling Language\n(UML) tailored for quantum software systems. QuanUML integrates\nquantum-specific constructs, such as qubits and quantum gates, into the UML\nframework, enabling the modeling of both quantum and hybrid quantum-classical\nsystems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic\nCircuits and Shor's Algorithm, demonstrating its utility in designing and\nvisualizing quantum algorithms. Our approach supports model-driven development\nof quantum software and offers a structured framework for quantum software\ndesign. We also highlight its advantages over existing methods and discuss\nfuture improvements.", "AI": {"tldr": "QuanUML is a UML extension for modeling quantum and hybrid systems, demonstrated with applications to quantum algorithms like Shor's Algorithm.", "motivation": "To provide a structured framework for the model-driven development of quantum and hybrid quantum-classical software systems.", "method": "Extend UML by integrating quantum-specific constructs, apply it to quantum algorithms for demonstration, and analyze its advantages.", "result": "QuanUML is successfully applied to Efficient Long-Range Entanglement and Shor's Algorithm, showing its effectiveness in quantum software design.", "conclusion": "QuanUML enhances quantum software modeling with structured visualization and supports quantum software development."}}
{"id": "2506.04539", "pdf": "https://arxiv.org/pdf/2506.04539", "abs": "https://arxiv.org/abs/2506.04539", "authors": ["Kordel K. France", "Ovidiu Daescu", "Anirban Paul", "Shalini Prasad"], "title": "Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation", "categories": ["cs.RO", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "comment": "Published as a full conference paper at the 2025 IEEE International\n  Symposium on Inertial Sensors & Systems", "summary": "Visual inertial odometry (VIO) is a process for fusing visual and kinematic\ndata to understand a machine's state in a navigation task. Olfactory inertial\nodometry (OIO) is an analog to VIO that fuses signals from gas sensors with\ninertial data to help a robot navigate by scent. Gas dynamics and environmental\nfactors introduce disturbances into olfactory navigation tasks that can make\nOIO difficult to facilitate. With our work here, we define a process for\ncalibrating a robot for OIO that generalizes to several olfaction sensor types.\nOur focus is specifically on calibrating OIO for centimeter-level accuracy in\nlocalizing an odor source on a slow-moving robot platform to demonstrate use\ncases in robotic surgery and touchless security screening. We demonstrate our\nprocess for OIO calibration on a real robotic arm and show how this calibration\nimproves performance over a cold-start olfactory navigation task.", "AI": {"tldr": "This paper introduces Olfactory Inertial Odometry (OIO), a method for robotic navigation using gas sensor fusion with inertial data, and provides a calibration approach for improved performance.", "motivation": "Navigation using scent via olfactory sensors in robots faces challenges due to disturbances from gas dynamics and environmental factors. The paper aims to make OIO accurate and accessible across different olfactory sensor types.", "method": "A generalizable calibration process is proposed for OIO on robots, targeting centimeter-level accuracy in odor source localization using a slow-moving platform.", "result": "The approach is tested on a robotic arm, demonstrating improved performance in olfactory navigation tasks compared to uncalibrated setups.", "conclusion": "The study validates the proposed OIO calibration process, highlighting its practical applications in fields like robotic surgery and touchless security screening."}}
{"id": "2506.04268", "pdf": "https://arxiv.org/pdf/2506.04268", "abs": "https://arxiv.org/abs/2506.04268", "authors": ["Jingyang Li", "Guoqiang Li"], "title": "MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid development of deep learning has led to challenges in deploying\nneural networks on edge devices, mainly due to their high memory and runtime\ncomplexity. Network compression techniques, such as quantization and pruning,\naim to reduce this complexity while maintaining accuracy. However, existing\nincremental verification methods often focus only on quantization and struggle\nwith structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided\nIncremental Verification), a novel framework for incremental verification of\ncompressed deep neural networks. It encodes both the original and compressed\nnetworks into SMT formulas, classifies changes, and use \\emph{Minimal Unsat\nCores (MUCs)} from the original network to guide efficient verification for the\ncompressed network. Experimental results show its effectiveness in handling\nquantization and pruning, with high proof reuse rates and significant speedup\nin verification time compared to traditional methods. MUC-G4 hence offers a\npromising solution for ensuring the safety and reliability of compressed neural\nnetworks in practical applications.", "AI": {"tldr": "The paper introduces MUC-G4, a verification method for compressed neural networks, effectively handling quantization and pruning with faster results.", "motivation": "Deep learning models often require high memory and runtime, making them challenging to deploy on edge devices. Existing network compression techniques like quantization and pruning maintain accuracy but lack robust verification methods for structural changes.", "method": "MUC-G4 encodes both original and compressed networks into SMT formulas, classifies changes, and uses Minimal Unsat Cores (MUCs) from the original network to guide efficient verification for the compressed network.", "result": "MUC-G4 achieved high proof reuse rates and significantly faster verification time compared to traditional methods, handling both quantization and pruning effectively.", "conclusion": "MUC-G4 is a promising framework for improving the safety and reliability of compressed neural networks in practical settings."}}
{"id": "2506.05320", "pdf": "https://arxiv.org/pdf/2506.05320", "abs": "https://arxiv.org/abs/2506.05320", "authors": ["Avery Hee-Woon Ryoo", "Nanda H. Krishna", "Ximeng Mao", "Mehdi Azabou", "Eva L. Dyer", "Matthew G. Perich", "Guillaume Lajoie"], "title": "Generalizable, real-time neural decoding with hybrid state-space models", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint. Under review", "summary": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications.", "AI": {"tldr": "The paper proposes POSSM, a hybrid architecture combining cross-attention and recurrent state-space models for efficient and accurate neural activity decoding, achieving significant speed improvement without compromising performance.", "motivation": "Neuroscience and neurotechnology need real-time decoding tools capable of generalizing well to unseen data while adhering to strict latency constraints.", "method": "POSSM integrates a cross-attention module for tokenizing neural spikes with a recurrent state-space model backbone. Multi-dataset pretraining enables better generalization across tasks and species.", "result": "POSSM achieves comparable decoding accuracy to state-of-the-art Transformers but is up to 9x faster on GPU. It also showcases cross-species transfer capabilities in decoding tasks.", "conclusion": "Hybrid SSMs like POSSM successfully balance accuracy, speed, and generalization in neural decoders, proving valuable for real-time and resource-constrained applications."}}
{"id": "2506.05202", "pdf": "https://arxiv.org/pdf/2506.05202", "abs": "https://arxiv.org/abs/2506.05202", "authors": ["Daniele Tramontano", "Yaroslav Kivva", "Saber Salehkaleybar Mathias Drton", "Negar Kiyavash"], "title": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Accepted at ICML 2025", "summary": "This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders.", "AI": {"tldr": "The paper addresses causal effect identification in latent variable lvLiNGAM using higher-order cumulants, proposing methods for challenging scenarios with proxies or instruments.", "motivation": "To handle scenarios with latent confounding in linear causal systems where standard causal inference methods fail, especially when using a single proxy variable or limited instruments.", "method": "Develops new estimation methods leveraging higher-order cumulants to handle causal identification under conditions with a single proxy or limited instruments.", "result": "Demonstrates through experiments that the proposed approaches achieve accurate and robust causal effect estimation, outperforming existing methods.", "conclusion": "Advances the field of causal inference in linear systems with latent confounders by proving identifiability and providing effective estimation methods."}}
{"id": "2506.05071", "pdf": "https://arxiv.org/pdf/2506.05071", "abs": "https://arxiv.org/abs/2506.05071", "authors": ["Shahram Ghandeharizadeh", "Sandy Irani", "Jenny Lam"], "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM", "categories": ["cs.DB", "cs.AR", "cs.DS"], "comment": "A shorter version appeared in the IEEE 34th International Conference\n  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:\n  10.1109/ICDE.2018.00155", "summary": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.", "AI": {"tldr": "This paper addresses optimal caching middleware configuration using Non-Volatile Memory (NVM) and other storage media by applying the Multiple Choice Knapsack Problem (MCKP) for design tradeoffs.", "motivation": "The paper aims to address the growing complexity in configuring an optimal caching memory hierarchy amidst evolving storage media, such as NVM, by guiding replication, partitioning, and media choices.", "method": "The authors model caching configuration decisions as a Multiple Choice Knapsack Problem (MCKP), leveraging its linear programming relaxation for practical and efficient approximation of solutions.", "result": "The study demonstrates that selective replication is beneficial under specific failure rates and workloads, while data tiering across storage media performs better under low failure rates and frequent updates.", "conclusion": "Optimal memory hierarchy design depends on balancing replication and tiering strategies based on failure rates and workload characteristics, enabling better system performance and recovery."}}
{"id": "2506.04429", "pdf": "https://arxiv.org/pdf/2506.04429", "abs": "https://arxiv.org/abs/2506.04429", "authors": ["Ananya Joshi", "Nolan Gormley", "Richa Gadgil", "Tina Townes", "Roni Rosenfeld", "Bryan Wilder"], "title": "An AI-Based Public Health Data Monitoring System", "categories": ["cs.AI"], "comment": null, "summary": "Public health experts need scalable approaches to monitor large volumes of\nhealth data (e.g., cases, hospitalizations, deaths) for outbreaks or data\nquality issues. Traditional alert-based monitoring systems struggle with modern\npublic health data monitoring systems for several reasons, including that\nalerting thresholds need to be constantly reset and the data volumes may cause\napplication lag. Instead, we propose a ranking-based monitoring paradigm that\nleverages new AI anomaly detection methods. Through a multi-year\ninterdisciplinary collaboration, the resulting system has been deployed at a\nnational organization to monitor up to 5,000,000 data points daily. A\nthree-month longitudinal deployed evaluation revealed a significant improvement\nin monitoring objectives, with a 54x increase in reviewer speed efficiency\ncompared to traditional alert-based methods. This work highlights the potential\nof human-centered AI to transform public health decision-making.", "AI": {"tldr": "The study explores a ranking-based AI anomaly detection system to improve the monitoring of large-scale public health data, achieving stronger efficiency than traditional methods.", "motivation": "Current alert-based monitoring systems for public health data face scalability and timeliness challenges when processing large amounts of health information.", "method": "The paper introduces a human-centered, AI-driven ranking-based anomaly detection system for monitoring public health data effectively and efficiently.", "result": "The system showed significant improvement, with a 54x increase in reviewer speed efficiency during the three-month evaluation period, compared to traditional approaches.", "conclusion": "The ranking-based AI anomaly detection method successfully transforms public health decision-making, demonstrating the effectiveness of human-centered AI solutions in health monitoring tasks."}}
{"id": "2506.04919", "pdf": "https://arxiv.org/pdf/2506.04919", "abs": "https://arxiv.org/abs/2506.04919", "authors": ["Fabien Dufoulon", "Gopal Pandurangan"], "title": "Improved Byzantine Agreement under an Adaptive Adversary", "categories": ["cs.DC", "cs.DS"], "comment": "PODC 2025, abstract shortened to fit arXiv constraints", "summary": "Byzantine agreement is a fundamental problem in fault-tolerant distributed\ncomputing that has been studied intensively for the last four decades. Much of\nthe research has focused on a static Byzantine adversary, where the adversary\nis constrained to choose the Byzantine nodes in advance of the protocol's\nexecution. This work focuses on the harder case of an adaptive Byzantine\nadversary that can choose the Byzantine nodes \\emph{adaptively} based on the\nprotocol's execution. While efficient $O(\\log n)$-round protocols ($n$ is the\ntotal number of nodes) are known for the static adversary (Goldwasser, Pavlov,\nand Vaikuntanathan, FOCS 2006) tolerating up to $t < n/(3+\\epsilon)$ Byzantine\nnodes, $\\Omega(t/\\sqrt{n \\log n})$ rounds is a well-known lower bound for\nadaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol\nfor adaptive adversary runs in $O(t/\\log n)$ rounds [Chor and Coan, IEEE Trans.\nSoft. Engg., 1985].\n  This work presents a synchronous randomized Byzantine agreement protocol\nunder an adaptive adversary that improves over previous results. Our protocol\nworks under the powerful \\emph{adaptive rushing adversary in the full\ninformation model}. That is, we assume that the Byzantine nodes can behave\narbitrarily and maliciously, have knowledge about the entire state of the\nnetwork at every round, including random choices made by all the nodes up to\nand including the current round, have unlimited computational power, and may\ncollude among themselves. Furthermore, the adversary can \\emph{adaptively}\ncorrupt up to $t < n/3$ nodes based on the protocol's execution. We present a\nsimple randomized Byzantine agreement protocol that runs in $O(\\min\\{t^2\\log\nn/n, t/\\log n\\})$ rounds that improves over the long-standing bound of\n$O(t/\\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].", "AI": {"tldr": "The paper introduces a randomized Byzantine agreement protocol for adaptive adversaries under a rushing adversary in the full information model, improving the round complexity to $O(\\min\\{t^2\\log n/n, t/\\log n\\})$, outperforming existing bounds.", "motivation": "The motivation is to address the harder problem of Byzantine agreement under adaptive adversaries, where the adversary can corrupt nodes dynamically during protocol execution. Existing solutions are either less efficient or cannot handle such adversaries effectively.", "method": "The authors propose a synchronous randomized protocol designed to tolerate up to $t < n/3$ corrupted nodes, leveraging adaptive adversaries' full access to network information while maintaining efficiency and robustness.", "result": "The new protocol achieves $O(\\min\\{t^2\\log n/n, t/\\log n\\})$ rounds in the adaptive adversary model, which is a significant improvement over the previously best-known $O(t/\\log n)$.", "conclusion": "This work advances the state-of-the-art in Byzantine agreement protocols by providing a faster and more efficient solution for adaptive adversaries, setting a new benchmark in the field."}}
{"id": "2506.04365", "pdf": "https://arxiv.org/pdf/2506.04365", "abs": "https://arxiv.org/abs/2506.04365", "authors": ["Liam Salass", "Jerrin Bright", "Amir Nazemi", "Yuhao Chen", "John Zelek", "David Clausi"], "title": "Ice Hockey Puck Localization Using Contextual Cues", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Puck detection in ice hockey broadcast videos poses significant challenges\ndue to the puck's small size, frequent occlusions, motion blur, broadcast\nartifacts, and scale inconsistencies due to varying camera zoom and broadcast\ncamera viewpoints. Prior works focus on appearance-based or motion-based cues\nof the puck without explicitly modelling the cues derived from player\nbehaviour. Players consistently turn their bodies and direct their gaze toward\nthe puck. Motivated by this strong contextual cue, we propose Puck Localization\nUsing Contextual Cues (PLUCC), a novel approach for scale-aware and\ncontext-driven single-frame puck detections. PLUCC consists of three\ncomponents: (a) a contextual encoder, which utilizes player orientations and\npositioning as helpful priors; (b) a feature pyramid encoder, which extracts\nmultiscale features from the dual encoders; and (c) a gating decoder that\ncombines latent features with a channel gating mechanism. For evaluation, in\naddition to standard average precision, we propose Rink Space Localization\nError (RSLE), a scale-invariant homography-based metric for removing\nperspective bias from rink space evaluation. The experimental results of PLUCC\non the PuckDataset dataset demonstrated state-of-the-art detection performance,\nsurpassing previous baseline methods by an average precision improvement of\n12.2% and RSLE average precision of 25%. Our research demonstrates the critical\nrole of contextual understanding in improving puck detection performance, with\nbroad implications for automated sports analysis.", "AI": {"tldr": "This paper introduces PLUCC, a novel method for detecting ice hockey pucks in broadcast videos using player behavior, with significant performance improvements over baselines.", "motivation": "Challenges in hockey puck detection, such as small size, occlusions, motion blur, and inconsistent scales, demand new approaches that leverage contextual cues like player behavior.", "method": "The PLUCC method uses a contextual encoder for modeling player positioning and orientation, a feature pyramid encoder for multiscale feature extraction, and a gating decoder to decode features effectively.", "result": "The proposed PLUCC method achieved a 12.2% improvement in average precision and 25% improvement in RSLE average precision compared to previous methods using the PuckDataset.", "conclusion": "The paper highlights the importance of contextual cues, particularly player behavior, in advancing puck detection technology, showing promise for broader automated sports analysis applications."}}
{"id": "2506.04405", "pdf": "https://arxiv.org/pdf/2506.04405", "abs": "https://arxiv.org/abs/2506.04405", "authors": ["Ran Xu", "Yuchen Zhuang", "Yishan Zhong", "Yue Yu", "Xiangru Tang", "Hang Wu", "May D. Wang", "Peifeng Ruan", "Donghan Yang", "Tao Wang", "Guanghua Xiao", "Carl Yang", "Yang Xie", "Wenqi Shi"], "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.", "AI": {"tldr": "MedAgentGYM is the first training platform aiming to improve coding-based medical reasoning in large language models. It encompasses a vast set of tasks and provides benchmarking capabilities.", "motivation": "To address the need for better coding-based medical reasoning in LLM-based biomedical agents while offering improved benchmarking and training resources.", "method": "MedAgentGYM includes 72,413 task instances from real biomedical scenarios, executable coding environments, interactive feedback mechanisms, and scalable training paths.", "result": "MedAgentGYM enabled Med-Copilot-7B to significantly enhance its performance through supervised fine-tuning (+36.44%) and reinforcement learning (+42.47%), making it competitive with gpt-4o while preserving affordability and privacy.", "conclusion": "MedAgentGYM serves as an integrated platform, advancing the development of LLM-based coding assistants for biomedical applications by providing robust training and benchmarking resources."}}
{"id": "2506.04785", "pdf": "https://arxiv.org/pdf/2506.04785", "abs": "https://arxiv.org/abs/2506.04785", "authors": ["Alisa Welter", "Niklas Schneider", "Tobias Dick", "Kallistos Weis", "Christof Tinnes", "Marvin Wyrich", "Sven Apel"], "title": "From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer", "categories": ["cs.SE"], "comment": null, "summary": "Knowledge transfer is fundamental to human collaboration and is therefore\ncommon in software engineering. Pair programming is a prominent instance. With\nthe rise of AI coding assistants, developers now not only work with human\npartners but also, as some claim, with AI pair programmers. Although studies\nconfirm knowledge transfer during human pair programming, its effectiveness\nwith AI coding assistants remains uncertain. To analyze knowledge transfer in\nboth human-human and human-AI settings, we conducted an empirical study where\ndeveloper pairs solved a programming task without AI support, while a separate\ngroup of individual developers completed the same task using the AI coding\nassistant GitHub Copilot. We extended an existing knowledge transfer framework\nand employed a semi-automated evaluation pipeline to assess differences in\nknowledge transfer episodes across both settings. We found a similar frequency\nof successful knowledge transfer episodes and overlapping topical categories\nacross both settings. Two of our key findings are that developers tend to\naccept GitHub Copilot's suggestions with less scrutiny than those from human\npair programming partners, but also that GitHub Copilot can subtly remind\ndevelopers of important code details they might otherwise overlook.", "AI": {"tldr": "The study compares knowledge transfer in human pair programming and human-AI coding settings using GitHub Copilot.", "motivation": "Understanding how knowledge transfer operates with AI coding assistants compared to traditional human pair programming.", "method": "An empirical study comparing programming tasks solved by human pairs and individual developers using GitHub Copilot, using an extended knowledge transfer framework and semi-automated evaluation.", "result": "Similar frequencies of successful knowledge transfer and topical overlap were observed across human-human and human-AI settings, but developers scrutinized AI suggestions less while AI assistants highlighted neglected code details.", "conclusion": "While GitHub Copilot facilitates knowledge transfer comparable to human pair programming, it changes the dynamics by reducing scrutiny and offering subtle reminders on code details."}}
{"id": "2506.04540", "pdf": "https://arxiv.org/pdf/2506.04540", "abs": "https://arxiv.org/abs/2506.04540", "authors": ["Kordel K. France"], "title": "Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques", "categories": ["cs.RO", "cs.LG", "physics.chem-ph", "physics.ins-det"], "comment": "Published at IEEE BioSensors 2025", "summary": "Chronoamperometry (CA) is a fundamental electrochemical technique used for\nquantifying redox-active species. However, in room-temperature ionic liquids\n(RTILs), the high viscosity and slow mass transport often lead to extended\nmeasurement durations. This paper presents a novel mathematical regression\napproach that reduces CA measurement windows to under 1 second, significantly\nfaster than previously reported methods, which typically require 1-4 seconds or\nlonger. By applying an inference algorithm to the initial transient current\nresponse, this method accurately predicts steady-state electrochemical\nparameters without requiring additional hardware modifications. The approach is\nvalidated through comparison with standard chronoamperometric techniques and is\ndemonstrated to maintain reasonable accuracy while dramatically reducing data\nacquisition time. The implications of this technique are explored in analytical\nchemistry, sensor technology, and battery science, where rapid electrochemical\nquantification is critical. Our technique is focused on enabling faster\nmultiplexing of chronoamperometric measurements for rapid olfactory and\nelectrochemical analysis.", "AI": {"tldr": "A novel mathematical regression approach predicts steady-state electrochemical parameters in under 1 second for chronoamperometry in room-temperature ionic liquids, faster than existing methods.", "motivation": "Room-temperature ionic liquids pose challenges for chronoamperometry due to high viscosity, which slows down mass transport and extends measurement times.", "method": "The authors developed a mathematical regression-based inference algorithm applied to the transient current response to predict electrochemical parameters without requiring hardware changes.", "result": "Their method reduced chronoamperometric measurement times to under 1 second while maintaining reasonable accuracy through validation against standard techniques.", "conclusion": "This method significantly enhances the speed of electrochemical analysis, with potential applications in analytical chemistry, sensor technology, and battery science for rapid quantification."}}
{"id": "2506.04272", "pdf": "https://arxiv.org/pdf/2506.04272", "abs": "https://arxiv.org/abs/2506.04272", "authors": ["Kyung Rok Kim", "Yumo Bai", "Chonghuan Wang", "Guanting Chen"], "title": "Understanding the Impact of Sampling Quality in Direct Preference Optimization", "categories": ["cs.LG"], "comment": "Submitted to NeurIPS2025", "summary": "We study the role of the sampling distribution in Direct Preference\nOptimization (DPO) and aim to understand its impact on DPO's training dynamics.\nOur analyses show that both the solution space and the convergence behavior of\nDPO depend on the support and quality of the generating distribution. We first\nanalyze how distribution of responses influences policy updates during gradient\ndescent, drawing connections to common phenomena found in practice. We then\ndesign a simplified yet well-structured alignment model as a proxy, and develop\nquantitative results showing how more frequent high-quality responses amplify\nthe gradient signal and improve the optimization landscape, leading to more\neffective policy learning. Our theoretical findings are supported by empirical\nexperiments and provide a principled justification for the online DPO framework\nin practice.", "AI": {"tldr": "The paper investigates how the sampling distribution affects Direct Preference Optimization (DPO) and its training dynamics, showing its influence on solution space, convergence, and policy learning.", "motivation": "The motivation is to understand the role of response distribution in shaping DPO training dynamics and to establish a theoretical foundation for its practical applications.", "method": "The paper used a combination of theoretical analysis and empirical experiments, including designing a simplified alignment model to assess how response quality impacts gradient signals and policy optimization.", "result": "It was found that high-quality, frequent responses improve the optimization landscape by amplifying gradient signals, facilitating more effective policy learning.", "conclusion": "The findings provide a theoretical justification for employing online DPO frameworks, emphasizing the importance of carefully managing the sampling distribution."}}
{"id": "2506.04289", "pdf": "https://arxiv.org/pdf/2506.04289", "abs": "https://arxiv.org/abs/2506.04289", "authors": ["Jesse Geerts", "Stephanie Chan", "Claudia Clopath", "Kimberly Stachenfeld"], "title": "Relational reasoning and inductive bias in transformers trained on a transitive inference task", "categories": ["cs.LG", "q-bio.NC"], "comment": "13 pages, 6 figures", "summary": "Transformer-based models have demonstrated remarkable reasoning abilities,\nbut the mechanisms underlying relational reasoning in different learning\nregimes remain poorly understood. In this work, we investigate how transformers\nperform a classic relational reasoning task from the Psychology literature,\n\\textit{transitive inference}, which requires inference about indirectly\nrelated items by integrating information across observed adjacent item pairs\n(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior\nacross two distinct learning regimes: in-weights learning (IWL), where models\nstore information in network parameters, and in-context learning (ICL), where\nmodels flexibly utilize information presented within the input sequence. Our\nfindings reveal that IWL naturally induces a generalization bias towards\ntransitive inference, despite being trained only on adjacent items, whereas ICL\nmodels trained solely on adjacent items do not generalize transitively.\nMechanistic analysis shows that ICL models develop induction circuits that\nimplement a simple match-and-copy strategy that performs well at relating\nadjacent pairs, but does not encoding hierarchical relationships among\nindirectly related items. Interestingly, when pre-trained on in-context linear\nregression tasks, transformers successfully exhibit in-context generalizable\ntransitive inference. Moreover, like IWL, they display both \\textit{symbolic\ndistance} and \\textit{terminal item effects} characteristic of human and animal\nperformance, without forming induction circuits. These results suggest that\npre-training on tasks with underlying structure promotes the development of\nrepresentations that can scaffold in-context relational reasoning.", "AI": {"tldr": "This paper explores the mechanisms of relational reasoning in transformers by studying transitive inference under two learning regimes: \"in-weights learning\" (IWL) and \"in-context learning\" (ICL).", "motivation": "Investigate how transformers perform relational reasoning tasks like transitive inference, a task well-documented in psychological studies, and understand differences in reasoning across learning regimes.", "method": "Analyzed transformers' ability to perform transitive inference in IWL and ICL settings, comparing behavior when trained solely on adjacent items. Mechanistic analysis was conducted, and experiments included pre-training on in-context linear regression tasks.", "result": "IWL models exhibited natural bias towards transitive inference and generalized well, while ICL models trained on adjacent items failed to generalize. Pre-training on structured tasks improved ICL transitive inference performance, showing symbolic distance and terminal item effects akin to human and animal cognition.", "conclusion": "Pre-training on structured tasks aids in developing representations that facilitate relational reasoning within the in-context learning paradigm, differing from the mechanisms observed in IWL."}}
{"id": "2506.05329", "pdf": "https://arxiv.org/pdf/2506.05329", "abs": "https://arxiv.org/abs/2506.05329", "authors": ["Guido Imbens", "Chao Qin", "Stefan Wager"], "title": "Admissibility of Completely Randomized Trials: A Large-Deviation Approach", "categories": ["stat.ML", "cs.LG", "econ.EM"], "comment": "A one-page abstract of this work will appear at the 26th ACM\n  Conference on Economics and Computation (EC'25)", "summary": "When an experimenter has the option of running an adaptive trial, is it\nadmissible to ignore this option and run a non-adaptive trial instead? We\nprovide a negative answer to this question in the best-arm identification\nproblem, where the experimenter aims to allocate measurement efforts\njudiciously to confidently deploy the most effective treatment arm. We find\nthat, whenever there are at least three treatment arms, there exist simple\nadaptive designs that universally and strictly dominate non-adaptive completely\nrandomized trials. This dominance is characterized by a notion called\nefficiency exponent, which quantifies a design's statistical efficiency when\nthe experimental sample is large. Our analysis focuses on the class of batched\narm elimination designs, which progressively eliminate underperforming arms at\npre-specified batch intervals. We characterize simple sufficient conditions\nunder which these designs universally and strictly dominate completely\nrandomized trials. These results resolve the second open problem posed in Qin\n[2022].", "AI": {"tldr": "This paper argues that adaptive trial designs dominate non-adaptive trials in best-arm identification problems when three or more treatment arms are involved.", "motivation": "To address whether non-adaptive trials are admissible when adaptive options exist in best-arm identification scenarios.", "method": "Analysis of batched arm elimination designs and efficiency exponent to compare adaptive and non-adaptive trials.", "result": "Adaptive designs universally and strictly outperform non-adaptive randomized trials in cases with three or more treatment arms.", "conclusion": "Adopting adaptive designs leads to higher statistical efficiency and resolves an open problem from Qin [2022]."}}
{"id": "2506.04478", "pdf": "https://arxiv.org/pdf/2506.04478", "abs": "https://arxiv.org/abs/2506.04478", "authors": ["Hadi Hosseini", "Samarth Khanna", "Ronak Singh"], "title": "Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences", "categories": ["cs.AI", "cs.GT", "econ.TH", "I.2.6; I.2.11; J.4"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has driven progress in reasoning\ntasks -- from program synthesis to scientific hypothesis generation -- yet\ntheir ability to handle ranked preferences and structured algorithms in\ncombinatorial domains remains underexplored. We study matching markets, a core\nframework behind applications like resource allocation and ride-sharing, which\nrequire reconciling individual ranked preferences to ensure stable outcomes. We\nevaluate several state-of-the-art models on a hierarchy of preference-based\nreasoning tasks -- ranging from stable-matching generation to instability\ndetection, instability resolution, and fine-grained preference queries -- to\nsystematically expose their logical and algorithmic limitations in handling\nranked inputs. Surprisingly, even top-performing models with advanced reasoning\nstruggle to resolve instability in large markets, often failing to identify\nblocking pairs or execute algorithms iteratively. We further show that\nparameter-efficient fine-tuning (LoRA) significantly improves performance in\nsmall markets, but fails to bring about a similar improvement on large\ninstances, suggesting the need for more sophisticated strategies to improve\nLLMs' reasoning with larger-context inputs.", "AI": {"tldr": "This paper explores large language models' (LLMs) limitations in handling ranked preferences in matching markets, with several reasoning tasks highlighting logical and algorithmic gaps.", "motivation": "To understand and evaluate how well LLMs can handle ranked preferences and algorithms within combinatorial domains like matching markets.", "method": "The researchers tested state-of-the-art LLMs on a hierarchy of preference-based reasoning tasks such as stable-matching generation, instability detection and resolution, and fine-grained preference queries.", "result": "LLMs perform poorly in resolving instabilities in large matching markets. Fine-tuning methods like LoRA improve performance in small markets, but not in larger contexts.", "conclusion": "Advanced strategies are needed to improve LLMs' reasoning over ranked preferences and handling larger-context scenarios."}}
{"id": "2506.04645", "pdf": "https://arxiv.org/pdf/2506.04645", "abs": "https://arxiv.org/abs/2506.04645", "authors": ["Ege Erdil"], "title": "Inference economics of language models", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We develop a theoretical model that addresses the economic trade-off between\ncost per token versus serial token generation speed when deploying LLMs for\ninference at scale. Our model takes into account arithmetic, memory bandwidth,\nnetwork bandwidth and latency constraints; and optimizes over different\nparallelism setups and batch sizes to find the ones that optimize serial\ninference speed at a given cost per token. We use the model to compute Pareto\nfrontiers of serial speed versus cost per token for popular language models.", "AI": {"tldr": "The paper presents a model to optimize the trade-off between cost per token and serial inference speed when using large language models (LLMs) at scale.", "motivation": "The motivation is to address the challenge of balancing cost efficiency and inference speed in deploying LLMs for scalable applications, which is crucial for maximizing practical utility.", "method": "The paper employs a theoretical model considering constraints like arithmetic, memory, network bandwidth, and latency to optimize parallelism setups and batch sizes.", "result": "The study computes Pareto frontiers showcasing the relationship between serial inference speed and cost efficiency for popular language models.", "conclusion": "The paper offers actionable insights into optimizing LLM inference setups for better performance and economic feasibility."}}
{"id": "2506.04367", "pdf": "https://arxiv.org/pdf/2506.04367", "abs": "https://arxiv.org/abs/2506.04367", "authors": ["Jubayer Ahmed Bhuiyan Shawon", "Hasan Mahmud", "Kamrul Hasan"], "title": "Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, 6 tables", "summary": "Sign Language Recognition (SLR) involves the automatic identification and\nclassification of sign gestures from images or video, converting them into text\nor speech to improve accessibility for the hearing-impaired community. In\nBangladesh, Bangla Sign Language (BdSL) serves as the primary mode of\ncommunication for many individuals with hearing impairments. This study\nfine-tunes state-of-the-art video transformer architectures -- VideoMAE, ViViT,\nand TimeSformer -- on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset\nwith 60 frequent signs. We standardized the videos to 30 FPS, resulting in\n9,307 user trial clips. To evaluate scalability and robustness, the models were\nalso fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401\nsign classes. Additionally, we benchmark performance against public datasets,\nincluding LSA64 and WLASL. Data augmentation techniques such as random\ncropping, horizontal flipping, and short-side scaling were applied to improve\nmodel robustness. To ensure balanced evaluation across folds during model\nselection, we employed 10-fold stratified cross-validation on the training set,\nwhile signer-independent evaluation was carried out using held-out test data\nfrom unseen users U4 and U8. Results show that video transformer models\nsignificantly outperform traditional machine learning and deep learning\napproaches. Performance is influenced by factors such as dataset size, video\nquality, frame distribution, frame rate, and model architecture. Among the\nmodels, the VideoMAE variant (MCG-NJU/videomae-base-finetuned-kinetics)\nachieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60\ndataset and 81.04% on the front-facing signs of BdSLW401 -- demonstrating\nstrong potential for scalable and accurate BdSL recognition.", "AI": {"tldr": "The paper fine-tunes state-of-the-art video transformer models to recognize Bangladesh Sign Language (BdSL), achieving high accuracy, particularly with the VideoMAE model.", "motivation": "To improve accessibility for the hearing-impaired community in Bangladesh by automating the recognition of Bangla Sign Language (BdSL) using advanced video transformer models.", "method": "Fine-tuning state-of-the-art video transformer architectures (VideoMAE, ViViT, TimeSformer) on two BdSL datasets (BdSLW60 and BdSLW401) using data augmentation, 10-fold stratified cross-validation, and signer-independent evaluation.", "result": "The VideoMAE model achieved 95.5% accuracy on the BdSLW60 dataset and 81.04% on BdSLW401, outperforming traditional ML and deep learning methods. Results are dependent on dataset size, video quality, frame distribution, frame rate, and model architecture.", "conclusion": "Video transformer models, particularly VideoMAE, show strong potential for scalable and robust recognition of BdSL, paving the way for improved communication accessibility for the hearing-impaired community."}}
{"id": "2506.04408", "pdf": "https://arxiv.org/pdf/2506.04408", "abs": "https://arxiv.org/abs/2506.04408", "authors": ["Wesley Scivetti", "Tatsuya Aoyama", "Ethan Wilcox", "Nathan Schneider"], "title": "Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humans have a remarkable ability to acquire and understand grammatical\nphenomena that are seen rarely, if ever, during childhood. Recent evidence\nsuggests that language models with human-scale pretraining data may possess a\nsimilar ability by generalizing from frequent to rare constructions. However,\nit remains an open question how widespread this generalization ability is, and\nto what extent this knowledge extends to meanings of rare constructions, as\nopposed to just their forms. We fill this gap by testing human-scale\ntransformer language models on their knowledge of both the form and meaning of\nthe (rare and quirky) English LET-ALONE construction. To evaluate our LMs we\nconstruct a bespoke synthetic benchmark that targets syntactic and semantic\nproperties of the construction. We find that human-scale LMs are sensitive to\nform, even when related constructions are filtered from the dataset. However,\nhuman-scale LMs do not make correct generalizations about LET-ALONE's meaning.\nThese results point to an asymmetry in the current architectures' sample\nefficiency between language form and meaning, something which is not present in\nhuman language learners.", "AI": {"tldr": "The paper investigates language models' ability to generalize rare grammatical phenomena, focusing on the LET-ALONE construction, and finds asymmetries in how they grasp form versus meaning.", "motivation": "To explore the extent to which human-scale language models generalize to rare grammatical constructs and their meanings, compared to humans.", "method": "A synthetic benchmark was created to test human-scale transformer language models on the form and meaning of the LET-ALONE construction, accounting for syntactic and semantic properties.", "result": "Language models showed sensitivity to grammatical forms but failed to generalize correctly to the meanings of the LET-ALONE construction.", "conclusion": "Current language models demonstrate an imbalance in learning efficiency between language form and meaning, which differs from the capabilities of human learners."}}
{"id": "2506.04987", "pdf": "https://arxiv.org/pdf/2506.04987", "abs": "https://arxiv.org/abs/2506.04987", "authors": ["Zanis Ali Khan", "Aayush Garg", "Qiang Tang"], "title": "A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint has been accepted in ARES AI&CCPS (International Workshop on\n  Artificial Intelligence, Cyber and Cyber-Physical Security)", "summary": "Software vulnerabilities pose significant security threats, requiring\neffective mitigation. While Automated Program Repair (APR) has advanced in\nfixing general bugs, vulnerability patching, a security-critical aspect of APR\nremains underexplored. This study investigates pre-trained language models,\nCodeBERT and CodeT5, for automated vulnerability patching across six datasets\nand four languages. We evaluate their accuracy and generalization to unknown\nvulnerabilities. Results show that while both models face challenges with\nfragmented or sparse context, CodeBERT performs comparatively better in such\nscenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.\nCodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned\nmodels on both in-distribution (trained) and out-of-distribution (unseen)\ndatasets. While fine-tuning improves in-distribution performance, models\nstruggle to generalize to unseen data, highlighting challenges in robust\nvulnerability detection. This study benchmarks model performance, identifies\nlimitations in generalization, and provides actionable insights to advance\nautomated vulnerability patching for real-world security applications.", "AI": {"tldr": "This paper explores the use of CodeBERT and CodeT5 for automated vulnerability patching, demonstrating mixed model performances and difficulties in generalizing to unseen vulnerabilities.", "motivation": "To address the gap in automated vulnerability patching, an underexplored yet critical aspect of software security in Automated Program Repair (APR).", "method": "The study employs pre-trained language models CodeBERT and CodeT5, testing their performance across six datasets and four programming languages with evaluations in in-distribution and out-of-distribution contexts.", "result": "CodeBERT showed better handling of fragmented contexts, while CodeT5 excelled in complex patterns and scalability. Fine-tuning improved trained data performance but failed to generalize well to unseen vulnerabilities.", "conclusion": "The study benchmarks model performances, identifies limitations in generalization, and offers insights for improving automated vulnerability patching for real-world applications."}}
{"id": "2506.04547", "pdf": "https://arxiv.org/pdf/2506.04547", "abs": "https://arxiv.org/abs/2506.04547", "authors": ["Jonathan Tirado", "Aida Parvaresh", "Burcu Seyido\u011flu", "Darryl A. Bedford", "Jonas J\u00f8rgensen", "Ahmad Rafsanjani"], "title": "Multimodal Limbless Crawling Soft Robot with a Kirigami Skin", "categories": ["cs.RO"], "comment": "Cyborg and Bionic Systems (2025)", "summary": "Limbless creatures can crawl on flat surfaces by deforming their bodies and\ninteracting with asperities on the ground, offering a biological blueprint for\ndesigning efficient limbless robots. Inspired by this natural locomotion, we\npresent a soft robot capable of navigating complex terrains using a combination\nof rectilinear motion and asymmetric steering gaits. The robot is made of a\npair of antagonistic inflatable soft actuators covered with a flexible kirigami\nskin with asymmetric frictional properties. The robot's rectilinear locomotion\nis achieved through cyclic inflation of internal chambers with precise phase\nshifts, enabling forward progression. Steering is accomplished using an\nasymmetric gait, allowing for both in-place rotation and wide turns. To\nvalidate its mobility in obstacle-rich environments, we tested the robot in an\narena with coarse substrates and multiple obstacles. Real-time feedback from\nonboard proximity sensors, integrated with a human-machine interface (HMI),\nallowed adaptive control to avoid collisions. This study highlights the\npotential of bioinspired soft robots for applications in confined or\nunstructured environments, such as search-and-rescue operations, environmental\nmonitoring, and industrial inspections.", "AI": {"tldr": "The research introduces a bioinspired soft robot that uses rectilinear motion and asymmetric gaits to navigate complex terrains, equipped with proximity sensors for adaptive control.", "motivation": "The paper aims to emulate limbless biological locomotion to design a robot that efficiently navigates confined and unstructured terrains.", "method": "The robot employs inflatable soft actuators covered with kirigami skin for movement and integrates onboard sensors for real-time feedback and adaptability.", "result": "The robot demonstrated effective locomotion and obstacle avoidance in an environment with coarse substrates and obstacles.", "conclusion": "Bioinspired soft robots show strong potential for applications like search-and-rescue, environmental monitoring, and industrial inspection in challenging terrains."}}
{"id": "2506.04281", "pdf": "https://arxiv.org/pdf/2506.04281", "abs": "https://arxiv.org/abs/2506.04281", "authors": ["Xu Zheng", "Chaohao Lin", "Sipeng Chen", "Zhuomin Chen", "Jimeng Shi", "Wei Cheng", "Jayantha Obeysekera", "Jason Liu", "Dongsheng Luo"], "title": "SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida", "categories": ["cs.LG"], "comment": "60 Pages", "summary": "Forecasting compound floods presents a significant challenge due to the\nintricate interplay of meteorological, hydrological, and oceanographic factors.\nAnalyzing compound floods has become more critical as the global climate\nincreases flood risks. Traditional physics-based methods, such as the\nHydrologic Engineering Center's River Analysis System, are often\ntime-inefficient. Machine learning has recently demonstrated promise in both\nmodeling accuracy and computational efficiency. However, the scarcity of\ncomprehensive datasets currently hinders systematic analysis. Existing\nwater-related datasets are often limited by a sparse network of monitoring\nstations and incomplete coverage of relevant factors. To address this\nchallenge, we introduce SF2Bench, a comprehensive time series collection on\ncompound floods in South Florida, which integrates four key factors: tide,\nrainfall, groundwater, and human management activities (gate and pump\ncontrolling). This integration allows for a more detailed analysis of the\nindividual contributions of these drivers to compound flooding and informs the\ndevelopment of improved flood forecasting approaches. To comprehensively\nevaluate the potential of various modeling paradigms, we assess the performance\nof six categories of methods, encompassing Multilayer Perceptrons,\nConvolutional Neural Networks, Recurrent Neural Networks, Graph Neural\nNetworks, Transformers, and Large Language Models. We verified the impact of\ndifferent key features on flood forecasting through experiments. Our analysis\nexamines temporal and spatial aspects, providing insights into the influence of\nhistorical data and spatial dependencies. The varying performance across these\napproaches underscores the diverse capabilities of each in capturing complex\ntemporal and spatial dependencies inherent in compound floods.", "AI": {"tldr": "Compound floods are challenging to forecast due to the interaction of various environmental factors. The paper introduces SF2Bench, a comprehensive dataset to analyze these floods and evaluates the performance of six machine learning methods.", "motivation": "Compound floods are becoming more critical in light of increasing global flood risks, yet forecasting them is hindered by the limitations of existing datasets and time-intensive physics-based methods.", "method": "The paper introduces the SF2Bench dataset, which incorporates tide, rainfall, groundwater, and human management activities. It evaluates six machine learning methods to analyze temporal and spatial aspects affecting flood forecasting.", "result": "The analysis highlights the varied capabilities of ML methods in handling complex temporal and spatial dependencies, with experiments verifying the impact of key features on forecasting.", "conclusion": "The integration of multifactorial data in SF2Bench and the comparative evaluation of modeling methods provide a foundation for enhancing flood forecasting approaches and understanding compound flood dynamics."}}
{"id": "2506.04379", "pdf": "https://arxiv.org/pdf/2506.04379", "abs": "https://arxiv.org/abs/2506.04379", "authors": ["Matthew W. Shinkle", "Mark D. Lescroart"], "title": "Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "Accepted to the Mechanistic Interpretability for Vision (MIV)\n  Workshop at the 2025 Conference on Computer Vision and Pattern Recognition\n  (CVPR) conference", "summary": "Deep neural networks (DNNs) trained on visual tasks develop feature\nrepresentations that resemble those in the human visual system. Although\nDNN-based encoding models can accurately predict brain responses to visual\nstimuli, they offer limited insight into the specific features driving these\nresponses. Here, we demonstrate that activation maximization -- a technique\ndesigned to interpret vision DNNs -- can be applied to DNN-based encoding\nmodels of the human brain. We extract and adaptively downsample activations\nfrom multiple layers of a pretrained Inception V3 network, then use linear\nregression to predict fMRI responses. This yields a full image-computable model\nof brain responses. Next, we apply activation maximization to generate images\noptimized for predicted responses in individual cortical voxels. We find that\nthese images contain visual characteristics that qualitatively correspond with\nknown selectivity and enable exploration of selectivity across the visual\ncortex. We further extend our method to whole regions of interest (ROIs) of the\nbrain and validate its efficacy by presenting these images to human\nparticipants in an fMRI study. We find that the generated images reliably drive\nactivity in targeted regions across both low- and high-level visual areas and\nacross subjects. These results demonstrate that activation maximization can be\nsuccessfully applied to DNN-based encoding models. By addressing key\nlimitations of alternative approaches that require natively generative models,\nour approach enables flexible characterization and modulation of responses\nacross the human visual system.", "AI": {"tldr": "This paper applies activation maximization to DNN-based models to better understand brain responses, demonstrating its potential for exploring visual selectivity in human visual cortex using generated images.", "motivation": "To address the challenge of identifying specific visual features driving activity in the human brain, using DNN-based encoding models that currently provide limited interpretability.", "method": "The study adapts activations from a pretrained DNN (Inception V3), predicts fMRI responses using linear regression, and applies activation maximization to generate images optimized for specific brain voxel or ROI responses, validated through fMRI experiments.", "result": "The generated images revealed visual features consistent with known visual selectivity, modulated activity in targeted brain regions across visual areas, and replicated results across different subjects.", "conclusion": "Activation maximization is effective in enhancing the interpretability of DNN-based encoding models, enabling a flexible and generative characterization of human visual system responses without relying solely on natively generative models."}}
{"id": "2506.04339", "pdf": "https://arxiv.org/pdf/2506.04339", "abs": "https://arxiv.org/abs/2506.04339", "authors": ["Kiyam Lin", "Alicja Polanska", "Davide Piras", "Alessio Spurio Mancini", "Jason D. McEwen"], "title": "Savage-Dickey density ratio estimation with normalizing flows for Bayesian model comparison", "categories": ["astro-ph.CO", "astro-ph.IM", "stat.ML"], "comment": "9 pages, 1 figure. Submitted to the Open Journal of Astrophysics.\n  Codes available at https://github.com/astro-informatics/harmonic", "summary": "A core motivation of science is to evaluate which scientific model best\nexplains observed data. Bayesian model comparison provides a principled\nstatistical approach to comparing scientific models and has found widespread\napplication within cosmology and astrophysics. Calculating the Bayesian\nevidence is computationally challenging, especially as we continue to explore\nincreasingly more complex models. The Savage-Dickey density ratio (SDDR)\nprovides a method to calculate the Bayes factor (evidence ratio) between two\nnested models using only posterior samples from the super model. The SDDR\nrequires the calculation of a normalised marginal distribution over the extra\nparameters of the super model, which has typically been performed using\nclassical density estimators, such as histograms. Classical density estimators,\nhowever, can struggle to scale to high-dimensional settings. We introduce a\nneural SDDR approach using normalizing flows that can scale to settings where\nthe super model contains a large number of extra parameters. We demonstrate the\neffectiveness of this neural SDDR methodology applied to both toy and realistic\ncosmological examples. For a field-level inference setting, we show that Bayes\nfactors computed for a Bayesian hierarchical model (BHM) and simulation-based\ninference (SBI) approach are consistent, providing further validation that SBI\nextracts as much cosmological information from the field as the BHM approach.\nThe SDDR estimator with normalizing flows is implemented in the open-source\nharmonic Python package.", "AI": {"tldr": "The paper proposes a Bayesian model comparison method using the Savage-Dickey density ratio (SDDR) with normalizing flows to handle high-dimensional settings, validated with cosmological data.", "motivation": "To overcome computational challenges in Bayesian evidence calculations for comparing increasingly complex scientific models, especially in high-dimensional settings.", "method": "Introduces a neural approach to SDDR using normalizing flows that scales well to high-dimensional settings and is applied to both toy and realistic cosmological examples.", "result": "Demonstrates that the proposed neural SDDR method effectively calculates Bayes factors, validates consistency with Bayesian hierarchical models and simulation-based inference, and is implemented in an open-source package.", "conclusion": "The neural SDDR with normalizing flows shows promise for scalable Bayesian model comparison in complex settings, offering consistent and efficient results for cosmological applications."}}
{"id": "2506.04481", "pdf": "https://arxiv.org/pdf/2506.04481", "abs": "https://arxiv.org/abs/2506.04481", "authors": ["Jiayu Liu", "Zhenya Huang", "Wei Dai", "Cheng Cheng", "Jinze Wu", "Jing Sha", "Song Li", "Qi Liu", "Shijin Wang", "Enhong Chen"], "title": "CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective", "categories": ["cs.AI"], "comment": null, "summary": "Although large language models (LLMs) show promise in solving complex\nmathematical tasks, existing evaluation paradigms rely solely on a coarse\nmeasure of overall answer accuracy, which are insufficient for assessing their\nauthentic capabilities. In this paper, we propose \\textbf{CogMath}, which\ncomprehensively assesses LLMs' mathematical abilities through the lens of human\ncognition. Specifically, inspired by psychological theories, CogMath formalizes\nhuman reasoning process into 3 stages: \\emph{problem comprehension},\n\\emph{problem solving}, and \\emph{solution summarization}. Within these stages,\nwe investigate perspectives such as numerical calculation, knowledge, and\ncounterfactuals, and design a total of 9 fine-grained evaluation dimensions. In\neach dimension, we develop an ``\\emph{Inquiry}-\\emph{Judge}-\\emph{Reference}''\nmulti-agent system to generate inquiries that assess LLMs' mastery from this\ndimension. An LLM is considered to truly master a problem only when excelling\nin all inquiries from the 9 dimensions. By applying CogMath on three\nbenchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs\nare overestimated by 30\\%-40\\%. Moreover, we locate their strengths and\nweaknesses across specific stages/dimensions, offering in-depth insights to\nfurther enhance their reasoning abilities.", "AI": {"tldr": "CogMath introduces a nuanced evaluation strategy for assessing the mathematical capabilities of large language models (LLMs), revealing significant overestimations in current benchmarks.", "motivation": "Existing evaluation methods for LLMs' mathematical abilities rely on overall accuracy, failing to capture the nuances of their authentic reasoning capabilities.", "method": "CogMath bases its methodology on human cognitive processes, measured through 9 detailed dimensions inspired by three stages of reasoning: problem comprehension, solving, and solution summarization. A multi-agent system, \"Inquiry-Judge-Reference,\" evaluates models within these dimensions.", "result": "CogMath tests 7 mainstream LLMs across three benchmarks and finds their mathematical abilities are overestimated by 30%-40%, while identifying strengths and weaknesses in specific reasoning stages.", "conclusion": "CogMath's fine-grained evaluation provides deeper insights into LLMs' cognitive reasoning and opens pathways for improving their mathematical and logical capabilities."}}
{"id": "2506.04763", "pdf": "https://arxiv.org/pdf/2506.04763", "abs": "https://arxiv.org/abs/2506.04763", "authors": ["Shuai Lu"], "title": "A highly scalable numerical framework for reservoir simulation on UG4 platform", "categories": ["physics.comp-ph", "cs.DC"], "comment": null, "summary": "The modeling and simulation of multiphase fluid flow receive significant\nattention in reservoir engineering. Many time discretization schemes for\nmultiphase flow equations are either explicit or semi-implicit, relying on the\ndecoupling between the saturation equation and the pressure equation. In this\nstudy, we delve into a fully coupled and fully implicit framework for\nsimulating multiphase flow in heterogeneous porous media, considering gravity\nand capillary effects. We utilize the Vertex-Centered Finite Volume Method for\nspatial discretization and propose an efficient implementation of interface\nconditions for heterogeneous porous media within the current scheme. Notably,\nwe introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error\nestimator, adapted for the first time to multiphase flow problems. To solve the\nresulting linear system, we employ the BiCGSTAB method with the Geometric\nMultigrid (GMG) preconditioner. The implementations of models and methods are\nbased on the open-source software: UG4. The results from parallel computations\non the supercomputer demonstrate that the scalability of our proposed framework\nis sufficient, supporting a scale of thousands of processors with Degrees of\nFreedom (DoF) extending up to billions.", "AI": {"tldr": "The paper focuses on a fully coupled and implicit simulation model for multiphase flow in heterogeneous porous media, employing advanced numerical techniques and achieving scalability on supercomputers.", "motivation": "The motivation is to improve the simulation accuracy and computational efficiency for modeling multiphase fluid flow in reservoir engineering, particularly under heterogeneous porous media conditions.", "method": "The authors employ a Vertex-Centered Finite Volume Method for spatial discretization and the novel Linearly Implicit Extrapolation Method (LIMEX) with an error estimator for time discretization. The linear system is solved using the BiCGSTAB method with a Geometric Multigrid (GMG) preconditioner, implemented via the open-source UG4 software.", "result": "Parallel computations on a supercomputer demonstrate that the framework can handle thousands of processors and billions of Degrees of Freedom (DoF) while maintaining sufficient scalability.", "conclusion": "The study presents a robust and efficient framework for simulating multiphase flow, showing promise for large-scale reservoir models and supercomputing applications."}}
{"id": "2506.04409", "pdf": "https://arxiv.org/pdf/2506.04409", "abs": "https://arxiv.org/abs/2506.04409", "authors": ["Lev Morozov", "Aleksandr Mogilevskii", "Alexander Shirnin"], "title": "Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes EmoRAG, a system designed to detect perceived emotions\nin text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We\nfocus on predicting the perceived emotions of the speaker from a given text\nsnippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,\nand disgust. Our approach does not require additional model training and only\nuses an ensemble of models to predict emotions. EmoRAG achieves results\ncomparable to the best performing systems, while being more efficient,\nscalable, and easier to implement.", "AI": {"tldr": "This paper presents EmoRAG, a system for multi-label emotion detection in text, achieving competitive results efficiently without additional model training.", "motivation": "The paper aims to address the challenge of detecting perceived emotions in text, specifically for SemEval-2025 Task 11 Subtask A, by providing an efficient and scalable solution.", "method": "The proposed method leverages an ensemble of models to predict emotions, without requiring additional training, focusing on efficiency and ease of implementation.", "result": "EmoRAG attains results comparable to the top systems in the task, proving its effectiveness in emotion detection.", "conclusion": "EmoRAG provides a competitive, efficient, and scalable approach for multi-label emotion detection in text, making it a practical alternative to more complex systems requiring extensive training."}}
{"id": "2506.04989", "pdf": "https://arxiv.org/pdf/2506.04989", "abs": "https://arxiv.org/abs/2506.04989", "authors": ["Dumitran Adrian Marius", "Dita Radu"], "title": "BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment", "categories": ["cs.SE"], "comment": "9 pages Preprint ACCEPTED at BBGI (ITS Workshop)", "summary": "Accessing quality preparation and feedback for the Romanian Bacalaureat exam\nis challenging, particularly for students in remote or underserved areas. This\npaper introduces BacPrep, an experimental online platform exploring Large\nLanguage Model (LLM) potential for automated assessment, aiming to offer a\nfree, accessible resource. Using official exam questions from the last 5 years,\nBacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb\n2025), guided by official grading schemes, to provide experimental feedback.\nCurrently operational, its primary research function is collecting student\nsolutions and LLM outputs. This focused dataset is vital for planned expert\nvalidation to rigorously evaluate the feasibility and accuracy of this\ncutting-edge LLM in the specific Bacalaureat context before reliable\ndeployment. We detail the design, data strategy, status, validation plan, and\nethics.", "AI": {"tldr": "This paper presents BacPrep, an experimental online platform utilizing Google's Gemini 2.0 Flash LLM for automated assessment and feedback on Romanian Bacalaureat exam questions.", "motivation": "Students in underserved and remote areas face difficulties accessing quality preparation and feedback for the Romanian Bacalaureat.", "method": "The platform uses Gemini 2.0 Flash LLM, guided by official grading schemes, with a focus on collecting student solutions and model outputs for expert validation.", "result": "BacPrep provides experimental feedback on exam solutions and gathers data to assess the feasibility and accuracy of the LLM.", "conclusion": "The platform is operational, with plans for rigorous validation to ensure the model's reliability before full deployment."}}
{"id": "2506.04577", "pdf": "https://arxiv.org/pdf/2506.04577", "abs": "https://arxiv.org/abs/2506.04577", "authors": ["Farshad Haghgoo Daryakenari", "Tara Farizeh"], "title": "A Novel Transformer-Based Method for Full Lower-Limb Joint Angles and Moments Prediction in Gait Using sEMG and IMU data", "categories": ["cs.RO"], "comment": "10 pages, 4 figures", "summary": "This study presents a transformer-based deep learning framework for the\nlong-horizon prediction of full lower-limb joint angles and joint moments using\nsurface electromyography (sEMG) and inertial measurement unit (IMU) signals.\nTwo separate Transformer Neural Networks (TNNs) were designed: one for\nkinematic prediction and one for kinetic prediction. The model was developed\nwith real-time application in mind, using only wearable sensors suitable for\noutside-laboratory use. Two prediction horizons were considered to evaluate\nshort- and long-term performance. The network achieved high accuracy in both\ntasks, with Spearman correlation coefficients exceeding 0.96 and R-squared\nscores above 0.92 across all joints. Notably, the model consistently\noutperformed a recent benchmark method in joint angle prediction, reducing RMSE\nerrors by an order of magnitude. The results confirmed the complementary role\nof sEMG and IMU signals in capturing both kinematic and kinetic information.\nThis work demonstrates the potential of transformer-based models for real-time,\nfull-limb biomechanical prediction in wearable and robotic applications, with\nfuture directions including input minimization and modality-specific weighting\nstrategies to enhance model efficiency and accuracy.", "AI": {"tldr": "The paper proposes a Transformer-based model for predicting lower-limb joint angles and moments using wearable sensors, achieving high accuracy and outperforming benchmarks.", "motivation": "Develop an accurate and real-time predictive framework for lower-limb biomechanics using wearable sensors suitable for outside-laboratory applications.", "method": "Two Transformer Neural Networks (TNNs) were designed to predict kinematic and kinetic information using signals from sEMG and IMU sensors. Performance was evaluated across short- and long-term horizons.", "result": "Achieved Spearman correlation coefficients over 0.96 and R-squared scores above 0.92, outperforming benchmark methods with significant error reductions.", "conclusion": "Transformer-based models are effective for wearable, real-time biomechanical predictions, with future improvements targeting efficiency and accuracy enhancements."}}
{"id": "2506.04282", "pdf": "https://arxiv.org/pdf/2506.04282", "abs": "https://arxiv.org/abs/2506.04282", "authors": ["Runxiang Wang", "Boxiao Wang", "Kai Li", "Yifan Zhang", "Jian Cheng"], "title": "DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic regression is a fundamental tool for discovering interpretable\nmathematical expressions from data, with broad applications across scientific\nand engineering domains. Recently, large language models (LLMs) have\ndemonstrated strong performance in this task, leveraging embedded scientific\npriors and reasoning capabilities to surpass traditional methods. However,\nexisting LLM-based approaches, such as LLM-SR, often over-rely on internal\npriors, lacking explicit data understanding and systematic reflection during\nequation generation. To address these limitations, we propose DrSR (Dual\nReasoning Symbolic Regression), a framework that combines data-driven insight\nwith reflective learning to enhance both robustness and discovery capability.\nSpecifically, DrSR guides LLMs to analyze structural relationships (e.g.,\nmonotonicity, nonlinearity, and correlation) within the data to generate\nstructured descriptions. Simultaneously, it monitors equation performance and\nestablishes a feedback loop to refine subsequent generations. By integrating\ndata understanding and generation reflection in a closed loop, DrSR enables\nmore efficient exploration of the symbolic expression space. Experiments across\ninterdisciplinary datasets in physics, chemistry, biology, and materials\nscience demonstrate that DrSR substantially improves the valid equation rate\nand consistently outperforms both classical and recent LLM-based methods in\nterms of accuracy, generalization, and search efficiency. These results\nunderscore its potential for scientific equation discovery.", "AI": {"tldr": "DrSR (Dual Reasoning Symbolic Regression) is a framework that enhances symbolic regression by combining data-driven insights and reflective learning, outperforming traditional and LLM-based methods in multiple domains.", "motivation": "The paper aims to address the limitations of LLM-based symbolic regression methods, which tend to rely excessively on internal priors and lack explicit data analysis and systematic feedback.", "method": "The proposed method, DrSR, guides large language models to analyze data's structural relationships and uses a feedback loop to refine equation generation iteratively.", "result": "DrSR consistently achieves better accuracy, generalization, and efficiency compared to classical and modern LLM-based symbolic regression methods across datasets from various scientific domains.", "conclusion": "DrSR demonstrates substantial improvements in equation discovery, emphasizing its utility in scientific applications by integrating data understanding and reflective learning."}}
{"id": "2506.04536", "pdf": "https://arxiv.org/pdf/2506.04536", "abs": "https://arxiv.org/abs/2506.04536", "authors": ["Luca Ghafourpour", "Valentin Duruisseaux", "Bahareh Tolooshams", "Philip H. Wong", "Costas A. Anastassiou", "Anima Anandkumar"], "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Characterizing the diverse computational properties of human neurons via\nmultimodal electrophysiological, transcriptomic, and morphological data\nprovides the foundation for constructing and validating bio-realistic neuron\nmodels that can advance our understanding of fundamental mechanisms underlying\nbrain function. However, current modeling approaches remain constrained by the\nlimited availability and intrinsic variability of experimental neuronal data.\nTo capture variability, ensembles of deterministic models are often used, but\nare difficult to scale as model generation requires repeating computationally\nexpensive optimization for each neuron. While deep learning is becoming\nincreasingly relevant in this space, it fails to capture the full biophysical\ncomplexity of neurons, their nonlinear voltage dynamics, and variability. To\naddress these shortcomings, we introduce NOBLE, a neural operator framework\nthat learns a mapping from a continuous frequency-modulated embedding of\ninterpretable neuron features to the somatic voltage response induced by\ncurrent injection. Trained on data generated from biophysically realistic\nneuron models, NOBLE predicts distributions of neural dynamics accounting for\nthe intrinsic experimental variability. Unlike conventional bio-realistic\nneuron models, interpolating within the embedding space offers models whose\ndynamics are consistent with experimentally observed responses. NOBLE is the\nfirst scaled-up deep learning framework validated on real experimental data,\nenabling efficient generation of synthetic neurons that exhibit trial-to-trial\nvariability and achieve a $4200\\times$ speedup over numerical solvers. To this\nend, NOBLE captures fundamental neural properties, opening the door to a better\nunderstanding of cellular composition and computations, neuromorphic\narchitectures, large-scale brain circuits, and general neuroAI applications.", "AI": {"tldr": "The paper introduces NOBLE, a neural operator framework that predicts neural dynamics while addressing variability and computational efficiency limitations in current neuron modeling approaches.", "motivation": "Existing neuron modeling methods are restricted by limited experimental data, intrinsic variability, and computationally intensive processes. Deep learning methods fail to effectively capture the biophysical complexity of neurons.", "method": "NOBLE uses a neural operator framework to map neuron characteristics into somatic voltage responses. It relies on a frequency-modulated embedding trained on data from biophysically realistic models and interpolates within this space for consistency.", "result": "NOBLE achieves a 4200x speedup compared to numerical solvers and is validated using real experimental data. It generates synthetic neurons that incorporate trial-to-trial variability and align with experimentally observed dynamics.", "conclusion": "NOBLE represents a significant advancement in scalable, bio-realistic neuron modeling, enhancing our ability to study brain function, cellular computations, and enabling broader neuroAI applications."}}
