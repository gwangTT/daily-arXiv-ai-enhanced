<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 57]
- [cs.CV](#cs.CV) [Total: 81]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.SE](#cs.SE) [Total: 16]
- [q-bio.NC](#q-bio.NC) [Total: 5]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [math.NA](#math.NA) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CC](#cs.CC) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CY](#cs.CY) [Total: 10]
- [eess.SP](#eess.SP) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: The paper revisits binary decision diagrams to generate a large Connect-Four solution look-up table, achieving strong results efficiently.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create a comprehensive look-up table for Connect-Four solutions, overcoming the belief that such a table was infeasible despite the game being mathematically solved.

Method: They utilized a symbolic search method based on binary decision diagrams, combining this with efficient implementation on a single CPU with 128 GB memory.

Result: The model generated an 89.6 GB look-up table for the standard 7x6 Connect-Four board in 47 hours, and incorporates an alpha-beta search for optimizing win or loss outcomes.

Conclusion: A strong solution for Connect-Four is feasible using symbolic search and binary decision diagrams, providing comprehensive game strategies and practical compatibility through an open-source tool.

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [2] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: The paper proposes a new computational model called Neurosymbolic Transition Systems to enhance automated reasoning tools by integrating symbolic algorithms with Large Language Models.


<details>
  <summary>Details</summary>
Motivation: Current neurosymbolic AR systems lack strong guarantees and deep integration between neural networks and symbolic reasoning, limiting their potential.

Method: Introduces Neurosymbolic Transition Systems as a model combining symbolic states with intuition, enabling parallel state transitions in symbols and neural reasoning.

Result: Provides a paradigm that can scale logical reasoning while maintaining guarantees of symbolic algorithms.

Conclusion: The new computational model paves the way for building sophisticated neurosymbolic automated reasoning tools with better integration and scalability.

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [3] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: The paper introduces Chat2SPaT, a method utilizing large language models to convert semi-structured traffic control plan descriptions into actionable signal phase and timing (SPaT) outputs with over 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of creating and managing pre-timed traffic signal control plans are labor-intensive and require repetitive manual input, leading to inefficiencies in traffic signal management.

Method: Chat2SPaT uses large language models (LLMs) to interpret ambiguous user descriptions and reformulate them into structured SPaT outputs, complemented by Python scripts for cycle localization and detailed planning.

Result: Experiments using over 300 plan descriptions achieved an accuracy of over 94% for both English and Chinese cases in generating comprehensive traffic signal control plans.

Conclusion: Chat2SPaT streamlines traffic signal plan management by leveraging LLMs, offering a highly accurate, user-friendly, and accessible solution for traffic practitioners and researchers to develop and update traffic control plans effectively.

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [4] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: This paper determines that the weighted arithmetic mean is the mandatory aggregation function under specific fuzzy classification conditions.


<details>
  <summary>Details</summary>
Motivation: To identify an optimal aggregation method for fuzzy classifications where independence, zero unanimity, and specific grouping conditions are maintained.

Method: The paper employs mathematical proofs to establish the weighted arithmetic mean as the unique aggregation function meeting the criteria.

Result: It demonstrates that optimal fuzzy classification aggregation conforming to the stated properties must fundamentally utilize the weighted arithmetic mean.

Conclusion: The findings solidify the weighted arithmetic mean's role as the exclusive aggregation method under these fuzzy classification prerequisites.

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [5] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: Introduces OLG++, a semantic extension of Obligation Logic Graph (OLG), for enhanced legal rule modeling through rich constructs and expressiveness.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing models like LegalRuleML and Obligation Logic Graphs, particularly in nuanced legal settings such as spatial, temporal, and exception hierarchies.

Method: OLG++ enhances graph-based legal modeling with new node and edge types and supports structured reasoning, applied to food business regulations for demonstration.

Result: OLG++ proves more expressive than prior models by effectively handling complex legal rules and queries.

Conclusion: OLG++ offers a sophisticated and more functional framework for representing and querying legal knowledge, exceeding capabilities of previous approaches.

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [6] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: The paper discusses Deep Research Comparator, a platform for evaluating AI agents that generate research reports, emphasizing side-by-side comparisons and detailed feedback.


<details>
  <summary>Details</summary>
Motivation: Deep research agents lack effective evaluation methods for both final reports and intermediate steps.

Method: The authors propose a platform to compare research agents, gather human feedback, and rank their performance, along with a baseline scaffold called Simple Deepresearch.

Result: The platform was tested with data from 17 annotators and three research agents, demonstrating its practical utility.

Conclusion: The platform advances the development and evaluation of AI-driven research agents through a structured comparison and feedback system.

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [7] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: This paper introduces a specialized dataset for augmented reality training, evaluates nine advanced vision-language models, and highlights limitations in fine-grained assembly tasks.


<details>
  <summary>Details</summary>
Motivation: The increasing need for AI-powered smart assistants to interpret multimodal environments in augmented reality training applications.

Method: Developing a tailored dataset featuring vision-language tasks and benchmarking nine state-of-the-art VLMs, such as GPT-4o, against these tasks.

Result: The models struggled with fine-grained assembly tasks, achieving a maximum F1 score of 40.54% in state detection, indicating limitations in current vision-language alignment.

Conclusion: Improved datasets, benchmarks, and focused research are needed to address fine-grained alignment challenges. The paper underscores the social benefits for visually impaired users and provides comprehensive resources to advance research.

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [8] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: The paper addresses implementation issues in deontic modal logic using answer set programming (ASP) for better handling paradoxes.


<details>
  <summary>Details</summary>
Motivation: To solve paradoxes and express concepts in deontic modal logic more effectively by utilizing tools from answer set programming.

Method: Use default negation, strong negation, and global constraints from ASP to express deontic modal logic operators.

Result: Their method allows elegant representation of obligations and impermissibilities, resolving major paradoxes in deontic modal logic.

Conclusion: The proposed ASP-based approach improves the representation and eliminates paradoxes associated with deontic modal logic.

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [9] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: The study focuses on improving visual question answering for dermatology using multimodal models, structured reasoning, and data augmentation, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and interpretability of automated diagnostic tools for dermatology in telemedicine settings.

Method: Fine-tuning open-source multimodal models, employing a structured reasoning layer, and utilizing retrieval-augmented generation to integrate additional context.

Result: Achieved competitive performance, securing second place overall and sixth for a specific submission in a dermatology question-answering competition.

Conclusion: The integration of systematic reasoning and contextual augmentation paves the way for more reliable automated diagnostic systems for skin condition evaluation.

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [10] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: The authors present WikiHowAgent, a multi-agent workflow with LLMs for teaching-learning interactions, creating a large educational dataset and demonstrating effectiveness across domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scalability and diverse course content in virtual educators, as well as limited frameworks for assessing pedagogic quality.

Method: They developed WikiHowAgent, involving teacher-learner agents, an interaction manager, and an evaluator to simulate educational dialogues. It includes a dataset with over 114,000 conversations and 17 domain topics.

Result: The proposed workflow showed effectiveness in various setups and provided insights into LLM capabilities across domains.

Conclusion: WikiHowAgent advances interactive teaching-learning applications, introducing a scalable, open-sourced framework with robust evaluation metrics and a significant dataset.

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [11] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: The paper critiques current AI red teaming practices for focusing too narrowly on model vulnerabilities and proposes a broader framework addressing systemic risks in AI systems.


<details>
  <summary>Details</summary>
Motivation: To address the gap between traditional critical thinking in red teaming and its current narrow application in AI.

Method: Proposes a dual-level framework for AI red teaming: macro-level for system-level risks and micro-level for model-level flaws, incorporating perspectives from systems theory and cybersecurity.

Result: The framework highlights the need for tackling emergent behaviors, systemic vulnerabilities, and integrating multidisciplinary approaches.

Conclusion: Effective AI red teaming must expand its scope to include sociotechnical systems and emergent risks, with involvement from multifunctional teams.

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [12] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: The paper explores using GPT-4o-mini, a large language model (LLM), for generating counterfactual explanations (CFs) in machine learning tasks, showcasing high plausibility, validity, and their utility for improving model performance.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are essential for understanding machine learning predictions, enabling both prevention strategies and robustness through augmented training data. Traditional methods, while useful, have limitations in plausibility and validity.

Method: This study employs GPT-4o-mini to generate CFs in zero-shot and three-shot scenarios. The approach is tested on the AI-Readi dataset for stress prediction and a public dataset for heart disease detection, comparing results to existing methods like DiCE, CFNOW, and NICE.

Result: The proposed LLM-based approach achieved significant plausibility (up to 99%), strong validity (up to 0.99), and comparable sparsity to other methods. Using the CFs as augmented data led to an accuracy improvement of 5% in low-data scenarios.

Conclusion: LLM-generated counterfactual explanations can effectively enhance both the explainability and robustness of machine learning models in clinical and physiological tasks, highlighting the potential of generative models in this application.

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [13] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: The paper introduces SingLoRA, a novel reformulation of the low-rank adaptation approach that addresses scale conflicts found in traditional methods, ensuring stable optimization and reducing parameter count.


<details>
  <summary>Details</summary>
Motivation: The authors observed that traditional LoRA approaches suffer from unstable training dynamics caused by scale disparities between matrix components, limiting performance and optimization stability.

Method: The proposed method, SingLoRA, simplifies weight updates by decomposing a single low-rank matrix into its transpose multiplication. This eliminates inter-matrix scale disparities and decreases parameter requirements.

Result: SingLoRA surpasses previous methods, achieving superior performance in tasks like common sense reasoning and image generation while reducing the parameter budget.

Conclusion: SingLoRA offers stable optimization by construction, ensures efficient model adaptation, outperforms existing methods in accuracy and fidelity, and achieves parameter-efficient fine-tuning benefits.

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [14] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: The paper discusses a framework for formalizing AI measurement to improve evaluation and comparison practices.


<details>
  <summary>Details</summary>
Motivation: The need for a structured approach to measuring AI to enhance reliable comparisons, connect AI evaluation with established risk analysis, and highlight that AI capability definitions depend on chosen measurement scales.

Method: The authors propose a layered measurement stack, distinguish between direct and indirect observables, and suggest these steps could lead to a unified AI taxonomy.

Result: The paper outlines the theoretical groundwork for a formal AI measurement framework, though results remain primarily conceptual.

Conclusion: Formalizing AI measurement would improve consistency, understanding, and safety in AI assessment and utilization.

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [15] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: The paper introduces MLlm-DR, a multimodal large language model designed for automated and explainable depression diagnosis using data from interview videos.


<details>
  <summary>Details</summary>
Motivation: Existing methods for automated depression diagnosis often lack explainability, limiting their clinical applicability, and current LLMs are not effective on interview-based diagnostic tasks.

Method: The paper proposes MLlm-DR, which combines a smaller LLM fine-tuned on a domain-specific dataset and a lightweight query module (LQ-former) to process multimodal information from interview videos.

Result: MLlm-DR achieves state-of-the-art performance on two benchmark datasets, CMDC and E-DAIC-WOZ, successfully handling depression score prediction while providing rationales.

Conclusion: MLlm-DR demonstrates the potential for effective and explainable depression diagnosis, bridging the gap between LLMs and clinical adoption.

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [16] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: This paper surveys the adaptation and application of large language models (LLMs) in geotechnical engineering, exploring tailored methodologies and their various practical applications, benefits, and limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to leverage advancements in large language models (LLMs) to streamline geotechnical engineering workflows, emphasizing the importance of domain-specific adaptation in this specialized engineering field.

Method: The paper conducts a survey outlining key methodologies for adapting general-purpose LLMs to geotechnics, such as prompt engineering, retrieval-augmented generation, domain-adaptive pretraining, and fine-tuning. It also examines state-of-the-art applications.

Result: The survey details effective applications of geotechnical-adapted LLMs in areas such as geological interpretation, site planning, design calculations, and safety assessments, highlighting both their benefits and current limitations.

Conclusion: The findings underscore the valuable role of LLMs in advancing geotechnical practice, offering guidelines for integration and laying a foundation for future interdisciplinary research in geotechnical engineering and engineering geology.

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [17] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: The paper introduces the ADMC framework to address missing modalities in emotion and intent recognition using an Attention-based Diffusion Network.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the issue of missing modalities in multimodal emotion and intent recognition caused by sensor malfunctions or incomplete data, where traditional reconstruction methods often fail.

Method: The ADMC trains independent feature extraction networks for each modality and uses an Attention-based Diffusion Network for generating missing modality features aligned with authentic distributions.

Result: The proposed ADMC achieves state-of-the-art results on IEMOCAP and MIntRec benchmarks under both missing and complete modality scenarios.

Conclusion: The ADMC framework effectively enhances multimodal emotion and intent recognition, proving its robustness against missing data and improving recognition in all scenarios.

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [18] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: The study found that Large Language Models (LLMs) can effectively generate retrieval practice questions, boosting student learning, though manual verification of the questions is still necessary.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the time-consuming and labor-intensive process of crafting high-quality retrieval practice questions, especially for rapidly evolving technical subjects.

Method: The study involved comparing student learning outcomes in two data science courses (around 60 students) by contrasting a week of LLM-generated retrieval practice with a week without such questions.

Result: Students exposed to LLM-generated questions achieved significantly better knowledge retention, with an average accuracy of 89% compared to 73% in the absence of such questions.

Conclusion: LLM-generated retrieval questions show potential as a scalable tool to enhance student learning through retrieval practice, though their quality is variable and requires instructor involvement for final validation.

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [19] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: This paper develops a method called SIP-CoT to improve how large language models simulate realistic human interactions by better capturing social and emotional dynamics in online information spread.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of social media has amplified the dissemination of both information and misinformation. Traditional models inadequately capture the complexities of this phenomenon, and advanced AI methods often overlook crucial psychological and behavioral dynamics. A new approach is needed to simulate realistic human aspects of online communication.

Method: The authors developed a mechanism called SIP-CoT (Social Information Processing-based Chain of Thought), enhanced by emotion-guided memory, to improve the interpretation of social cues, goal personalization, and feedback evaluation in LLM agents. They used Social Information Processing Theory to identify gaps in current models and incorporated this into LLM design.

Result: The study found that SIP-CoT-enhanced LLM agents demonstrated improved social information processing, better mimicking real human behaviors, attitudes, and emotional responses.

Conclusion: Integrating SIP-CoT and emotional memory into LLMs enhances their ability to simulate human-like social intelligence and realism, addressing critical deficiencies in existing propagation simulation models.

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [20] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: The paper proposes a novel method to predict city-level foreign direct investment (FDI) using judicial data instead of traditional economic data, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional city-level FDI predictions rely on economic data like GDP, which can be manipulated, leading to unreliable results. The paper aims to address this issue by leveraging judicial data.

Method: The authors build an index system from over 12 million adjudication documents to assess judicial performance. They propose a Tabular Learning method on Judicial Data (TLJD), which combines row and column data with a mixture of experts model to account for regional differences.

Result: The TLJD method outperforms ten state-of-the-art baselines in city-level FDI prediction tasks, achieving a high R2 score of at least 0.92 across multiple metrics.

Conclusion: Judicial data can serve as a reliable alternative to traditional economic indicators for predicting FDI, and the proposed TLJD method is effective and superior in this context.

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [21] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: The study examines how human experts and AI models evaluate dermatology treatment plans differently, highlighting a significant evaluator effect.


<details>
  <summary>Details</summary>
Motivation: The paper investigates challenges in assessing the quality of AI-generated treatment plans and compares them to human-made ones, pinpointing the gap in evaluation methodologies.

Method: Treatment plans from human experts, a generalist AI (GPT-4o), and a reasoning AI (o3) were scored for complex dermatology cases by human experts and a superior AI judge (Gemini 2.5 Pro).

Result: Human evaluators rated human-generated plans higher, while the AI judge rated plans from advanced AI models higher, reversing the rankings completely.

Conclusion: The study underscores a crucial gap between human clinical heuristics and AI-driven logic, suggesting the need for collaborative, explainable human-AI systems in future clinical care.

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [22] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: ModelAuditor enhances accuracy and reliability in AI deployment for clinical imaging by auditing failure modes efficiently and affordably.


<details>
  <summary>Details</summary>
Motivation: AI models face reliability challenges in clinical practice due to real-world variations, like demographic or equipment shifts, causing performance degradation.

Method: ModelAuditor uses a multi-agent architecture for context-aware simulations, interpretable reporting, and actionable recommendations on consumer-grade hardware.

Result: ModelAuditor recovers 15-25% performance loss due to deployment shifts, outperforming other methods across diverse clinical scenarios.

Conclusion: ModelAuditor is an efficient, cost-effective solution for auditing and mitigating AI failure modes in clinical applications.

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [23] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: The paper proposes a novel method for real-time battery health monitoring using discharge pulse analysis, achieving high accuracy with only 1% error.


<details>
  <summary>Details</summary>
Motivation: Existing battery health monitoring methods face challenges in real-time applications, especially in microgrids with operational constraints.

Method: Developed an approach analyzing the parameters of a discharge pulse at the end of a battery's charge phase, based on an equivalent electrical model.

Result: Experimental results showed the method could predict battery degradation with a mean absolute error of around 1%, achieving high accuracy and explainability.

Conclusion: If confirmed, the method offers seamless integration into BMS, enabling optimized and continuous battery management.

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [24] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: The paper introduces GTA1, a GUI Test-time Scaling Agent, addressing challenges in task planning and visual grounding for GUI agents through test-time scaling and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: GUI agents face difficulties in ambiguous task planning and precise visual grounding within complex interfaces.

Method: The authors use a test-time scaling method with action proposal sampling and a judge model assessment, paired with reinforcement learning for improved visual grounding.

Result: GTA1 demonstrates state-of-the-art accuracy across benchmarks with top performances in Screenspot-Pro, Screenspot-V2, and OSWorld-G.

Conclusion: The proposed approach improves task planning and interaction accuracy significantly, setting a new standard in GUI agents’ autonomous operation performance.

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [25] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: The study introduces the CROP benchmark dataset and Affective-ROPTester framework to evaluate large language models' capacity to predict retinopathy of prematurity (ROP) risk, highlighting limited intrinsic prediction capabilities, the role of structured inputs, and the influence of emotional framing on predictive biases.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the current gap in understanding large language models' ability to predict ROP risk and to analyze the role of emotional framing in influencing model bias and performance.

Method: A novel Chinese dataset "CROP" with 993 annotated records was created, and the Affective-ROPTester framework was used for evaluation. This framework includes three prompting strategies (Instruction-based, Chain-of-Thought, In-Context Learning) and integrates emotional elements at the prompt level.

Result: The study finds that LLMs are limited in predicting ROP risk using solely intrinsic knowledge but improve significantly with structured external support. Emotional framing affects bias, with positive framing reducing overestimation of risk levels compared to negative emotions.

Conclusion: Emotional framing and structured prompts are critical for improving diagnostic reliability in clinical AI models, emphasizing the need for affect-sensitive prompt engineering for effective evaluation and bias mitigation.

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [26] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: AI outperforms humans in games but lacks human-like intuitive decision-making; proposed CogniPlay model aims to bridge this gap.


<details>
  <summary>Details</summary>
Motivation: Expand AI's capabilities to mimic human cognition and intuitive decision-making in gameplay.

Method: Analyzing cognitive psychology insights and existing human-like AI models to develop the CogniPlay model.

Result: Introduction of the CogniPlay model, aimed at replicating human-like behavior in games.

Conclusion: Bridging the gap between AI and human-like decision-making could lead to more adaptable and intuitive AI systems.

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [27] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: The paper introduces a systematic method to improve time series forecasting by breaking the process into three key stages and evaluating various architectural configurations, achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address core challenges in time series forecasting, including effective sequence representation, meaningful information extraction, and precise target projection.

Method: The method involves breaking the forecasting process into three stages: input sequence representation, information extraction (with modules like convolutional layers and self-attention mechanisms), and target projection, tested across seven benchmark datasets.

Result: The proposed models achieve state-of-the-art accuracy, improved computational efficiency, and reduced training/inference times while using fewer parameters.

Conclusion: The systematic approach successfully overcomes task-specific forecasting challenges and sets new standards in accuracy and efficiency for time series forecasting.

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [28] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: The paper introduces MusiScene, a music captioning model capable of imagining complementary scenes associated with music, improving upon existing models and leveraging MSI captions for Video Background Music Generation.


<details>
  <summary>Details</summary>
Motivation: Existing music captioning models are limited to describing musical elements but lack the ability to associate music with cross-modal scene imagination, a gap this research aims to address.

Method: The researchers constructed a large-scale video-audio caption dataset, finetuned Music Understanding LLaMA for Music Scene Imagination (MSI) tasks, and evaluated the model's performance in generating relevant captions.

Result: MusiScene outperformed MU-LLaMA in generating contextually relevant MSI captions and demonstrated its capability in improving Video Background Music Generation from text.

Conclusion: The study highlights MusiScene as a significant step forward in music captioning, bridging the gap between audio and video scenes, which enhances multimedia applications like VBMG.

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [29] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: The paper introduces BlueLM-2.5-3B, a compact 3B-parameter multimodal language model designed for edge-device deployment, offering efficient reasoning and control over thinking modes while excelling in both multimodal and text-only benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance high-performance multimodal large language models that are efficient, compact, and suitable for edge-device deployment, providing general-purpose reasoning alongside multimodal capabilities.

Method: The method includes diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and utilizing a high-performance training infrastructure.

Result: BlueLM-2.5-3B achieves competitive text-only benchmark performance, multimodal capacity comparable to larger models, and exceptional data efficiency with significantly less training data, surpassing baseline models in non-thinking mode.

Conclusion: The paper concludes that BlueLM-2.5-3B contributes to the progression of high-performance on-device MLLMs and offers valuable insights to the research community to optimize efficiency and performance in multimodal systems.

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [30] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: The paper presents a foundation model designed for multi-task prediction in wireless networks, addressing limitations in task generalization of traditional DL approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional deep learning methods in adapting to diverse scenarios and tasks within wireless network predictions.

Method: The model unifies heterogeneous tasks using univariate decomposition, interval awareness through granularity encoding, and employs a causal Transformer backbone. Added innovations include a patch masking strategy for flexible input lengths.

Result: The foundation model, after training on large-scale datasets, exhibits strong generalization to unseen scenarios and outperforms traditional methods in zero-shot predictions.

Conclusion: This approach offers a robust and generalizable solution for prediction tasks in wireless networks, broadening the scope of task adaptability and interval diversity.

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [31] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: The paper proposes an attribution-based approach to enhance the interpretability of AI predictions in healthcare for assessing lymphedema risk after breast cancer radiotherapy.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and interpretability in data-driven AI methods hampers their integration into healthcare decision-making.

Method: The paper employs a statistical analysis of attributes in a rule-based Explainable AI model, using metrics from Information Retrieval to evaluate and present attribute relevance.

Result: A user study showed that the proposed approach improved interpretability and usefulness compared to the raw Explainable AI model's output.

Conclusion: The approach enhances the practical applicability of AI risk assessments by offering interpretable insights into risk factors.

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [32] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: This study presents 'HopeBot,' a chatbot that administers the PHQ-9 depression screening using large language models, showing strong agreement with traditional self-administered methods and user preference for its clarity and supportiveness.


<details>
  <summary>Details</summary>
Motivation: The study aimed to improve depression screening by addressing the limitations of static tools like the PHQ-9, which lack interactivity and adaptability, by leveraging advancements in large language models.

Method: The researchers developed HopeBot, a voice-based chatbot using retrieval-augmented generation and real-time clarification to administer the PHQ-9. They conducted a within-subject study with 132 participants in the UK and China using both self-administered and chatbot-based approaches, followed by comparative feedback analysis.

Result: The chatbot showed strong agreement with traditional methods (ICC = 0.91; 45% identical responses). Of 75 participants offering feedback, 71% trusted the chatbot more for its clarity, guidance, and supportive tone. Ratings for comfort (8.4), clarity (7.7), handling sensitivity (7.6), and helpful recommendations (7.4) were high, with 87.1% indicating willingness to reuse or recommend it.

Conclusion: Voice-based LLM chatbots like HopeBot are effective, scalable, and user-preferred tools that can serve as low-burden adjuncts for routine depression screening, with potential for widespread adoption and personalized interaction.

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [33] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: The authors present CogniSQL-R1-Zero, a reinforcement learning model for translating natural language into SQL, achieving state-of-the-art performance with efficient training methods and releasing new datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in generating accurate and executable SQL from natural language, particularly for complex queries, despite advancements brought by large language models.

Method: The paper introduces CogniSQL-R1-Zero, a reinforcement learning framework utilizing lightweight reward signals like execution correctness and format-tag compliance, avoiding intermediate supervision and complex reward shaping.

Result: CogniSQL-R1-Zero outperforms prior supervised and instruction-tuned baselines on the BIRD Text-to-SQL benchmark using a 7B backbone, trained efficiently on just four NVIDIA A100 GPUs.

Conclusion: The work demonstrates the scalability and efficiency of their RL-based approach for Text-to-SQL tasks and contributes resources like reasoning traces and a weakly supervised query corpus to foster further research.

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [34] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: The paper introduces Feature-Guided Neighbor Selection (FGNS), a method that improves AI interpretability by using class-representative examples guided by feature importance.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods often fail to provide clear and interpretable outputs for non-experts, limiting their utility in practical scenarios.

Method: FGNS is a post hoc approach that uses local and global feature importance to select class-representative examples. It was also evaluated in a user study with 98 participants using Kannada script classifications.

Result: FGNS significantly helped non-experts identify model errors, improved decision speed and accuracy, and chose neighbors more representative of class prototypes compared to k-NN explanations.

Conclusion: FGNS augments human-aligned model assessment, improving interpretability and decision-making speed, but further work is needed to address trust concerns in explanations.

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [35] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: The paper addresses issues in Lockean belief sets, proposing methods to close them under logical deduction and providing a minimally invasive approach for probabilistic belief revision.


<details>
  <summary>Details</summary>
Motivation: Lockean belief sets are not naturally closed under classical logic, creating challenges in contexts like belief change theory. The paper seeks to address these shortcomings.

Method: The authors characterize belief sets that comply with logical closure and design a probabilistic update mechanism that minimally revises the belief set.

Result: The paper demonstrates how to deductively close a belief set through minimal revisions while maintaining consistency with new information.

Conclusion: The proposed approach resolves the logical closure issue in Lockean belief sets, enabling both deductive closure and minimal belief adjustments upon updates.

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [36] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: FEVO proposes a framework to improve large language models (LLMs) in the financial domain using continued pre-training, supervised fine-tuning, and reinforcement learning. FEVO models outperform larger and specialist models on financial benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of tailored solutions for LLMs in the financial domain, despite advancements in reasoning capabilities for other fields such as mathematics and programming.

Method: FEVO employs a multi-stage training framework with three phases: (1) continued pre-training (CPT) for domain knowledge expansion, (2) supervised fine-tuning (SFT) to develop logical reasoning, and (3) reinforcement learning (RL) for integration of domain knowledge and reasoning. High-quality, task-specific datasets were curated using state-of-the-art reasoning models and filters.

Result: The FEVO models, specifically FEVO-R32B, achieved state-of-the-art performance across five financial benchmarks and demonstrated clear performance improvements when compared to simpler RL-only training approaches.

Conclusion: FEVO effectively demonstrates the importance of domain-specific training and structured reasoning in enhancing LLM performance in specialized fields. Its methodology could set a precedent for adapting LLMs to other knowledge-intensive domains.

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [37] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: The paper proposes an AI-based framework using LSTM, genetic algorithm, and SHAP for improving energy management in healthcare facilities, achieving superior forecasting performance and model transparency.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inefficiencies and costs associated with fluctuating energy demands in healthcare facilities, where traditional energy management methods are often inadequate.

Method: An integrated AI-based framework combining LSTM for time-series energy forecasting, genetic algorithms for parameter optimization, and SHAP for improving model interpretability and transparency was developed.

Result: The LSTM model outperformed traditional models such as ARIMA and Prophet with a significantly lower Mean Absolute Error (21.69) and Root Mean Square Error (29.96). The genetic algorithm improved load balancing, and SHAP provided insights into feature influences, enhancing trust.

Conclusion: The proposed LSTM-GA-SHAP framework improves energy forecasting, efficiency, and sustainability in healthcare facilities, paving the way for scalable AI-based energy solutions.

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [38] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: The paper proposes OpenAgentSafety, a framework for evaluating AI agents' safety across real-world tasks and tools, addressing limitations in prior benchmarks.


<details>
  <summary>Details</summary>
Motivation: The potential for unsafe behavior in AI agents has become a critical concern as they are deployed for complex, real-world tasks, requiring more rigorous and realistic evaluation methods.

Method: OpenAgentSafety evaluates agents using real-world tools and environments over 350 tasks across eight risk categories, employing both rule-based analysis and LLM-as-judge assessments for detection.

Result: The framework tested five prominent LLMs, finding unsafe behavior in 51.2% to 72.7% of safety-vulnerable tasks, depending on the model.

Conclusion: The study highlights significant vulnerabilities in current AI agents, emphasizing the need for robust safeguards to ensure safe deployment in real-world applications.

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [39] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: The paper demonstrates that paired preference data, even from weak sources, can improve language model performance beyond traditional strong supervision.


<details>
  <summary>Details</summary>
Motivation: Strong supervision data is often scarce, limiting improvements in language model training.

Method: The authors propose 'delta learning,' using relative quality differences between weak paired data through preference tuning to enhance learning. They post-train an 8B model using paired responses from smaller models (3B and 1.5B).

Result: Their approach matches state-of-the-art model performance using simpler and more cost-effective methods, validated through controlled and large-scale experiments.

Conclusion: Delta learning enables significant performance improvements in language models using weaker data, making advanced post-training simpler and cheaper.

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [40] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: The paper presents methods for identifying causal effects using simplified causal diagrams (causal abstractions) when full causal diagrams are unknown, and proposes a hierarchical framework for identifiability criteria.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying treatment effects from observational data in situations where full causal diagrams are unknown, especially in complex or high-dimensional settings.

Method: The authors formalize causal abstractions as collections of causal diagrams and introduce a set of identifiability criteria, organizing them into a hierarchical framework to clarify the relationships between different levels of causal knowledge.

Result: The paper establishes a hierarchy of identifiability criteria and provides examples and tools to apply the framework in situations with partial causal knowledge.

Conclusion: The framework enhances the understanding of causal identifiability with limited knowledge of causal structures and offers practical tools for dealing with these scenarios.

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [41] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: This paper introduces the Aligned Scoring Rule (ASR) to improve alignment of proper scoring rules with human preferences for text elicitation.


<details>
  <summary>Details</summary>
Motivation: There's a misalignment between proper scoring rules and human preference when eliciting textual information, despite these rules being proper from a mathematical perspective.

Method: The authors design the ASR by minimizing the mean squared error between a reference (human) score and a proper scoring rule.

Result: The ASR is more aligned with human preference than previous methods while maintaining its properness.

Conclusion: The ASR offers a novel approach to scoring textual predictions that enhances alignment with human evaluations without compromising the core property of properness.

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [42] [Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads](https://arxiv.org/abs/2507.05556)
*Jumin Kim,Seungmin Baek,Minbok Wi,Hwayong Nam,Michael Jaemin Kim,Sukhan Lee,Kyomin Sohn,Jung Ho Ahn*

Main category: cs.AR

TL;DR: The study evaluates the real-machine performance of PRAC, a DRAM disturbance mitigation technique, showing significantly lower overheads than simulator estimates.


<details>
  <summary>Details</summary>
Motivation: Simulator-based studies reported that PRAC caused high performance overheads. Discrepancies between simulator results and real hardware require real-machine experiments to determine accurate performance.

Method: Real-machine experiments on PRAC were conducted with timing modifications verified through microbenchmarks, analyzing performance on SPEC CPU2017 workloads.

Result: PRAC's real-machine average and maximum overheads were 1.06% and 3.28%, significantly lower (up to 9.15x) than simulator-based reports. The close page policy minimized overheads by handling elongated DRAM row precharge operations.

Conclusion: PRAC's performance impact on real machines is negligible compared to simulator estimates, validating the effectiveness of close page policy in reducing overhead.

Abstract: Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation
method, modifies key DRAM timing parameters, reportedly causing significant
performance overheads in simulator-based studies. However, given known
discrepancies between simulators and real hardware, real-machine experiments
are vital for accurate PRAC performance estimation. We present the first
real-machine performance analysis of PRAC. After verifying timing modifications
on the latest CPUs using microbenchmarks, our analysis shows that PRAC's
average and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017
workloads -- up to 9.15x lower than simulator-based reports. Further, we show
that the close page policy minimizes this overhead by effectively hiding the
elongated DRAM row precharge operations due to PRAC from the critical path.

</details>


### [43] [GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks](https://arxiv.org/abs/2507.05681)
*Muhammad Hadir Khan,Matthew Guthaus*

Main category: cs.AR

TL;DR: Clock meshes are critical for minimizing skew in VLSI systems, but analysis is complex and traditional methods are slow or inaccurate. GATMesh, a GNN framework, drastically improves both speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in analyzing clock meshes in VLSI systems, including reconvergent paths, multi-source driving, and skew handling, which traditional methods fail to efficiently tackle.

Method: Developed GATMesh, a Graph Neural Network-based framework that uses augmented structural and physical features of clock meshes, trained on SPICE simulation data.

Result: GATMesh achieves 47146x speed-up over multi-threaded SPICE simulation, with an average delay error of 5.27ps on unseen benchmarks.

Conclusion: GATMesh provides a significant advancement in clock mesh analysis, combining accuracy and drastic improvements in computation speed.

Abstract: Clock meshes are essential in high-performance VLSI systems for minimizing
skew and handling PVT variations, but analyzing them is difficult due to
reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE
simulations are accurate but slow; yet simplified models miss key effects like
slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based
framework that models the clock mesh as a graph with augmented structural and
physical features. Trained on SPICE data, GATMesh achieves high accuracy with
average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups
of 47146x over multi-threaded SPICE simulation.

</details>


### [44] [RTGPU: Real-Time Computing with Graphics Processing Units](https://arxiv.org/abs/2507.06069)
*Atiyeh Gheibi-Fetrat,Amirsaeed Ahmadi-Tonekaboni,Farzam Koohi-Ronaghi,Pariya Hajipour,Sana Babayan-Vanestan,Fatemeh Fotouhi,Elahe Mortazavian-Farsani,Pouria Khajehpour-Dezfouli,Sepideh Safari,Shaahin Hessabi,Hamid Sarbazi-Azad*

Main category: cs.AR

TL;DR: The paper investigates the use of GPUs in real-time systems, identifying their benefits and the challenges they present, while reviewing existing solutions and suggesting research avenues.


<details>
  <summary>Details</summary>
Motivation: As GPUs transition from graphics-focused tasks to time-critical applications like machine learning and robotics, understanding their integration into real-time systems becomes crucial due to unique challenges like execution unpredictability.

Method: The paper surveys existing literature on GPUs in real-time systems, examining challenges and current mitigation techniques such as scheduling, resource management, and synchronization solutions.

Result: The authors provide a consolidated view of existing solutions to GPU challenges in real-time contexts and identify gaps in research to improve predictability and performance.

Conclusion: The study underscores the growing importance of GPUs in real-time domains, the current limitations, and calls for further research to overcome barriers to their effective and predictable integration.

Abstract: In this work, we survey the role of GPUs in real-time systems. Originally
designed for parallel graphics workloads, GPUs are now widely used in
time-critical applications such as machine learning, autonomous vehicles, and
robotics due to their high computational throughput. Their parallel
architecture is well-suited for accelerating complex tasks under strict timing
constraints. However, their integration into real-time systems presents several
challenges, including non-preemptive execution, execution time variability, and
resource contention; factors that can lead to unpredictable delays and deadline
violations. We examine existing solutions that address these challenges,
including scheduling algorithms, resource management techniques, and
synchronization methods, and highlight open research directions to improve GPU
predictability and performance in real-time environments.

</details>


### [45] [PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization](https://arxiv.org/abs/2507.06127)
*Dongsheng Zuo,Jiadong Zhu,Yang Luo,Yuzhe Ma*

Main category: cs.AR

TL;DR: This paper introduces PrefixAgent, a framework leveraging large language models (LLMs) to optimize prefix adders with reduced search space and better area efficiency.


<details>
  <summary>Details</summary>
Motivation: Optimizing prefix adders is challenging due to the exponential growth of their design space with bit-width, and existing methods face scalability and performance limitations.

Method: PrefixAgent reformulates prefix adder optimization into subtasks like backbone synthesis and structure refinement, and employs E-graph reasoning traces for effective LLM fine-tuning.

Result: The framework produces prefix adders with smaller areas than baseline methods and demonstrates scalability and generalization in commercial EDA flows.

Conclusion: PrefixAgent offers a promising avenue for prefix adder optimization by leveraging LLMs to overcome traditional scalability and performance hurdles.

Abstract: Prefix adders are fundamental arithmetic circuits, but their design space
grows exponentially with bit-width, posing significant optimization challenges.
Previous works face limitations in performance, generalization, and
scalability. To address these challenges, we propose PrefixAgent, a large
language model (LLM)-powered framework that enables efficient prefix adder
optimization. Specifically, PrefixAgent reformulates the problem into subtasks
including backbone synthesis and structure refinement, which effectively
reduces the search space. More importantly, this new design perspective enables
us to efficiently collect enormous high-quality data and reasoning traces with
E-graph, which further results in an effective fine-tuning of LLM. Experimental
results show that PrefixAgent synthesizes prefix adders with consistently
smaller areas compared to baseline methods, while maintaining scalability and
generalization in commercial EDA flows.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [46] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: TokenShapley is a novel method providing token-level attribution using Shapley values and KNN retrieval, yielding better accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in verifying token-level correctness in responses generated by Large Language Models (LLMs), an area where current attribution methods are inadequate.

Method: TokenShapley combines Shapley value-based data attribution with KNN-based retrieval. A precomputed datastore is leveraged for contextual retrieval, and Shapley values are computed to quantify token importance for fine-grained attribution.

Result: TokenShapley achieves an 11-23% improvement in accuracy over state-of-the-art baselines in token-level attribution across four benchmarks.

Conclusion: TokenShapley effectively enhances token-level attribution in LLMs, meeting user demand for finer-grained response verification and improving token attribution accuracy.

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [47] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: The paper proposes a new framework for evaluating the generalization ability of large language models (LLMs) based on user behavior prediction, instead of knowledge-retrieval and reasoning tasks, and validates it using movie and music recommendation datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of measuring LLMs' generalization, especially in the face of data contamination and the infeasibility of ensuring unseen test information during training.

Method: The authors propose user behavior prediction as a robust alternative for testing LLMs' generalization abilities, applying this framework on movie and music recommendation datasets with models like GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.

Result: GPT-4o outperformed GPT-4o-mini and Llama in user behavior prediction tasks, but all models demonstrated substantial room for improvement, particularly Llama.

Conclusion: User behavior prediction is a scalable and robust test for evaluating the generalization of LLMs, offering an alternative to traditional reasoning or knowledge-retrieval tasks, though there is significant progress yet to be made in this area.

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [48] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: The paper introduces ASCEND, a new framework for detecting implicit sexism in text, exploiting threshold-based contrastive learning for improved accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods often fail to identify implicit forms of sexism on social media, which is increasingly spreading hateful content globally.

Method: ASCEND uses a threshold-based supervised contrastive learning approach combined with a cross-entropy loss. It aligns semantically similar embeddings while separating dissimilar ones using cosine similarity and enhances textual analysis through word-level attention and external features like sentiment, emotion, and toxicity.

Result: ASCEND demonstrated significant performance improvements in detecting implicit sexism, with notable Macro F1 score increases of 9.86%, 29.63%, and 32.51% on the EXIST2021 and MLSC datasets.

Conclusion: The proposed ASCEND framework successfully detects implicit sexism, outperforming existing methods by effectively refining embedding spaces and considering semantic subtleties.

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [49] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: This paper presents an AI framework combining advanced techniques like RAG-enhanced sentiment analysis, prompt engineering, and cross-modal fusion to improve student dropout prediction, achieving 89% accuracy and 0.88 F1-score.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of student dropout in distance learning by incorporating nuanced emotional and contextual factors into predictive models.

Method: The framework employs Retrieval-Augmented Generation for sentiment analysis, prompt engineering to identify academic distress indicators, and cross-modal attention fusion to align textual, behavioral, and socio-demographic data.

Result: The model achieves 89% accuracy and an F1-score of 0.88 on a dataset of 4,423 students, surpassing traditional models by 7% and reducing false negatives by 21%.

Conclusion: The framework significantly improves dropout prediction and provides contextually aligned intervention strategies, bridging predictive analytics with actionable educational solutions.

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [50] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: This paper introduces LCDS, a system to improve discharge summary generation by addressing hallucination and attribution issues in LLMs using a source mapping table and logical rules.


<details>
  <summary>Details</summary>
Motivation: Address hallucination issues and lack of attribution in LLM-based discharge summary generation, especially given the complexity of handling long-form EMRs.

Method: Develop LCDS, a framework that employs a source mapping table for content constraining, logical rules for reliability, and source attribution for easier feedback to improve discharge summary generation.

Result: LCDS generates silver discharge summaries that are more reliable and tailored to clinical contexts. The golden discharge summaries generated after modifications can be used for further fine-tuning of LLMs.

Conclusion: LCDS offers a promising solution to improve the reliability, transparency, and modular adaptability of discharge summaries generated by LLMs in medical practice.

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [51] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow is the first open-source multimodal LLM designed specifically for e-commerce, integrating advanced reasoning and handling complex queries to improve user satisfaction and reduce costs.


<details>
  <summary>Details</summary>
Motivation: LLMs have limitations in complex, multimodal scenarios, especially in e-commerce customer service, necessitating a tailored solution.

Method: MindFlow uses the CoALA framework with integrated memory, decision-making, and action modules, employing a modular 'MLLM-as-Tool' approach for visual-textual reasoning.

Result: MindFlow achieves significant improvements in real-world applications, including a 93.53% relative improvement in handling complex queries via online A/B testing and simulations.

Conclusion: MindFlow successfully extends LLM capabilities to multimodal e-commerce scenarios, enhancing customer service and operational efficiency.

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [52] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: The paper introduces LAG, a method for leveraging task-specific LoRA adapters in language models to improve performance without needing extra training or data.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for effective ways to use fine-tuned language models specifically designed for certain tasks or domains.

Method: LAG selects, combines, and applies LoRA adapters efficiently per-token and per-layer without requiring additional training or data.

Result: LAG outperforms other data-free methods in various knowledge-intensive tasks, and it also integrates well with approaches like retrieval-augmented generation (RAG).

Conclusion: LAG is an efficient and versatile solution for optimizing task-specific performance in fine-tuned language models, even in data-scarce situations.

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [53] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: Models trained on inefficient reasoning traces, which are long but coherent and incremental, outperform those trained on optimal traces in generalization tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate how test-time compute and reasoning structure affect a model’s ability to generalize in reasoning tasks.

Method: The paper evaluates reasoning by training decoder-only transformers on shortest-path tasks with dynamic programming traces versus traces involving backtracking.

Result: Training with inefficient traces led to better generalization on unseen graphs, with confidence in next-token prediction as an important factor.

Conclusion: Long, coherent, and incremental traces improve generalization performance as they offer a better training signal for optimization.

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [54] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: EduCoder is an open-source tool for annotating educational dialogue transcripts, addressing challenges like diverse interactions, flexible coding, and data reliability improvement.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of tools specifically designed for the nuanced task of annotating educational dialogue transcripts, covering complexities such as varied pedagogical features and contextual factors.

Method: EduCoder integrates categorical and open-ended coding, supports the creation of complex codebooks, and allows for comparative annotation to enhance reliability.

Result: The result is the development of EduCoder, a domain-specific platform that enables collaborative coding and annotation for educational dialogue analysis, with features enhancing precision and consistency.

Conclusion: EduCoder advances research tools in educational dialogue annotation by offering tailored functionalities, fostering collaboration, and improving data reliability in education transcript analysis.

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [55] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: The paper introduces "InfoRidge," an information-theoretic framework to trace predictive information through transformer layers during training, revealing that intermediate layers play a pivotal role in generalization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how predictive information flows and generalization arises across transformer layers, addressing an unclear aspect of NLG model mechanisms.

Method: InfoRidge estimates mutual information between hidden representations and target outputs across layers. Trainable residual scaling coefficients are introduced to probe layer importance under varied distributions.

Result: Predictive information peaks in upper-middle layers, forming a "generalization ridge" in transformers, while models rely on these ridge layers over final layers during distribution shifts.

Conclusion: Intermediate layers of transformers are critical for generalization performance, and the findings highlight their importance in model robustness.

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [56] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: The paper explores a framework where natural language privacy profiles guide data modification for protecting user privacy while querying commercial LLM APIs.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns with commercial LLMs that require user data exposure.

Method: A framework was developed where local models rewrite queries based on user-defined privacy profiles before being sent to external LLMs. The PEEP dataset is introduced to evaluate this concept.

Result: Lightweight LLMs demonstrated partial capability in adhering to user privacy instructions, but exhibited consistent challenges.

Conclusion: The study underscores the need for models that can better interpret and adhere to privacy preferences set by users.

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [57] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: The paper reviews the underdeveloped multilingual reasoning capabilities of LLMs and introduces a new benchmark, GeoFact-X, alongside a novel method, BRIDGE, to address multilingual reasoning issues. Results show improved fidelity in multilingual reasoning.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the shortcomings in multilingual reasoning capabilities of LLMs, particularly focusing on low-resource languages which are often overlooked or defaulted to English in reasoning tasks.

Method: It introduces GeoFact-X, a geography-based benchmark with reasoning traces in multiple languages, and BRIDGE, a training strategy using a language-consistency reward during supervised fine-tuning and reinforcement learning. Automatic evaluation with LLM-as-a-judge is also developed.

Result: BRIDGE significantly improves reasoning fidelity in multilingual tasks, as shown through enhanced performance across language consistency and reasoning quality metrics.

Conclusion: The paper concludes that reasoning-aware multilingual reinforcement learning, via methods like BRIDGE, is pivotal for robust cross-lingual generalization in LLMs.

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [58] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: The paper presents CoPE, a framework for evaluating contextual and parametric knowledge integration in large language models, identifying a bias towards earlier context and proposing improved prompting methods.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models prioritize and integrate contextual and parametric knowledge for better information processing.

Method: Introduces CoPE framework, MultiWikiAtomic dataset in three languages, and analyzes lost-in-the-later biases and the impact of reasoning models and prompts like chain-of-thought.

Result: Observes a positional bias where LLMs deprioritize later context, and finds reasoning and chain-of-thought prompts less effective at contextual grounding and recall.

Conclusion: Proposes prompt-based methods to enhance contextual grounding, with case study results suggesting improvements in factual summarization and reduced hallucination.

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [59] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: The paper examines how gender and local politics shape online abortion discourse post-Dobbs ruling, analyzing 10 million X (formerly Twitter) posts.


<details>
  <summary>Details</summary>
Motivation: To investigate how gender, ideology, and location interact to influence abortion-related discourse in the wake of a major Supreme Court ruling.

Method: Analyzed almost 10 million abortion-related posts on X, accounting for inferred gender, ideological alignment, and geographic location.

Result: Gender plays a significant role in moderating abortion attitudes and emotional expression, especially in conservative regions, creating a growing gender gap in attitudes. The Dobbs draft leak prompted increased online activism, mostly from pro-abortion women in regions with restrictive laws.

Conclusion: Abortion discussions are ideologically and gender-structured, with local sociopolitical conditions playing a critical role, and identity significantly influences political expression during pivotal moments.

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [60] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: PhoniTale is a system that uses LLMs to generate cross-lingual mnemonics to assist in second-language vocabulary acquisition, specifically for typologically distant languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of vocabulary acquisition in second-language learners due to phonological and structural mismatches in typologically distant languages.

Method: Developed PhoniTale, a system that retrieves phonologically similar L1 keywords and uses LLMs to generate mnemonics, evaluated via automated metrics, human evaluation, and recall tests.

Result: PhoniTale’s mnemonics performed comparably to human-created ones in helping users recall vocabulary.

Conclusion: PhoniTale offers a promising approach for automated mnemonic generation but requires refinement to further improve quality and methodology.

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [61] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: The paper explores whether LLMs genuinely understand language by investigating their semantic capabilities at the word and sentence level, using classical theories for analysis.


<details>
  <summary>Details</summary>
Motivation: Understanding the semantic capabilities of LLMs is crucial to evaluate their potential mimicking human-like comprehension.

Method: The authors analyze LLMs' internal language processing and generated representations through classical semantics theories by Frege and Russell.

Result: Revealed a nuanced understanding of LLMs' semantic capabilities, linking their workings to established semantic frameworks.

Conclusion: LLMs exhibit complex semantic representations but may not fully grasp language as humans do, according to classical semantic theories.

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [62] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: This paper addresses the subjectivity of toxic language detection by introducing MODELCITIZENS, a dataset with diverse annotations for social media posts, and finetuning models that outperform prior tools.


<details>
  <summary>Details</summary>
Motivation: Toxic language detection on social media is subjective and often erases community-specific contexts, like reclaimed language, due to current models collapsing diverse perspectives into a single truth.

Method: The paper introduces a novel dataset, MODELCITIZENS, including 6.8K posts and 40K toxicity annotations across diverse identity groups. These posts are supplemented with conversational contexts generated by LLMs. They also release two finetuned models, LLAMACITIZEN-8B and GEMMACITIZEN-12B.

Result: State-of-the-art tools underperform on the MODELCITIZENS dataset, especially on posts augmented with conversational context. The proposed finetuned models outperform competing models like GPT-o4-mini by 5.5%.

Conclusion: Community-informed annotations and modeling, as introduced in this paper, are crucial for developing inclusive content moderation tools that acknowledge diverse perspectives on toxicity.

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [63] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: This paper evaluates LLMs on structured tabular reporting and medical order extraction tasks, and introduces two open-source datasets.


<details>
  <summary>Details</summary>
Motivation: To reduce documentation burdens in healthcare workflows and improve focus on patient care.

Method: Uses private and open-source datasets for evaluation, proposes a pipeline for synthetic nurse dictations.

Result: LLMs show variable performance, with strengths observed in specific clinical NLP tasks. SYNUR and SIMORD datasets are introduced.

Conclusion: These approaches and datasets support advancing clinical NLP and addressing real-world challenges in the field.

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [64] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: The paper proposes R2-LLMs, a hierarchical reasoning framework for large language models, enhancing test-time performance without requiring advanced distillation or chain-of-thought training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving test-time performance in large language models by developing a framework that doesn't require training with more advanced models or chain-of-thought data.

Method: It introduces dual-level retrieval-augmented in-context learning: coarse retrieval of similar problem-answer templates and fine-grained intermediate solution step retrieval using Monte Carlo Tree Search (MCTS) with a process reward model (PRM) for scoring.

Result: Empirical evaluations on datasets like MATH500, GSM8K, and OlympiadBench-TO showed up to a 16% performance boost using LLaMA-3.1-8B compared to baselines.

Conclusion: R2-LLMs effectively enhances reasoning and decision-making in large language models through hierarchical retrieval and a reward-based refinement approach, showcasing its potential in complex reasoning tasks.

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [65] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: Re5, a self-evaluation and revision framework, is introduced to improve instruction adherence and response quality of large language models (LLMs) efficiently.


<details>
  <summary>Details</summary>
Motivation: Current iterative revision methods for improving LLM performance face challenges such as high monetary costs and quality degradation due to excessive revisions.

Method: Re5 utilizes task and constraint extraction, structural evaluations to prevent accumulated errors, and fine-grained evaluations for selective revisions. It ensures quality while improving adherence and uses outputs for alignment tuning.

Result: Re5 demonstrates comparable instruction-following performance to models trained with GPT-4o-mini data while maintaining a 64.24%-win rate over unrevised responses.

Conclusion: Re5 provides a resource-efficient way to enhance instruction-following performance and response quality for LLMs, reducing reliance on external supervision.

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [66] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: The paper proposes a flipped knowledge distillation framework where a Large Language Model (LLM) learns from a Smaller Language Model (SLM), leveraging specialized domain knowledge.


<details>
  <summary>Details</summary>
Motivation: To combine the domain-specific effectiveness of smaller fine-tuned language models with the rich semantic understanding of large language models in tasks like text matching.

Method: The LLM is restructured as an encoder-decoder model using LoRA. An encoder generates compressed representations, and a decoder maps them to outputs. These representations are aligned with SLM-produced similarity scores using a proposed Margin-aware Contrastive Learning (MCL) method.

Result: The flipped distillation approach improves the LLM's performance, achieving success in financial and healthcare benchmarks and integration into real-world applications.

Conclusion: Flipping the distillation process to have LLMs learn from SLMs enhances domain-specific performance and can be effectively deployed in practical settings.

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [67] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: This paper introduces SARA, a Retrieval-augmented Generation (RAG) framework, which balances context efficiency and factual accuracy by integrating fine-grained text spans and semantic compression vectors.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems face challenges of limited effective context length and redundancy in retrieved documents, often losing details needed for factual accuracy.

Method: SARA improves context representation by combining natural-language text snippets with semantic compression vectors to optimize both local precision and global knowledge coverage. Evidence-selection allows dynamic reranking of contexts.

Result: SARA demonstrated consistent improvements in answer relevance (+17.71), correctness (+13.72), and semantic similarity (+15.53) across 9 datasets and 5 LLMs.

Conclusion: The integration of textual and compressed representations in SARA enhances robustness and context efficiency in RAG frameworks, proving its effectiveness across diverse datasets and model configurations.

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [68] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: The paper introduces ECom-Bench, a benchmark framework to evaluate LLM agents with multimodal capabilities in e-commerce customer support scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the need for a structured way to evaluate multimodal capabilities of LLM agents in complex e-commerce customer support tasks.

Method: Developed ECom-Bench, featuring dynamic user simulations with persona data and realistic task datasets derived from authentic e-commerce dialogues.

Result: Even advanced models like GPT-4 achieved only 10-20% pass metric on the benchmark, showcasing the challenges posed by real-world e-commerce scenarios.

Conclusion: ECom-Bench represents a significant step in challenging and improving LLM agents for realistic e-commerce situations; code and data will be made available for further research.

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [69] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: The Smoothie-Qwen approach tackles language bias in multilingual LLMs, successfully reducing undesired outputs without retraining.


<details>
  <summary>Details</summary>
Motivation: To address language confusion in multilingual LLMs, where they generate responses in a default or dominant language against the prompt’s intent.

Method: Developed Smoothie-Qwen, a post-hoc method that adjusts token-level probabilities to suppress unwanted language outputs, bypassing the need for retraining.

Result: Demonstrated success with the Qwen model, achieving a 95% reduction in unintended Chinese outputs while maintaining task accuracy on multilingual benchmarks.

Conclusion: Smoothie-Qwen offers an efficient, retraining-free method to improve language controllability, enhancing the reliability of LLMs for diverse language tasks.

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [70] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: DualDistill introduces a fine-tuning framework that trains a model, Agentic-R1, to dynamically decide and effectively execute reasoning strategies, achieving improved accuracy across logical and computational tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of long-CoT models and tool-augmented agents that struggle with complex logical reasoning and error-prone traces.

Method: The authors propose DualDistill, a framework that fine-tunes a unified student model by distilling complementary strategies from multiple teacher models. Agentic-R1, a resultant model, adaptively deploys tools for computation and uses text-based reasoning for abstract tasks.

Result: Agentic-R1 demonstrates increased accuracy across diverse tasks, including computationally intensive and standard benchmarks.

Conclusion: Multi-strategy distillation via DualDistill enables robust and efficient reasoning, making it a significant advancement for tasks requiring diverse logical approaches.

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [71] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: The paper introduces DRAGON, a dynamic evaluation benchmark for Retrieval-Augmented Generation (RAG) systems in Russian that uses a regularly updated news corpus.


<details>
  <summary>Details</summary>
Motivation: There is a lack of dynamic and comprehensive RAG evaluation benchmarks in non-English languages, such as Russian, that reflect real-world and evolving data scenarios.

Method: The authors created DRAGON, a benchmark built on an updated Russian news corpus. It utilizes automatic question generation based on a Knowledge Graph and evaluates RAG systems' retriever and generator components.

Result: The study provides an evaluation framework with scripts and data, potentially extendable to other languages. A leaderboard is launched to engage the research community.

Conclusion: DRAGON serves as the first dynamic RAG benchmark in Russian, aiming to advance multilingual and real-time RAG system development.

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [72] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: The paper introduces HIRAG, a fine-tuning method to enhance Retrieval-Augmented Generation (RAG) models, improving their performance on reasoning-intensive datasets by applying a hierarchical chain-of-thought approach.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address shortcomings in traditional RAG systems, including inconsistent document quality and insufficient exploration of chain-of-thought processes in fine-tuning RAG models.

Method: The proposed HIRAG method employs a hierarchical instruction-tuning approach with a 'think before answering' strategy to build progressively enhanced RAG capabilities: filtering, combining semantic information, and RAG-specific reasoning.

Result: HIRAG demonstrated significant performance improvements on multiple datasets, including RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

Conclusion: Hierarchical chain-of-thought fine-tuning, as exemplified by HIRAG, can effectively enhance the reasoning and knowledge integration abilities of RAG models for various complex tasks.

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [73] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: The paper introduces the Omni-router Transformer, a shared-router model that improves coordination in MoE layers, leading to enhanced ASR performance.


<details>
  <summary>Details</summary>
Motivation: Current MoE routing strategies lack coordination across layers, limiting expert specialization and overall performance.

Method: The study designs a shared router mechanism across MoE layers for increased cooperation and specialization, resulting in the Omni-router Transformer.

Result: The Omni-router Transformer reduces word error rates by 11.2% compared to dense models and 8.2% compared to Switch Transformer models. It also improves robustness across diverse ASR benchmarks.

Conclusion: The shared router approach boosts the ASR performance and ensures structured expert utilization, indicating its significance in advancing MoE architectures.

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [74] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: GPTKB v1.5 is a large-scale knowledge base constructed from GPT-4.1, containing 100-million interlinked triples, enabling structured exploration via SPARQL queries and analysis of LLM knowledge.


<details>
  <summary>Details</summary>
Motivation: Factual knowledge of language models, particularly its exploration and analysis, remains challenging due to limitations in accessibility through conventional methods.

Method: The study utilizes GPTKB methodology for massive-recursive knowledge generation using GPT-4.1, producing dense interlinks for comprehensive exploration and querying, demonstrated through three key use cases.

Result: The outcome is GPTKB v1.5, an accessible platform enabling link traversal, structured querying, and insights into LLM knowledge capabilities and limitations.

Conclusion: The research paves the way for systematic analysis of LLM knowledge and automated construction of large-scale knowledge bases, transforming how we interact with and evaluate LLM-derived information.

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [75] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: The paper introduces "DocTalk," a synthesized dialogue dataset created from Wikipedia articles, proposing it as a means to improve multi-turn conversational capabilities of LLMs during pre-training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the mismatch between the predominantly prose-based pre-training data of LLMs and their increasing application in multi-turn conversational tasks.

Method: The authors developed a pipeline that transforms clusters of related documents into long, multi-turn, multi-topic dialogues. They applied this to Wikipedia articles to create a dataset called DocTalk.

Result: Using DocTalk for pre-training resulted in up to 40% improvement in context memory and understanding of LLMs, with no loss of base performance.

Conclusion: Synthesizing conversational structures and integrating DocTalk during pre-training can significantly enhance LLMs' conversational abilities without detrimental effects on their core functionalities.

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [76] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: The paper introduces Flippi, an advanced conversational assistant tailored for e-commerce, which utilizes LLM-powered techniques to enhance product discovery and shopping experiences for users.


<details>
  <summary>Details</summary>
Motivation: To address challenges of navigating vast product landscapes in e-commerce and provide a personalized shopping experience through conversational interaction.

Method: Flippi employs advanced NLP techniques, including Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction. It is designed for integration across e-commerce platforms.

Result: Flippi enables efficient product discovery, personalized suggestions, cost-effective decisions, and comparative product analysis. It enhances customer engagement and conversion rates.

Conclusion: Flippi redefines e-commerce shopping by merging digital convenience with personalized assistance, setting a benchmark for improved customer satisfaction and engagement.

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [77] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: The paper develops benchmarks to test large vision-language models (LVLMs) on tasks related to amodal completion, showing mixed accuracy across object types and challenges with Japanese-specific prompts.


<details>
  <summary>Details</summary>
Motivation: To explore the inferential abilities of LVLMs in understanding texts related to amodal completion, bridging a gap in multimodal reasoning capabilities.

Method: Construction of a benchmark grounded in Basic Formal Ontology to systematically classify amodal completion and evaluate LVLM performance.

Result: Models showed human-comparable accuracy overall but struggled with specific object types and performed worse on Japanese prompts, indicating linguistic limitations.

Conclusion: LVLMs demonstrate potential in multimodal reasoning but reveal deficiencies in cross-language competence and object-specific understanding, highlighting areas for improvement.

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [78] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: The paper discusses biases in automatic speech recognition (ASR) systems concerning speaker groups and explores different measures to evaluate performance and bias in Dutch ASR systems, recommending supplementary metrics beyond averaged error rates.


<details>
  <summary>Details</summary>
Motivation: Growing evidence of bias in ASR systems due to speaker attributes like gender, age, or accent necessitates better evaluation frameworks to address these disparities and improve fairness.

Method: The authors compare existing and novel performance and bias measures, conduct experiments using bias mitigation strategies, and analyze state-of-the-art end-to-end ASR systems for Dutch.

Result: The study reveals that traditional metrics like averaged error rates are insufficient to capture system performance and bias comprehensively, necessitating alternative evaluation measures.

Conclusion: Recommendations are given for adopting more comprehensive metrics to report ASR system performance and bias, promoting equitable representation for diverse speaker groups and reducing overall bias.

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [79] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: The paper proposes a framework using large language models (LLMs) to simulate virtual survey respondents, thereby improving the construct validity of psychometric survey items in a scalable and cost-effective way.


<details>
  <summary>Details</summary>
Motivation: The growing use of psychometric surveys to evaluate LLMs introduces a need for efficiently generating construct-valid survey items without relying on costly human data collection.

Method: The framework involves simulating virtual respondents with diverse mediators—factors that impact how different traits influence survey responses—using LLMs, and identifying robust survey items via these simulations.

Result: Experiments across three psychological trait theories (Big5, Schwartz, VIA) demonstrate that LLMs can generate plausible mediators and simulate respondent behavior effectively, validating high-quality survey items.

Conclusion: This approach not only enhances the cost-effectiveness of survey development but also offers insights into how LLMs mimic human-like behaviors in psychometric contexts. The publicly released dataset and code aim to foster further innovation.

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [80] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: The paper presents the Unibuc - NLP team's methods and results for the SemEval 2025 Task on emotion detection, leveraging large language models with few-shot prompting and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To advance emotion detection capabilities across multiple languages, including less-resourced ones, using cutting-edge NLP models.

Method: Employing large language models (Gemini, Qwen, DeepSeek) with few-shot prompting and fine-tuning techniques to detect emotions in a multi-label setup.

Result: Achieved distinct results: F1-macro scores of 0.7546 (English, rank 26/96), 0.1727 (Portuguese, rank 35/36), and 0.325 (Emakhuwa, rank 1/31).

Conclusion: The results demonstrate that their methods perform well for Emakhuwa but exhibit challenges in handling Portuguese due to resource limitations or linguistic complexities.

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [81] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: The paper explores the robustness of evaluation methodologies for knowledge model editing, revealing how metrics and edit batch sizes can affect rankings of editors and questioning string-matching's reliability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of scrutiny regarding evaluation methodologies for knowledge editor robustness and their impact on model performance.

Method: The authors investigate varying metrics, different edit batch sizes, and carry out manual assessments of string-matching evaluation methods.

Result: Different evaluation approaches led to changes in rankings of knowledge editors, and string-matching methods showed tendencies for false positives.

Conclusion: Evaluation methodologies greatly influence the perceived effectiveness of knowledge editors, and careful selection is vital to avoid unfair biases and unreliable results.

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [82] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: The paper focuses on creating a method, named DAEDCMD, to improve Multimodal Misinformation Detection (MMD) by leveraging online data streams and addressing challenges such as past knowledge forgetting and evolving social environments.


<details>
  <summary>Details</summary>
Motivation: Misinformation, especially multimodal articles on social media, causes serious negative impacts. Existing methods fail to address the problem of new, continuously emerging events making models outdated and ineffective.

Method: The authors propose a method DAEDCMD, based on a Dirichlet process-based mixture-of-expert structure to prevent past knowledge forgetting and a continuous-time dynamics model to adapt to evolving environments.

Result: DAEDCMD outperforms six MMD baselines and three continual learning methods in experiments, demonstrating its effectiveness.

Conclusion: The method successfully tackles challenges in continual MMD and offers a promising solution for detecting misinformation in dynamic, real-world scenarios.

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [83] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: The paper investigates the problem of ghosting (inline query auto-completion) for chat-based systems, using human-human and human-bot dialogue datasets. It evaluates deep learning and non-deep learning methods and introduces a novel strategy, finding significant performance variation between methods depending on query type.


<details>
  <summary>Details</summary>
Motivation: To address the under-researched problem of chat-ghosting, which plays a critical role in enhancing user experience in chat systems but lacks benchmarks and comparative analyses of traditional and modern methods.

Method: The study uses four dialogue datasets (human-human and human-bot) to evaluate various query auto-completion methods, including statistical models, deep learning methods, and a proposed entropy-based dynamic early stopping strategy, with and without conversational context.

Result: Statistical n-gram methods and tries outperform deep learning models for seen prefixes in both performance and efficiency. Neural models like T5 and Phi-2 are superior for unseen queries, and incorporating conversational context improves ghosting quality significantly.

Conclusion: Both traditional and neural approaches have their strengths depending on query characteristics. Adding conversational context strengthens ghosting performance, and future research should explore this critical area further, using the benchmarks and findings provided in this paper.

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [84] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: OpenFActScore is an open-source tool for evaluating the factual accuracy of AI-generated text, replicating FActScore while using open models instead of closed, commercial ones.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to promote transparency and reproducibility in evaluating the factuality of AI-generated text by providing an open-source alternative to the closed-source FActScore framework.

Method: OpenFActScore uses Atomic Fact Generation (AFG) to generate individual claims from text and Atomic Fact Validation (AFV) to validate these claims against trusted sources. It is designed to work with any model compatible with Hugging Face.

Result: Open-source models tested with OpenFActScore approximated the performance of closed-source systems. Specifically, the Gemma model achieved the best performance, and the framework's results had a 0.99 Pearson correlation with the original FActScore experiments.

Conclusion: OpenFActScore offers a transparent, reproducible, and cost-effective way to evaluate factual accuracy in AI-generated text, making it a valuable alternative to closed-source systems.

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [85] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: The paper states that only a tiny fraction of NLP papers (0.1%) evaluate real-world impact, urging a shift in focus from metrics to understanding real-world applications.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of focus in the NLP community on evaluating the practical, real-world usefulness of their systems, which could accelerate technology adoption.

Method: A structured survey of the ACL Anthology was conducted to quantify the proportion of papers evaluating real-world impact versus metric-based evaluations.

Result: The survey revealed that roughly 0.1% of NLP papers evaluate real-world impact, and most do so briefly, focusing more on metric evaluations.

Conclusion: To make NLP technology more useful and accelerate adoption, the community should prioritize assessing the real-world impact of their systems over purely focusing on metrics.

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [86] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: The paper introduces RabakBench, a multilingual safety benchmark localized for Singapore's linguistic context, addressing the challenges LLMs face in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: LLMs and their safety classifiers struggle in low-resource languages due to a lack of comprehensive training data and evaluation tools.

Method: The researchers developed RabakBench using a three-stage pipeline: generating adversarial examples with LLM red teaming, labeling data with LLM-based annotations validated by human judgments, and translating content into multiple languages while maintaining nuances and toxicity.

Result: RabakBench contains over 5,000 safety-labeled examples in four languages and six safety categories, revealing significant performance drops in popular safety classifiers when evaluated.

Conclusion: RabakBench enhances safety evaluations for LLMs in Southeast Asia's multilingual settings and provides a reproducible methodology for developing localized safety datasets for low-resource environments.

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [87] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: The paper introduces a method to improve language model training by reducing dependence on extensive human annotation and cutting carbon emissions.


<details>
  <summary>Details</summary>
Motivation: Current methods of training language models using human-annotated and expanded datasets are costly, energy-intensive, and prone to data leakage.

Method: The authors propose a self-evolution method with two key steps: Multi-level Principle Generation (using a large-scale model to derive task principles) and Principle-based Instance Generation (employing a smaller-scale model to create extensive training data from these principles).

Result: The experimental results demonstrate that the proposed approach enhances model performance compared to smaller models directly generating data and significantly reduces carbon emissions.

Conclusion: The self-evolution approach shows promise in addressing the challenges of high costs, data leakage, and environmental concerns in language model training.

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [88] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: This paper introduces an LLM-based automated pipeline for generating synthetic data and in-context learning for document-level entity and relation extraction.


<details>
  <summary>Details</summary>
Motivation: High-quality, annotated datasets for document-level entity and relation extraction are scarce, particularly in zero-shot or few-shot scenarios.

Method: The method combines synthetic data generation with retrieval-based in-context learning using reasoning-optimized LLMs, avoiding manual annotations, and dynamically retrieves examples for inference.

Result: Generated a synthetic dataset of 5k Wikipedia abstracts with 59k entities and 30k relation triples. Evaluated performance on the DocIE shared task.

Conclusion: Document-level entity and relation extraction in a zero-shot setting remains a difficult task, even for advanced LLMs.

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [89] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: The paper introduces a multistage failure recovery framework using zero-shot chain prompting for embodied agents and achieves state-of-the-art results on a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Embodied agents often fail during complex task execution, necessitating effective recovery mechanisms to ensure their success.

Method: A conditional multistage failure recovery framework with four error-handling stages was proposed. It employs zero-shot chain prompting and leverages LLMs to analyze and address challenges in environmental contexts.

Result: State-of-the-art performance is achieved on the TfD benchmark of the TEACH dataset, showing an 11.5% improvement over a baseline without recovery and a 19% improvement over the strongest existing model.

Conclusion: The proposed framework demonstrates the potential of LLM-driven reasoning in improving the robustness and efficacy of embodied agents' failure recovery.

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [90] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: The paper investigates memorization in LLMs, presenting the Entropy-Memorization Law, which links data entropy with memorization difficulty, and introduces a strategy for Dataset Inference.


<details>
  <summary>Details</summary>
Motivation: To understand how the difficulty of memorization in LLMs can be characterized, specifically exploring the relationship between data entropy and memorization levels.

Method: The authors conduct experiments on OLMo models, analyzing correlations between data entropy and memorization scores while examining cases like randomized gibberish strings.

Result: Key result indicates a linear correlation between data entropy and memorization in LLMs, and an ability to distinguish between training and testing data using insights from their findings.

Conclusion: Data entropy serves as a predictive metric for memorization challenges in LLMs, offering practical tools for Dataset Inference and a deeper understanding of LLM behavior.

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [91] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: This paper surveys prompt tuning, a technique for efficiently adapting language models by adding trainable vectors to frozen models. It classifies and analyzes current methodologies, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a comprehensive overview of parameter-efficient prompt tuning methods, identify their innovations, pros/cons, and challenges, and explore ways to improve them.

Method: The method involves categorizing prompt tuning techniques into direct prompt learning and transfer learning, analyzing their designs, and visually comparing different frameworks.

Result: This survey identifies computational efficiency and training stability as challenges, provides insights into existing methodologies, and highlights potential areas for improvement.

Conclusion: Prompt tuning is a promising approach for adapting language models efficiently, but it faces challenges in robustness and computational demands. Research should focus on these aspects to broaden its applications.

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [92] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel is a state-of-the-art multilingual image generation framework supporting six languages, solving limitations in existing English-centric systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by English-centric text-to-image generation, such as semantic drift, cultural misalignment, and inequity for non-English speakers.

Method: NeoBabel employs large-scale multilingual pretraining and high-resolution instruction tuning to improve inclusivity, efficiency, and performance in image generation.

Result: NeoBabel achieves superior multilingual performance (+0.11 and +0.09 over competitors) while preserving robust English functionality, despite being much smaller in size.

Conclusion: NeoBabel sets a precedent that multilingual capabilities enhance AI robustness, efficiency, and cultural fidelity, releasing resources to advance inclusive AI efforts.

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [93] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: The paper introduces the Code Triangle framework to evaluate LLMs in code generation, identifying their limitations in comparison to human programmers and proposing methods for improvement.


<details>
  <summary>Details</summary>
Motivation: Explore the programming competence of LLMs and address their shortcomings in generating diverse and robust solutions.

Method: Developed the Code Triangle framework to evaluate LLMs in editorial analysis, code implementation, and test case generation using competitive programming benchmarks.

Result: LLMs showed self-consistency across dimensions but lacked diversity and robustness due to biases and reasoning limitations. Improvements were achieved via human-generated content and model mixtures.

Conclusion: Enhancing LLMs with diverse training data, human guidance, and mixed models can improve their coding abilities and robustness, paving the way for more advanced coding models.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [94] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3 is an open-source vision-language model leveraging reinforcement learning (RL) post-training to excel in visual reasoning without additional pre-training.


<details>
  <summary>Details</summary>
Motivation: To enhance visual reasoning in vision-language models by transferring reasoning skills from Large Language Models (LLMs) and improving cross-modal alignment.

Method: Developed an RL-powered post-training framework, introduced a novel entropy-based reasoning capability indicator, and analyzed multimodal reasoning strategies to optimize the model's performance.

Result: Skywork-R1V3 achieved state-of-the-art results on MMMU (76.0%, up from 64.3%) and performed comparably to leading closed-source models.

Conclusion: The model demonstrates RL’s effectiveness in boosting visual reasoning, marking a major step forward in open-source vision-language model capabilities.

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [95] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: The paper introduces CriticLean, a framework improving the critic phase in translating natural language math into formal code using CriticLeanGPT and new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked critic phase in automated theorem proving, improving the evaluation of whether generated formalizations capture the semantic intent of the original problem.

Method: The introduction of CriticLeanGPT trained with supervised fine-tuning and reinforcement learning to evaluate semantic fidelity, and CriticLeanBench, a new benchmark for measuring the accuracy of such evaluations.

Result: CriticLeanGPT significantly outperforms both open- and closed-source baselines. Additionally, the FineLeanCorpus dataset was created, showcasing rich diversity, difficulty coverage, and correctness.

Conclusion: Optimizing the critic phase is critical for reliable formalizations, and the CriticLean framework sets a strong foundation for advancements in formal mathematical reasoning.

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [96] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: The authors developed a method for improving subjectivity detection in English news text using transfer-learning and a controlled data augmentation pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of classifying subjective and objective sentences in English news text more effectively.

Method: The authors used transfer-learning on fine-tuned transformers, introduced GPT-4o for controlled augmentation to generate subjectivity-styled paraphrases, and ensured consistency via label refinement.

Result: Transfer-learning with specified encoders outperformed general-purpose fine-tuning, and curated augmentation significantly boosted robustness in detecting subjective content.

Conclusion: Combining encoder specialization with label-consistent augmentation is effective for subjectivity detection. Their method placed 16th out of 24 participants in the CheckThat! Lab competition.

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [97] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: The paper evaluates strategies for predicting the veracity of numerical claims using the QuanTemp dataset and a custom evidence retrieval system, finding evidence quality to be the key challenge.


<details>
  <summary>Details</summary>
Motivation: The aim is to improve automated fact-checking of numerical claims, which are particularly challenging due to their reliance on comparisons, quantities, and temporal references.

Method: The study tests different modeling strategies, including expanded context windows and right-to-left (R2L) tokenization, within a veracity-prediction pipeline leveraging ModernBERT.

Result: The results show that neither longer context windows nor R2L tokenization notably improve veracity prediction; instead, evidence quality remains the chief limiting factor. The system achieves a competitive F1 score of 0.57, placing among the Top-4 in CheckThat! 2025 Task 3.

Conclusion: Improving evidence quality is more critical than using longer contexts or R2L tokenization for better performance in verifying numerical claims.

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [98] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: UQLM is a Python package developed to detect hallucinations in LLMs using advanced uncertainty quantification techniques.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs compromise the safety and trustworthiness of their outputs, creating a need for robust detection methods.

Method: UQLM uses state-of-the-art uncertainty quantification techniques to compute confidence scores for LLM outputs, enabling hallucination detection.

Result: The package provides an accessible, off-the-shelf solution for hallucination detection with confidence scores ranging from 0 to 1.

Conclusion: UQLM can be used to improve the reliability of LLM outputs by integrating its uncertainty quantification-based detection features.

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [99] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: Latent reasoning allows Large Language Models (LLMs) to perform inference entirely within their hidden states, bypassing natural language token-level steps. This paper surveys latent reasoning by detailing neural computational layers, strategies, and advanced paradigms to enhance reasoning without explicit verbal traces.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of natural language-based chain-of-thought reasoning in LLMs, which restrict the model's expressive capacity, and to advance research in latent reasoning by establishing a clearer framework for understanding this emerging field.

Method: The paper examines neural network layers as substrates for reasoning, investigates latent reasoning methods like activation-based recurrence and hidden state propagation, and explores advanced models like infinite-depth reasoning via masked diffusion.

Result: The survey unifies latent reasoning methodologies, showcases how computational structures internalize reasoning traces, and highlights advanced paradigms that expand LLM reasoning capabilities.

Conclusion: The paper offers a conceptual framework of latent reasoning for LLMs, promoting understanding and guiding future research to innovate methods at the intersection of reasoning and cognition with LLMs.

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [100] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: The paper details DS@GT team's participation in CLEF 2025 CheckThat! Task 4a on classifying scientific discourse in tweets, using transformer finetuning, few-shot prompting, and ensemble modeling.


<details>
  <summary>Details</summary>
Motivation: To develop methods for effectively identifying scientific claims, references, or mentions in tweets for a multiclass classification task.

Method: The authors employed three methods: transformer fine-tuning, few-shot large language model (LLM) prompting, and an ensemble model informed by prior experiments.

Result: The team achieved a macro-averaged F1 score of 0.8611, surpassing the baseline score of 0.8375, and placed 7th in the competition.

Conclusion: The combination of transformer, prompt-based LLM approaches, and ensemble modeling yielded competitive results, demonstrating their methods' effectiveness in the task. The code is publicly available.

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [101] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: The paper introduces a framework for evaluating the efficiency and effectiveness of large language model (LLM)-based rerankers using the hardware-independent metrics RPP and QPP.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM-based reranking systems rely on hardware-dependent or proxy metrics, making the efficiency-effectiveness assessment inconsistent and challenging to interpret.

Method: The authors propose two new hardware-agnostic metrics (RPP and QPP) and develop a FLOPs estimator for calculating the computational demands of LLM-based rerankers without running experiments.

Result: Comprehensive experiments were conducted using the proposed evaluation framework, revealing insights into the efficiency-effectiveness trade-offs of various LLM-based rerankers.

Conclusion: The novel metrics and accompanying FLOPs estimator offer a standardized approach for evaluating LLM-based rerankers, encouraging further discussion and research in this area from a computational efficiency standpoint.

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [102] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: The paper introduces Agent KB, a framework that improves agent-based problem solving via shared knowledge and experience transfer, achieving significant performance boosts in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Language agents struggle with error correction and reusing experiences across domains, which hinders complex problem solving.

Method: The authors propose a Reason-Retrieve-Refine pipeline in the Agent KB framework, utilizing hierarchical knowledge capturing both high-level strategies and detailed logs for cross-agent knowledge transfer.

Result: Agent KB improves success rates in GAIA benchmark by up to 16.28%, with Claude-3 enhancing from 38.46% to 57.69%, and GPT-4 from 53.49% to 73.26%. It also increases Claude-3's performance on SWE-bench code repair from 41.33% to 53.33%.

Conclusion: Agent KB provides a modular and framework-agnostic infrastructure, allowing agents to learn from past experiences and adapt successful strategies to new tasks, making it a valuable tool for advancing language agents.

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [103] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: Generative text-to-image models struggle with prompt adherence due to noisy datasets, which necessitates heavy prompt engineering. This paper proposes the use of a consistent caption structure during training, resulting in significant improvements in alignment and controllability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of poor text-to-image alignment caused by the noisy and unstructured nature of large-scale datasets like LAION-5B, necessitating excessive prompt engineering.

Method: The study introduces a curated dataset (Re-LAION-Caption 19M) with captions following a four-part structure (subject, setting, aesthetics, camera details) and fine-tunes text-to-image models (PixArt-Σ and Stable Diffusion 2) on structured versus randomized captions to evaluate improvements in alignment.

Result: Models trained with structured captions consistently outperformed those trained with randomized captions, achieving higher text-image alignment scores using visual question answering (VQA) models.

Conclusion: Enforcing consistent caption structures during training significantly enhances model controllability and text-to-image alignment, reducing dependency on prompt engineering. The dataset is made publicly available for further research.

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [104] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: CorrDetail introduces a framework to tackle the shortcomings of existing deepfake detection methods by enhancing interpretability and addressing hallucination issues.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of facial deepfakes due to advancements in image generation technology poses security challenges, necessitating better detection methods.

Method: CorrDetail employs a self-correction framework with error-guided questioning, visual fine-grained detail enhancement, and a fusion decision strategy to improve deepfake detection.

Result: Experimental results show CorrDetail achieves state-of-the-art performance, accurately identifies forgery details, and exhibits strong generalization capabilities.

Conclusion: CorrDetail provides an interpretable and effective approach to deepfake detection, enhancing reliability and discriminative capacity against extreme samples.

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [105] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: YOLO-APD is a customized YOLOv8-based deep learning framework developed for efficient and accurate pedestrian detection on geometrically complex roadways, achieving significant performance improvements over baseline models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard RGB camera-based pedestrian detection systems on complex road surfaces, particularly curved roadways, for autonomous vehicles.

Method: The paper proposes YOLO-APD, enhancing the YOLOv8 framework with innovations such as SimAM, C3Ghost modules, SimSPPF, Mish activation, and an Intelligent Gather & Distribute (IGD) module. It also incorporates adaptive region-of-interest processing using vehicle steering dynamics.

Result: YOLO-APD achieves 77.7% mAP@0.5:0.95 and a pedestrian recall rate exceeding 96% on a custom CARLA dataset. It outperforms baseline models, including YOLOv8, while maintaining 100 FPS real-time processing.

Conclusion: The study introduces a highly effective, efficient, and affordable perception system suitable for autonomous vehicles in challenging environments. It highlights the potential for future applications but emphasizes the need for domain adaptation.

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [106] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Spotlight is a virtual staining method that enhances learning of cellular structures by employing selective loss functions, improving downstream tasks in microscopy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of training virtual staining models that focus on biologically meaningful signals rather than reproducing background noise and artifacts.

Method: Spotlight employs histogram-based foreground estimation to mask pixel-wise loss and utilizes Dice loss for shape-aware predictions, enhancing focus on cellular structures.

Result: Spotlight demonstrates superior morphological representation and pixel-level accuracy, optimizing virtual stains for tasks like segmentation and profiling.

Conclusion: Spotlight represents a robust advancement in virtual staining methods, prioritizing biologically relevant structures and enhancing microscopy analysis.

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [107] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: The paper evaluates foundational models for agriculture-related tasks and advocates for the development of a specialized agricultural foundational model.


<details>
  <summary>Details</summary>
Motivation: Address challenges of food security amidst population growth and climate change by exploring innovative solutions through foundational models.

Method: Survey and empirically evaluate existing foundational models for agricultural tasks, and propose a requirements framework for an ideal agricultural model (CropFM).

Result: Current foundational models show potential but also demonstrate the necessity for a specialized model tailored specifically to agriculture.

Conclusion: Dedicated foundational models for agriculture are essential to address domain-specific challenges and improve monitoring and productivity.

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [108] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: The paper introduces a deep learning-based method for enhancing underwater image quality using publicly available datasets and human subjective assessments.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of improving the visual quality of underwater images, which is essential for various applications in marine exploration and research.

Method: The approach involves training a classifier network to differentiate between high/low-quality images, followed by using GANs to enhance low-quality ones with metrics like PSNR, SSIM, and UIQM.

Result: The model demonstrates significant improvement in both perceived and measured underwater image quality, especially focusing on criteria such as color fidelity and sharpness.

Conclusion: The integration of subjective human assessments into deep learning frameworks can effectively improve underwater image quality, showing promise for other image enhancement tasks.

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [109] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: This paper introduces pFedMMA, a personalized federated learning framework using multi-modal adapters for vision-language tasks, achieving improved trade-offs between personalization and generalization.


<details>
  <summary>Details</summary>
Motivation: While Vision-Language Models (VLMs) like CLIP generalize well in zero- and few-shot settings, adapting them to decentralized and heterogeneous data in federated learning contexts remains a challenge, often sacrificing generalization for personalization.

Method: The proposed method, pFedMMA, employs modality-specific up- and down-projection layers and a globally shared projection within multi-modal adapters. Clients locally adapt to their data distributions while collaboratively training a shared projection for global generalization, exchanging only the shared component to maintain communication efficiency.

Result: pFedMMA demonstrates state-of-the-art performance in balancing personalization and generalization across eleven datasets, including challenging domain- and label-shift scenarios, surpassing existing federated prompt tuning methods.

Conclusion: This study validates pFedMMA as an effective framework for personalized federated learning, leveraging multi-modal adapters to address challenges in vision-language tasks while maintaining communication efficiency and achieving robust performance.

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [110] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: LoongX introduces a hands-free image editing approach enabled by multimodal neurophysiological signals combined with diffusion models and advanced fusion techniques.


<details>
  <summary>Details</summary>
Motivation: To make image editing accessible for individuals with limited motor control or language abilities through brain-computer interface technology.

Method: The system employs multimodal neurophysiological signals, integrates cross-scale state space (CS3) and dynamic gated fusion (DGF) modules, uses diffusion models fine-tuned for semantic alignment, and pre-trains encoders with contrastive learning.

Result: LoongX achieves comparable performance to text-driven methods and performs better when neural signals are integrated with speech inputs.

Conclusion: The paper demonstrates the promise of neurophysiological signal-driven image editing systems in creating accessible and intuitive creative technologies, with datasets and code provided for future development.

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [111] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: The paper reviews motion generation techniques, categorizing them by their generative strategies and analyzing recent advancements in the field from 2023 onward.


<details>
  <summary>Details</summary>
Motivation: The field of motion generation has rapidly evolved, incorporating diverse modeling paradigms, yet there lacks a comprehensive review focused on categorizing recent methodologies for improving clarity in comparisons and identifying challenges.

Method: The authors categorize motion generation techniques based on their generative strategies, analyze key components such as architecture, conditioning mechanisms, and generation settings, and compile data on evaluation metrics and datasets used.

Result: The survey provides a structured categorization of methods, summarizes recent advancements, and highlights open challenges within the field.

Conclusion: This review serves as a foundational reference for researchers, offering clarity on recent motion generation developments, their methodologies, and future directions.

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [112] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: This paper introduces a method for efficient local editing in 3D scenes by combining 2D diffusion-based semantic parsing with iterative 3D Gaussian Splatting, achieving state-of-the-art performance and significant speedup.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D scene editing often struggles with targeted manipulations due to limitations in 3D semantic parsing compared to 2D approaches. Enhancing precision and efficiency in this domain is crucial.

Method: The method uses 2D diffusion editing to identify modification regions, applies inverse rendering for 3D localization, refines frontal views, and initializes a coarse 3D Gaussian Splatting consistent across views using depth maps predicted by a 2D foundation model.

Result: Experiments show the proposed technique achieves state-of-the-art results in 3D scene local editing tasks while delivering up to four times faster performance compared to previous methods.

Conclusion: The approach effectively enhances fidelity in 3D scene editing by combining 2D parsing with iterative 3D reconstruction, ensuring coherence across perspectives while significantly improving efficiency.

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [113] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: A new activation function, SoftReMish, surpasses ReLU, Tanh, and Mish in CNN image classification tasks, achieving best loss and accuracy on MNIST.


<details>
  <summary>Details</summary>
Motivation: To enhance CNN performance in image classification by introducing an optimized activation function.

Method: SoftReMish was integrated into a standard CNN using the MNIST dataset, tested against ReLU, Tanh, and Mish, and evaluated through training loss and validation accuracy.

Result: SoftReMish achieved the best minimum training loss (3.14e-8) and validation accuracy (99.41%), outperforming other activation functions.

Conclusion: SoftReMish provides better convergence and generalization in CNNs, making it a strong candidate for visual recognition tasks.

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [114] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM extends SAM2 to open-vocabulary tasks by combining it with a vision-language model, offering a unified, efficient, and generalizable segmentation solution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of segmenting objects based on open-ended language prompts for diverse and unseen categories.

Method: The framework combines SAM2 with multi-modal embeddings from a lightweight vision-language model, freezing pre-trained components and training only 4.5M parameters, with enhancements for instance awareness and generalization.

Result: OpenWorldSAM achieves state-of-the-art performance in open-vocabulary segmentation tasks on benchmarks like ADE20k, PASCAL, ScanNet, and SUN-RGBD.

Conclusion: The method generalizes well to unseen categories and performs effectively in zero-shot scenarios, providing a flexible and efficient segmentation solution.

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [115] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: The paper introduces an AI-driven, vision-guided sprayer system for precise herbicide application to reduce costs and environmental impact. Indoor trials confirm its effectiveness in detecting and adjusting for weed canopy size.


<details>
  <summary>Details</summary>
Motivation: The current use of excessive herbicides in agriculture increases costs, causes environmental pollution, and leads to resistant weeds. This paper seeks to provide precise herbicide application solutions to address these challenges.

Method: The system uses YOLO11n and YOLO11n-seg deep learning models with NVIDIA Jetson Orin Nano for weed detection and canopy size estimation. The spray nozzle activation is controlled via an Arduino-based relay interface. Performance was tested with Hibiscus plants in indoor trials.

Result: The YOLO11n model achieved high object detection accuracy (mAP@50 of 0.98). Spray coverage accuracy was validated, showing an upward adjustment based on canopy size: 16.22% for small canopies and over 21% for medium to large canopies.

Conclusion: The AI-driven system demonstrates feasibility and precision for selective herbicide application using real-time deep learning on low-cost hardware. Future work will expand capabilities to detect weed species in crop fields and validate results in varied conditions.

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [116] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: The paper proposes using large vision models and driving video analysis to identify cognitive status in older drivers, aiming to detect early signs of decline such as Alzheimer's and MCI.


<details>
  <summary>Details</summary>
Motivation: Current diagnostic methods for cognitive decline are expensive and time-consuming, leading to underdiagnosis.

Method: The framework analyzes real-world driving videos using large vision models to extract behavioral patterns linked to cognitive impairment.

Result: The method allows classification of cognitive status and prediction of disease progression using everyday driving data.

Conclusion: This approach offers a scalable, non-invasive solution for early cognitive decline detection, benefiting proactive interventions and reducing societal impacts.

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [117] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: The paper introduces 'Cloud Diffusion Models,' which replace traditional white noise in diffusion models with scale-invariant noise profiles to improve image generation.


<details>
  <summary>Details</summary>
Motivation: Natural images exhibit scale-invariance in their statistical properties, unlike the white noise used in current diffusion models. Incorporating scale-invariant noise could improve image generation.

Method: Introduce the concept of 'Cloud Diffusion Models' that leverage scale-invariant noise profiles instead of white noise in diffusion models.

Result: Proposed benefits include faster inference, better high-frequency details, and enhanced controllability, but no experimental results are included in this paper.

Conclusion: Cloud Diffusion Models could result in superior image generation performance; experimental comparison with traditional models is forthcoming.

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [118] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet is introduced as a novel multi-view diffusion model tackling the challenge of generating consistent multi-view images from a single input while preserving spatial and 3D reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating spatially consistent multi-view images for better 3D surface reconstruction, which often suffers from degraded quality due to inconsistency across views.

Method: The method involves using a multi-view diffusion architecture. It employs parallel diffusion models for each viewpoint that collaboratively create a shared latent space for consistent image generation. The approach aggregates encodings on orthogonal planes from different views and processes them to propagate information and fill in missing details.

Result: LoomNet achieves efficient generation of 16 high-quality, coherent multi-view images in 15 seconds and outperforms state-of-the-art methods in terms of image quality and reconstruction accuracy.

Conclusion: LoomNet successfully addresses multi-view consistency issues, offering a robust solution with improved reconstructed quality and showcasing creative capability by producing diverse and plausible novel views.

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [119] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: Introduced a state-of-the-art text-image retrieval model excelling at multiple benchmarks with high retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: The growing demand for retrieval systems that effectively operate across text and image modalities.

Method: Modified NVIDIA Eagle2 VLM with bidirectional attention and integrated ColBERT-style late interaction mechanism in a unified embedding space alongside two-stage training.

Result: Achieved state-of-the-art NDCG@5 scores of 91.0 (ViDoRe V1) and 63.5 (ViDoRe V2), leading both leaderboards (as of June 2025).

Conclusion: The model provides strong retrieval accuracy across modalities but involves trade-offs in storage and efficiency, which are comprehensively analyzed.

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [120] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: The paper develops a pipeline for augmenting monocular dashcam footage with realistic distortions to aid perception research for autonomous vehicles in African driving scenarios.


<details>
  <summary>Details</summary>
Motivation: The key barrier identified is the lack of autonomous vehicle datasets from African regions, which hinders progress in perception models for low-resource settings.

Method: The paper creates a procedural augmentation pipeline using modules to simulate optical effects (lens distortion, TPS, etc.) and weather effects (fog, lens flare).

Result: They establish a benchmark by testing image restoration models and releasing an augmented dataset and distortion toolkit for further research.

Conclusion: This work effectively supports cost-efficient perception research in underrepresented African contexts by addressing the data scarcity challenge.

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [121] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout addresses limitations in layout generation by introducing relation-CoT for improved spatial relationships and aesthetics.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods underperform in capturing spatial relationships among design elements, creating structural issues in layout generation.

Method: ReLayout introduces enhanced layout annotations with explicit relation definitions, as well as a rebalance sampler to manage layout prototypes.

Result: Experiments show ReLayout outperforms baselines, generating aesthetically pleasing and diverse layouts aligned with human preferences.

Conclusion: ReLayout improves layout generation through structured designs and relation-based enhancements, achieving better human-aligned aesthetics and explainability.

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [122] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: The paper introduces a novel Cross-modal Transition-guided Network (CTNet) to address challenges in multi-modal face anti-spoofing, particularly handling distribution discrepancies, out-of-distribution attacks, and missing modalities.


<details>
  <summary>Details</summary>
Motivation: To enhance biometric authentication systems by improving robustness and reliability in multi-modal face anti-spoofing tasks, addressing challenges like distribution discrepancies and modality unavailability.

Method: The paper proposes CTNet, which learns consistent cross-modal transitions among live samples, contrasts them with inconsistent transitions between live and spoof samples, and reconstructs missing modalities' features during inference.

Result: The experimental results show that CTNet outperforms previous two-class multi-modal face anti-spoofing methods across various testing protocols.

Conclusion: CTNet improves robustness in multi-modal FAS by addressing distribution discrepancies, detecting out-of-distribution attacks, and handling missing modalities, leading to better overall performance.

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [123] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: The paper introduces a semi-supervised defect detection framework (DSYM) that achieves high precision with reduced labeling requirements, using conditional diffusion, collaborative training, and optimization strategies.


<details>
  <summary>Details</summary>
Motivation: Existing defect detection methods in industrial quality inspection, such as manual inspections and basic algorithms, are inefficient, costly, and lack robustness.

Method: The proposed framework uses conditional diffusion for multi-scale pseudo-defect sample generation, incorporates pseudo-labels from unlabeled data, and employs noise filtering using a CLIP-based mechanism.

Result: Experimental results show the framework achieves 78.4% mAP@0.5 with typical labeled data and 75.1% mAP@0.5 with only 40% labeled data, demonstrating high efficiency and performance.

Conclusion: This semi-supervised framework offers a robust, data-efficient solution for defect detection in industrial applications and is open-sourced for broader use.

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [124] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: The paper introduces GSVR, a 2D Gaussian-based video representation achieving high reconstruction quality, fast training (2s/frame) and decoding (800+ FPS), outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing video representation methods prioritize reconstruction quality but face challenges with slow decoding speeds, high computation costs, and long training times.

Method: GSVR proposes a hybrid deformation field to model video dynamics via tri-plane and polynomial motion. It includes a Dynamic-aware Time Slicing strategy to adaptively segment videos for varied motion and uses quantization-aware fine-tuning with image codecs for compact representation.

Result: GSVR achieves 800+ FPS and 35+ PSNR on Bunny with only 2 seconds of training per frame. It also delivers faster decoding speed and better convergence compared to prior techniques while excelling in video compression and interpolation tasks.

Conclusion: GSVR addresses limitations of existing methods by enhancing efficiency in training and decoding without compromising visual quality, establishing itself as a promising video representation solution.

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [125] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0 is an open-source toolkit for OCR and document parsing that introduces competitive, efficient solutions for multilingual text recognition, hierarchical parsing, and key information extraction.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient document understanding during the rise of large language models.

Method: Develop and deploy compact models like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4 to achieve high accuracy and efficiency compared to large-scale vision-language models.

Result: Models with fewer than 100 million parameters demonstrated competitive performance, rivaling billion-parameter vision-language models.

Conclusion: PaddleOCR 3.0 offers accessible, efficient solutions and tools for developers aiming to build intelligent document applications.

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [126] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: This paper introduces Accordion, a framework for converting non-editable AI-generated designs into editable layered designs, enhancing usability for graphic designers.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge posed by AI-generated graphic designs, which, despite their high quality, are often non-editable, limiting their practical utility for designers.

Method: Accordion employs a vision language model guided by curated prompts across three stages, utilizes visual harmony for global image decomposition, and leverages tools like SAM and element removal models. It is trained on the Design39K dataset with enhanced AI-generated image-ground truth pairs.

Result: Accordion performs effectively on various graphic design tasks, such as text-to-template generation, adding text to backgrounds, text de-rendering, and generating design variations, as evidenced by experimental evaluations and user studies.

Conclusion: Accordion offers a novel top-down approach for creating editable layered designs, bridging the gap between AI-generated designs and practical usability for graphic designers, and it sets a benchmark for design refinement and layering.

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [127] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: The paper introduces Kernel Density Steering (KDS), a method to improve image restoration using diffusion models, addressing issues of fidelity and artifacts through ensemble-based local mode-seeking.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for image restoration often fail due to inconsistent fidelity and artifacts, and there is a need for a robust framework to mitigate these issues.

Method: Kernel Density Steering (KDS) uses an $N$-particle ensemble of diffusion samples and applies patch-wise kernel density estimation gradients to steer samples towards high-density, shared regions for improved outputs.

Result: KDS achieves superior quantitative and qualitative results, particularly in challenging image restoration tasks like super-resolution and inpainting, requiring no retraining or external verification.

Conclusion: KDS is a plug-and-play solution enhancing diffusion models through collective mode-seeking, improving restoration quality at some computational expense.

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [128] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: This paper introduces "Generative HMC (GenHMC)", a generative model to enable photorealistic avatar animations in VR/AR without requiring extensive and paired HMC-dome captures.


<details>
  <summary>Details</summary>
Motivation: The difficulty in obtaining synchronized ground truth states of faces between partial observations from head-mounted cameras and full observations from dome cameras severely limits the feasibility of generating photorealistic avatar animations efficiently.

Method: The authors present "Generative HMC (GenHMC)", a novel generative approach that uses large unpaired HMC captures and dome captures to synthesize high-quality HMC images by disentangling facial expression, viewpoint, and appearance.

Result: GenHMC successfully achieves better ground truth accuracy by disentangling expression and appearance, handles unseen identities without paired datasets, and enhances data efficiency and accuracy for avatar animation.

Conclusion: The method provides a scalable and reusable solution to creating high-quality avatar animations, advancing the state-of-the-art in VR/AR photorealism while addressing limitations of operational costs and dataset requirements.

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [129] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: The paper proposes AdaptaGen, a framework for domain-specific image generation that addresses limitations in semantic accuracy and detail fidelity while incorporating hierarchical optimization and cross-modal adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively combine semantic understanding and visual representation in specialized domains, as well as inadequately integrate domain-specific semantic constraints, leading to hallucinations and deviations.

Method: AdaptaGen employs a hierarchical semantic optimization system integrating matrix-based prompt optimization and multi-perspective relationships, along with a cross-modal adaptation mechanism and two-phase caption transformation to enhance semantic fidelity and visual diversity.

Result: Experimental results show that AdaptaGen effectively improves image quality, diversity, and semantic consistency across 40 categories using just 16 images per category, emphasizing its effectiveness in domain-specific tasks.

Conclusion: AdaptaGen successfully addresses key challenges in domain-specific image generation, offering improvements in semantic coherence, thematic element preservation, and the generation of diverse and high-quality images while adhering to domain constraints.

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [130] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: The paper introduces OFFSET, a network for improved Composed Image Retrieval (CIR), addressing query degradation and visual focus biases by leveraging dominant portion segmentation and textual input prioritization.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in CIR where noise in visual data and lack of prioritization for textual input degrade retrieval accuracy, thus hindering its potential.

Method: Proposes OFFSET, which includes a focus mapping-based feature extractor with dominant portion segmentation and dual focus mapping modules, and a textually guided focus revision module, enhancing focus and reducing noise.

Result: Comprehensive experiments on four benchmark datasets confirmed that the OFFSET model outperforms existing methods in CIR.

Conclusion: OFFSET effectively mitigates the limitations of CIR using innovative feature extraction and focus revision techniques, significantly improving retrieval performance.

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [131] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: This paper introduces a structural knowledge-guided complex diffusion model for PolSAR image classification, effectively capturing phase information and preserving structural details.


<details>
  <summary>Details</summary>
Motivation: Traditional real-valued diffusion models struggle with capturing complex-valued phase information in PolSAR data and preserving fine image structures.

Method: The method leverages the Contourlet transform to decompose PolSAR data into frequency subbands. It uses a knowledge-guided complex diffusion network to process statistical properties in the low-frequency subbands while high-frequency information guides diffusion to enhance edge detail preservation.

Result: Experiments on three real-world PolSAR datasets show the method outperforms state-of-the-art approaches, particularly in edge preservation and region homogeneity in complex terrains.

Conclusion: The proposed approach effectively improves PolSAR image classification performance by integrating structural information and multiscale representations for enhanced edge and region preservation.

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [132] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes Dynamic Rank Adaptation (DRA), a method aimed at enhancing the generalization ability of large vision-language models for recognizing new classes.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to maintain generalization abilities, particularly for unseen classes, due to equal treatment of all image and text model tokens, causing overfitting and degraded general representations.

Method: DRA dynamically allocates adaptation ranks to features based on their importance during training, using token importance grouping, rank adaptation, a channel response mechanism, and L1 regularization.

Result: DRA outperforms existing methods, particularly in recognizing new classes across benchmarks like base-new class tasks, cross-dataset evaluations, and domain generalization tests.

Conclusion: Dynamic Rank Adaptation successfully improves the ability of vision-language models to generalize, particularly to unseen classes, by focusing adaptation on more informative features.

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [133] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: The paper develops a fusion method combining RGB and event sensors for spacecraft pose estimation to overcome lighting challenges and will release a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RGB and event sensors for robust spacecraft pose estimation under harsh lighting conditions.

Method: The study uses a beam-splitter prism for precise alignment between RGB and event sensors, incorporates a RANSAC-based fusion technique, and integrates uncertainty estimation to mitigate challenges.

Result: The proposed method demonstrated strong pose estimation performance on a benchmark dataset gathered in controlled laboratory conditions, validating the advantages of sensor fusion.

Conclusion: The event-RGB sensor fusion approach effectively combats lighting challenges in spacecraft pose estimation, suggesting event sensors' potential and fostering community research with the release of a public dataset.

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [134] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: The paper proposes a framework using diffusion models to segment brain lesions, reverse tissue deformations, and reconstruct healthy pre-lesion brain images, demonstrating better accuracy than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current lesion segmentation methods fail to distinguish between irreversibly damaged brain tissue and deformed tissue caused by lesion growth or swelling.

Method: The framework includes a pipeline to segment abnormal brain regions, reverses tissue deformations to isolate the core lesion area, and uses inpainting to reconstruct an estimation of the healthy pre-lesion brain.

Result: The approach shows improved accuracy in lesion segmentation, characterization, and brain labeling compared to conventional methods.

Conclusion: This diffusion model-based framework offers a novel, accurate, and robust method for brain lesion analysis, with potential applications in both clinical and research settings.

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [135] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: The paper presents R-VLM, a GUI automation method enhancing grounding accuracy through zoomed-in region proposals and IoU-aware objectives, achieving significant improvements in benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in GUI automation caused by inaccurate grounding due to cluttered screenshots and simplistic learning objectives.

Method: R-VLM leverages zoomed-in region proposals for element localization and introduces an IoU-aware objective for superior grounding performance.

Result: The model improved GUI grounding accuracy by 13% on benchmarks like ScreenSpot and AgentStudio, and enhanced navigation task accuracy by 3.2-9.7% on AITW and Mind2Web.

Conclusion: R-VLM successfully combines VLM techniques with object detection strategies, advancing the precision and applicability of GUI automation systems.

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [136] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: Medical video generation lags despite advancements in general video generation due to dataset limitations. MedVideoCap-55K and MedGen are proposed to address this gap.


<details>
  <summary>Details</summary>
Motivation: To address the lack of medical video generation tools that combine high visual fidelity and medical accuracy, essential for training, education, and simulation.

Method: Introducing MedVideoCap-55K, a curated dataset of 55,000 medical video clips, and building MedGen, a model trained on the dataset to generate accurate and visually high-quality medical videos.

Result: MedGen achieves leading performance among open-source models and performs comparably to commercial systems in benchmarks for visual quality and medical accuracy.

Conclusion: MedVideoCap-55K and MedGen can significantly contribute to the development of medical video generation, fostering advancements in clinical training and research.

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [137] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces the Integrated Structural Prompt (ISP) for Vision-Language Models (VLMs) to improve the interaction between text and image data and enhance generalization. Extensive experiments confirm its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Prompt learning methods for VLMs often neglect the structural relationships between prompts and tokens in or across text and image modalities, and balancing performance across base and new classes remains challenging.

Method: The proposed Integrated Structural Prompt (ISP) incorporates self-structural and cross-structural prompt modules to capture relationships within and between modalities. A sample probing module dynamically adjusts loss coefficients based on sample difficulty to avoid overfitting and enhance generalization.

Result: The ISP method achieves competitive performance on tasks involving base-to-new generalization, cross-dataset evaluation, and domain generalization, performing well against state-of-the-art methods.

Conclusion: The paper concludes that ISP effectively boosts the information interaction between text and image domains while maintaining feature stability and improving generalization to unseen data, addressing key limitations of existing prompt learning methods.

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [138] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: The paper introduces LiON-LoRA, a framework that addresses challenges in precise video control using Video Diffusion Models by rethinking Low-Rank Adaptation fusion principles.


<details>
  <summary>Details</summary>
Motivation: To enable precise control over camera trajectories and object motion in video synthesis, addressing issues like unstable fusion and non-linear scalability in existing LoRA approaches.

Method: LiON-LoRA incorporates three key elements: analyzing orthogonality in shallow VDM layers, ensuring norm consistency for stable fusion, and integrating controllable tokens into the diffusion transformer for linear motion adjustment.

Result: LiON-LoRA improves trajectory control accuracy and motion strength adjustment, outperforming state-of-the-art methods while requiring minimal training data.

Conclusion: LiON-LoRA provides a unified framework for spatial and temporal controllability in video synthesis using VDMs, offering superior generalization and precise control for constrained data scenarios.

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [139] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: This paper compares diverse hyperspectral anomaly detection (HAD) methods, categorizing them and evaluating their performance across 17 datasets to uncover strengths, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by existing HAD methods, such as computational complexity, noise sensitivity, and generalization issues, and provide a comprehensive comparison of detection techniques.

Method: The study categorizes HAD methods into four groups: statistical models, representation-based methods, classical machine learning, and deep learning. Evaluation was performed on 17 benchmarking datasets using metrics like ROC, AUC, and separability maps.

Result: Deep learning models were found to provide the highest detection accuracy, while statistical models excelled in computational efficiency, offering varying trade-offs in performance and speed.

Conclusion: The paper highlights the strengths and limitations of diverse HAD methods, offering insights for advancing research and practical applications in hyperspectral anomaly detection.

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [140] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: Introduction of SenseShift6D dataset to evaluate 6D object-pose estimation under varying sensor and lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Investigating the impact of real-world variations in illumination, exposure, gain, and depth-sensor modes, which are largely unexplored in existing benchmarks.

Method: Creation of a novel RGB-D dataset (SenseShift6D) with varied sensor and lighting configurations, and evaluation of adaptive test-time sensor controls vs data augmentation using state-of-the-art models.

Result: Sensor control during test-time improves performance, outperforming digital data augmentation and matching or exceeding increases in real-world training data.

Conclusion: SenseShift6D shifts 6D-pose evaluation toward sensor-aware robustness, enabling adaptive self-tuning systems for real-world environments.

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [141] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: An automated algorithm is introduced for white balance correction of microscopic images, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To tackle the difficulty of achieving properly balanced colored images in optical microscopy, even for experienced operators.

Method: Proposed a fully automatic white balance correction algorithm and validated it using 200 microscopic images containing commonly used pathomorphology specimens.

Result: Demonstrated the proposed algorithm's superior white balance performance compared to traditional photography algorithms, especially for hematoxylin-phloxine-saffron and immunohistochemical-stained images.

Conclusion: The presented white balance algorithm is more effective for color correction in microscopic imaging than conventional photographic methods.

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [142] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: The paper introduces DreamArt, a generative framework for creating high-fidelity, articulated 3D assets from single-view images using a three-stage pipeline.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing methods in generating articulated 3D objects, which either neglect part decomposition and articulation or require dense multi-view/interactive data.

Method: DreamArt uses a three-stage pipeline: reconstructing part-segmented 3D meshes, fine-tuning a video diffusion model for articulation priors, and optimizing motion/texture for coherent, high-quality output.

Result: Experimental results show that DreamArt generates high-quality articulated objects with accurate parts, realistic appearance, and plausible movement.

Conclusion: DreamArt provides a scalable solution for articulated 3D asset generation without requiring dense multi-view data, making it suitable for applications in Embodied AI and AR/VR.

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [143] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: The paper introduces TalkFashion, a multi-functional virtual try-on system guided by text instructions, enabling tasks such as full outfit changes and local editing with enhanced flexibility and automation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of versatility and flexibility in current virtual try-on systems, which mainly rely on end-to-end networks for single try-on tasks.

Method: A proposed TalkFashion intelligent assistant leverages large language models to analyze user instructions. It integrates an instruction-based local repainting model for automated local editing using multi-modal models.

Result: The system exhibited better semantic consistency and visual quality compared to existing methods based on experimental evaluations.

Conclusion: TalkFashion provides a more versatile and flexible approach to virtual try-on, supporting multifunctional tasks guided by natural text instructions and improving both editing quality and automation.

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [144] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: This paper introduces SPADE, an innovative framework for Panoptic Scene Graph Generation (PSG) aimed at improving spatial relation reasoning and relation prediction by utilizing a denoising diffusion model's inversion process.


<details>
  <summary>Details</summary>
Motivation: To address limitations in spatial relation reasoning in pre-trained vision-language models (VLMs) for PSG, leveraging the denoising model's inversion process to preserve spatial structure.

Method: SPADE consists of two steps: (1) inversion-guided UNet adaptation with LoRA-based fine-tuning, and (2) spatial-aware relation graph transformer for capturing contextual information.

Result: SPADE achieves superior performance compared to state-of-the-art methods on PSG and Visual Genome datasets, particularly in spatial relationship prediction and open-vocabulary settings.

Conclusion: SPADE offers a novel approach to PSG by integrating spatial-aware reasoning capabilities to overcome VLM limitations, setting new benchmarks in both closed- and open-set scenarios.

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [145] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: The paper introduces an autoregressive model, DREAM, improving document reconstruction by addressing errors and layout issues. It surpasses existing methods and aligns with several subtasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing document analysis methods, such as error propagation in multi-stage models and the inability of generative models to retain layout information.

Method: An end-to-end autoregressive model, DREAM, which translates text images into document reconstruction sequences. It also defines a standardized task, proposes a Document Similarity Metric (DSM), and introduces the "DocRec1K" dataset.

Result: DREAM achieves state-of-the-art results in document reconstruction tasks and demonstrates strong performance in related subtasks like layout analysis, text recognition, and formula recognition.

Conclusion: DREAM effectively resolves key challenges in document reconstruction, proving its superiority in performance and adaptability to multiple subtasks.

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [146] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: The paper introduces a method to condition synthetic camera sensor data with solar altitude to address gaps in daytime variation research using diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of real-world data acquisition for autonomous driving, such as labeling cost and scenario coverage, and the research gap in understanding daylight variation in sensor data.

Method: Proposes solar altitude as a conditioning variable for generating synthetic sensor data and employs normalization techniques with diffusion models to handle daylight sensitivity.

Result: Successfully captures lighting characteristics and illumination-dependent image noise while eliminating the need for manual labeling.

Conclusion: Utilizing solar altitude as a conditioning variable provides an effective way to synthetically generate accurate daytime variation data tailored for autonomous driving applications.

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [147] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: The paper introduces a framework for generating synthetic 3D bridge point cloud data with detailed annotations, enabling advancements in bridge inspection automation.


<details>
  <summary>Details</summary>
Motivation: Traditional manual bridge inspection is inefficient and 3D point cloud technology's potential is hindered by data incompleteness, requiring new methods to overcome these limitations.

Method: A framework is developed to generate complete 3D point clouds with fine annotations and simulate realistic incomplete data, supporting the training of segmentation and completion networks.

Result: PointNet++ showed 84.2% mIoU in real-world semantic segmentation using the synthetic data, while KT-Net excelled in component completion tasks.

Conclusion: The framework and dataset offer a new avenue for automating bridge infrastructure management and maintenance through advanced 3D visual analysis.

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [148] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper introduces a "2D-3D-2D" framework for image editing, which improves consistency and object identity preservation compared to traditional 2D editing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 2D image editing methods struggle with maintaining consistency and preserving object identity because they manipulate pixels directly.

Method: The proposed framework converts 2D objects into 3D representations for editing in a 3D environment. Then, the edited objects are reprojected and seamlessly blended back into the original 2D image.

Result: The framework achieves superior performance over methods like DragGAN and DragDiffusion, ensuring consistent edits and better object identity preservation.

Conclusion: This innovative approach to image editing provides a more effective and physically plausible solution through the integration of 3D manipulation, outperforming traditional 2D-only methods.

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [149] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: The paper introduces a model that combines vision and language systems to enhance video understanding for tasks like causal reasoning and future prediction, overcoming the current reliance on raw recognition.


<details>
  <summary>Details</summary>
Motivation: Existing video models are good at identifying actions but lack the necessary commonsense knowledge for cognitive tasks like reasoning and prediction.

Method: The framework integrates a Vision Foundation Model for perception with a Large Language Model for reasoning, utilizing a fusion module inspired by Q-Former to distill visual features into a language-compatible format.

Result: The model achieves state-of-the-art performance across various benchmarks, particularly in zero-shot generalization, and ablation studies confirm the importance of its components.

Conclusion: This work enhances video understanding capabilities by fusing visual and reasoning systems, potentially advancing AI applications in fields like robotics and human-computer interaction.

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [150] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces I$^2$R, a few-shot segmentation method addressing semantic gaps and visually similar regions that degrade segmentation performance, achieving improved benchmarks results on PASCAL-5$^i$ and COCO-20$^i$.


<details>
  <summary>Details</summary>
Motivation: Few-shot segmentation addresses the challenge of reducing annotation efforts while enabling segmentation models to generalize to novel classes with minimal examples. Existing approaches suffer from semantic mismatches and visually similar yet distinct regions that cause prediction inaccuracies.

Method: The proposed I$^2$R method employs category-specific representations to aggregate global semantic cues for accurate inter-image localization and introduces a directional masking strategy to suppress conflicting pixel pairs.

Result: I$^2$R outperformed state-of-the-art methods with improvements of 1.9% and 2.1% in mIoU under the 1-shot setting on PASCAL-5$^i$ and COCO-20$^i$, respectively.

Conclusion: I$^2$R effectively mitigates inter- and intra-image discrepancies in few-shot segmentation, showcasing promising advancements over existing methods and improving segmentation accuracy.

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [151] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: This paper introduces the USIGAN model for virtual IHC staining using H&E images, achieving improved semantic consistency despite weakly paired input conditions.


<details>
  <summary>Details</summary>
Motivation: Virtual IHC staining provides a cost-effective method for pathological analysis but faces challenges from spatial heterogeneity under weakly paired conditions.

Method: The USIGAN model uses unbalanced self-information transport, eliminating weakly paired terms from joint distributions and integrating the UOT-CTM and PC-SCM mechanisms for better correlation and semantic consistency.

Result: Experiments on publicly available datasets show superior performance of USIGAN according to clinically relevant measurements like IoD and Pearson-R correlation.

Conclusion: USIGAN improves the accuracy of virtual IHC staining through innovative modeling, addressing weak pairing issues and maintaining pathological relevance.

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [152] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: The paper introduces DFYP, a framework for improved crop yield prediction using remote sensing, overcoming existing spatial and generalization challenges via innovative spectral and spatial modeling techniques.


<details>
  <summary>Details</summary>
Motivation: Crop yield prediction faces difficulties due to complex spatial and dynamic agricultural patterns as well as limited capacity of existing methods to generalize across crops and years.

Method: DFYP employs three components: a Resolution-aware Channel Attention module for spectral reweighting, an Adaptive Operator Learning Network for better spatial edge extraction, and a dual-branch architecture for flexible data fusion across resolutions and crop types.

Result: DFYP achieves superior prediction accuracy compared to current methods, validated by experiments on multi-year MODIS and Sentinel-2 datasets, with significant improvements in RMSE, MAE, and R2 metrics.

Conclusion: The proposed DFYP framework is highly effective and robust for agricultural monitoring, addressing spatial modeling and generalization deficiencies in remote sensing methods.

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [153] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: The paper introduces D-FCGS, a feedforward framework for efficiently compressing dynamic 3D Gaussian Splatting sequences, achieving high performance without per-scene optimization.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in compression for dynamic 3D scenes and improve scalability in immersive applications.

Method: The authors propose a Group-of-Frames structure with I-P frame coding, sparse control points for motion extraction, and a dual prior-aware entropy model for compression, requiring no optimization per scene.

Result: D-FCGS matches the performance of optimization-based methods, achieves over 40x compression in under 2 seconds, and retains visual quality across viewpoints.

Conclusion: The paper demonstrates that the feedforward compression of D-FCGS can enhance scalability and efficiency for Free-viewpoint video transmission and storage, laying groundwork for immersive 3D experiences.

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [154] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: GeoMag (Geographical Magnifier) is a novel large model framework for remote sensing image analysis that excels in pixel-level tasks and reduces computational demands via adaptive resolution adjustment techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current Vision-Language Models (VLMs) in remote sensing, which struggle with pixel-level tasks, small-object recognition, and computational efficiency when handling high-resolution imagery.

Method: GeoMag employs Task-driven Multi-granularity Resolution Adjustment (TMRA) and Prompt-guided Semantic-aware Cropping (PSC) to dynamically adapt the focus of the model, improving its ability to target crucial areas while reducing background noise and computational overhead.

Result: GeoMag demonstrates superior performance in pixel-level tasks and competitive results across other granularities, as evidenced by experiments on 10 benchmarks.

Conclusion: The GeoMag framework significantly enhances remote sensing image parsing by optimizing visual representations of task-relevant areas, reducing computational loads, and achieving robust multi-granularity capabilities.

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [155] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: This paper addresses the challenge of temporally incomplete multimodal data in visual tracking by proposing a flexible framework that adapts to missing data rates and scene complexity, achieving state-of-the-art performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve visual tracking robustness against appearance variations and to effectively handle missing modalities caused by sensor synchronization issues, which is an underexplored area.

Method: The paper introduces a Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity and a video-level masking strategy to ensure temporal consistency and spatial completeness, enabling dynamic activation of computational units based on missing data rates.

Result: The proposed model adapts to varying missing data rates and scene complexity, achieving state-of-the-art performance on 9 benchmarks in both complete and missing modality scenarios.

Conclusion: The study emphasizes the importance of adaptable architectures for multimodal tracking in the context of incomplete data, providing a robust framework that could set a new standard in handling such challenges.

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [156] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: This paper evaluates the applicability of explainable AI (xAI) methods and metrics, initially designed for computer vision, in remote sensing (RS) scene classification tasks. It identifies key limitations and offers guidelines for their effective deployment.


<details>
  <summary>Details</summary>
Motivation: Most xAI methods are developed for natural images in computer vision, but their direct application in remote sensing may not be effective due to contextual differences in RS data.

Method: The study methodologically and experimentally analyzes 10 explanation metrics across 5 categories and assesses 5 feature attribution methods applied to 3 RS datasets. Key limitations in methods and metrics are identified.

Result: Findings highlight that perturbation-based methods like Occlusion and LIME depend on spatial characteristics, gradient-based methods like GradCAM struggle with multi-label images, and relevance propagation methods like LRP can lead to disproportionate attribution. Robustness and randomization metrics are more stable compared to localization or complexity metrics.

Conclusion: The paper offers practical guidelines for choosing suitable xAI methods, metrics, and hyperparameters for RS image scene classification, emphasizing the need for tailored approaches due to identified limitations.

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [157] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper proposes MGPO, a reinforcement learning framework for LMMs to improve visual grounding by using iterative cropping and binary rewards, achieving enhanced performance on visual question answering tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges that large multi-modal models (LMMs) face in processing high-resolution images and improve their visual grounding capabilities without costly grounding annotations.

Method: The authors developed MGPO, an RL framework that uses multi-turn conversations and policy optimization with binary rewards, enabling LMMs to better focus on key visual regions via iterative cropping.

Result: MGPO demonstrated a 5.4% improvement on in-distribution performance (MME-Realworld) and a 5.2% improvement on out-of-distribution tasks (V* Bench), outperforming models like GPT-4o after post-training.

Conclusion: MGPO enhances LMMs' visual grounding abilities, offering a practical and scalable solution to improve their performance in visual question answering tasks without relying on expensive grounding annotations.

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [158] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: The paper introduces three paradigms leveraging monocular depth estimation to enhance the robustness of Video Instance Segmentation (VIS), achieving state-of-the-art results with its methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges such as object occlusions, motion blur, and appearance variations during temporal association in Video Instance Segmentation.

Method: The approach involves systematically integrating monocular depth estimation using three paradigms: Expanding Depth Channel (EDC), Sharing ViT (SV), and Depth Supervision (DS).

Result: Benchmark evaluations revealed significant improvements in VIS robustness with EDC and SV methods. Specifically, EDC achieved a new state-of-the-art result of 56.2 AP on the OVIS benchmark using a Swin-L backbone.

Conclusion: This study demonstrates that depth cues are critical for enhancing video understanding and establishes them as powerful tools for robust VIS performance.

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [159] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: This paper introduces a sparse representation method for generalizable neural surface reconstruction, enabling higher-resolution reconstructions with significant memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Improve the scalability and resolution of generalizable neural surface reconstruction models, which are traditionally limited by memory inefficiency of dense 3D feature volumes.

Method: Uses a two-stage approach: 1) Train a network to predict voxel occupancies using posed images and depth maps. 2) Perform volume rendering only in voxels with high occupancy estimates. Custom algorithms for efficient sampling, feature aggregation, and querying in sparse volumes are introduced.

Result: Reduces storage requirements by over 50 times without performance loss, achieves reconstructions at 512^3 resolution compared to the typical 128^3, and outperforms state-of-the-art methods in reconstruction accuracy.

Conclusion: The proposed sparse representation method addresses memory inefficiency issues and allows significantly higher-resolution reconstructions on standard hardware while maintaining high accuracy.

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [160] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2 enhances motion-guided video generation by improving appearance and motion customization, introducing better design methods like decoupled personalization and gated self-attention.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in video generation, focusing on appearance and motion customization for multi-entity scenarios and reducing multimodal conditioning misalignment.

Method: It introduces a decoupled personalization extractor for detailed embeddings, a gated self-attention mechanism to harmonize modalities, and a contrastive loss to optimize motion and personalization consistency.

Result: Experimental results show that Tora2 performs competitively with state-of-the-art methods while offering improved motion control and multi-entity customization.

Conclusion: Tora2 marks a significant step forward in multi-condition video generation, achieving simultaneous multi-entity motion and appearance customization with advanced design innovations.

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [161] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: This paper introduces T-LoRA, a novel framework for fine-tuning diffusion models using only a single concept image, addressing overfitting issues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of overfitting when adapting diffusion models in data-limited scenarios, particularly with single concept images.

Method: T-LoRA employs a timestep-sensitive fine-tuning strategy and orthogonal initialization, enhancing diffusion timesteps and preserving adapter independence.

Result: Experiments show that T-LoRA outperforms existing techniques in achieving concept fidelity and text alignment under constrained data conditions.

Conclusion: T-LoRA offers effective personalization of diffusion models with minimal data, proving superior reliability and diversity compared to standard methods.

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [162] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: The paper introduces a scalable pipeline for automatic triplet generation and proposes a fully synthetic dataset (CIRHS) for Composed Image Retrieval (CIR), alongside a novel framework named CoAlign.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations of reliance on costly, manually labeled triplets in existing CIR methods to enhance scalability and zero-shot capability.

Method: The paper uses a large language model to generate diverse prompts, which control a text-to-image generative model to produce image pairs with identical elements. These are filtered, reorganized, and transformed into the CIRHS dataset. It also proposes the CoAlign framework for better contextual alignment.

Result: The CoAlign framework trained on the synthetic CIRHS dataset achieves outstanding zero-shot performance on benchmarks and surpasses state-of-the-art methods under supervised training.

Conclusion: Training CIR models on a fully synthetic dataset is feasible, and the authors’ proposed methods show superior performance both in zero-shot and supervised scenarios, making a significant contribution to scalable CIR.

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [163] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: The paper introduces SCINet, a novel framework for partial multi-label learning, leveraging multimodal models and semantic techniques to improve label-instance relationships.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of identifying ambiguous relationships between labels and instances in partial multi-label learning.

Method: Propose a bi-dominant prompter module for text-image correlations, a cross-modality fusion module to model co-occurrence patterns, and rich semantic augmentation using image transformations.

Result: SCINet achieves superior performance compared to existing methods on four benchmark datasets.

Conclusion: SCINet effectively tackles the challenges of partial multi-label learning using innovative semantic and multimodal strategies.

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [164] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: The paper proposes an ensemble-based approach to improve deepfake detection across diverse datasets, overcoming out-of-distribution data challenges.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models often falter on out-of-distribution data, making generalization crucial for real-world applications.

Method: Ensemble-based predictions leverage combining probabilities from state-of-the-art asymmetric models evaluated on diverse datasets.

Result: Experiments show that ensemble methods consistently outperform individual models in terms of stability and reliability across out-of-domain settings.

Conclusion: Asymmetric ensembling offers a scalable and robust framework for deepfake detection in scenarios with limited prior information on forgery characteristics.

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [165] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: The paper proposes a method to improve the geo-registration of LiDAR point clouds in GNSS-denied urban areas by aligning 3D point clouds with satellite images. It achieved significant localization accuracy improvements on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate localization in urban areas with high buildings and bridges is challenging due to GNSS signal denial, and existing methods relying on GNSS/IMU data fail in such conditions.

Method: The method aligns 3D point clouds with satellite images using a pre-trained Point Transformer model for road segmentation, global rigid alignment using intersection points, local refinement via RBF interpolation, and elevation correction with SRTM terrain data.

Result: The approach improved alignment accuracy significantly, achieving 55.3% and 77.4% planimetric alignment improvements on KITTI and Perth datasets, respectively, along with significant elevation correlation gains.

Conclusion: The proposed method offers a robust and effective solution for geo-registering LiDAR point clouds in dense urban environments without reliance on GNSS localization.

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [166] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: The paper proposes a framework called GCDA to address the issue of generating meaningful and legible text in text-to-image diffusion models, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Modern diffusion models generate impressive imagery but fail to produce readable text, limiting their practical utility in areas such as advertising and design.

Method: GCDA includes three innovations: dual-stream text encoding for glyph-aware representation, character-aware attention with attention segregation loss, and OCR-in-the-loop fine-tuning optimizing text readability and accuracy.

Result: Experimental benchmarks demonstrate GCDA's superiority in text rendering metrics (Character Error Rate: 0.08 vs 0.21; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality (FID: 14.3).

Conclusion: GCDA enhances text-to-image models by effectively integrating text rendering capabilities without compromising overall image quality, opening up new possibilities for practical applications.

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [167] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: The paper introduces VisualSpeaker, a method that generates realistic 3D facial animations using photorealistic rendering and perceptual lip-reading loss for improved visual speech recognition.


<details>
  <summary>Details</summary>
Motivation: Improving the quality of 3D facial animations for expressive avatar systems used in human-computer interaction, particularly focusing on accessibility, sign language, and accurate depictions of mouth movements.

Method: VisualSpeaker integrates photorealistic differentiable rendering with supervised visual speech recognition, introducing a perceptual lip-reading loss using a pretrained Visual Automatic Speech Recognition model to train the system.

Result: Tests on the MEAD dataset show that VisualSpeaker achieves a 56.1% improvement in Lip Vertex Error metrics and enhances perceptual animation quality while maintaining the controllability of mesh-driven animation.

Conclusion: VisualSpeaker bridges 2D computer vision and 3D facial animation domains, resulting in more natural and accurate animations, particularly benefiting sign language avatar systems for accessibility.

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [168] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: MEDTalk proposes a framework for dynamic emotional 3D facial animation, overcoming the limitations of static emotion labels by disentangling content from emotion embedding spaces and integrating multimodal inputs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diversity and naturalness in existing audio-driven emotional facial animation methods constrained by static emotion labels.

Method: Disentangles content and emotion using cross-reconstruction, dynamically predicts intensity variations from audio and text, and incorporates multimodal inputs for personalized facial expression generation.

Result: Generated facial animations exhibit synchronized lip movements, realistic emotional expressions, and user-specified customization, with streamlined integration into industrial pipelines.

Conclusion: MEDTalk enables fine-grained, dynamic emotional facial animations that enhance diversity, realism, and user control in both entertainment and industrial applications.

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [169] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: The paper introduces MCAM, a model for understanding driving behavior using multimodal causality, achieving state-of-the-art performance in matching visual and linguistic data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for driving behavior analysis fail to address spurious correlations among modalities and lack ego-vehicle causality modeling.

Method: The MCAM model uses multi-level feature extraction, directed acyclic graphs for causal analysis, and a vision-language transformer for feature alignment.

Result: MCAM outperforms on BDD-X and CoVLA datasets, excelling in visual-language causal relationship learning and capturing causal dynamics in videos.

Conclusion: MCAM demonstrates exceptional performance and applicability in autonomous driving scenarios, paving the way for improved multimodal causal modeling in driving video analysis.

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [170] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: This paper proposes a novel method for normal integration that explicitly handles depth discontinuities and works with generic central cameras, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for normal integration fail to explicitly address depth discontinuities and are restricted to orthographic or ideal pinhole cameras.

Method: The proposed method leverages a local planarity assumption, formulating constraints between surface normals and ray directions to model the relationship between depth and normals more accurately.

Result: The proposed approach achieves state-of-the-art performance on standard normal integration benchmarks and introduces the first method capable of directly handling generic central cameras.

Conclusion: The novel formulation outperforms existing methods and extends the applicability of normal integration to a wider range of camera models by explicitly addressing depth discontinuities.

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [171] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: The paper introduces ScoreAdv, a novel natural adversarial example generator using diffusion models, achieving high attack success rates and image quality.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks using $
abla_{p}$-norm perturbations often fail to align with human perception, prompting a shift toward creating unrestricted adversarial examples (UAEs) with natural-looking images.

Method: ScoreAdv integrates a diffusion model with an adversarial guidance mechanism to adjust the sampling distribution toward adversarial outcomes while using saliency maps to enhance visual relevancy.

Result: Extensive experimentation on ImageNet and CelebA datasets demonstrates state-of-the-art attack success rates and image quality for both black-box and white-box models across ten targets.

Conclusion: ScoreAdv offers an innovative and robust solution for generating unlimited natural adversarial examples, overcoming limitations of prior methods and showing robustness even under defensive measures.

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [172] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: This paper introduces CAST-Phys, a robust dataset for remote emotion recognition, utilizing facial and physiological cues.


<details>
  <summary>Details</summary>
Motivation: The motivation centers on addressing the limitations of contact-based emotion elicitation and the lack of affective multi-modal datasets which hinders precise emotion recognition.

Method: The research involves developing a dataset with diverse physiological signals (PPG, EDA, RR) and high-resolution facial video recordings, enabling the study of remote physiological emotion recognition.

Result: The analysis validates the importance of physiological signals, especially in scenarios where facial cues are insufficient, and demonstrates successful modality fusion for advanced emotion recognition.

Conclusion: CAST-Phys represents a significant step toward enhancing contactless and multi-modal emotion recognition, integrating facial and physiological data to improve accuracy in real-world conditions.

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [173] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: DS@GT's solution utilizes a pipeline combining Vision Transformer, tiling strategy, and domain-prior adaptations for plant identification in vegetation quadrat images, achieving second place in PlantCLEF 2025.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve multi-species plant identification in vegetation quadrat images, addressing challenges in precision and scalability using domain-specific adaptations.

Method: The pipeline integrates a fine-tuned Vision Transformer for patch-level inference, a tiling strategy to align image patches with network receptive fields, and domain-prior adaptations such as clustering and geolocation filtering, followed by prediction aggregation and Bayesian re-weighting.

Result: The proposed approach achieved a macro-averaged F1 score of 0.348 on the private leaderboard of the PlantCLEF 2025 challenge, marking a second-place finish.

Conclusion: The study demonstrates an effective and reproducible approach to multi-species plant identification using advanced machine learning techniques, without requiring additional training.

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [174] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: The paper introduces Ref-Unlock, a framework improving reflective surface rendering in 3D Gaussian Splatting (3DGS), addressing geometric inconsistencies and reflection misclassification issues.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve the challenge of accurately rendering reflective surfaces during novel view synthesis, where existing methods misinterpret reflections as physical geometry, leading to reconstruction artifacts and degraded visual quality.

Method: Ref-Unlock introduces a dual-branch 3D Gaussian Splatting framework that disentangles transmitted and reflected components. It uses high-order spherical harmonics for capturing high-frequency reflection details, a reflection removal module for pseudo reflection-free supervision, and additional constraints for enhancing 3D geometric consistency.

Result: Experiments show that Ref-Unlock achieves superior performance over traditional GS-based reflection methods, produces results comparable with advanced NeRF-based models, and enables flexible reflection editing via vision foundation models.

Conclusion: Ref-Unlock provides an efficient and practical solution for improving reflective scene rendering by addressing geometry and reflection challenges, demonstrating its potential for future applications in novel view synthesis.

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [175] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: The paper introduces Omni-Video, a unified framework for efficient video understanding, generation, and editing via multimodal large language models (MLLMs) and diffusion decoders.


<details>
  <summary>Details</summary>
Motivation: To address the gap in foundational models that focus predominantly on image processing while neglecting unified video understanding and generation.

Method: Omni-Video combines MLLMs with diffusion decoders using a lightweight architectural design and an efficient multi-stage training scheme to link visual tokens to diffusion decoder's conditional input.

Result: The framework demonstrates strong generalization across diverse video tasks, including generation, editing, and understanding.

Conclusion: Omni-Video is effective and efficient for unified video modeling and offers new possibilities in advancing video-related technologies.

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [176] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel prompt-free conditional diffusion model for improving multi-object image augmentation, overcoming challenges of traditional methods like lack of diversity and deviation issues with generated images.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-object image augmentation face issues of either deviating from the original dataset (when relying completely on text conditions) or producing less diverse images (when heavily reliant on original images), limiting their usefulness in downstream tasks.

Method: The authors propose a prompt-free conditional diffusion framework augmented by a local-global semantic fusion strategy to extract image-specific semantics (replacing text). LoRA is incorporated to reduce category deviations, and a reward-model-based counting loss is introduced to enhance diversity while addressing object count deviations.

Result: Experimental results demonstrate that the proposed method outperforms multiple state-of-the-art methods in generating diverse and accurate multi-object images, leading to improvements in downstream tasks and strong out-of-domain generalization.

Conclusion: The proposed framework addresses key limitations of existing methodologies by better aligning generated images with the original data distribution while enhancing diversity, making it effective for dataset augmentation in multi-object scenarios.

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [177] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: The paper introduces a method to generalize Laplacian-like smoothing using normalized similarity matrices for irregular domains.


<details>
  <summary>Details</summary>
Motivation: Existing smoothing methods such as Laplacians require well-defined domain structures, which are often unavailable, leading to bias in alternative techniques for irregular domains.

Method: A symmetric variant of the Sinkhorn algorithm is used to normalize similarity matrices into diffusion-like operators that mimic the behavior of Laplacians, enabling processing of irregular data formats.

Result: The proposed operators approximate heat diffusion while preserving spectral properties of the Laplacian, showing usefulness in tasks like shape analysis and matching.

Conclusion: The introduced approach generalizes smoothing techniques to irregular domains, bridging gaps in traditional methods while leveraging the core principles of heat diffusion.

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [178] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart introduces a novel framework for generative 3D object creation with editable part structures, prioritizing semantic decoupling and structural cohesion.


<details>
  <summary>Details</summary>
Motivation: The need for editable part-based structures in 3D assets to improve interactive and adaptable applications beyond monolithic 3D generation.

Method: OmniPart operates in two stages: (1) an autoregressive module generates sequences of bounding boxes guided by flexible 2D masks; (2) a spatially-conditioned flow model synthesizes consistent 3D parts within the planned layout.

Result: OmniPart achieves state-of-the-art results in part-aware 3D generation, supporting user-defined granularity and diverse applications.

Conclusion: OmniPart enables interpretable, editable, and versatile 3D content generation, addressing limitations of previous methods.

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [179] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: The paper tackles the challenge of scientific data interpretation in visual question answering (VQA) using semi-structured data like charts and figures. It presents experiments with large models, achieving strong performance through techniques like prompt optimization and ensemble modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the ability of current VQA approaches to handle scientific data, particularly where precision with numerical values, multi-step reasoning, and consistency are required.

Method: They conducted experiments on models ranging from 5B to 8B parameters, highlighting prompt optimization, chain-of-thought reasoning, and ensemble modeling as key techniques. Their best model, InternVL3, and an ensemble of multiple VLMs were evaluated.

Result: The best standalone model, InternVL3, achieved strong scores (ROUGE-1: 0.740, BERTScore: 0.983) on the SciVQA 2025 benchmark. The ensemble approach improved upon most individual models but not InternVL3’s standalone performance.

Conclusion: Prompt optimization, chain-of-thought reasoning, and ensemble modeling significantly improve VQA capabilities for interpreting scientific figures. InternVL3 emerged as a robust and reliable model for these visual reasoning tasks.

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [180] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: The paper introduces CultureCLIP, a model fine-tuned on a synthetic dataset (CulTwin) to enhance fine-grained cultural differentiation in vision-language models like CLIP.


<details>
  <summary>Details</summary>
Motivation: Pretrained vision-language models struggle with distinguishing visually similar but culturally distinct concepts due to a lack of quality datasets, integrated contextual knowledge, and hard negatives.

Method: The researchers created a synthetic cultural dataset called CulTwin using VLMs and text-to-image diffusion models. They fine-tuned CLIP on this dataset using customized contrastive learning to align cultural concepts with enhanced captions and images.

Result: CultureCLIP shows up to a 5.49% improvement in fine-grained concept recognition for culturally relevant tasks, while retaining the generalization strength of its CLIP backbone.

Conclusion: The study demonstrates the effectiveness of data synthesis and fine-tuning paradigms in addressing cultural distinctions within vision-language models.

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [181] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: SceneDINO introduces an unsupervised method for Semantic Scene Completion (SSC), relying solely on multi-view consistency self-supervision, without semantic or geometric ground truth.


<details>
  <summary>Details</summary>
Motivation: SSC traditionally depends on expensive annotations. The goal is to create a solution for SSC, leveraging unsupervised techniques to reduce reliance on labeled data.

Method: SceneDINO uses self-supervised representation learning, 3D feature distillation, and multi-view consistency to infer both 3D geometry and semantics from single images.

Result: SceneDINO achieves state-of-the-art segmentation accuracy in 3D and 2D unsupervised scene understanding, rivaling current supervised methods in linear probing. It excels in domain generalization and multi-view consistency.

Conclusion: SceneDINO sets a strong foundation for progressing toward single-image 3D scene understanding using unsupervised learning techniques.

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [182] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: RSRefSeg 2 improves remote sensing image segmentation by introducing a two-stage process: coarse localization followed by fine segmentation to enhance accuracy and address semantic ambiguities.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current remote sensing segmentation models, such as poor semantic alignment, error propagation, and reduced generalizability due to architectural coupling.

Method: A decoupling paradigm leveraging CLIP for semantic localization and prompts feeding into SAM for fine-grained segmentation. It includes a second-order prompter to enhance accuracy in complex multi-entity scenarios.

Result: RSRefSeg 2 achieves a ~3% improvement in segmentation accuracy (gIoU) and better handles complex semantic relationships compared to prior methods.

Conclusion: RSRefSeg 2 demonstrates that a decoupled segmentation framework can offer superior precision, generalizability, and cross-modal alignment for remote sensing tasks.

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [183] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces an automated pipeline, AnthroTAP, to generate pseudo-labeled data for point tracking in human motion using the SMPL model. The model achieves state-of-the-art results on the TAP-Vid benchmark with significantly less computational resources.


<details>
  <summary>Details</summary>
Motivation: Human motion provides complex challenges like deformations and occlusions, which is valuable for developing robust point trackers. However, manual annotation for extensive training datasets is labor-intensive.

Method: The AnthroTAP pipeline automates training data generation through the Skinned Multi-Person Linear (SMPL) model. It fits the SMPL model to humans in video, projects 3D mesh vertices onto 2D planes, handles occlusions with ray-casting, and filters unreliable trajectories using optical flow consistency.

Result: A model trained on AnthroTAP-annotated data achieved top performance on the TAP-Vid benchmark, outperforming others using 10,000 times less data and minimal computational resources.

Conclusion: AnthroTAP is a highly efficient and effective method for generating training data for point tracking, demonstrating its utility with superior performance and reduced resource requirements.

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [184] [High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction](https://arxiv.org/abs/2507.05308)
*Zehuan Chen,Xiangwei Lai*

Main category: cs.DC

TL;DR: This paper proposes HC-FGNN, a Federated Graph Neural Network model that improves QoS prediction accuracy while ensuring privacy by considering implicit user-user interactions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing FGNN-based QoS predictors, which only utilize explicit user-service graphs and fail to incorporate implicit user-user interactions.

Method: The HC-FGNN model applies an attention mechanism to enhance user-service graphs with high-order collaboration, reflecting implicit user-user interactions. It also uses lightweight message aggregation for higher computational efficiency.

Result: Experiments on two real-world QoS datasets demonstrate that HC-FGNN achieves high prediction accuracy and strong privacy protection.

Conclusion: HC-FGNN successfully combines accurate QoS prediction with privacy preservation, making it an effective solution for QoS data prediction challenges in cloud services.

Abstract: Predicting Quality of Service (QoS) data crucial for cloud service selection,
where user privacy is a critical concern. Federated Graph Neural Networks
(FGNNs) can perform QoS data prediction as well as maintaining user privacy.
However, existing FGNN-based QoS predictors commonly implement on-device
training on scattered explicit user-service graphs, thereby failing to utilize
the implicit user-user interactions. To address this issue, this study proposes
a high order collaboration-oriented federated graph neural network (HC-FGNN) to
obtain accurate QoS prediction with privacy preservation. Concretely, it
magnifies the explicit user-service graphs following the principle of attention
mechanism to obtain the high order collaboration, which reflects the implicit
user-user interactions. Moreover, it utilizes a lightweight-based message
aggregation way to improve the computational efficiency. The extensive
experiments on two QoS datasets from real application indicate that the
proposed HC-FGNN possesses the advantages of high prediction accurate and
privacy protection.

</details>


### [185] [Archetype-Aware Predictive Autoscaling with Uncertainty Quantification for Serverless Workloads on Kubernetes](https://arxiv.org/abs/2507.05653)
*Guilin Zhang,Srinivas Vippagunta,Raghavendra Nandagopal,Suchitra Raman,Jeff Xu,Marcus Pfeiffer,Shree Chatterjee,Ziqi Tan,Wulan Guo,Hailong Jiang*

Main category: cs.DC

TL;DR: The paper introduces AAPA, an autoscaling system optimized for varying workloads on serverless platforms, achieving high accuracy classification and significant SLO improvements.


<details>
  <summary>Details</summary>
Motivation: Address the efficiency challenges in managing dynamic workloads for HPEC platforms adopting serverless paradigms while maintaining SLOs.

Method: Introduce AAPA, which uses weak supervision to classify workload windows into four archetypes with 99.8% accuracy, enabling predictive autoscaling.

Result: AAPA achieves reduction in SLO violations by up to 50%, improves response time by 40%, but increases resource costs significantly under spike-heavy loads.

Conclusion: AAPA offers an effective autoscaling solution for dynamic workloads, balancing SLO compliance with increased resource costs, particularly under challenging conditions.

Abstract: High-performance extreme computing (HPEC) platforms increasingly adopt
serverless paradigms, yet face challenges in efficiently managing highly
dynamic workloads while maintaining service-level objectives (SLOs). We propose
**AAPA**, an archetype-aware predictive autoscaling system that leverages weak
supervision to automatically classify 300\,000\,+ workload windows into four
archetypes (PERIODIC, SPIKE, RAMP, STATIONARY\_NOISY) with 99.8\% accuracy.
Evaluation on publicly available Azure Functions traces shows that AAPA reduces
SLO violations by up to 50\%, improves response time by 40\%, albeit with a
2--8\,$\times$ increase in resource cost under spike-heavy loads.

</details>


### [186] [Air-FedGA: A Grouping Asynchronous Federated Learning Mechanism Exploiting Over-the-air Computation](https://arxiv.org/abs/2507.05704)
*Qianpiao Ma,Junlong Zhou,Xiangpeng Hou,Jianchun Liu,Hongli Xu,Jianeng Miao,Qingmin Jia*

Main category: cs.DC

TL;DR: The paper proposes Air-FedGA, a mechanism combining over-the-air computation (AirComp) and asynchronous federated learning (FL) to address synchronization and heterogeneity issues in FL.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in federated learning such as communication constraints, edge heterogeneity, and data Non-IID, while exploring the potential benefits of over-the-air computation aggregations despite its strict synchronization requirement.

Method: An AirComp-based grouping asynchronous federated learning mechanism is introduced, which groups workers for synchronous over-the-air aggregation while enabling asynchronous global model updates. Additionally, an optimization algorithm addresses power scaling, denoising, and worker grouping.

Result: Experiments demonstrate that Air-FedGA accelerates FL training by 29.9%-71.6% compared to state-of-the-art methods, and its convergence is theoretically proven.

Conclusion: Air-FedGA successfully combines AirComp and asynchronous FL to efficiently overcome synchronization and communication challenges, demonstrating marked improvements in training speed and adaptability.

Abstract: Federated learning (FL) is a new paradigm to train AI models over distributed
edge devices (i.e., workers) using their local data, while confronting various
challenges including communication resource constraints, edge heterogeneity and
data Non-IID. Over-the-air computation (AirComp) is a promising technique to
achieve efficient utilization of communication resource for model aggregation
by leveraging the superposition property of a wireless multiple access channel
(MAC). However, AirComp requires strict synchronization among edge devices,
which is hard to achieve in heterogeneous scenarios. In this paper, we propose
an AirComp-based grouping asynchronous federated learning mechanism
(Air-FedGA), which combines the advantages of AirComp and asynchronous FL to
address the communication and heterogeneity challenges. Specifically, Air-FedGA
organizes workers into groups and performs over-the-air aggregation within each
group, while groups asynchronously communicate with the parameter server to
update the global model. In this way, Air-FedGA accelerates the FL model
training by over-the-air aggregation, while relaxing the synchronization
requirement of this aggregation technology. We theoretically prove the
convergence of Air-FedGA. We formulate a training time minimization problem for
Air-FedGA and propose the power control and worker grouping algorithm to solve
it, which jointly optimizes the power scaling factors at edge devices, the
denoising factors at the parameter server, as well as the worker grouping
strategy. We conduct experiments on classical models and datasets, and the
results demonstrate that our proposed mechanism and algorithm can speed up FL
model training by 29.9%-71.6% compared with the state-of-the-art solutions.

</details>


### [187] [ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge](https://arxiv.org/abs/2507.06011)
*Daghash K. Alqahtani,Maria A. Rodriguez,Muhammad Aamir Cheema,Hamid Rezatofighi,Adel N. Toosi*

Main category: cs.DC

TL;DR: The paper presents ECORE, a framework balancing energy efficiency and detection performance for edge-based object detection, achieving significant latency and energy reductions with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Edge computing's potential for real-time vision tasks is hindered by the resource constraints of edge devices. The need to jointly optimize energy and accuracy for such tasks motivated the development of a new approach.

Method: The authors propose ECORE, a framework using dynamic routing strategies that direct image processing to appropriate edge device-model pairs. Techniques include estimation-based methods and a greedy selection algorithm. Evaluation involves real-world datasets, models like YOLO and SSD, and platforms such as Raspberry Pi and TPU accelerators.

Result: ECORE achieved a 45% reduction in energy consumption and a 49% reduction in latency, with only a minimal 2% loss in detection accuracy compared to traditional accuracy-focused methods.

Conclusion: ECORE successfully balances energy efficiency and detection performance, making it viable for real-time, resource-constrained edge applications.

Abstract: Edge computing enables data processing closer to the source, significantly
reducing latency an essential requirement for real-time vision-based analytics
such as object detection in surveillance and smart city environments. However,
these tasks place substantial demands on resource constrained edge devices,
making the joint optimization of energy consumption and detection accuracy
critical. To address this challenge, we propose ECORE, a framework that
integrates multiple dynamic routing strategies including estimation based
techniques and a greedy selection algorithm to direct image processing requests
to the most suitable edge device-model pair. ECORE dynamically balances energy
efficiency and detection performance based on object characteristics. We
evaluate our approach through extensive experiments on real-world datasets,
comparing the proposed routers against widely used baseline techniques. The
evaluation leverages established object detection models (YOLO, SSD,
EfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry
Pi 4 and 5, and TPU accelerators. Results demonstrate that our proposed
context-aware routing strategies can reduce energy consumption and latency by
45% and 49%, respectively, while incurring only a 2% loss in detection accuracy
compared to accuracy-centric methods.

</details>


### [188] [Efficient Federated Learning with Timely Update Dissemination](https://arxiv.org/abs/2507.06031)
*Juncheng Jia,Ji Liu,Chao Huo,Yihui Shen,Yang Zhou,Huaiyu Dai,Dejing Dou*

Main category: cs.DC

TL;DR: The paper introduces efficient Federated Learning (FL) methods, FedASMU and FedSSMU, which leverage additional bandwidth and dynamic techniques to improve update dissemination, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve efficiency and accuracy in Federated Learning systems while effectively managing distributed data and leveraging additional bandwidth resources.

Method: The methods include asynchronous and synchronous FL frameworks (FedASMU and FedSSMU). FedASMU integrates server-side dynamic model aggregation and device-side adaptive model adjustments. FedSSMU extends this approach in a synchronous setup.

Result: Experiments on six models and five datasets demonstrate substantial improvements in accuracy (up to 145.87%) and efficiency (up to 97.59%) over baseline methods.

Conclusion: The proposed methods, FedASMU and FedSSMU, successfully enhance Federated Learning performance by improving convergence, accuracy, and efficiency through innovative techniques.

Abstract: Federated Learning (FL) has emerged as a compelling methodology for the
management of distributed data, marked by significant advancements in recent
years. In this paper, we propose an efficient FL approach that capitalizes on
additional downlink bandwidth resources to ensure timely update dissemination.
Initially, we implement this strategy within an asynchronous framework,
introducing the Asynchronous Staleness-aware Model Update (FedASMU), which
integrates both server-side and device-side methodologies. On the server side,
we present an asynchronous FL system model that employs a dynamic model
aggregation technique, which harmonizes local model updates with the global
model to enhance both accuracy and efficiency. Concurrently, on the device
side, we propose an adaptive model adjustment mechanism that integrates the
latest global model with local models during training to further elevate
accuracy. Subsequently, we extend this approach to a synchronous context,
referred to as FedSSMU. Theoretical analyses substantiate the convergence of
our proposed methodologies. Extensive experiments, encompassing six models and
five public datasets, demonstrate that FedASMU and FedSSMU significantly
surpass baseline methods in terms of both accuracy (up to 145.87%) and
efficiency (up to 97.59%).

</details>


### [189] [A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems](https://arxiv.org/abs/2507.06107)
*Junaid Ahmed Khan,Andrea Bartolini*

Main category: cs.DC

TL;DR: This paper introduces the first unified ontology for Operational Data Analytics in HPC systems to enable semantic interoperability across data centers, optimizing knowledge graph storage.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in telemetry analysis for HPC systems, such as limited semantic integration and storage inefficiencies, given the growing complexity and scale of workloads like generative AI.

Method: Introducing a unified ontology that integrates telemetry data from two major ODA datasets, validated with 36 competency questions, and includes storage optimization techniques for knowledge graphs.

Result: The proposed ontology reduces knowledge graph storage overhead by up to 38.84% and an additional 26.82% depending on deployment configurations, while supporting cross-system analysis.

Conclusion: This work establishes a framework for scalable and interoperable ODA knowledge graphs in HPC systems, enabling comprehensive analysis within and across systems.

Abstract: Modern high-performance computing (HPC) systems generate massive volumes of
heterogeneous telemetry data from millions of sensors monitoring compute,
memory, power, cooling, and storage subsystems. As HPC infrastructures scale to
support increasingly complex workloads-including generative AI-the need for
efficient, reliable, and interoperable telemetry analysis becomes critical.
Operational Data Analytics (ODA) has emerged to address these demands; however,
the reliance on schema-less storage solutions limits data accessibility and
semantic integration. Ontologies and knowledge graphs (KG) provide an effective
way to enable efficient and expressive data querying by capturing domain
semantics, but they face challenges such as significant storage overhead and
the limited applicability of existing ontologies, which are often tailored to
specific HPC systems only. In this paper, we present the first unified ontology
for ODA in HPC systems, designed to enable semantic interoperability across
heterogeneous data centers. Our ontology models telemetry data from the two
largest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA
(Fugaku, Japan)-within a single data model. The ontology is validated through
36 competency questions reflecting real-world stakeholder requirements, and we
introduce modeling optimizations that reduce knowledge graph (KG) storage
overhead by up to 38.84% compared to a previous approach, with an additional
26.82% reduction depending on the desired deployment configuration. This work
paves the way for scalable ODA KGs and supports not only analysis within
individual systems, but also cross-system analysis across heterogeneous HPC
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [190] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: The paper explores over-smoothing in Graph Neural Networks (GNNs) using an analogy to Anderson localization and introduces participation degree as a measurement metric.


<details>
  <summary>Details</summary>
Motivation: Over-smoothing in GNNs causes node representations to lose their distinctiveness with increasing network depth, hindering effective graph data analysis.

Method: The study draws an analogy between over-smoothing in GNNs and Anderson localization in disordered systems, analyzing behavior through participation degree metrics and theoretical frameworks.

Result: The research establishes a link between low-frequency mode expansion and high-frequency mode localization in GNNs, explaining over-smoothing via participation degree metrics.

Conclusion: Alleviating over-smoothing is possible through reducing the disorder in information propagation, as indicated by theoretical analysis within the paper.

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [191] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: This paper investigates the vulnerability of Graph Neural Networks (GNNs) to hardware-based fault attacks and proposes a novel Gradual Bit-Flip Fault Attack (GBFA) to target trained weight parameters.


<details>
  <summary>Details</summary>
Motivation: GNNs are widely used in applications with graph-structured data, but their vulnerability to hardware-based fault injection attacks has not been adequately studied.

Method: The authors propose GBFA, a two-step attack method that creates a Markov model to predict layer execution and identifies vulnerable bits via gradient ranking, specifically targeting minimal bit flips in GNN weight parameters.

Result: GBFA, when evaluated on GNN models using Cora and PubMed datasets, shows a significant degradation in predictive accuracy. For instance, a single bit flip in the last layer decreased GraphSAGE's accuracy by 17% on Cora.

Conclusion: The results underscore the need for adopting a layer-aware attack strategy and show that even minimal fault injections can compromise GNN performance significantly. This raises concerns about the security of GNN models against hardware attacks.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [192] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: The paper proposes a method to reduce redundancy and better incorporate exogenous inputs for improving time series forecasting performance, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Despite the significance of exogenous inputs in transformer-based time series models, existing methods struggle with redundancy and limited long-term dependency capture, necessitating an enhanced approach.

Method: The authors whiten exogenous inputs based on global statistics to reduce redundancy and improve awareness of long-term patterns, integrating them with endogenous inputs without extending the lookback window.

Result: The proposed method outperforms 11 baseline models across four benchmark datasets, achieving state-of-the-art results in time series forecasting.

Conclusion: The presented approach effectively utilizes exogenous inputs to enhance forecasting performance, offering a robust solution to existing limitations in time series forecasting models.

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [193] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: A novel DNN compression approach using a gradient-based explainable AI technique, LRP, reduces model size by 64% and improves accuracy by 42% compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: DNNs involve high computational costs and memory usage, making it challenging to use them on resource-constrained devices. While XAI provides insights into the inner workings of DNNs, leveraging XAI for targeted parameter reduction could address these limitations.

Method: The proposed method uses the gradient-based XAI technique Layer-wise Relevance Propagation (LRP) to compute importance scores for weights. Weights with negative or zero scores are pruned, and mixed-precision quantization is applied based on score significance.

Result: The approach achieves a 64% reduction in model size while simultaneously improving the accuracy of the compressed model by 42% compared to the existing XAI-based compression method.

Conclusion: This paper illustrates that incorporating XAI techniques like LRP into DNN compression pipelines not only significantly reduces model size but can also enhance model accuracy, offering a promising direction for efficient deployment of DNNs on edge devices.

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [194] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: This paper introduces the first dataless neural network framework for solving the Resource-Constrained Project Scheduling Problem (RCPSP), extending previous research on dataless approaches.


<details>
  <summary>Details</summary>
Motivation: Despite the progress in dataless neural networks, no prior work has applied this approach to RCPSP, which motivates this study to bridge the gap.

Method: The study reformulates RCPSP's discrete scheduling constraints into differentiable objectives using smooth relaxations and automatic differentiation, enabling GPU parallelization.

Result: The implementation is ongoing, with experiments being conducted on standard PSPLIB benchmarks (J30, J60, J120). Results will be shared in a future version of this paper.

Conclusion: This paper signifies a critical step toward applying dataless neural networks to combinatorial optimization problems like RCPSP, combining modern mathematical frameworks with the computational efficiency of GPU-accelerated methods.

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [195] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: The paper presents P-DivGNN, a physics-informed machine learning method for reconstructing local stress fields at the micro-scale using graph neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately predicting local stress fields for applications like fracture analysis and fatigue criteria, while improving computational efficiency over traditional methods such as FE simulations.

Method: P-DivGNN combines message-passing graph neural networks with periodic micro-structure representation and incorporates physical constraints into the training process.

Result: The method successfully predicts stress fields for both linear and non-linear hyperelastic responses, demonstrating computational speed-ups in non-linear cases, particularly for large-scale problems.

Conclusion: P-DivGNN is an effective, computationally efficient alternative to traditional finite element methods for reconstructing local stress fields, offering potential impact in applications requiring localized stress analysis.

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [196] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: NeVe introduces neural velocity as a dynamic mechanism for training neural networks, reducing dependence on validation data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reduce reliance on validation loss by introducing a metric (neural velocity) that enables efficient training optimization.

Method: It proposes sampling neural velocity to adjust learning rates and establish stopping criteria, eliminating the need for held-out datasets.

Result: The study demonstrates the effectiveness of neural velocity in optimizing neural network training without heavy reliance on validation loss.

Conclusion: Neural velocity has strong potential as a novel metric for dynamic and efficient neural network training optimization.

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [197] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: This paper introduces a model-free reinforcement learning approach using Proximal Policy Optimization (PPO) for the in situ training of diffractive optical processors, enabling better convergence and performance under real-world physical constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of optimizing and aligning diffractive optical networks for physical systems, which are hindered by hardware imperfections, noise, and misalignments.

Method: A model-free reinforcement learning approach based on Proximal Policy Optimization (PPO) was developed to train diffractive optical processors in situ, utilizing data measurement efficiently and stabilizing convergence.

Result: The approach was experimentally validated on tasks such as energy focusing, holographic image generation, aberration correction, and optical image classification, showing improved convergence and performance.

Conclusion: This approach enables faster and more accurate in situ training of optical systems without requiring prior system knowledge or explicit modeling, offering a scalable solution for complex, feedback-driven physical systems.

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [198] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: This paper presents a data-driven conditional graph neural network (cGNN) for simulating soft tissue deformation and interaction forces by training with a combination of mass-spring simulations and experimental data.


<details>
  <summary>Details</summary>
Motivation: Soft tissue simulation is critical for medical and other applications, but its high deformability and integration with haptic feedback bring significant challenges in existing methods.

Method: A conditional graph neural network (cGNN) is designed to predict soft tissue deformation and forces using surface points and applied force locations, with transfer learning combining synthetic and experimental data.

Result: The model achieves precise predictions of tissue deformations with a distance error of 0.35±0.03 mm and force estimations with an error of 0.37±0.05 N across specified ranges.

Conclusion: The cGNN-based data-driven approach improves generalization and accuracy in soft tissue simulation, holding promise for medical simulations and other fields requiring realistic tissue dynamics.

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [199] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: The paper introduces Multitask Preplay, a novel approach where experience from one task is used to simulate counterfactual scenarios for unpursued tasks, aiding generalization across multiple tasks in humans and artificial agents.


<details>
  <summary>Details</summary>
Motivation: Humans can only actively pursue a limited number of tasks simultaneously, but they seem to learn solutions to other accessible but unpursued tasks, leveraging generalization capabilities.

Method: The authors propose the Multitask Preplay algorithm, which uses experience replay from a completed task to preplay and simulate potential scenarios for unpursued tasks, developing predictive representations to enable adaptive task performance.

Result: Experiments revealed Multitask Preplay improves predictions on generalization to unpursued tasks, validated in environments such as a grid-world and a Minecraft-like setting (Craftax), and allows artificial agents to transfer learned behaviors to novel worlds.

Conclusion: Multitask Preplay offers a scalable framework for understanding human task generalization, and implementing it in artificial agents enhances their adaptability in complex multitask settings.

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [200] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: This paper introduces Heuristic Enhanced Policy Optimization (HEPO), a framework that improves reinforcement learning (RL) by leveraging heuristics to avoid reward hacking and enhance policy performance.


<details>
  <summary>Details</summary>
Motivation: In reinforcement learning, designing heuristic rewards is labor-intensive and prone to inefficiencies, as heuristics are often suboptimal. Current theoretical approaches fail to lead to policy improvements in practice.

Method: The paper proposes HEPO, a framework that maximizes policy improvement by effectively leveraging heuristics while mitigating reward hacking issues. It focuses on practical optimization rather than theoretical policy invariance.

Result: HEPO demonstrates superior performance on standard RL benchmarks with well-engineered reward functions. Additionally, it performs well even with poorly designed heuristics from non-experts, reducing dependency on sophisticated human input.

Conclusion: HEPO is a practical and robust optimization method for integrating heuristics into RL, reducing human efforts in designing reward functions while achieving strong performance.

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [201] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: The paper introduces Local-Global Convolutional Neural Network (LGCNN) for modeling groundwater flows with heat transport, combining numerical surrogates and CNNs to overcome real-world training data limitations.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in modeling groundwater heat transport due to limited, low-quality training data and difficulties with advection processes.

Method: They propose the LGCNN that integrates numerical surrogates for global transport processes with CNNs for local groundwater velocity and heat diffusion modeling.

Result: The LGCNN successfully modeled a city-wide subsurface temperature field in Munich, scaling to larger domains without requiring retraining.

Conclusion: The approach offers scalable and efficient models for groundwater flow with heat transport, and all datasets and codes are shared for reproducibility.

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [202] [Bridging Prediction and Intervention Problems in Social Systems](https://arxiv.org/abs/2507.05216)
*Lydia T. Liu,Inioluwa Deborah Raji,Angela Zhou,Luke Guerdan,Jessica Hullman,Daniel Malinsky,Bryan Wilder,Simone Zhang,Hammaad Adam,Amanda Coston,Ben Laufer,Ezinne Nwankwo,Michael Zanger-Tishler,Eli Ben-Michael,Solon Barocas,Avi Feller,Marissa Gerchick,Talia Gillis,Shion Guha,Daniel Ho,Lily Hu,Kosuke Imai,Sayash Kapoor,Joshua Loftus,Razieh Nabi,Arvind Narayanan,Ben Recht,Juan Carlos Perdomo,Matthew Salganik,Mark Sendak,Alexander Tolbert,Berk Ustun,Suresh Venkatasubramanian,Angelina Wang,Ashia Wilson*

Main category: cs.LG

TL;DR: The paper argues for shifting from a prediction-focused paradigm to an intervention-oriented approach when considering the societal impact of automated decision systems (ADS).


<details>
  <summary>Details</summary>
Motivation: Traditional automated decision systems aim at predicting behaviors but fall short of addressing the broader societal and policy effects in deployment.

Method: The authors outline a shift towards treating ADS as interventionist tools, combining predictions with decision-making processes and their societal outcomes.

Result: The study unifies statistical and practical frameworks to highlight the limitations of isolated prediction tasks and suggests an intervention-focused framework for ADS design, implementation, and evaluation.

Conclusion: A paradigm shift is necessary to frame ADS as intervention tools rather than solely prediction engines, paving the way for more impactful societal integration and ethical deployment.

Abstract: Many automated decision systems (ADS) are designed to solve prediction
problems -- where the goal is to learn patterns from a sample of the population
and apply them to individuals from the same population. In reality, these
prediction systems operationalize holistic policy interventions in deployment.
Once deployed, ADS can shape impacted population outcomes through an effective
policy change in how decision-makers operate, while also being defined by past
and present interactions between stakeholders and the limitations of existing
organizational, as well as societal, infrastructure and context. In this work,
we consider the ways in which we must shift from a prediction-focused paradigm
to an interventionist paradigm when considering the impact of ADS within social
systems. We argue this requires a new default problem setup for ADS beyond
prediction, to instead consider predictions as decision support, final
decisions, and outcomes. We highlight how this perspective unifies modern
statistical frameworks and other tools to study the design, implementation, and
evaluation of ADS systems, and point to the research directions necessary to
operationalize this paradigm shift. Using these tools, we characterize the
limitations of focusing on isolated prediction tasks, and lay the foundation
for a more intervention-oriented approach to developing and deploying ADS.

</details>


### [203] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: The paper introduces a novel foundation model for structured time series data that disentangles physical signals from instrumental distortions using a dual-encoder architecture and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in model generalization caused by the entanglement of physical phenomena and systematic measurement distortions in multi-instrument time series data.

Method: A dual-encoder architecture trained with structured contrastive learning, leveraging naturally occurring observational triplets to disentangle latent representations of physical signals from instrumental effects.

Result: The model outperforms traditional single-latent space models in tasks, especially in low-data settings, as shown through tests on simulated astronomical time series resembling variable stars observed by TESS.

Conclusion: Incorporating causal structure into representation learning significantly enhances the capabilities of foundation models, supporting tasks like few-shot generalization and efficient adaptation.

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [204] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: The study introduces a method to manipulate financial reports, demonstrating its effectiveness in inflating earnings while reducing fraud detection scores.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem where distressed firms manipulate financial reports for gain, leveraging publicly available financial models for this purpose.

Method: The paper proposes the Maximum Violated Multi-Objective (MVMO) attacks, which adjust attackers' strategies to meet dual objectives necessary for successful fraud.

Result: MVMO attacks enable firms to inflate their earnings by 100-200% while simultaneously reducing their fraud score by 15%, achieving a 20x increase in successful attacks compared to standard methods.

Conclusion: The developed MVMO attack model aligns well with real-world fraud practices and significantly enhances the success of financial manipulation schemes.

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [205] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: The paper compares supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) for continual post-training and concludes RFT is superior due to its capability to prevent catastrophic forgetting and enhance general knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting foundation models to continually changing tasks while retaining previously learned information.

Method: Conduct a comparative analysis of SFT and RFT on seven multimodal tasks using the model Qwen2.5-VL-7B-Instruct, and propose an algorithm for improving RFT stability and efficiency.

Result: SFT results in significant forgetting of prior tasks, while RFT preserves prior knowledge, maintains performance across tasks, and enhances general model capabilities.

Conclusion: RFT emerges as the preferred paradigm over SFT for continual post-training due to its implicit regularization benefits and ability to mitigate forgetting.

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [206] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: This paper reframes dynamic regret minimization in online convex optimization as a static problem in a Reproducing Kernel Hilbert Space (RKHS), leading to improved guarantees and practical algorithms.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to achieve low cumulative loss relative to an arbitrary sequence, addressing limitations in prior reductions that were confined to linear losses. They sought a framework to extend dynamic-to-static reductions to more general loss settings.

Method: The paper constructs a suitable function space in the form of RKHS to reframe the dynamic regret minimization problem. This approach enables optimal guarantees and supports a wide range of loss sequences, beyond linear losses.

Result: The paper recovers optimal dynamic regret bounds for linear losses and introduces new guarantees that are scale-free and directionally adaptive. It also provides rigorous bounds for exp-concave and improper linear regression scenarios, leveraging the complexity measure of RKHS.

Conclusion: By utilizing RKHS, the authors extend the validity of dynamic-to-static reductions to non-linear scenarios, yielding practical, efficient algorithms for broader loss types and achieving optimal results in dynamic regret minimization.

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [207] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: PT-LiRPA introduces a method that improves formal verification of neural networks by combining linear relaxation and probabilistic sampling techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of formal verification for neural networks, particularly where conventional methods face challenges.

Method: PT-LiRPA utilizes linear over-approximation paired with sampling-based analysis to tighten the bounds of reachable sets in neural network verification.

Result: PT-LiRPA achieves superior robustness certificates compared to prior methods, especially excelling in complex cases with high confidence levels.

Conclusion: The framework significantly advances formal verification methods by reducing computational overhead, improving reliability, and addressing previously unsolved verification challenges.

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [208] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: This paper introduces Stein Diffusion Guidance (SDG), a new training-free diffusion guidance method for diffusion models, balancing computational efficiency and accuracy by correcting approximate posteriors.


<details>
  <summary>Details</summary>
Motivation: Current stochastic optimal control (SOC) approaches for diffusion model fine-tuning are computationally expensive, while training-free methods using Tweedie's formula often yield unreliable results due to approximation errors.

Method: The authors propose a unified framework leveraging SOC principles, correcting approximate posteriors with Stein variational inference, and introducing a novel running cost functional to improve guidance accuracy.

Result: Experiments in complex molecular generation tasks show that SDG surpasses existing training-free guidance methods, particularly in effectiveness within low-density regions.

Conclusion: The Stein Diffusion Guidance (SDG) framework combines the strengths of different paradigms to provide computationally efficient and reliable guidance for diffusion models, proving to be highly effective in practical scenarios.

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [209] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn is a deep learning system designed for modularity and high efficiency, emphasizing easy integration of features and support for heterogeneous hardware infrastructure.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of facilitating scalable training of large models while maintaining modularity and performance, particularly in systems with heterogeneous hardware.

Method: AXLearn introduces modular internal software interfaces, strict encapsulation, and a novel metric to quantify modularity using Lines-of-Code (LoC)-complexity.

Result: The system achieves constant complexity as it scales components, offers easy feature integration (e.g., adding RoPE with just 10 LoC), and delivers performance comparable to other state-of-the-art training systems.

Conclusion: AXLearn is a modular, efficient, and practical system for high-performance model training, demonstrating unique advancements in complexity management and flexibility.

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [210] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: The paper introduces MACE-TNP, a meta-learning-based model for Bayesian causal inference that predicts intervention distributions without committing to a single causal structure.


<details>
  <summary>Details</summary>
Motivation: Causal inference often requires estimating intervention effects, but observational data usually allows for multiple compatible causal graphs. Current methods face challenges of overconfidence and computational intractability when dealing with structural uncertainty in causal graphs.

Method: The authors propose MACE-TNP, a meta-learning model trained to predict Bayesian model-averaged interventional posterior distributions. This end-to-end approach circumvents the need for computationally expensive Bayesian calculations over a super-exponentially growing number of causal structures.

Result: The MACE-TNP model outperformed state-of-the-art Bayesian baselines in empirical evaluations, demonstrating its effectiveness at estimating causal effects while managing structural uncertainty.

Conclusion: Meta-learning is proposed as a scalable and flexible paradigm to approximate complex Bayesian causal inference, opening avenues for tackling larger and more challenging causal inference problems in the future.

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [211] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: This paper introduces RepLIn, an algorithm to improve model robustness against interventional distribution shifts by enforcing independence conditions from a causal model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address performance disparities in predictive models caused by interventional distribution shifts, which arise because existing methods overlook independence relations in interventional data.

Method: Using insights about the correlation between representation adherence to causal independence and performance disparity, the authors derive conditions for linear models and propose a training algorithm called RepLIn to enforce these independence conditions.

Result: RepLIn is tested on synthetic, image, and text datasets, demonstrating improved robustness in facial attribute classification and toxicity detection tasks while being scalable with causal graph complexity.

Conclusion: The proposed method leverages causal independence relations to enhance the robustness of learned representations across interventional distribution shifts for continuous and discrete latent variables.

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [212] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: The paper addresses the challenge of air pollution from agricultural emissions by proposing two novel deep learning models, EmissionNet and EmissionNet-Transformer, for predicting N$_2$O emissions.


<details>
  <summary>Details</summary>
Motivation: Air pollution from agricultural emissions is a significant yet underestimated contributor to environmental and public health issues. Existing physics-based forecasting models lack the ability to handle complex pollutant interactions.

Method: Deep learning-based architectures, namely EmissionNet and EmissionNet-Transformer, are developed using convolutional and transformer methodologies to analyze spatial-temporal patterns in emissions data.

Result: The proposed models show potential to improve upon traditional methods by leveraging advanced architecture designs tailored for capturing intricate interactions in emission data.

Conclusion: EmissionNet and EmissionNet-Transformer represent promising alternatives to conventional models, offering enhanced forecasting capabilities for agricultural N$_2$O emissions.

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [213] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: The paper presents the Features at Convergence Theorem (FACT) to explain neural network feature learning at convergence, introduces the FACT-RFM algorithm based on this theorem, and demonstrates its performance on tabular data.


<details>
  <summary>Details</summary>
Motivation: To better understand how neural networks learn and represent features during training and at convergence.

Method: Proves the Features at Convergence Theorem (FACT) and validates it empirically. Develops a new algorithm, FACT-RFM, by modifying Recursive Feature Machines to incorporate FACT.

Result: FACT theorem is empirically validated, and FACT-RFM achieves strong performance on tabular data, capturing intricate neural network training behaviors like grokking and phase transitions.

Conclusion: The research advances the theoretical understanding of feature learning in neural networks and offers a novel algorithm (FACT-RFM) with practical implications for tabular data analysis and feature representation.

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [214] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: The paper addresses delayed and sparse rewards in reinforcement learning, using 2048 as a testbed, introducing Horizon-DQN that significantly outperforms other RL methods in maximizing game performance.


<details>
  <summary>Details</summary>
Motivation: To improve RL agents' ability to handle sparse and delayed rewards, which are prevalent in challenging strategic domains like the 2048 game.

Method: The authors propose a new framework, Horizon-DQN, that combines distributional learning, multi-step RL, and advanced architectural features like dueling networks and prioritized replay.

Result: Horizon-DQN outperforms other RL methods, achieving max scores of 18.21K and scaling up to 41.828K, reaching 4096 in 2048.

Conclusion: Distributional, multi-step reinforcement learning is powerful for sparse-reward problems, with potential further improvements through model-based techniques and curriculum learning.

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [215] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: The paper introduces a method for predicting graph dynamics using a combination of time series prediction and an adapted flux balance analysis, tested on synthetic and real datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting graph structures over time have limitations, such as assuming static vertices, which constrains their applicability.

Method: A fusion of time series prediction and modified flux balance analysis (originally from biochemistry) is introduced to model growing graphs with additional constraints.

Result: Experiments on synthetic and real-world datasets reveal the effectiveness of the proposed graph prediction approach.

Conclusion: The adapted methodology successfully forecasts dynamic graph structures, overcoming limitations of prior techniques.

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [216] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: This paper proposes leveraging Forward-Backward (FB) representations specifically for exploration to improve zero-shot reinforcement learning, achieving better sample complexity.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot reinforcement learning approaches, like FB representations, do not integrate exploration capabilities, relying on external exploration techniques which can limit learning efficiency.

Method: The authors design exploration policies that leverage the epistemic uncertainty in FB representations to minimize posterior variance and enhance exploration efficiency.

Result: Empirical results demonstrate that the proposed exploration strategies significantly improve sample complexity compared to traditional exploration methods.

Conclusion: Using FB representations for exploration is an effective principle to advance zero-shot reinforcement learning, leading to more efficient data collection and learning processes.

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [217] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: The paper proposes a federated learning framework that uses prototypes and lightweight adapter modules to address challenges of communication overhead and statistical heterogeneity, achieving interpretability and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of communication overhead and statistical heterogeneity in federated learning while maintaining model privacy and interpretability.

Method: The method involves using prototypes for inherent interpretations and lightweight adapter modules to align local models to a global structure, reducing communication load by only sharing prototypes and adapters instead of entire model weights.

Result: The proposed framework demonstrated improved accuracy in a classification task on a real-world retinal fundus image dataset and showed its interpretive capabilities.

Conclusion: The approach reduces communication overhead, enhances interpretability, tackles statistical heterogeneity, and achieves improved performance in federated learning.

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [218] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: The paper studies reward hacking in RL systems, presenting a detection framework with 78.4% precision and mitigation techniques reducing hacking by 54.6%.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of reward hacking where RL agents exploit reward functions, impacting reliable autonomous deployment.

Method: Conducted a large-scale empirical study across 15 environments and 5 algorithms. Implemented detection for 6 reward hacking categories and tested mitigation approaches.

Result: The detection methods achieved 78.4% precision and 81.7% recall with minimal computational cost. Mitigation strategies reduced reward hacking by up to 54.6%.

Conclusion: Reward density and alignment significantly influence hacking. While mitigation is effective in controlled cases, challenges remain such as concept drift and adversarial behaviors.

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [219] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: This paper introduces Ex-HiDeNN, a neural network architecture for discovering interpretable and accurate expressions using symbolic regression and separability checks, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of discovering interpretable and accurate closed-form expressions efficiently from complex datasets.

Method: This paper proposes Ex-HiDeNN, a two-step neural network approach combining symbolic regression and a separability checker for accurate and scalable model construction.

Result: Ex-HiDeNN demonstrated excellent results in benchmarks and engineering applications, producing smaller error margins and better performance than traditional methods.

Conclusion: Ex-HiDeNN successfully discovers closed-form expressions with high accuracy and scalability, showing potential to outperform conventional approaches in diverse applications.

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [220] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: The paper introduces a graph-based neural network model (GCLSTM) to estimate campus building occupancy and movements using Wi-Fi logs and schedules, showing improved prediction accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to predict campus occupancy and inter-building movement patterns accurately while preserving individual privacy, leveraging available data sources like Wi-Fi logs and building schedules.

Method: The problem is formulated as a graph structure with nodes (buildings) and edges (routes), and addressed using a Graph Convolution plus LSTM Neural Network (GCLSTM), a technique designed to model complex traffic flow patterns.

Result: The experiments reveal that the GCLSTM model notably outperforms traditional pedestrian flow estimators such as Multi-Layer Perceptron (MLP) and Linear Regression.

Conclusion: The proposed GCLSTM model is effective in learning traffic flow patterns and estimating occupancy within campus buildings while maintaining privacy, offering superior performance over conventional methods.

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [221] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: The paper introduces a Monte Carlo-based gradient compression scheme that uses biased compressors to create unbiased estimates, aiming to reduce communication overhead in distributed learning.


<details>
  <summary>Details</summary>
Motivation: Communication costs in distributed learning are a significant bottleneck, prompting the need for efficient compression methods that balance empirical performance and theoretical guarantees.

Method: The authors propose a Multilevel Monte Carlo (MLMC) approach that utilizes biased compressors to build statistically unbiased gradients and test its performance using Top-$k$ and bit-wise compressors.

Result: The method shows improved performance in distributed learning tasks and produces enhanced compression variants like adaptive forms.

Conclusion: The MLMC compression scheme effectively blends the strengths of biased and unbiased compressors, offering a versatile improvement in gradient compression for distributed deep learning.

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [222] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: The paper introduces a novel deep learning-based method to improve marketing campaign efficiency by optimizing user selection and reward allocation.


<details>
  <summary>Details</summary>
Motivation: To enhance the impact and cost-efficiency of user growth strategies for consumer internet companies by addressing limitations of traditional prediction-based methods.

Method: A novel approach using deep learning that models uplifts in business metrics and incorporates softmax gating to optimize parameters for an aggregated loss function under complex constraints.

Result: The proposed method outperforms state-of-the-art techniques by over 20% in experimental evaluations and demonstrates effectiveness in real-world applications.

Conclusion: This methodology significantly improves marketing campaign optimization and has versatile applicability across different product scenarios, validated through successful deployments worldwide.

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [223] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: This paper proposes a novel deep learning framework to estimate and rank Heterogeneous Treatment Effects (HTE) across complex treatment policies with multiple factors.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need to estimate HTE in settings where the treatment action space extends beyond binary variables to more complex structured treatment policies, such as continuous or discrete factors.

Method: A deep learning methodology incorporating a Neural-Augmented Naive Bayes layer is developed to handle multiple treatment factors. The approach accommodates continuous treatment variables and aggregates HTE functions for direct ranking of subjects.

Result: The proposed algorithms are validated on public datasets, demonstrating their effectiveness in improving prediction performance within various HTE applications.

Conclusion: The study contributes a flexible and generic deep learning framework for understanding and ranking heterogeneous treatment policies, advancing the field of HTE analysis in complex multi-factor settings.

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [224] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: The paper introduces Interpolated Learning (InterpoLL) to mitigate shortcut reliance in machine learning models, enhancing minority example generalization across NLP tasks without hurting majority example performance.


<details>
  <summary>Details</summary>
Motivation: Shortcut exploitation in training data limits model generalization, particularly for minority examples where spurious correlations do not apply.

Method: InterpoLL interpolates feature representations between majority and intra-class minority examples to reduce shortcut reliance and improve model prediction consistency.

Result: Experimental results show InterpoLL outperforms ERM and alternative approaches in minority example generalization across various natural language architectures, maintaining majority example accuracy.

Conclusion: InterpoLL effectively mitigates shortcuts, enabling better generalization in minority examples without sacrificing majority example performance, and is broadly applicable across model architectures.

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [225] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: The paper investigates the theoretical understanding of Graph Convolutional Networks (GCNs) incorporating jumping connections and graph sparsification, showing how these techniques affect generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding regarding the generalization of GCNs that combine jumping connections and graph sparsification, which are empirically successful techniques but have not been deeply analyzed together.

Method: The authors analyzed the learning dynamics and generalization of GCNs by focusing on the sparse effective adjacency matrix $A^*$ and examined how sparsifications interact differently with jumping connections across layers.

Result: The results show that graph sparsification retains generalization performance if the critical edges are preserved in $A^*$. Additionally, the jumping connections impose varying sparsification needs on different layers, with the first layer being more sensitive than subsequent layers.

Conclusion: Jumping connections and graph sparsification can be combined effectively to maintain GCN generalization performance, but the sparsification process should carefully consider layer-specific requirements to preserve meaningful propagation.

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [226] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: The paper introduces LSC-GNN to handle noisy edges in graphs, showing improved performance via latent alignment.


<details>
  <summary>Details</summary>
Motivation: Noisy edges in Graph Neural Networks (GNNs) significantly affect performance, necessitating methods to handle this issue.

Method: The paper proposes training two encoders: one with the full graph and another excluding noisy edges. Then, it introduces a latent space constraint by penalizing differences between their representations.

Result: LSC-GNN outperforms standard and noise-resilient GNN models under moderate graph noise.

Conclusion: LSC-GNN improves predictive performance and interpretability in noisy relational structures, including its successful extension to heterogeneous graphs.

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [227] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: The paper proposes an advanced machine learning method to improve the accuracy of estimating carried load using gait motion data from wearable sensors.


<details>
  <summary>Details</summary>
Motivation: Current approaches for load estimation rely heavily on direct mappings that limit generalization and accuracy. Addressing these limitations, the paper introduces auxiliary information such as baseline gait patterns and carrying style.

Method: It uses a model combining deep latent variable modeling, temporal convolutional networks, and bi-directional cross-attention to fuse loaded and unloaded gait patterns. It eliminates the need for labeled carrying styles at inference time.

Result: Experiments on real-world data showed increased accuracy by leveraging auxiliary information and validated the effectiveness of explicit mechanisms for feature integration.

Conclusion: The study demonstrates the value of incorporating domain knowledge and advanced fusion techniques to enhance load estimation, even without manual labeling of carrying style during deployment.

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [228] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: This paper surveys feature-based and GAN-based approaches to learning from demonstrations, analyzing their reward structures, advantages, limitations, and suggesting task-specific selection criteria.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-offs in reward structures and provide insights into choosing between feature-based and GAN-based methods for learning from demonstrations.

Method: Comparative analysis of feature-based and GAN-based methods, focusing on their reward structures and implications for task-specific use.

Result: The paper highlights advantages (such as interpretability and adaptability) and limitations (training instability, generalization issues) of both paradigms, and identifies converging advancements in structured motion representations.

Conclusion: Selection of methods should depend on task-specific priorities like fidelity, diversity, and adaptability rather than favoring one paradigm universally.

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [229] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: The paper investigates memorization in Large Language Models (LLMs), including its causes, detection methods, mitigation strategies, and broader implications.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLM memorization, its impact on model behavior, privacy, and ethics.

Method: Synthesizing existing studies to analyze factors influencing memorization, detection techniques, and mitigation strategies.

Result: Identifying how memorization happens in LLMs, evaluating detection methods, and proposing mitigation solutions.

Conclusion: The paper provides a comprehensive review of memorization in LLMs, emphasizing technical, ethical, and privacy challenges and offering mitigation strategies.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [230] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: The authors propose "Uncertainty-Aware Reinforcement Learning" (UARL), a method to improve real-world deployment of RL policies by focusing on safety and robustness without direct target domain interactions.


<details>
  <summary>Details</summary>
Motivation: Real-world deployment of RL policies faces issues like distribution shifts, safety risks, and impracticality of direct interactions during refinement, which are inadequately addressed by current methods.

Method: The UARL framework uses an ensemble of critics to measure uncertainty, applies progressive environmental randomization, and iteratively refines policies in simulated high-uncertainty regions, avoiding direct target domain interaction.

Result: The authors tested UARL on MuJoCo benchmarks and a quadrupedal robot, showing improved OOD detection, performance, and sample efficiency compared to existing approaches.

Conclusion: UARL paves the way for safer and more robust RL applications in the real world by enhancing generalization and eliminating the need for risky direct interactions with the target domain.

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [231] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: This paper introduces a Fourier Spectral Transformer network that leverages spectral methods and neural architectures to achieve accurate long-term predictions of PDE dynamics.


<details>
  <summary>Details</summary>
Motivation: Current numerical and machine learning methods struggle to balance precision and efficiency in predicting complex PDE dynamics like fluid flows.

Method: The study transforms PDEs into spectral ODEs, generates precise training data with numerical solvers, and uses a Transformer network to model spectral coefficient evolution.

Result: Using test cases like the Navier-Stokes and Burgers' equations, the method outperformed traditional approaches and ML in forecasting fluid dynamics, even with limited data.

Conclusion: This framework facilitates real-time prediction and control of dynamical systems, offering generalization across unseen data and a promising future paradigm.

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [232] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: Graph learning is vital in AI for modeling complex relationships where traditional methods fall short. This survey covers advancements in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI, and responsible AI, addressing challenges like scalability and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing need for a comprehensive understanding of graph learning developments, especially its transformative impact across various real-world applications while identifying challenges like interpretability and scalability.

Method: This survey categorizes and reviews state-of-the-art techniques across six key dimensions of graph learning: scalability, temporal modeling, multimodal integration, generative processes, explainability, and ethical considerations.

Result: The survey systematizes existing techniques, identifies challenges like heterogeneity and trustworthiness, and underscores their ethical implications, offering a roadmap for responsible and effective graph learning innovation.

Conclusion: This review highlights the progress, challenges, ethical dimensions, and future directions in graph learning. It aims to serve as a resource for navigating and advancing the field in real-world applications.

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [233] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: This paper presents a deep learning approach using wearable inertial sensors to classify canine gait abnormalities with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in distinguishing between neurological and orthopedic gait abnormalities in dogs, even by experienced clinicians.

Method: A deep learning model was developed and trained on inertial sensor readings from 29 dogs to classify gait types into healthy, orthopedic, or neurological categories.

Result: The model achieved 96% accuracy for multiclass classification (healthy/orthopedic/neurological) and 82% accuracy for binary classification (healthy/non-healthy) when generalizing to unseen dogs.

Conclusion: Inertial-based deep learning models hold promise as practical and objective tools for diagnosing orthopedic and neurological gait abnormalities in canine clinical assessments.

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [234] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: This paper addresses system-level challenges in federated training of large-scale AI models with a mixture-of-experts (MoE) architecture. It proposes strategies for dynamic client-expert alignment to optimize scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to overcome the inefficiencies in federated learning of large, MoE-structured AI models caused by heterogeneous client resources and complex expert coordination.

Method: The method proposed involves intelligent client-expert alignment using dynamic fitness scoring, global expert load monitoring, and client capacity profiling.

Result: The approach potentially reduces communication rounds required for convergence, making federated training more scalable and efficient.

Conclusion: The work offers a foundational strategy for enabling robust and efficient deployment of large-scale federated MoE AI models in decentralized edge systems.

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [235] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: The paper introduces AutoTriton, an RL-powered model for Triton programming that automates optimization of GPU kernel performance.


<details>
  <summary>Details</summary>
Motivation: Optimizing GPU kernels for deep learning involves significant manual effort, requiring developers to fine-tune parameters and hardware-specific optimizations. While domain-specific languages like Triton simplify this, the manual tuning of parameters remains a challenge. This paper aims to reduce these barriers and improve performance.

Method: The researchers developed AutoTriton, which uses supervised fine-tuning (SFT) to learn Triton programming techniques and reinforcement learning (RL) using the GRPO algorithm. The RL process incorporates both rule-based rewards and execution-based rewards to further improve kernel generation.

Result: AutoTriton achieves performance comparable to mainstream large models like Claude-4-Sonnet and DeepSeek-R1-0528 across five evaluation benchmarks. Experimental analysis highlights the significance of each module within AutoTriton.

Conclusion: The study demonstrates the feasibility and potential of reinforcement learning for automatically generating high-performance GPU kernels, laying a foundation for more efficient AI systems. The model and code are open-sourced for future advancements.

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [236] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: The paper introduces MobileGUI-RL, a method that trains vision-based GUI agents in online environments for automating mobile and web tasks, achieving better performance compared to existing offline approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of offline training methods for GUI agents, such as lack of scalability, overfitting, and poor adaptability in unseen environments.

Method: The proposed MobileGUI-RL framework trains GUI agents online by synthesizing learnable tasks via self-exploration and filtering, and modifies the GRPO approach with trajectory-aware advantages and composite rewards.

Result: Experiments across three online benchmarks demonstrate consistent improvements in task success and execution efficiency.

Conclusion: MobileGUI-RL is a scalable and effective solution for GUI navigation challenges, overcoming the shortcomings of previous offline training methods.

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [237] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: The paper proposes a dual-layer UAV-assisted edge computing architecture with a hierarchical offloading scheme to optimize system delay, energy consumption, and task reliability in vehicular networks.


<details>
  <summary>Details</summary>
Motivation: Existing UAV-assisted offloading strategies face challenges in coordinating heterogeneous resources and adapting to dynamic network conditions.

Method: A dual-layer architecture using partial offloading is designed with high-altitude UAVs for relay capability and low-altitude UAVs for computing. A Markov decision process-based approach is employed to optimize decisions via a soft actor-critic algorithm.

Result: Simulations demonstrate enhanced task completion rate, better system efficiency, and improved convergence speed over baseline strategies.

Conclusion: The proposed architecture and optimization method show robustness and applicability for dynamic vehicular environments, suggesting promise for future edge computing solutions.

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [238] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: The paper introduces WeatherMixer, an AI model for atmospheric forecasting, using Jigsaw parallelization to achieve state-of-the-art performance in scaling and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges in accurate atmospheric forecasting, particularly with large data samples and complex dynamics, by creating a scalable and efficient AI model.

Method: The researchers developed WeatherMixer, a scalable MLP-based architecture, and a novel model parallelization scheme called Jigsaw that combines domain and tensor parallelism to reduce computational overheads.

Result: WeatherMixer and Jigsaw achieved state-of-the-art performance in scaling, with peak performances of 9 and 11 PFLOPs on 256 GPUs, and improved scaling efficiencies of 68% and 72%.

Conclusion: The proposed WeatherMixer and Jigsaw approaches demonstrate significant progress in overcoming memory and bandwidth bottlenecks, making AI-based atmospheric forecasting more efficient and scalable.

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [239] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: This study presents a novel approach that integrates deep learning-based image registration with physics-informed regularization for biomechanical analysis of cardiac tissues and disease classification, achieving highly accurate results.


<details>
  <summary>Details</summary>
Motivation: Cardiac diseases are prevalent and require accurate diagnostic methods. The study aims to develop a methodology to improve diagnostic accuracy and provide insights into cardiac biomechanics.

Method: The approach combines deep learning image registration with physics-informed regularization, modeling tissue deformations using Neo-Hookean material formulation. Features extracted from estimated local strains are used for disease classification with five classifier algorithms.

Result: The method achieved high dice scores (up to 0.945) for ventricular segmentation and perfect classification accuracy (98% training, 100% testing) on the ACDC dataset.

Conclusion: This explainable method enhances diagnostic accuracy and reliability for cardiac diseases while providing clinicians with insights into cardiac mechanics, supporting personalized patient care.

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [240] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: This paper focuses on the limitations of Large Language Models (LLMs) in physics reasoning and introduces a benchmark, ABench-Physics, with static (Phy_A) and dynamic (Phy_B) problem sets to assess their capabilities.


<details>
  <summary>Details</summary>
Motivation: Identify and address the underexplored capabilities of LLMs in solving physics problems requiring physical modeling and conceptual reasoning.

Method: They developed ABench-Physics with two components: Phy_A (static graduate-level problems) and Phy_B (dynamic problems with variability) requiring precise numerical answers for evaluation.

Result: Evaluation of state-of-the-art LLMs demonstrated significant limitations, particularly in generalizing to dynamic physics problems.

Conclusion: ABench-Physics reveals gaps in LLMs' scientific reasoning, offering a robust framework to improve physical modeling capabilities.

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [241] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: The paper introduces Soup-Adapter, a new method that tackles hyperparameter tuning limitations and model robustness in few-shot domain adaptation by combining multiple adapters.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of hyperparameter tuning without large validation datasets and improving model robustness under distribution shifts.

Method: Training multiple independent adapters with diverse hyperparameters, averaging their outputs, and reparameterizing them into a single adapter using a principled concatenation approach.

Result: The ensemble method improves performance, robustness to distribution shifts, and reduces sensitivity to key hyperparameters like the residual ratio of CLIP-Adapter.

Conclusion: Soup-Adapter effectively resolves key limitations in few-shot domain adaptation, offering robustness and performance improvements, and is also the first study comparing CLIP adapter-style techniques for DINOv2.

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [242] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: The paper introduces BAGEL, a framework and tool for global interpretability, focusing on semantic concepts and their propagation in neural networks, enabling enhanced trustworthiness and understanding of model behavior.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional local interpretability methods by creating a tool for global mechanistic analysis of model behavior and dataset biases.

Method: Developing a model-agnostic framework that uses semantic attributes (concepts) to analyze and visualize their interactions across layers through a structured knowledge graph.

Result: The framework reveals latent circuits and the information flow underlying decision-making in deep learning models, offering insights into concept-class relationships and spurious correlations.

Conclusion: BAGEL enhances understanding of model generalization and biases, fostering trust and deeper insights into deep learning mechanisms.

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [243] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: This paper addresses the challenge of achieving both domain generalization and algorithmic fairness in machine learning. It proposes the Fair Domain Generalization (FairDG) problem and presents a method (PAFDG) to optimize utility-fairness trade-offs in unseen domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to reconcile two critical ML challenges: robust generalization to unseen domains (domain generalization) and ensuring algorithmic fairness, which are often addressed separately. Current DG methods neglect fairness, while fairness approaches fail to consider domain shifts.

Method: The paper introduces PAFDG, a framework designed to solve the FairDG problem by employing Pareto optimization to balance utility and fairness. This approach is guided by novel mutual information-based bounds that optimize both risk and fairness violations.

Result: The proposed PAFDG method demonstrates superior trade-offs between utility and fairness compared to existing approaches when evaluated on real-world vision and language datasets.

Conclusion: The work successfully bridges domain generalization and algorithmic fairness by introducing a theoretically grounded and practical method (PAFDG). It lays the groundwork for addressing these challenges together in future applications.

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [244] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed neural network (PINN) for accurate and robust state estimation in power systems, demonstrating superior performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Power systems face challenges in accurate and fast state estimation, especially under faults or cyber-attacks, necessitating robust and secure solutions.

Method: A hybrid method using physics-informed neural networks (PINNs) is proposed, embedding physical laws into the neural network architecture to improve performance.

Result: The PINN approach achieves up to 83% higher accuracy on unseen training subsets and 65% on unrelated datasets, as well as 93% better accuracy during data manipulation attacks compared to traditional neural networks.

Conclusion: Integrating physical principles into neural networks significantly enhances power system state estimation, improving accuracy, robustness, and security.

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [245] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: The paper presents a framework for generating universal embeddings of tabular data using graph structures and Graph Auto-Encoders, enabling effective downstream tasks like regression and classification without predefined targets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing and interpreting industrial tabular data for various tasks that are not defined beforehand.

Method: Transforms relational database tables into graph structures, applies Graph Auto-Encoders for entity embeddings, and aggregates these embeddings to represent table rows.

Result: The proposed method achieves better performance than existing tabular data embedding techniques on real-world datasets.

Conclusion: This method provides a task-independent solution for embedding tabular data, paving the way for efficient downstream tasks using similarity measures in the embedding space.

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [246] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: The paper introduces a Diffusion Dataset Condensation (D2C) framework to reduce the data and computational requirements for training diffusion models, achieving significant speed-up and retaining high result quality.


<details>
  <summary>Details</summary>
Motivation: Training diffusion models is resource-intensive, requiring large datasets and significant GPU computation time. The study aims to address this data- and compute-heavy limitation by creating a synthetic sub-dataset for efficient model training.

Method: The D2C framework comprises two phases—Select and Attach. The Select phase identifies a compact and diverse subset based on a diffusion difficulty score and interval sampling. The Attach phase enriches the dataset with semantic and visual representations to enhance conditional signals.

Result: Experiments show that D2C achieves a significant reduction in data and time required for training diffusion models while retaining high output quality. For instance, it enables 100x faster training and a FID score of 4.3 using 0.8% of the original dataset.

Conclusion: D2C provides an effective solution for accelerating the training of diffusion models with limited data without compromising performance, making it a pioneering approach in dataset condensation for generative tasks.

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [247] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: The paper addresses the problem of noisy labels in AI training for classifying canine heart murmurs. Label noise was reduced by incorporating multiple expert opinions and high-quality data selection, resulting in significant performance improvements, especially with the XGBoost algorithm.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the negative impact of label noise on AI model performance in diagnosing canine heart murmurs, which is critical in veterinary cardiology. Noisy labels create ambiguity and hinder accurate detection, especially in the context of Myxomatous Mitral Valve Disease (MMVD).

Method: The authors annotated heart sound recordings (HSR) with multiple expert opinions to reduce label noise and selected 70 high-quality samples. Heart cycles were leveraged to expand the dataset. They then trained and evaluated three classification algorithms (AdaBoost, XGBoost, Random Forest) to assess impacts on classification performance.

Result: XGBoost outperformed the other classifiers, showing the most significant improvement in accuracy after label noise reduction. Sensitivity and specificity metrics for detecting mild, moderate, and loud/thrilling heart murmurs showed significant increases, with the most notable gains in the XGBoost model.

Conclusion: Incorporating multiple expert opinions and reducing label noise significantly improves classification performance in detecting canine heart murmurs. The findings stress the importance of data quality in veterinary AI applications, with XGBoost proving highly effective.

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [248] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper proposes a new interpretation of the Adam optimizer, simplifying its convergence analysis and improving theoretical results.


<details>
  <summary>Details</summary>
Motivation: Existing Adam convergence analysis has been unsatisfactory due to strong assumptions and complex techniques required for its existing interpretation.

Method: The authors reinterpret Adam as a sign-like optimizer, simplifying the convergence analysis and proving better theoretical convergence rates under mild conditions.

Result: The paper achieves a tighter convergence rate of \(O(1/T^{1/4})\), improves understanding of Adam's momentum, and provides practical learning rate tuning guidelines.

Conclusion: The new interpretation offers theoretical and practical improvements, resolving gaps in Adam's convergence analysis and advancing its utility in training deep neural networks.

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [249] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt is a Python framework for creating and interpreting models for time series data.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and interpretable tool for knowledge discovery in complex time series data.

Method: A Python toolkit with modular interfaces to integrate datasets, architectures, and interpretability methods.

Result: Offers a platform for building, exploring, and interpreting deep learning models for time series analysis.

Conclusion: KnowIt promotes deep time series modeling and facilitates interpretability through collaborative improvement.

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [250] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: The paper introduces Kamae, a Python library that ensures consistent feature preprocessing across production recommender systems' training and inference by converting PySpark pipelines to Keras models.


<details>
  <summary>Details</summary>
Motivation: Duplicating preprocessing logic across offline and online environments in production recommender systems leads to increased engineering efforts and risks like dataset shift; hence, a unified approach is needed.

Method: The authors developed Kamae, an open-source library with configurable Spark transformers and estimators, mapped to equivalent Keras layers to maintain consistent preprocessing across machine learning pipelines.

Result: Kamae demonstrated its effectiveness through applications on real-world datasets, including MovieLens and Expedia's Learning-to-Rank use cases.

Conclusion: Kamae bridges the gap between PySpark and Keras, enabling efficient, consistent end-to-end preprocessing across the machine learning lifecycle and mitigating engineering challenges.

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [251] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: This paper presents a universal HDLSS learning approach using multi-view mid fusion methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of HDLSS settings where feature dimensions greatly exceed available samples.

Method: Proposed three view-construction methods to split high-dimensional features into smaller subsets, leveraging multi-view mid fusion techniques.

Result: Experimental validations show effective generalization and performance across model types and tasks.

Conclusion: The study highlights the potential and universal benefits of using multi-view mid fusion learning in HDLSS settings, providing a foundation for further research.

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [252] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec is a neural compressor for barometric data from wind turbines that achieves high compression rates while maintaining low reconstruction error and real-time operation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of efficiently compressing barometric data from wind turbine blades to optimize wireless transmission energy consumption and adapt to changing network conditions.

Method: EdgeCodec uses an asymmetric autoencoder trained with a discriminator, augmented by a Residual Vector Quantizer, and capable of adapting bitrates on a sample-by-sample basis.

Result: The approach achieves compression rates of up to 10,240:1 with reconstruction errors under 3%, operates on a GAP9 microcontroller in real-time, and reduces wireless transmission energy consumption by 2.9x in its high compression mode.

Conclusion: EdgeCodec provides a significant advance in data compression for wind turbine sensors, enabling efficient energy use, real-time adaptability, and extended operational lifetimes.

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [253] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: Structured State Space Models (SSMs) are optimized for edge computing using Quantization-Aware Training (QAT), enhancing efficiency and robustness during deployment.


<details>
  <summary>Details</summary>
Motivation: With increasing demand for processing long sequences in resource-constrained environments, SSMs offer a scalable solution compared to memory-intensive Transformers.

Method: The paper utilizes Quantization-Aware Training (QAT) and analyzes its effects on model size, numerical precision, analog noise robustness, and pruning for edge hardware like AIMC chips.

Result: Quantization-Aware Training (QAT) reduces SSM complexity by up to two orders of magnitude while improving robustness and enabling structural optimizations.

Conclusion: QAT enhances the efficiency and deployability of SSMs on edge hardware, especially memristive analog in-memory computing substrates, achieving significant computational improvements.

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [254] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: The paper introduces CoRE, a mechanism for label-free self-evaluation in Large Reasoning Models to enhance efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models show strong performance but struggle with inefficiencies caused by overthinking and excessive reasoning steps.

Method: The authors propose CoRE, a latent space embedding approach to analyze intermediate reasoning steps and detect cyclical inefficiencies. They further develop CoRE-Eval, a self-evaluation framework that uses geometric pattern detection to dynamically terminate reasoning.

Result: Experiments demonstrate that CoRE-Eval reduces reasoning redundancy (chain-of-thought length) by 13.7% to 33.2% while boosting answer accuracy by about 10%, with noteworthy performance on mathematical reasoning benchmarks.

Conclusion: CoRE and CoRE-Eval improve reasoning efficiency and accuracy in LRMs, showcasing practical applicability for enhancing reasoning models without external labels.

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [255] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: The ZO-SAH algorithm accelerates zeroth-order optimization by incorporating approximate second-order information through subspace eigen-analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance convergence in zeroth-order optimization by addressing the challenges of estimating curvature information (Hessian) due to its high computational cost.

Method: The ZO-SAH method uses randomly selected 2D subspaces to fit quadratic models and estimate Hessians, along with a subspace-switching strategy to reuse function evaluations and reduce query costs.

Result: ZO-SAH demonstrates faster convergence compared to existing zeroth-order methods in experiments across eight benchmark datasets, including tasks like logistic regression and deep neural network training.

Conclusion: Incorporating efficient second-order approximations into zeroth-order optimization is feasible, and ZO-SAH proves to be a practical algorithm for achieving faster convergence with lower function-query costs.

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [256] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: This paper proposes an AI-driven hierarchical link prediction framework that integrates matrix factorization methods for identifying missing associations in large, sparse, and noisy scientific literature networks or knowledge graphs, particularly applied to transition-metal dichalcogenides (TMDs).


<details>
  <summary>Details</summary>
Motivation: Scientific literature networks and knowledge graphs often suffer from missing or weakly connected links due to their large and sparse nature, which makes uncovering hidden associations and fostering scientific discovery challenging.

Method: The approach combines Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean Matrix Factorization (BNMFk) with automatic model selection, and Logistic Matrix Factorization (LMF). These methods generate a three-level topic tree from a large corpus, while an ensemble BNMFk + LMF provides interpretability and probabilistic scoring. These were implemented to make latent material-topic associations visible.

Result: Their method successfully reveals hidden connections and predicts missing links in the data, specifically identifying relationships between TMDs and topics like superconductivity. Validation demonstrated that the model could infer these associations accurately, even with data gaps.

Conclusion: The framework demonstrates its utility in uncovering hidden links in a multidisciplinary graph of scientific materials. By enabling hypothesis generation and facilitating human-in-the-loop discovery through an interactive dashboard, the method promotes cross-community exploration and scientific advancements.

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [257] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: The paper addresses aliasing effects in convolutional layers, deriving efficient methods to suppress aliasing and ensure stability.


<details>
  <summary>Details</summary>
Motivation: Current methods lack a comprehensive analysis of aliasing's impact on numerical stability in convolutional layers.

Method: A frame-theoretic approach is adapted to analyze aliasing in convolutional layers with 1D kernels, leading to optimization objectives and closed-form expressions for random kernels.

Result: The study produces practical stability estimates, efficient optimization objectives to minimize aliasing, and insights into aliasing behavior in initialized layers.

Conclusion: The developed methods systematically suppress aliasing, enhance Parseval stability, and offer fundamental insights into aliasing effects in convolutional layers.

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [258] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: This paper proposes a method for optimizing connections in Deep Differentiable Logic Gate Networks (LGNs), achieving improved performance and efficiency across datasets like MNIST.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance LGNs' efficiency and performance using fewer logic gates by partially optimizing their connections.

Method: The method involves training LGNs with a probability distribution over a subset of connections per gate input, selecting the best connection, and subsequently determining the gate types.

Result: The connection-optimized LGNs outperformed fixed-connection LGNs on benchmarks such as Yin-Yang, MNIST, and Fashion-MNIST, with significant reductions in required logic gates.

Conclusion: This work demonstrates that LGNs can achieve high performance with significantly fewer resources, paving the way for fully trainable Boolean logic networks.

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [259] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: The paper addresses the issue of sequence models overallocating attention to irrelevant contexts, deteriorating their performance. It adapts differential mechanisms from Transformers to Mamba architecture, offering modified designs for higher efficiency and improved retrieval abilities.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness, retrieval capabilities, and long-range performance of sequence models by addressing their tendency to allocate excessive attention to irrelevant context.

Method: The paper adapts differential design principles from Transformers to Mamba architecture, introducing a novel mechanism for efficient handling of attention allocation.

Result: Empirical evaluations show that the proposed differential mechanism for Mamba improves retrieval capabilities and model performance compared to its vanilla version.

Conclusion: The modified Mamba architecture effectively mitigates overallocation problems and advances its efficiency and capability compared to existing models like transformers. The approach is broadly applicable, and code resources are shared publicly.

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [260] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: This paper explores Associative Memories, focusing on their modern interpretations, connections to AI models, and practical applications.


<details>
  <summary>Details</summary>
Motivation: The study aims to revisit Associative Memories in light of new theoretical advancements and their relevance to modern AI architectures.

Method: The tutorial integrates mathematical derivations, coding notebooks, and discussions on Lagrangian formulations to illustrate concepts and applications.

Result: The research bridges theoretical insights into Associative Memories with practical applications, highlighting their parallels with SOTA AI models.

Conclusion: Associative Memories are foundational in AI, with contemporary approaches broadening their utility in interpreting and designing neural networks.

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [261] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: The paper focuses on optimizing antenna activation in waveguides using neural networks to maximize communication rates, tackling the challenge of user location uncertainty.


<details>
  <summary>Details</summary>
Motivation: The need for energy-efficient, cost-effective, and flexible antenna technologies to keep pace with evolving wireless communication demands.

Method: Formulate the problem as a combinatorial fractional 0-1 quadratic program and use neural networks of varying complexity to learn activation policies from data.

Result: Simulation results confirm the effectiveness and robustness of the proposed models, even under user location uncertainty.

Conclusion: The proposed neural network-based approach successfully optimizes antenna activation, contributing to advanced wireless communication systems under realistic conditions.

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [262] [Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics](https://arxiv.org/abs/2507.05534)
*Erik Hemberg,Eric Liu,Lucille Fuller,Stephen Moskal,Una-May O'Reilly*

Main category: cs.NE

TL;DR: The paper explores the performance of different controller representations and algorithms for evolving cyber agent teams, comparing single-side optimization with coevolution across scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess the relative effectiveness of controller representations and evolutionary algorithms for optimizing the performance of cyber agent teams.

Method: Comparison of single-side training and coevolution scenarios using grammatical evolution algorithms with code-like logic and a novel LLM-supported mutation operator.

Result: Grammatical evolution algorithms excel in team performance, and coevolution reduces performance extremes but induces dynamics, while single-side optimization sustains higher peaks.

Conclusion: The trade-off between coevolution and single-side optimization highlights distinct impacts on performance and fluctuations, offering insights for cyber agent team strategy development.

Abstract: We investigate two representation alternatives for the controllers of teams
of cyber agents. We combine these controller representations with different
evolutionary algorithms, one of which introduces a novel LLM-supported mutation
operator. Using a cyber security scenario, we evaluate agent learning when one
side is trained to compete against a side that does not evolve and when two
sides coevolve with each other. This allows us to quantify the relative merits
and tradeoffs of representation and algorithm combinations in terms of team
performance. Our versions of grammatical evolution algorithms using grammars
that allow a controller to be expressed in code-like logic can achieve the best
team performance. The scenario also allows us to compare the performance impact
and dynamics of coevolution versus evolution under different combinations.
Across the algorithms and representations, we observe that coevolution reduces
the performance highs and lows of both sides while it induces fluctuations on
both sides. In contrast, when only one-side is optimized, performance peaks are
higher and is more sustained than when both sides are optimized with
coevolution.

</details>


### [263] [A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion](https://arxiv.org/abs/2507.05847)
*Jia-Cheng Li,Min-Rong Chen,Guo-Qiang Zeng,Jian Weng,Man Wang,Jia-Lin Mai*

Main category: cs.NE

TL;DR: The paper introduces a universal framework inspired by particle drift and diffusion to improve evolutionary algorithms for large-scale multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: Existing evolutionary algorithms struggle with maintaining convergence and diversity for optimization problems with high-dimensional decision variables.

Method: The framework divides the optimization into three sub-stages (two coarse-tuning and one fine-tuning), applying drift-diffusion operations at each stage; evolutionary algorithms are integrated into the framework.

Result: Experiments with up to 5000 decision variables and neural network training show significant improvements in convergence, diversity, and computational efficiency.

Conclusion: The proposed framework effectively addresses challenges in large-scale multi-objective optimization, enhancing algorithm performance and practical problem-solving capabilities.

Abstract: Large-scale multi-objective optimization poses challenges to existing
evolutionary algorithms in maintaining the performances of convergence and
diversity because of high dimensional decision variables. Inspired by the
motion of particles in physics, we propose a universal framework for
large-scale multi-objective optimization based on particle drift and diffusion
to solve these challenges in this paper. This framework innovatively divides
the optimization process into three sub-stages: two coarse-tuning sub-stages
and one fine-tuning sub-stage. Different strategies of drift-diffusion
operations are performed on the guiding solutions according to the current
sub-stage, ingeniously simulating the movement of particles under diverse
environmental conditions. Finally, representative evolutionary algorithms are
embedded into the proposed framework, and their effectiveness are evaluated
through comparative experiments on various large-scale multi-objective problems
with 1000 to 5000 decision variables. Moreover, comparative algorithms are
conducted on neural network training problems to validate the effectiveness of
the proposed framework in the practical problems. The experimental results
demonstrate that the framework proposed in this paper significantly enhances
the performance of convergence and diversity of MOEAs, and improves the
computational efficiency of algorithms in solving large-scale multi-objective
optimization problems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [264] [A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation](https://arxiv.org/abs/2507.05331)
*TRI LBM Team,Jose Barreiros,Andrew Beaulieu,Aditya Bhat,Rick Cory,Eric Cousineau,Hongkai Dai,Ching-Hsin Fang,Kunimatsu Hashimoto,Muhammad Zubair Irshad,Masha Itkina,Naveen Kuppuswamy,Kuan-Hui Lee,Katherine Liu,Dale McConachie,Ian McMahon,Haruki Nishimura,Calder Phillips-Grafflin,Charles Richter,Paarth Shah,Krishnan Srinivasan,Blake Wulfe,Chen Xu,Mengchao Zhang,Alex Alspach,Maya Angeles,Kushal Arora,Vitor Campagnolo Guizilini,Alejandro Castro,Dian Chen,Ting-Sheng Chu,Sam Creasey,Sean Curtis,Richard Denitto,Emma Dixon,Eric Dusel,Matthew Ferreira,Aimee Goncalves,Grant Gould,Damrong Guoy,Swati Gupta,Xuchen Han,Kyle Hatch,Brendan Hathaway,Allison Henry,Hillel Hochsztein,Phoebe Horgan,Shun Iwase,Donovon Jackson,Siddharth Karamcheti,Sedrick Keh,Joseph Masterjohn,Jean Mercat,Patrick Miller,Paul Mitiguy,Tony Nguyen,Jeremy Nimmer,Yuki Noguchi,Reko Ong,Aykut Onol,Owen Pfannenstiehl,Richard Poyner,Leticia Priebe Mendes Rocha,Gordon Richardson,Christopher Rodriguez,Derick Seale,Michael Sherman,Mariah Smith-Jones,David Tago,Pavel Tokmakov,Matthew Tran,Basile Van Hoorick,Igor Vasiljevic,Sergey Zakharov,Mark Zolotas,Rares Ambrus,Kerri Fetzer-Borelli,Benjamin Burchfiel,Hadas Kress-Gazit,Siyuan Feng,Stacie Ford,Russ Tedrake*

Main category: cs.RO

TL;DR: This paper evaluates the performance of multitask robot manipulation policies, called Large Behavior Models (LBMs), by extending Diffusion Policy on simulated and real-world robot data. The study demonstrates that multi-task pretraining improves robustness and task efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of evaluating real-world performance of general-purpose robot foundation models, a critical step to advance robot manipulation and understand their capabilities.

Method: The authors propose and validate an evaluation pipeline for multitask models by comparing multi-task LBMs with single-task baselines through statistical analysis, using blind, randomized trials in both simulated and real-world environments.

Result: The results indicate that multi-task pretraining improves policy robustness and success rates, while also enabling faster learning of complex new tasks with reduced data compared to single-task baselines. Performance further scales predictably with increased pretraining data diversity.

Conclusion: Multi-task pretraining for robot manipulation policies enhances robustness, efficiency, and scalability of task performance, demonstrating the potential of Large Behavior Models with sufficient pretraining data diversity.

Abstract: Robot manipulation has seen tremendous progress in recent years, with
imitation learning policies enabling successful performance of dexterous and
hard-to-model tasks. Concurrently, scaling data and model size has led to the
development of capable language and vision foundation models, motivating
large-scale efforts to create general-purpose robot foundation models. While
these models have garnered significant enthusiasm and investment, meaningful
evaluation of real-world performance remains a challenge, limiting both the
pace of development and inhibiting a nuanced understanding of current
capabilities. In this paper, we rigorously evaluate multitask robot
manipulation policies, referred to as Large Behavior Models (LBMs), by
extending the Diffusion Policy paradigm across a corpus of simulated and
real-world robot data. We propose and validate an evaluation pipeline to
rigorously analyze the capabilities of these models with statistical
confidence. We compare against single-task baselines through blind, randomized
trials in a controlled setting, using both simulation and real-world
experiments. We find that multi-task pretraining makes the policies more
successful and robust, and enables teaching complex new tasks more quickly,
using a fraction of the data when compared to single-task baselines. Moreover,
performance predictably increases as pretraining scale and diversity grows.
Project page: https://toyotaresearchinstitute.github.io/lbm1/

</details>


### [265] [Feature Geometry for Stereo Sidescan and Forward-looking Sonar](https://arxiv.org/abs/2507.05410)
*Kalin Norman,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: This paper proposes a method for fusing acoustic data from forward-looking and sidescan sonar using geometry-based projections to enable cross-modal feature correlation and 3D information recovery in marine robotics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance stereo acoustic data fusion for marine robotics by addressing the challenge of correlating feature observations between two disparate sonar modalities (forward-looking and sidescan sonar).

Method: The authors introduce an acoustic geometry inspired by epipolar geometry for stereo cameras and use relative pose information to project features observed in one sonar into the other sonar's image.

Result: Simulated tests demonstrate how relative configuration between sonars impacts projection accuracy, providing insights into optimal stereo configurations for tasks like feature matching and 3D feature reconstruction.

Conclusion: The study highlights the viability of using geometric projections for cross-modal stereo sonar setups, offering practical guidance for their deployment in field robotics applications.

Abstract: In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.

</details>


### [266] [CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning](https://arxiv.org/abs/2507.05458)
*Yi-Shiuan Tung,Bradley Hayes,Alessandro Roncone*

Main category: cs.RO

TL;DR: This paper introduces CRED, a trajectory generation method for Active Preference Learning (APL), focusing on improving human reward function estimation and enhancing robot adaptability to human preferences.


<details>
  <summary>Details</summary>
Motivation: Robots need to adapt to human preferences such as balancing distance, time, and safety in tasks like delivery routing, and current methods face challenges in exploring trajectory spaces and forming informative queries.

Method: The authors propose CRED, which combines environment design and trajectory selection using counterfactual reasoning, imagining scenarios and sampling rewards to create diverse query sets for ranking human preferences.

Result: Experiments demonstrate that CRED enhances reward learning and generalizes effectively across different environments, including GridWorld and OpenStreetMap navigation.

Conclusion: CRED facilitates better estimation of human reward functions in Active Preference Learning, promising improved robot adaptability to diverse human preferences.

Abstract: For effective real-world deployment, robots should adapt to human
preferences, such as balancing distance, time, and safety in delivery routing.
Active preference learning (APL) learns human reward functions by presenting
trajectories for ranking. However, existing methods often struggle to explore
the full trajectory space and fail to identify informative queries,
particularly in long-horizon tasks. We propose CRED, a trajectory generation
method for APL that improves reward estimation by jointly optimizing
environment design and trajectory selection. CRED "imagines" new scenarios
through environment design and uses counterfactual reasoning -- by sampling
rewards from its current belief and asking "What if this reward were the true
preference?" -- to generate a diverse and informative set of trajectories for
ranking. Experiments in GridWorld and real-world navigation using OpenStreetMap
data show that CRED improves reward learning and generalizes effectively across
different environments.

</details>


### [267] [Gaussian Process-Based Active Exploration Strategies in Vision and Touch](https://arxiv.org/abs/2507.05522)
*Ho Jin Choi,Nadia Figueroa*

Main category: cs.RO

TL;DR: This paper introduces a method for robots to actively learn object properties by integrating visual and tactile inputs into a unified Gaussian Process Distance Field (GPDF), enabling precise 3D shape modeling without extensive pretraining.


<details>
  <summary>Details</summary>
Motivation: Robots struggle to manipulate objects in unstructured environments due to limited understanding of object properties like shape and material, while humans excel through interactive multi-sensor exploration.

Method: The proposed approach uses Gaussian Process Distance Fields (GPDF) to fuse visual and tactile observations, iteratively refining object geometry via vision and tactile sensing. GPDF allows uncertainty quantification and exploratory motion planning for information gain, with experiments conducted using robotic manipulators equipped with tactile sensors and RGB-D cameras.

Result: The experiments demonstrate effective exploration and precise mapping of static object geometries, with scalability improvements achieved using approximation methods.

Conclusion: This probabilistic multi-modal fusion framework enables robots to actively explore and map complex object geometries, potentially extending to surface properties beyond geometry.

Abstract: Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.

</details>


### [268] [PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs](https://arxiv.org/abs/2507.05555)
*Obin Kwon,Sankalp Yamsani,Noboru Myers,Sean Taylor,Jooyoung Hong,Kyungseo Park,Alex Alspach,Joohyung Kim*

Main category: cs.RO

TL;DR: The paper introduces PAPRLE, a modular system for robotic limb control that supports flexible configurations and diverse input devices, enabling versatile teleoperation. The system offers bilateral control, force feedback, and compatibility across robot setups.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable robotic limb systems that can support various teleoperation scenarios and configurations in research and practical contexts.

Method: PAPRLE's modular ecosystem allows user-customizable setups, employing pluggable puppeteer devices and compatibility with diverse input devices. It supports joint-space and task-space control with real-time force feedback.

Result: The PAPRLE system demonstrated effective performance in real-world scenarios across various leader devices and follower robot configurations, validating its adaptability and scalability.

Conclusion: PAPRLE's modular and versatile design advances research in embodied AI and control systems. Its open-source release aims to encourage adoption and further development by the community.

Abstract: We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular
ecosystem that enables flexible placement and control of robotic limbs. With
PAPRLE, a user can change the arrangement of the robotic limbs, and control
them using a variety of input devices, including puppeteers, gaming
controllers, and VR-based interfaces. This versatility supports a wide range of
teleoperation scenarios and promotes adaptability to different task
requirements. To further enhance configurability, we introduce a pluggable
puppeteer device that can be easily mounted and adapted to match the target
robot configurations. PAPRLE supports bilateral teleoperation through these
puppeteer devices, agnostic to the type or configuration of the follower robot.
By supporting both joint-space and task-space control, the system provides
real-time force feedback, improving user fidelity and physical interaction
awareness. The modular design of PAPRLE facilitates novel spatial arrangements
of the limbs and enables scalable data collection, thereby advancing research
in embodied AI and learning-based control. We validate PAPRLE in various
real-world settings, demonstrating its versatility across diverse combinations
of leader devices and follower robots. The system will be released as open
source, including both hardware and software components, to support broader
adoption and community-driven extension. Additional resources and
demonstrations are available at the project website:
https://uiuckimlab.github.io/paprle-pages

</details>


### [269] [Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube](https://arxiv.org/abs/2507.05607)
*Chongshan Fan,Shenghai Yuan*

Main category: cs.RO

TL;DR: Auto-RubikAI is a modular system integrating a knowledge base, vision-language, and large language models for interpretable, low-data robotic manipulation of tasks like solving a Rubik's Cube.


<details>
  <summary>Details</summary>
Motivation: To create a robust, interpretable, and data-efficient robotic planning system that can perform complex tasks like Rubik's Cube restoration without requiring large-scale training data or demonstrations.

Method: The paper proposes a tri-module system: a knowledge base for symbolic reasoning, a vision-language model for scene representation, and a large language model for robotic control code, combined for robust task execution in variable environments.

Result: Auto-RubikAI achieved a 79% success rate in randomized Rubik’s Cube tasks, outperformed traditional and modern baselines in solution steps, and demonstrated Sim-to-Real adaptability without retraining.

Conclusion: Auto-RubikAI provides a foundational, efficient framework for robotic planning, with applications in manufacturing, education, and autonomous systems, achieving interpretability, safety, and cost-efficiency.

Abstract: This paper presents Auto-RubikAI, a modular autonomous planning framework
that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),
and a large language model (LLM) to solve structured manipulation tasks
exemplified by Rubik's Cube restoration. Unlike traditional robot systems based
on predefined scripts, or modern approaches relying on pretrained networks and
large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step
task execution with minimal data requirements and no prior demonstrations. The
proposed system employs a KB module to solve group-theoretic restoration steps,
overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to
construct a semantic 3D scene representation, while the LLM generates
structured robotic control code via prompt chaining. This tri-module
architecture enables robust performance under spatial uncertainty. We deploy
Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic
arm, demonstrating effective Sim-to-Real adaptation without retraining.
Experiments show a 79% end-to-end task success rate across randomized
configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our
KB-enhanced method reduces average solution steps while maintaining
interpretability and safety. Auto-RubikAI provides a cost-efficient, modular
foundation for embodied task planning in smart manufacturing, robotics
education, and autonomous execution scenarios. Code, prompts, and hardware
modules will be released upon publication.

</details>


### [270] [DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation](https://arxiv.org/abs/2507.05627)
*Young Hun Kim,Seungyeon Kim,Yonghyeon Lee,Frank Chongwoo Park*

Main category: cs.RO

TL;DR: The paper proposes DreamGrasp, a framework for partial-view 3D recognition using pre-trained image generative models to handle cluttered, occluded environments.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to generalize in real-world scenarios for 3D reconstruction and object identification from minimal RGB images due to occlusions and lack of reliable depth data.

Method: DreamGrasp leverages large-scale generative models for imagination, combines coarse 3D reconstruction, contrastive learning for instance segmentation, and text-guided instance-wise refinement.

Result: DreamGrasp achieves accurate 3D object geometry reconstruction and performs well in tasks like decluttering and target retrieval in complex multi-object environments.

Conclusion: DreamGrasp provides a robust solution for 3D recognition in challenging scenarios, improving performance over prior methods and enabling practical applications.

Abstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying
object instances from a few sparse RGB images -- is an exceptionally
challenging yet practically essential task, particularly in cluttered, occluded
real-world settings where full-view or reliable depth data are often
unavailable. Existing methods, whether based on strong symmetry priors or
supervised learning on curated datasets, fail to generalize to such scenarios.
In this work, we introduce DreamGrasp, a framework that leverages the
imagination capability of large-scale pre-trained image generative models to
infer the unobserved parts of a scene. By combining coarse 3D reconstruction,
instance segmentation via contrastive learning, and text-guided instance-wise
refinement, DreamGrasp circumvents limitations of prior methods and enables
robust 3D reconstruction in complex, multi-object environments. Our experiments
show that DreamGrasp not only recovers accurate object geometry but also
supports downstream tasks like sequential decluttering and target retrieval
with high success rates.

</details>


### [271] [A Physics-Based Continuum Model for Versatile, Scalable, and Fast Terramechanics Simulation](https://arxiv.org/abs/2507.05643)
*Huzaifa Unjhawala,Luning Bakke,Harry Zhang,Michael Taylor,Ganesh Arivoli,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: The paper introduces Chrono::CRM, a physics-based, GPU-accelerated terramechanics simulation model leveraging SPH. It offers large-scale feasibility and open-source accessibility.


<details>
  <summary>Details</summary>
Motivation: Current semi-empirical terramechanics methods are limited in addressing complex tasks like digging and flexible wheel interactions, necessitating a scalable physics-based solution.

Method: The authors developed Chrono::CRM on the SPH framework, validated it against experimental and DEM data, and enabled efficient large-scale terrain simulations via GPU acceleration and active domains.

Result: Chrono::CRM successfully handles large terrain scales (10 km with 100M particles) and exhibits near interactive computational efficiency while maintaining accuracy through comparisons against physical tests and DEM benchmarks.

Conclusion: Chrono::CRM is a versatile, scalable, and efficient physics-based terramechanics modeling tool, with open-source availability to advance research.

Abstract: This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.

</details>


### [272] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: This paper introduces 3DGS-LSR, a localization framework that uses 3D Gaussian Splatting for reliable, centimeter-level positioning in complex urban environments using only a monocular RGB camera, addressing GNSS and traditional mapping limitations.


<details>
  <summary>Details</summary>
Motivation: Autonomous robotic systems need precise localization for safe navigation, but GNSS often becomes unreliable in urban environments due to signal occlusion and multipath effects. Additionally, traditional mapping methods have issues with storage and computational inefficiency.

Method: The paper proposes a method that constructs high-accuracy 3D Gaussian Splatting maps using multi-sensor data. On the robot side, it uses monocular RGB input combined with feature extraction tools like SuperPoint and SuperGlue. Localization is refined iteratively using step-by-step rendering.

Result: The 3DGS-LSR framework achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, outperforming other methods while relying only on monocular RGB input.

Conclusion: 3DGS-LSR is a robust localization framework suitable for urban environments, offering reliable centimeter-level positioning for autonomous robots without relying on GNSS or extensive computational resources.

Abstract: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

</details>


### [273] [Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains](https://arxiv.org/abs/2507.05663)
*Neelay Joglekar,Fei Liu,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: This paper develops a stable control system for cable-driven RCM robotic manipulators to address joint reading errors, ensuring reliable kinematics computations essential for autonomous surgical tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of joint reading errors in cable-driven RCM robotic manipulators, which compromise kinematics and impede the integration of autonomous surgical subtasks.

Method: The authors designed a provably stable tracking-in-the-loop controller for the out-of-view kinematic chain and implemented a bilevel control scheme for the entire kinematic chain. Their method was validated in simulated and real-world settings.

Result: Their controller ensured stability and reliable performance in correcting errors in the out-of-view kinematic chain, as verified through theoretical and experimental benchmarks.

Conclusion: The approach enables robust kinematics control for RCM robots even with joint reading errors, paving the way for incorporating autonomous functionalities into surgical procedures.

Abstract: Remote Center of Motion (RCM) robotic manipulators have revolutionized
Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation
within the patient's body cavity without disturbing the insertion point on the
patient. Accurate RCM tool control is vital for incorporating autonomous
subtasks like suturing, blood suction, and tumor resection into robotic
surgical procedures, reducing surgeon fatigue and improving patient outcomes.
However, these cable-driven systems are subject to significant joint reading
errors, corrupting the kinematics computation necessary to perform control.
Although visual tracking with endoscopic cameras can correct errors on in-view
joints, errors in the kinematic chain prior to the insertion point are
irreparable because they remain out of view. No prior work has characterized
the stability of control under these conditions. We fill this gap by designing
a provably stable tracking-in-the-loop controller for the out-of-view portion
of the RCM manipulator kinematic chain. We additionally incorporate this
controller into a bilevel control scheme for the full kinematic chain. We
rigorously benchmark our method in simulated and real world settings to verify
our theoretical findings. Our work provides key insights into the next steps
required for the transition from teleoperated to autonomous surgery.

</details>


### [274] [Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control](https://arxiv.org/abs/2507.05674)
*Xinyao Qin,Xiaoteng Ma,Yang Qi,Qihan Liu,Chuanyi Xue,Ning Gui,Qinyu Dong,Jun Yang,Bin Liang*

Main category: cs.RO

TL;DR: DMLoco employs diffusion models combined with PPO finetuning to achieve language-conditioned, stable, and generalizable legged robot locomotion.


<details>
  <summary>Details</summary>
Motivation: The aim is to overcome limitations in legged locomotion tasks, such as task transition difficulties and compounding errors, using diffusion models and reinforcement learning approaches.

Method: DMLoco leverages multi-task pretraining via diffusion models and online PPO finetuning, using DDIM for sampling efficiency and TensorRT for optimized real-world deployment.

Result: The framework achieves language-conditioned control, robust task transitions, and operates onboard quadruped robots at 50Hz, demonstrating scalability and efficiency.

Conclusion: DMLoco expands the application of generative models into legged locomotion, offering a practical and robust solution for adaptable, real-world robotic operations.

Abstract: Recent research has highlighted the powerful capabilities of imitation
learning in robotics. Leveraging generative models, particularly diffusion
models, these approaches offer notable advantages such as strong multi-task
generalization, effective language conditioning, and high sample efficiency.
While their application has been successful in manipulation tasks, their use in
legged locomotion remains relatively underexplored, mainly due to compounding
errors that affect stability and difficulties in task transition under limited
data. Online reinforcement learning (RL) has demonstrated promising results in
legged robot control in the past years, providing valuable insights to address
these challenges. In this work, we propose DMLoco, a diffusion-based framework
for quadruped robots that integrates multi-task pretraining with online PPO
finetuning to enable language-conditioned control and robust task transitions.
Our approach first pretrains the policy on a diverse multi-task dataset using
diffusion models, enabling language-guided execution of various skills. Then,
it finetunes the policy in simulation to ensure robustness and stable task
transition during real-world deployment. By utilizing Denoising Diffusion
Implicit Models (DDIM) for efficient sampling and TensorRT for optimized
deployment, our policy runs onboard at 50Hz, offering a scalable and efficient
solution for adaptive, language-guided locomotion on resource-constrained
robotic platforms.

</details>


### [275] [Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning](https://arxiv.org/abs/2507.05695)
*Xiatao Sun,Yuxuan Wang,Shuo Yang,Yinxing Chen,Daniel Rakita*

Main category: cs.RO

TL;DR: This paper introduces hPGA-DP, a robot learning diffusion policy framework that integrates Projective Geometric Algebra (PGA) to embed geometric inductive biases for improved training and performance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion policies in robot learning require relearning of fundamental spatial operations each time they are trained, leading to inefficiencies.

Method: The hPGA-DP framework incorporates Projective Geometric Algebra (PGA) into a novel diffusion policy architecture featuring the P-GATr model, which is E(3)-equivariant. A hybrid strategy uses P-GATr for encoding and decoding while employing U-Net or Transformer modules for denoising.

Result: Experiments and ablation studies demonstrate improved task performance, faster training convergence, and greater efficiency compared to models lacking geometric biases or relying solely on P-GATr.

Conclusion: Integrating PGA into diffusion policies offers significant performance gains by embedding geometric biases, showcasing the benefits of combining P-GATr with hybrid model architectures.

Abstract: Diffusion policies have become increasingly popular in robot learning due to
their reliable convergence in motion generation tasks. At a high level, these
policies learn to transform noisy action trajectories into effective ones,
conditioned on observations. However, each time such a model is trained in a
robotics context, the network must relearn fundamental spatial representations
and operations, such as translations and rotations, from scratch in order to
ground itself and operate effectively in a 3D environment. Incorporating
geometric inductive biases directly into the network can alleviate this
redundancy and substantially improve training efficiency. In this paper, we
introduce hPGA-DP, a diffusion policy approach that integrates a mathematical
framework called Projective Geometric Algebra (PGA) to embed strong geometric
inductive biases. PGA is particularly well-suited for this purpose as it
provides a unified algebraic framework that naturally encodes geometric
primitives, such as points, directions, and rotations, enabling neural networks
to reason about spatial structure through interpretable and composable
operations. Specifically, we propose a novel diffusion policy architecture that
incorporates the Projective Geometric Algebra Transformer (P-GATr), leveraging
its E(3)-equivariant properties established in prior work. Our approach adopts
a hybrid architecture strategy, using P-GATr as both a state encoder and action
decoder, while employing U-Net or Transformer-based modules for the denoising
process. Several experiments and ablation studies in both simulated and
real-world environments demonstrate that hPGA-DP not only improves task
performance and training efficiency through the geometric bias of P-GATr, but
also achieves substantially faster convergence through its hybrid model
compared to architectures that rely solely on P-GATr.

</details>


### [276] [DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving](https://arxiv.org/abs/2507.05710)
*Hyeongchan Ham,Heejin Ahn*

Main category: cs.RO

TL;DR: The paper presents a new motion planning framework for autonomous vehicles that uses evidential deep learning to handle perception uncertainties and ensure safety.


<details>
  <summary>Details</summary>
Motivation: The study addresses the safety risks in autonomous vehicles caused by the uncertainties in neural network-based perception, which can lead to unsafe control decisions.

Method: The authors propose a novel Distributionally Robust Optimization framework that incorporates evidential deep learning to dynamically adjust constraints based on perception uncertainties. This is integrated into a Model Predictive Control algorithm.

Result: Through validation in the CARLA simulator, the approach demonstrated efficiency under high perception confidence and conservative enforcement under low confidence, ensuring safety.

Conclusion: The proposed DRO-EDL-MPC framework offers a computationally feasible and effective solution for safe motion planning under varying perception confidence levels in autonomous vehicles.

Abstract: Safety is a critical concern in motion planning for autonomous vehicles.
Modern autonomous vehicles rely on neural network-based perception, but making
control decisions based on these inference results poses significant safety
risks due to inherent uncertainties. To address this challenge, we present a
distributionally robust optimization (DRO) framework that accounts for both
aleatoric and epistemic perception uncertainties using evidential deep learning
(EDL). Our approach introduces a novel ambiguity set formulation based on
evidential distributions that dynamically adjusts the conservativeness
according to perception confidence levels. We integrate this uncertainty-aware
constraint into model predictive control (MPC), proposing the DRO-EDL-MPC
algorithm with computational tractability for autonomous driving applications.
Validation in the CARLA simulator demonstrates that our approach maintains
efficiency under high perception confidence while enforcing conservative
constraints under low confidence.

</details>


### [277] [Simultaneous Triggering and Synchronization of Sensors and Onboard Computers](https://arxiv.org/abs/2507.05717)
*Morten Nissov,Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper introduces a versatile, low-cost system for precise sensor data timestamping critical for robotics applications, addressing timing challenges in real-time estimation.


<details>
  <summary>Details</summary>
Motivation: Inaccurate sensor data timestamping negatively affects high-fidelity estimation in robotics, especially in real-time scenarios where post-processing may not be feasible.

Method: The authors propose a robust, low-cost timestamping system using readily-available components, focusing on synchronization and sensor triggering.

Result: The system demonstrates effective capabilities for synchronizing and triggering both high- and low-rate sensors.

Conclusion: Real-time, accurate sensor timestamping is achievable using established methods and affordable components, showing potential to reduce timing issues in robotics applications.

Abstract: High fidelity estimation algorithms for robotics require accurate data.
However, timestamping of sensor data is a key issue that rarely receives the
attention it deserves. Inaccurate timestamping can be compensated for in
post-processing but is imperative for online estimation. Simultaneously, even
online mitigation of timing issues can be achieved through a relaxation of the
tuning parameters from their otherwise more performative optimal values, but at
a detriment to performance. To address the need for real-time, low-cost
timestamping, a versatile system which utilizes readily-available components
and established methods for synchronization is introduced. The synchronization
and triggering (of both high- and low-rate sensors) capabilities of the system
are demonstrated.

</details>


### [278] [A Learning-based Planning and Control Framework for Inertia Drift Vehicles](https://arxiv.org/abs/2507.05748)
*Bei Zhou,Zhouheng Li,Lei Xie,Hongye Su,Johannes Betz*

Main category: cs.RO

TL;DR: The paper presents a Bayesian optimization-based learning framework for smooth inertia drift transitions in autonomous racing.


<details>
  <summary>Details</summary>
Motivation: To improve path tracking and performance during inertia drift transitions in autonomous racing, addressing challenges like dynamic coupling and model inaccuracies.

Method: A Bayesian optimization framework for planning smooth transitions and minimizing velocity loss, while deriving a performance-driven control policy to mitigate model errors.

Result: Simulation on an 8-shape path shows smooth and stable inertia drift during sharp corners.

Conclusion: The proposed learning-based approach effectively manages inertia drift in autonomous racing through enhanced stability and control.

Abstract: Inertia drift is a transitional maneuver between two sustained drift stages
in opposite directions, which provides valuable insights for navigating
consecutive sharp corners for autonomous racing.However, this can be a
challenging scenario for the drift controller to handle rapid transitions
between opposing sideslip angles while maintaining accurate path tracking.
Moreover, accurate drift control depends on a high-fidelity vehicle model to
derive drift equilibrium points and predict vehicle states, but this is often
compromised by the strongly coupled longitudinal-lateral drift dynamics and
unpredictable environmental variations. To address these challenges, this paper
proposes a learning-based planning and control framework utilizing Bayesian
optimization (BO), which develops a planning logic to ensure a smooth
transition and minimal velocity loss between inertia and sustained drift
phases. BO is further employed to learn a performance-driven control policy
that mitigates modeling errors for enhanced system performance. Simulation
results on an 8-shape reference path demonstrate that the proposed framework
can achieve smooth and stable inertia drift through sharp corners.

</details>


### [279] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: The paper presents LeAD, an autonomous driving architecture combining end-to-end frameworks with language models to handle complex urban driving scenarios effectively.


<details>
  <summary>Details</summary>
Motivation: To address the inability of existing urban autonomous driving systems to handle complex scenarios and interpret intentions effectively, thus improving alignment with human reasoning.

Method: LeAD integrates two subsystems: a high-frequency E2E module for real-time operations, and a low-frequency LLM module for improved scenario analysis and decision-making using chain-of-thought reasoning.

Result: LeAD demonstrated superior performance in unconventional scenarios, achieving 71 points on CARLA's Leaderboard V1 benchmark with a 93% route completion rate.

Conclusion: The architecture provides a promising solution for improving urban autonomous driving through enhanced semantic understanding and decision-making, outperforming current benchmarks.

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [280] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: This paper proposes a communication-efficient federated learning framework to train grasp pose detection (GPD) models for robots, enhancing privacy and performance under constrained resources.


<details>
  <summary>Details</summary>
Motivation: GPD is essential for robotic autonomy but faces challenges like data privacy, centralization, and high communication costs in FL, particularly for resource-constrained robots.

Method: A module-wise FL framework that identifies slower-converging components of the GPD model and allocates focused communication effort to these components during a two-phase training process.

Result: Empirical results on the GraspNet-1B dataset and real-world robotic experiments show this method achieves higher accuracy and grasp success rates for a given communication budget compared to baseline methods.

Conclusion: The proposed approach effectively balances communication costs and model performance, offering a robust way to train decentralized GPD models while preserving privacy.

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [281] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: This paper evaluates path planning algorithms (A*, Dijkstra, RRT*, NIACO) for autonomous navigation in unstructured environments using road networks derived from satellite and LiDAR data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in autonomous vehicle navigation in unstructured environments like forests and mountains due to irregular terrains and complex road conditions.

Method: Tested algorithms on 2D and 3D weighted road networks from satellite and LiDAR datasets, using metrics like path cost, computation time, and memory consumption under identical conditions.

Result: Dijkstra algorithm showed the most stable and efficient performance for both 2D and 3D path planning on dense geospatial road-maps.

Conclusion: The study confirms Dijkstra's reliability for static terrain navigation and provides a basis for future research on dynamic path planning in complex terrains.

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [282] [FineGrasp: Towards Robust Grasping for Delicate Objects](https://arxiv.org/abs/2507.05978)
*Yun Du,Mengao Zhao,Tianwei Lin,Yiwei Jin,Chaodong Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: FineGrasp improves robotic grasping for small and delicate objects using network modifications, refined label strategies, and mixed sim-to-real training.


<details>
  <summary>Details</summary>
Motivation: Existing robotic grasping systems struggle to handle small and delicate objects, which hinders pipeline effectiveness.

Method: The paper introduces FineGrasp with network modifications, refined graspness label normalization, and mixed sim-to-real training.

Result: Experimental results demonstrate significant enhancements in grasping small objects and validating semantic grasping efficiency.

Conclusion: FineGrasp is an effective solution for improving precise robotic grasping capabilities in challenging scenarios.

Abstract: Recent advancements in robotic grasping have led to its integration as a core
module in many manipulation systems. For instance, language-driven semantic
segmentation enables the grasping of any designated object or object part.
However, existing methods often struggle to generate feasible grasp poses for
small objects or delicate components, potentially causing the entire pipeline
to fail. To address this issue, we propose a novel grasping method, FineGrasp,
which introduces improvements in three key aspects. First, we introduce
multiple network modifications to enhance the ability of to handle delicate
regions. Second, we address the issue of label imbalance and propose a refined
graspness label normalization strategy. Third, we introduce a new simulated
grasp dataset and show that mixed sim-to-real training further improves grasp
performance. Experimental results show significant improvements, especially in
grasping small objects, and confirm the effectiveness of our system in semantic
grasping.

</details>


### [283] [AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central Venous Catheterization](https://arxiv.org/abs/2507.05979)
*Deepak Raina,Lidia Al-Zogbi,Brian Teixeira,Vivek Singh,Ankur Kapoor,Thorsten Fleiter,Muyinatu A. Lediju Bell,Vinciya Pandian,Axel Krieger*

Main category: cs.RO

TL;DR: Researchers developed a robotic system for ultrasound-guided central venous catheterization (CVC), achieving high accuracy and first-attempt success in a controlled setting.


<details>
  <summary>Details</summary>
Motivation: Errors during CVC due to anatomical variations and operator dependency can lead to severe complications; thus, an autonomous robot-guided system is sought to improve accuracy and safety.

Method: The study introduced a robotic pipeline with three main modules: (1) identifying anatomical landmarks using deep learning, (2) scanning and reconstructing vessels via robot motion planning, and (3) guiding needle insertion under ultrasound with operator feedback.

Result: The system demonstrated 100% first-attempt success, reconstructed vessels with a mean error of 2.15 mm, and achieved needle insertion errors of under 1 mm in 10 simulated clinical scenarios using a high-fidelity commercial phantom.

Conclusion: This robotic pipeline represents a significant advancement in autonomous ultrasound-guided CVC and shows substantial promise for future clinical use.

Abstract: Purpose: Central venous catheterization (CVC) is a critical medical procedure
for vascular access, hemodynamic monitoring, and life-saving interventions. Its
success remains challenging due to the need for continuous ultrasound-guided
visualization of a target vessel and approaching needle, which is further
complicated by anatomical variability and operator dependency. Errors in needle
placement can lead to life-threatening complications. While robotic systems
offer a potential solution, achieving full autonomy remains challenging. In
this work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,
from scan initialization to needle insertion. Methods: We introduce a
deep-learning model to identify clinically relevant anatomical landmarks from a
depth image of the patient's neck, obtained using RGB-D camera, to autonomously
define the scanning region and paths. Then, a robot motion planning framework
is proposed to scan, segment, reconstruct, and localize vessels (veins and
arteries), followed by the identification of the optimal insertion zone.
Finally, a needle guidance module plans the insertion under ultrasound guidance
with operator's feedback. This pipeline was validated on a high-fidelity
commercial phantom across 10 simulated clinical scenarios. Results: The
proposed pipeline achieved 10 out of 10 successful needle placements on the
first attempt. Vessels were reconstructed with a mean error of 2.15
\textit{mm}, and autonomous needle insertion was performed with an error less
than or close to 1 \textit{mm}. Conclusion: To our knowledge, this is the first
robotic CVC system demonstrated on a high-fidelity phantom with integrated
planning, scanning, and insertion. Experimental results show its potential for
clinical translation.

</details>


### [284] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: The paper introduces a real-time algorithm for estimating speech workload to enhance human-machine system adaptability.


<details>
  <summary>Details</summary>
Motivation: To improve task performance in demanding environments by mitigating undesirable workload states through real-time workload estimation.

Method: Developed an algorithm to estimate speech workload in real-time and analyzed its generalizability and accuracy.

Result: The algorithm demonstrated effective accuracy and generalizability across individuals and human-machine teaming paradigms.

Conclusion: Real-time speech workload estimation is key for advancing adaptive human-machine systems.

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [285] [SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles](https://arxiv.org/abs/2507.06053)
*Jakub F. Kowalewski,Keeyon Hajjafar,Alyssa Ugent,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: This paper presents a soft robotic arm capable of scrubbing adhered residues with high effectiveness using neural network-enabled force and position control.


<details>
  <summary>Details</summary>
Motivation: To enable soft robots to perform physically demanding scrubbing tasks traditionally challenging for them, while ensuring safety and adaptability in human environments.

Method: The researchers trained a neural network for the robot arm to learn its inverse kinematics and elasticity, allowing open-loop force and position control for scrubbing tasks.

Result: The soft robotic arm successfully removed an average of 99.7% of contamination from different surfaces, such as burnt food residue from a plate and sticky fruit preserve from a toilet seat.

Conclusion: Soft robots can be adapted to exert continuous torque and pressure, making them suitable for safe and effective scrubbing, overcoming traditional limitations in handling contamination.

Abstract: Scrubbing surfaces is a physically demanding and time-intensive task.
Removing adhered contamination requires substantial friction generated through
pressure and torque or high lateral forces. Rigid robotic manipulators, while
capable of exerting these forces, are usually confined to structured
environments isolated from humans due to safety risks. In contrast, soft robot
arms can safely work around humans and adapt to environmental uncertainty, but
typically struggle to transmit the continuous torques or lateral forces
necessary for scrubbing. Here, we demonstrate a soft robotic arm scrubbing
adhered residues using torque and pressure, a task traditionally challenging
for soft robots. We train a neural network to learn the arm's inverse
kinematics and elasticity, which enables open-loop force and position control.
Using this learned model, the robot successfully scrubbed burnt food residue
from a plate and sticky fruit preserve from a toilet seat, removing an average
of 99.7% of contamination. This work demonstrates how soft robots, capable of
exerting continuous torque, can effectively and safely scrub challenging
contamination from surfaces.

</details>


### [286] [Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty](https://arxiv.org/abs/2507.06129)
*Abhish Khanal,Joseph Prince Mathew,Cameron Nowzari,Gregory J. Stein*

Main category: cs.RO

TL;DR: The paper introduces a multi-robot planning framework utilizing graph neural networks for time-critical search tasks under uncertainty, demonstrating efficiency improvements in simulated experiments and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and impracticality of deploying response teams or inspection robots to every location during disaster response or surveillance operations, by prioritizing areas needing immediate attention while optimizing travel time.

Method: The method includes using graph neural networks to estimate the likelihood of points of interest needing attention based on noisy sensor data. These predictions guide a multi-robot model-based planner to create cost-effective plans.

Result: Simulated experiments show performance improvements of 16.3%, 26.7%, and 26.2% for 1, 3, and 5 robots respectively compared to baseline methods. Real-world validation is achieved using quad-copters.

Conclusion: The framework enhances the efficiency and coordination of multi-robot systems in conducting time-critical searches under uncertainty, making it practical for response operations in real-world scenarios.

Abstract: In disaster response or surveillance operations, quickly identifying areas
needing urgent attention is critical, but deploying response teams to every
location is inefficient or often impossible. Effective performance in this
domain requires coordinating a multi-robot inspection team to prioritize
inspecting locations more likely to need immediate response, while also
minimizing travel time. This is particularly challenging because robots must
directly observe the locations to determine which ones require additional
attention. This work introduces a multi-robot planning framework for
coordinated time-critical multi-robot search under uncertainty. Our approach
uses a graph neural network to estimate the likelihood of PoIs needing
attention from noisy sensor data and then uses those predictions to guide a
multi-robot model-based planner to determine the cost-effective plan. Simulated
experiments demonstrate that our planner improves performance at least by
16.3\%, 26.7\%, and 26.2\% for 1, 3, and 5 robots, respectively, compared to
non-learned and learned baselines. We also validate our approach on real-world
platforms using quad-copters.

</details>


### [287] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: This paper introduces a fast and accurate algorithm to estimate collision probabilities between objects with uncertain trajectories.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of collision probabilities for objects with uncertain paths is crucial for safety in dynamic environments, particularly in autonomous systems.

Method: An adaptive sigma-point sampling technique is devised to efficiently calculate collision probabilities, accounting for temporal dependencies of poses described by Gaussian distributions.

Result: The algorithm achieves a median error of 3.5% and a median runtime of 0.21ms on an Intel Xeon Gold 6226R processor, and demonstrates robust performance across 400 real-world autonomous vehicle scenarios.

Conclusion: This approach provides an improved estimation of collision probabilities by avoiding temporal dependency neglect, potentially benefiting applications in autonomous systems.

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [288] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: The paper evaluates large language models for embodied robotic tasks within the Meta PARTNER benchmark, finding reasoning models more effective than non-reasoning models.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of large language models in solving embodied robotic tasks and assess their capabilities in collaborative robotic environments.

Method: Evaluation of multiple large language models on random kitchen scenes in the Meta PARTNER benchmark, focusing on various configurations including full and partial observability.

Result: Reasoning models like OpenAI o3-mini outperformed non-reasoning models such as GPT-4o and Llama 3 across all configurations.

Conclusion: Reasoning-based models show promise for advancements in embodied robotics, particularly in collaborative and variable environments.

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


### [289] [Learning Agile Tensile Perching for Aerial Robots from Demonstrations](https://arxiv.org/abs/2507.06172)
*Kangle Yuan,Atar Babgei,Luca Romanello,Hai-Nguyen Nguyen,Ronald Clark,Mirko Kovac,Sophie F. Armanini,Basaran Bahadir Kocer*

Main category: cs.RO

TL;DR: The paper introduces a reinforcement learning-based trajectory framework for aerial robots to enable secure tether-based perching on various structures.


<details>
  <summary>Details</summary>
Motivation: Extending aerial robot endurance by enabling energy-efficient perching on diverse structures, which presents challenges like tether slack/tension and precise momentum control.

Method: Developing a trajectory framework using reinforcement learning (Soft Actor-Critic from Demonstrations) to manage the complexities of tether dynamics and achieve precise wrapping and anchoring.

Result: The framework showed effective and reliable trajectory generation for tensile perching in both simulations and real-world experiments, with enhanced training efficiency.

Conclusion: This novel approach facilitates agile, secure perching for aerial robots, providing potential energy-saving and versatility benefits for various applications.

Abstract: Perching on structures such as trees, beams, and ledges is essential for
extending the endurance of aerial robots by enabling energy conservation in
standby or observation modes. A tethered tensile perching mechanism offers a
simple, adaptable solution that can be retrofitted to existing robots and
accommodates a variety of structure sizes and shapes. However, tethered tensile
perching introduces significant modelling challenges which require precise
management of aerial robot dynamics, including the cases of tether slack &
tension, and momentum transfer. Achieving smooth wrapping and secure anchoring
by targeting a specific tether segment adds further complexity. In this work,
we present a novel trajectory framework for tethered tensile perching,
utilizing reinforcement learning (RL) through the Soft Actor-Critic from
Demonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal
demonstrations, our approach enhances training efficiency and responsiveness,
achieving precise control over position and velocity. This framework enables
the aerial robot to accurately target specific tether segments, facilitating
reliable wrapping and secure anchoring. We validate our framework through
extensive simulation and real-world experiments, and demonstrate effectiveness
in achieving agile and reliable trajectory generation for tensile perching.

</details>


### [290] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: This paper proposes a method for fast teleoperation of low-cost manipulators without force sensors using 4-channel bilateral control, enabling improved performance in imitation learning.


<details>
  <summary>Details</summary>
Motivation: To enable efficient teleoperation and high-quality data collection using low-cost manipulators, overcoming limitations in existing unilateral control systems, especially for fast or contact-rich operations.

Method: Utilization of 4-channel bilateral control incorporating manipulator dynamics, nonlinear terms compensation, velocity and force estimation, and adjustable gains. Data from this system is further used to enhance learned policies in imitation learning.

Result: Demonstrated feasibility of fast teleoperation with force feedback and improved imitation learning performance by including force information in both input and output policies.

Conclusion: The proposed system enables high-fidelity teleoperation and effective data collection on affordable manipulators, making it practical for imitation learning applications.

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [291] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: The paper examines the role of data diversity in robotic manipulation, challenging assumptions that more diversity is always better and providing insights for effective data scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand how data scaling principles, successful in NLP and CV, can be effectively applied to robotic manipulation tasks.

Method: The paper explores data diversity across three dimensions—task, embodiment, and expert—through extensive experiments on various robot platforms, proposing a debiasing method to address velocity ambiguity.

Result: Key findings include the significance of task diversity over demonstration quantity, limited necessity of multi-embodiment data, and confounds caused by expert diversity. The proposed method yielded a performance boost of 15%.

Conclusion: The study offers actionable insights for scaling robotic manipulation datasets, emphasizing task diversity and addressing challenges in expert-induced data bias.

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [292] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: EC-Flow is a language-guided robotic manipulation framework that predicts embodiment-centric flow from action-unlabeled videos to handle versatile scenarios like deformable objects and occlusions.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation systems often rely on action-labeled datasets, which limit generalization to complex scenarios such as handling deformable objects or occlusions. Current flow prediction approaches are object-centric and inadequate in addressing such versatile manipulation requirements.

Method: EC-Flow introduces embodiment-centric flow prediction by combining an embodiment's kinematics with a goal-alignment module that optimizes movement consistency and goal-image prediction. Robot execution uses a standard URDF file for ease of practical use.

Result: EC-Flow outperformed state-of-the-art object-centric flow methods on simulation and real-world tasks, achieving improvements of 62% in occluded object handling, 45% in deformable object manipulation, and 80% in non-object-displacement tasks.

Conclusion: EC-Flow offers a significant advancement in robotic manipulation by enabling generalized skill transfer from action-unlabeled videos to complex real-world scenarios, improving efficiency and versatility in robotic systems.

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [293] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper introduces CoRe, a benchmark for evaluating the semantic reasoning capabilities of large language models (LLMs) in software engineering tasks, revealing challenges in deeper reasoning despite good dependency identification performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to thoroughly assess the semantic reasoning capabilities of LLMs in software engineering tasks such as code dependency analysis.

Method: CoRe benchmark is introduced, consisting of 12,553 human-verified task instances and using a semantics-aware sampling strategy to ensure diversity and complexity.

Result: Evaluation of 10 mainstream LLMs shows effective dependency identification but struggles in tasks requiring deep semantic reasoning and multi-step analysis.

Conclusion: LLMs exhibit limitations in code reasoning tasks involving complex semantics, suggesting areas for improvement in understanding control flow and dependencies.

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [294] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: The paper reviews challenges in open-source software licensing, analyzing existing research and providing future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of open-source licenses and generative software techniques demand better tools for managing license-related risks.

Method: Conducted a systematic literature review (SLR) on 80 papers, classifying research into license identification, risk assessment, and risk mitigation.

Result: Highlighted existing challenges, proposed future research directions, and offered practical guidance to address OSS license risks.

Conclusion: The paper aims to bridge academia and industry gaps, promoting better governance of software risks in the ecosystem.

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [295] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: This paper integrates Large Language Models (LLMs) with fuzz testing to generate weakest preconditions (WPs) for program verification, using feedback from test executions to refine LLM output.


<details>
  <summary>Details</summary>
Motivation: Generating weakest preconditions (WPs) is crucial for verifying program correctness and detecting runtime errors. However, traditional methods face challenges in automation and accuracy.

Method: The paper introduces Fuzzing Guidance (FG), which leverages feedback from fuzz testing to refine LLM-generated WPs. FG assesses the validity of candidate WPs and sends execution feedback back to LLMs for contextual improvement.

Result: The proposed approach was tested on deterministic array programs in Java. Results show that LLMs can produce valid WPs, and their capability is significantly enhanced by incorporating FG.

Conclusion: By combining LLMs with fuzz testing and execution-driven context refinement, this method provides an effective way to generate weakest preconditions, contributing to program verification research.

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [296] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: The paper presents a model to enhance code development and answers in Reservoir Computing using LLMs, emphasizing reduced hallucinations via RAG and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: To improve the ability of LLMs in assisting with Python coding tasks and resolving complex questions in Reservoir Computing.

Method: The tool integrates Retrieval-Augmented Generation (RAG) and knowledge graphs to enrich LLM responses, while focusing on reducing hallucinations and boosting accuracy.

Result: The model surpassed its base model (Codestral-22B) in coding tasks and performed competitively compared to proprietary models (e.g., ChatGPT-4o) in ReservoirPy-specific knowledge.

Conclusion: The system offers an interactive, specialized platform that enhances coding proficiency and delivers reliable domain-specific insights better than existing models on niche tasks.

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [297] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: The paper introduces CorePipe, an automated pipeline, and CoreCodeBench, a repository-level benchmark, to evaluate Large Language Models (LLMs) in real engineering scenarios across diverse tasks. Experiments with 16 LLMs reveal multi-dimensional insights into their performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the inadequacies of current benchmarks for evaluating LLMs on real-world engineering-level code tasks, which often lack diversity, complexity, and precision.

Method: The authors developed CorePipe, an automated system that converts software repositories into test cases, and designed CoreCodeBench, a benchmark that includes atomic and composite questions of varying difficulty for evaluating LLMs in engineering scenarios.

Result: Experiments with 16 LLMs across different scenarios using CoreCodeBench showed varying capabilities of the models, providing insights into their performance and applicability in engineering contexts.

Conclusion: CorePipe and CoreCodeBench are effective tools for assessing the applicability of LLMs in real engineering workflows, addressing previous benchmarking limitations and highlighting areas for LLMs' improvement in software development tasks.

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [298] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: The paper addresses Large Language Model robustness testing using search-based approaches to optimize Metamorphic Relations for effective failure detection.


<details>
  <summary>Details</summary>
Motivation: Existing robustness assessments of LLMs face limitations in scalable MR selection and expandability of test spaces beyond single perturbation MRs.

Method: Developed a search-based method with four algorithms (Single-GA, NSGA-II, SPEA2, MOEA/D) to optimize MR groups and cover combinatorial perturbations.

Result: MOEA/D algorithm outperformed others in maximizing failure detection; identified dominant MRs capable of confusing LLMs.

Conclusion: The research advances LLM robustness testing by optimizing MR selection, expanding MR test spaces, and providing insights into effective testing strategies through search-based approaches.

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [299] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: The paper investigates using Large Language Models (LLMs) to evaluate code readability, performing experiments with nine LLMs across interventions like removing comments, altering identifiers, and refactoring. Results show LLMs are sensitive to changes and align well with reference models for certain scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing code readability, which is influenced by various factors but lacks reliable and consistent measurement methods in both industry and academia.

Method: A quasi-experimental study tested nine LLMs by applying three interventions (removing comments, altering identifier names, and refactoring code) and compared their evaluations with a reference tool. Data was collected on response consistency and variations.

Result: LLMs exhibited sensitivity to code changes, with strong agreement with reference models on original and refactored code scenarios. They showed semantic sensitivity that the reference tool lacked and revealed response variability in 9.37% to 14.58% of executions.

Conclusion: LLMs are promising for evaluating code readability and semantic quality, offering a standardized and consistent method to assess attributes like coherence between identifier names, comments, and code purpose.

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [300] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: The paper introduces zkSDK, a modular framework that simplifies Zero-Knowledge (ZK) application development by abstracting backend complexities and automating ZK-proving backend selection using a Python-like language called Presto.


<details>
  <summary>Details</summary>
Motivation: Due to the fragmented developer experience and steep learning curve in selecting the appropriate ZK backend, developers often remain tied to a single ZK backend, limiting flexibility and efficiency.

Method: zkSDK incorporates Presto, a Python-like language, to profile and analyze program workloads. It employs a dynamic selection algorithm to automatically choose the optimal ZK-proving backend based on user-defined criteria.

Result: The framework was evaluated using real-world workloads, demonstrating its ability to effectively identify the best-suited ZK backend, thus enhancing development processes.

Conclusion: zkSDK streamlines ZK application development, offering developers a modular and user-friendly tool that removes backend complexities and optimizes backend selection, promoting flexibility and efficiency in the ZK space.

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [301] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE is a testing framework for AI-powered browser extensions, addressing their unique challenges and improving testing efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional testing methods fail to account for the complexities of LLM-powered browser extensions, creating gaps in reliability assurance.

Method: ASSURE incorporates test case generation, automated execution, and a validation pipeline to evaluate AI-powered extensions systematically.

Result: ASSURE identified 531 issues across six extensions, showing 6.4x improved testing throughput and detecting vulnerabilities quickly.

Conclusion: ASSURE is a practical, efficient solution for testing AI browser extensions, suitable for integration into development workflows.

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [302] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder transforms unstructured API documentation into machine-readable OpenAPI specs using LLMs and rule-based algorithms, saving time and effort.


<details>
  <summary>Details</summary>
Motivation: Manual conversion of unstructured online API documentation into structured formats for machine use is time-consuming and tedious.

Method: The framework integrates large language models (LLMs) and rule-based algorithms, leveraging domain-specific knowledge to convert long API documentation into OpenAPI specifications.

Result: Experiments show OASBuilder performs well across diverse APIs, generating valid and comprehensive OpenAPI specs, implemented enterprise-wide to save substantial manual effort.

Conclusion: OASBuilder simplifies API accessibility for AI by automating the creation of structured specifications, enhancing efficiency and usability in business and technical contexts.

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [303] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: The paper investigates the role of empathy in Software Engineering (SE) by defining it, identifying barriers, suggesting practices, and exploring outcomes through web article analysis and expert feedback.


<details>
  <summary>Details</summary>
Motivation: Empathy, crucial for effective communication and collaboration, is under-researched in Software Engineering, leaving SE practitioners without structured insights.

Method: Qualitative content analysis of 55 web articles and a follow-up survey with empathy experts were employed to derive findings.

Result: The paper identifies barriers like toxic culture and excessive technical focus, suggests practices to enhance team empathy, and proposes a framework highlighting positive outcomes like better collaboration and reduced stress.

Conclusion: The study's framework is deemed impactful and has potential for use in training to improve SE team dynamics. Future research will address empathy's broader implications in SE.

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [304] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: The paper introduces SLEEC-LLM, a tool leveraging large language models to interpret inconsistencies in normative requirements, making them accessible to non-technical stakeholders.


<details>
  <summary>Details</summary>
Motivation: There is a need to make normative requirements like social, legal, ethical, and cultural norms more accessible and manageable by non-technical stakeholders, due to the complexity and error-prone nature of existing methods.

Method: The authors developed SLEEC-LLM, a tool that employs large language models to translate formal verification results into natural language, aiding in understanding and validation of requirements.

Result: The application of SLEEC-LLM in two real-world case studies demonstrated improved efficiency and explainability in eliciting and analyzing normative requirements.

Conclusion: SLEEC-LLM empowers non-technical stakeholders by simplifying consistency analysis of normative rules, enhancing collaboration, and streamlining the requirement elicitation process.

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [305] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: The paper proposes TigAug, a system for automatically augmenting labeled traffic light images to test and improve traffic light detection models in autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: There is a lack of automated testing solutions for traffic light detection models in autonomous driving systems, and manual data collection is labor-intensive and often insufficient for diverse environments.

Method: The authors create TigAug, which utilizes metamorphic relations and image transformations based on weather, camera, and traffic light properties to generate synthetic images. These are then used to detect errors and retrain traffic light detection models.

Result: Experimental evaluation with four state-of-the-art traffic light detection models and two datasets shows that TigAug effectively tests, improves model performance, and synthesizes images efficiently with acceptable levels of naturalness.

Conclusion: TigAug is a practical and efficient tool for enhancing the testing and retraining of traffic light detection models, addressing gaps in reliability for autonomous driving systems.

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [306] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: The paper explores enhancing large language model (LLM) performance in Requirements Engineering (RE) tasks through Multi-Agent Debate (MAD) strategies.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of treating LLMs as black boxes and proposes leveraging collaborative approaches like MAD to enhance robustness and adaptability.

Method: The authors conducted a systematic analysis of MAD strategies across domains, created a taxonomy of their attributes, and tested a preliminary MAD framework for RE classification.

Result: The study categorized MAD strategies into a taxonomy and demonstrated that MAD is feasible for improving RE classification accuracy.

Conclusion: MAD strategies show promise for enhancing LLM accuracy in RE tasks, providing a basis for future research and development in this domain.

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [307] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune is a novel tool for tuning software configurations using causally purified rules, outperforming existing methods significantly and offering better explainability.


<details>
  <summary>Details</summary>
Motivation: Modern software systems require efficient configuration tuning to ensure performance, but existing methods struggle with balancing exploration and exploitation due to uncertainties in identifying promising regions of the configuration landscape.

Method: PromiseTune learns rules representing promising regions of the configuration landscape and refines them using causal inference, emphasizing these regions during tuning to optimize performance while improving explainability.

Result: PromiseTune outperforms 11 state-of-the-art tuners across 12 systems, achieving a 42% higher rank compared to the second-best method and providing additional insights into system characteristics.

Conclusion: PromiseTune mitigates the exploration-exploitation trade-off effectively while offering spatial explainability of configurations, making it a superior solution compared to existing methods.

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [308] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: The paper explores the existing gap between ethical requirements and current practices in AI model documentation, proposing a taxonomy to improve model cards.


<details>
  <summary>Details</summary>
Motivation: To address inadequate AI model documentation practices and ensure compliance with ethical requirements.

Method: Thematic analysis of 26 ethics guidelines, three AI documentation frameworks, three quantitative studies, and ten real model cards.

Result: Identified 43 ethical requirements, organized them into a taxonomy of four themes and twelve sub-themes.

Conclusion: Current documentation overly emphasizes model capabilities and reliability, neglecting aspects like fairness and explainability. A revised framework for model cards is needed.

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [309] [Cross-Subject DD: A Cross-Subject Brain-Computer Interface Algorithm](https://arxiv.org/abs/2507.05268)
*Xiaoyuan Li,Xinru Xue,Bohan Zhang,Ye Sun,Shoushuo Xi,Gang Liu*

Main category: q-bio.NC

TL;DR: The paper presents a universal Brain-Computer Interface (BCI) model for better cross-subject compatibility in motor imagery decoding.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of poor adaptability and generalizability in existing BCI models caused by inter-individual variability in brain activity.

Method: The proposed Cross-Subject DD (CSDD) algorithm involves training personalized models, extracting relation spectrums, identifying common features using statistical analysis, and constructing a cross-subject universal model based on these features.

Result: Using the BCIC IV 2a dataset, the proposed method shows a 3.28% improvement in decoding performance compared to existing methods.

Conclusion: This research introduces a novel method for extracting universal common features, enabling broader and more effective applications of BCI technology across subjects.

Abstract: Brain-computer interface (BCI) based on motor imagery (MI) enables direct
control of external devices by decoding the electroencephalogram (EEG)
generated in the brain during imagined movements. However, due to
inter-individual variability in brain activity, existing BCI models exhibit
poor adaptability across subjects, thereby limiting their generalizability and
widespread application. To address this issue, this paper proposes a
cross-subject BCI algorithm named Cross-Subject DD (CSDD), which constructs a
universal BCI model by extracting common features across subjects. The specific
methods include: 1) training personalized models for each subject; 2)
transforming personalized models into relation spectrums; 3) identifying common
features through statistical analysis; and 4) constructing a cross-subject
universal model based on common features. The experiments utilized the BCIC IV
2a dataset, involving nine subjects. Eight of these subjects were selected for
training and extracing the common features, and the cross-subject decoding
performance of the model was validated on the remaining subject. The results
demonstrate that, compared with existing similar methods, our approach achieves
a 3.28% improvement in performance. This paper introduces for the first time a
novel method for extracting pure common features and constructing a universal
cross-subject BCI model, thereby facilitating broader applications of BCI
technology.

</details>


### [310] [Hierarchy or Heterarchy? A Theory of Long-Range Connections for the Sensorimotor Brain](https://arxiv.org/abs/2507.05888)
*Jeff Hawkins,Niels Leadholm,Viviane Clay*

Main category: q-bio.NC

TL;DR: This paper introduces the "Thousand Brains Theory," suggesting that each cortical column functions as an independent sensorimotor learning system, challenging the hierarchical model of neocortical organization.


<details>
  <summary>Details</summary>
Motivation: Traditional hierarchical models of neocortical processing struggle to account for anatomical and functional observations that suggest parallel processing and more flexible interactions.

Method: The authors propose a heterarchical model where cortical columns autonomously learn via sensory integration over multiple movements, supported by an analysis of long-range neocortical and thalamic connections.

Result: The study concludes that hierarchical connections help learn compositional structures of objects, while the thalamus facilitates pose transformations between sensors and objects.

Conclusion: The "Thousand Brains Theory" provides a novel framework with implications for understanding neocortical processing and advancements in artificial intelligence.

Abstract: In the traditional understanding of the neocortex, sensory information flows
up a hierarchy of regions, with each level processing increasingly complex
features. Information also flows down the hierarchy via a different set of
connections. Although the hierarchical model has significant support, many
anatomical connections do not conform to the standard hierarchical
interpretation. In addition, hierarchically arranged regions sometimes respond
in parallel, not sequentially as would occur in a hierarchy. This and other
evidence suggests that two regions can act in parallel and hierarchically at
the same time. Given this flexibility, the word "heterarchy" might be a more
suitable term to describe neocortical organization. This paper proposes a new
interpretation of how sensory and motor information is processed in the
neocortex. The key to our proposal is what we call the "Thousand Brains
Theory", which posits that every cortical column is a sensorimotor learning
system. Columns learn by integrating sensory input over multiple movements of a
sensor. In this view, even primary and secondary regions, such as V1 and V2,
can learn and recognize complete 3D objects. This suggests that the
hierarchical connections between regions are used to learn the compositional
structure of parent objects composed of smaller child objects. We explain the
theory by examining the different types of long-range connections between
cortical regions and between the neocortex and thalamus. We describe these
connections, and then suggest the specific roles they play in the context of a
heterarchy of sensorimotor regions. We also suggest that the thalamus plays an
essential role in transforming the pose between objects and sensors. The novel
perspective we argue for here has broad implications for both neuroscience and
artificial intelligence.

</details>


### [311] [Spin-Based Modeling of Perception as Emergent from contextualized Internal Evaluation](https://arxiv.org/abs/2507.06041)
*Laura Fanfarillo,Gustavo Diez,Victor Gómez Mayordomo,Miguel Bosch,J. Ricardo Arias-Gonzalez,Belén Valenzuela*

Main category: q-bio.NC

TL;DR: The study presents a spin-based model to explain interoceptive perception, incorporating neutral evaluations that reshape the perceptual landscape.


<details>
  <summary>Details</summary>
Motivation: To clarify the link between microscopic evaluations of sensations and macroscopic states of perception using a formalized, physics-inspired framework.

Method: A lattice-based spin model incorporates positive, negative, and neutral states, later coarse-grained into a Landau-type functional to analyze phenomenological outcomes.

Result: Neutral evaluative states significantly increase perceptual entropy, reduce sensitivity thresholds, and enhance responsiveness to contexts.

Conclusion: This model bridges neuroscience, cognitive science, and theoretical physics, providing a tool for studying perception and related disorders like chronic pain.

Abstract: We develop a microscopic model of perception of an interoceptive sensation in
which spin-like variables encode an organism's internal evaluation of embodied
vital norms associated with the sensation. Spins can take positive, negative,
or neutral values. These local valorizations interact on a lattice embedded in
the environmental context, and their collective configuration gives rise to a
macroscopic perceptual state. By applying a coarse-graining procedure to a
family of symmetric spin models, we derive a macroscopic Landau-type functional
that makes explicit the mechanism by which key phenomenological features of
perception, emerge from microscopic evaluative interactions. A central result
is that the inclusion of a neutral evaluative state fundamentally alters the
structure of the perceptual landscape, enhancing entropy, lowering the critical
threshold, and increasing sensitivity to contextual input. These results
establish a principled link between microscopic evaluative processes and
large-scale perceptual organization, offering a flexible framework for
integrating perceptual regulation and neurobiological modeling. The model
integrates notions of neuroscience and cognitive science using the formalism of
condensed matter field theory and providing novel theoretical insights and
experimental predictions for conditions such as mental disorders and chronic
pain.

</details>


### [312] [Miniaturized optically-generated Bessel beam ultrasound for volumetric transcranial brain stimulation](https://arxiv.org/abs/2507.06108)
*Yueming Li,Guo Chen,Tiago R. Oliveira,Nick Todd,Yong-Zhi Zhang,Carolyn Marar,Nan Zheng,Lu Lan,Nathan McDannold,Ji-Xin Cheng,Chen Yang*

Main category: q-bio.NC

TL;DR: The paper introduces a new miniaturized device, OBUS, that uses Bessel beam ultrasound for precise and effective non-invasive stimulation of small brain sub-regions.


<details>
  <summary>Details</summary>
Motivation: Understanding brain functions requires non-invasive and precise targeting of small and variably shaped brain sub-regions. Current ultrasound neuromodulation technologies face limitations such as trade-offs between device miniaturization, spatial resolution, volumetric control, and transcranial capability.

Method: The researchers developed the Optically-Generated Bessel Beam Ultrasound (OBUS) device with a 2.33 mm diameter. The device generates a column-shaped ultrasound field with high spatial resolution (152 μm laterally; 1.93 mm axially) and tests its capabilities through immunofluorescence imaging, electrophysiological recordings, and functional MRI.

Result: OBUS successfully stimulated mouse brain cells at a depth of 2.2 mm and demonstrated superior transcranial transmission efficiency and beam shape preservation compared to conventional Gaussian ultrasound. The stimulation evoked measurable neural circuit activity in rodents.

Conclusion: OBUS enables precise, non-invasive brain stimulation with significant spatial control, offering a promising tool for advancing brain function studies.

Abstract: Non-invasive stimulation of small, variably shaped brain sub-regions is
crucial for advancing our understanding of brain functions. Current ultrasound
neuromodulation faces two significant trade-offs when targeting brain
sub-regions: miniaturization versus volumetric control and spatial resolution
versus transcranial capability. Here, we present an optically-generated Bessel
beam ultrasound (OBUS) device designed to overcome these limitations. This 2.33
mm-diameter miniaturized device delivers a column-shaped field achieving a
lateral resolution of 152 um and an axial resolution of 1.93 mm, targeting
brain sub-regions with an elongated volume of tissue activation.
Immunofluorescence imaging of mouse brain slices confirms its ability to
stimulate cells at a depth of 2.2 mm. Additionally, OBUS outperforms
conventional Gaussian ultrasound in transcranial transmission efficiency and
beam shape preservation. Electrophysiological recordings and functional MRI
captured rodent brain responses evoked by OBUS, demonstrating OBUS's ability to
non-invasively activate neural circuits in intact brains. This technology
offers new possibilities for studying brain functions with precision and
volumetric control.

</details>


### [313] [A Linear Generative Framework for Structure-Function Coupling in the Human Brain](https://arxiv.org/abs/2507.06136)
*Sam Frank Kelemen,Joaquín Gõni,Sérgio Pequito,Arian Ashourvan*

Main category: q-bio.NC

TL;DR: This study uses a generative linear model to link brain structural connectivity (SC) derived from diffusion-weighted imaging (DWI) with functional connectivity (FC) observed in resting-state fMRI, providing insights into how brain structural architecture drives functional dynamics.


<details>
  <summary>Details</summary>
Motivation: The research motivation is to systematically untangle the complex relationship between structural connectivity and functional connectivity in the brain, enabling predictions about how disruptions to SC impact FC and potentially lead to cognitive and behavioral impairments.

Method: The researchers employ a generative linear model to derive explicit rules connecting DWI-derived SC with resting-state fMRI FC. Validation is performed using topological null models, and virtual lesion experiments elucidate the contributions of cortical and subcortical systems.

Result: The study identifies classifications of brain regions into integrator hubs and mediator hubs, highlighting their distinct roles in synchronizing and orchestrating brain dynamics. Distinct contributions of brain systems to global functional organization are demonstrated.

Conclusion: This framework deciphers how structural connectivity influences functional dynamics, setting the groundwork for predicting the effects of pathological or surgical disruptions on brain networks and their impact on cognitive and behavioral functions.

Abstract: Brain function emerges from coordinated activity across anatomically
connected regions, where structural connectivity (SC) -- the network of white
matter pathways - provides the physical substrate for functional connectivity
(FC) -- the correlated neural activity between brain areas. While these
structural and functional networks exhibit substantial overlap, their
relationship involves complex, indirect mechanisms, including the dynamic
interplay of direct and indirect pathways, recurrent network interactions, and
neuromodulatory influences. To systematically untangle how structural
architecture shapes functional patterns, this work aims to establish a set of
rules that decode how direct and indirect structural connections and motifs
give rise to FC between brain regions. Specifically, using a generative linear
model, we derive explicit rules that predict an individual's resting-state fMRI
FC from diffusion-weighted imaging (DWI)-derived SC, validated against
topological null models. Examining the rules reveals distinct classes of brain
regions, with integrator hubs acting as structural linchpins promoting
synchronization and mediator hubs serving as structural fulcrums orchestrating
competing dynamics. Through virtual lesion experiments, we demonstrate how
different cortical and subcortical systems distinctively contribute to global
functional organization. Together, this framework disentangles the mechanisms
by which structural architecture drives functional dynamics, enabling the
prediction of how pathological or surgical disruptions to brain connectivity
cascade through functional networks, potentially leading to cognitive and
behavioral impairments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [314] [Enjoying Non-linearity in Multinomial Logistic Bandits](https://arxiv.org/abs/2507.05306)
*Pierre Boudart,Pierre Gaillard,Alessandro Rudi*

Main category: stat.ML

TL;DR: The paper addresses the multinomial logistic bandit problem, extending prior work on binary logistic bandits to a more complex setting with multiple choices. It introduces problem-dependent constants to reduce regret bounds and proposes an efficient algorithm with improved guarantees.


<details>
  <summary>Details</summary>
Motivation: To generalize the study of multinomial logistic bandits and understand the impact of non-linearity in such settings, making the framework suitable for real-world applications like reinforcement learning and recommender systems.

Method: The paper extends the definition of a problem-dependent constant, $\\kappa_*$, to multinomial settings. Using this, it proposes an algorithm that effectively accounts for non-linearity, achieving tighter regret bounds.

Result: The proposed approach results in a problem-dependent regret bound of $\\mathcal{O}( Kd \\\sqrt{{T}/{\\kappa_*}})$, improving on previous bounds of $\\mathcal{O}( Kd \\\sqrt{T})$ by leveraging the non-linearity captured in $\\kappa_*$.

Conclusion: The algorithm achieves optimal dependence on the problem-specific constant, improving regret bounds for multinomial logistic bandits and confirming the utility of their approach.

Abstract: We consider the multinomial logistic bandit problem, a variant of generalized
linear bandits where a learner interacts with an environment by selecting
actions to maximize expected rewards based on probabilistic feedback from
multiple possible outcomes. In the binary setting, recent work has focused on
understanding the impact of the non-linearity of the logistic model (Faury et
al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant
$\kappa_*$, that may be exponentially large in some problem parameters and
which is captured by the derivative of the sigmoid function. It encapsulates
the non-linearity and improves existing regret guarantees over $T$ rounds from
$\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the
dimension of the parameter space. We extend their analysis to the multinomial
logistic bandit framework, making it suitable for complex applications with
more than two choices, such as reinforcement learning or recommender systems.
To achieve this, we extend the definition of $\kappa_*$ to the multinomial
setting and propose an efficient algorithm that leverages the problem's
non-linearity. Our method yields a problem-dependent regret bound of order $
\smash{\widetilde{\mathcal{O}}( Kd \sqrt{{T}/{\kappa_*}})} $, where $K$ is the
number of actions and $\kappa_* \ge 1$. This improves upon the best existing
guarantees of order $ \smash{\widetilde{\mathcal{O}}( Kd \sqrt{T} )} $.
Moreover, we provide a $\smash{ \Omega(d\sqrt{T/\kappa_*})}$ lower-bound,
showing that our dependence on $\kappa_*$ is optimal.

</details>


### [315] [Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine Learning Framework for Adaptive Risk Forecasting](https://arxiv.org/abs/2507.05470)
*Agnideep Aich,Ashit Baran Aich,Dipak C. Jain*

Main category: stat.ML

TL;DR: The paper presents Temporal Conformal Prediction (TCP), a hybrid framework for constructing accurate and adaptive prediction intervals for financial time-series.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of non-stationarity, volatility clustering, and regime shifts in financial time-series without relying on parametric models.

Method: TCP combines quantile regression with an online adaptive conformal calibration layer using a decaying learning rate.

Result: TCP outperforms established methods like GARCH and Historical Simulation by delivering sharper prediction intervals with superior or competitive coverage in various asset classes, especially in high-volatility scenarios.

Conclusion: TCP offers a flexible and distribution-free approach to financial uncertainty quantification, advancing risk forecasting by balancing coverage and sharpness effectively.

Abstract: We propose Temporal Conformal Prediction (TCP), a novel framework for
constructing prediction intervals in financial time-series with guaranteed
finite-sample validity. TCP integrates quantile regression with a conformal
calibration layer that adapts online via a decaying learning rate. This hybrid
design bridges statistical and machine learning paradigms, enabling TCP to
accommodate non-stationarity, volatility clustering, and regime shifts which
are hallmarks of real-world asset returns, without relying on rigid parametric
assumptions. We benchmark TCP against established methods including GARCH,
Historical Simulation, and static Quantile Regression across equities (S&P
500), cryptocurrency (Bitcoin), and commodities (Gold). Empirical results show
that TCP consistently delivers sharper intervals with competitive or superior
coverage, particularly in high-volatility regimes. Our study underscores TCP's
strength in navigating the coverage-sharpness tradeoff, a central challenge in
modern risk forecasting. Overall, TCP offers a distribution-free, adaptive, and
interpretable alternative for financial uncertainty quantification, advancing
the interface between statistical inference and machine learning in finance.

</details>


### [316] [A Malliavin calculus approach to score functions in diffusion generative models](https://arxiv.org/abs/2507.05550)
*Ehsan Mirafzali,Frank Proske,Utkarsh Gupta,Daniele Venturi,Razvan Marinescu*

Main category: stat.ML

TL;DR: The paper derives an exact closed-form expression for the score function in nonlinear diffusion generative models, enhancing their applicability and advancing score estimation methods.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding and practical applicability of score functions in score-based diffusion generative models, which are crucial for modeling complex data distributions.

Method: The authors utilize advanced stochastic analysis tools like Malliavin derivatives, their adjoint operators (Skorokhod integrals), and a new Bismut-type formula to derive a closed-form expression for the score function.

Result: A theoretically sound, closed-form expression for the score function is presented, systematically eliminating Malliavin derivatives and enhancing practical utility.

Conclusion: The proposed method advances the theoretical foundation of score estimation in generative modeling, enabling new sampling algorithms and extending to broader classes of stochastic differential equations.

Abstract: Score-based diffusion generative models have recently emerged as a powerful
tool for modelling complex data distributions. These models aim at learning the
score function, which defines a map from a known probability distribution to
the target data distribution via deterministic or stochastic differential
equations (SDEs). The score function is typically estimated from data using a
variety of approximation techniques, such as denoising or sliced score
matching, Hyv\"arien's method, or Schr\"odinger bridges. In this paper, we
derive an exact, closed form, expression for the score function for a broad
class of nonlinear diffusion generative models. Our approach combines modern
stochastic analysis tools such as Malliavin derivatives and their adjoint
operators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type
formula. The resulting expression for the score function can be written
entirely in terms of the first and second variation processes, with all
Malliavin derivatives systematically eliminated, thereby enhancing its
practical applicability. The theoretical framework presented in this work
offers a principled foundation for advancing score estimation methods in
generative modelling, enabling the design of new sampling algorithms for
complex probability distributions. Our results can be extended to broader
classes of stochastic differential equations, opening new directions for the
development of score-based diffusion generative models.

</details>


### [317] [Property Elicitation on Imprecise Probabilities](https://arxiv.org/abs/2507.05857)
*James Bailie,Rabanus Derr*

Main category: stat.ML

TL;DR: This paper investigates property elicitation in imprecise probabilities (IP), offering conditions and clarifications for eliciting IP-properties.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by extending the classical machine learning paradigm to multi-distribution learning using IPs and $\u0393$-maximin risk minimization.

Method: The authors derive necessary conditions for eliciting IP-properties while leveraging Bayes pairs to clarify the elicitation process.

Result: They establish conditions and show that elicitable IP-properties are tied to the standard property of the maximum Bayes risk distribution.

Conclusion: Property elicitation can be extended to imprecise probabilities by contextualizing through Bayes pairs and necessary conditions for effective elicitation.

Abstract: Property elicitation studies which attributes of a probability distribution
can be determined by minimising a risk. We investigate a generalisation of
property elicitation to imprecise probabilities (IP). This investigation is
motivated by multi-distribution learning, which takes the classical machine
learning paradigm of minimising a single risk over a (precise) probability and
replaces it with $\Gamma$-maximin risk minimization over an IP. We provide
necessary conditions for elicitability of a IP-property. Furthermore, we
explain what an elicitable IP-property actually elicits through Bayes pairs --
the elicited IP-property is the corresponding standard property of the maximum
Bayes risk distribution.

</details>


### [318] [Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis](https://arxiv.org/abs/2507.05913)
*Gholamali Aminian,Idan Shenfeld,Amir R. Asadi,Ahmad Beirami,Youssef Mroueh*

Main category: stat.ML

TL;DR: This paper introduces Smooth Best-of-N (SBoN), a method to address the limitations of traditional Best-of-N (BoN) by analyzing its performance and scaling behavior in generative models.


<details>
  <summary>Details</summary>
Motivation: The work aims to improve the Best-of-N method, which lacks robustness to lower-quality proxy reward models during inference-time alignment in generative models.

Method: The authors propose and analyze SBoN, introducing theoretical bounds on KL divergence and studying the regret gap to better understand and optimize its scaling behavior.

Result: Theoretical and empirical results indicate that smoothing in SBoN alleviates overoptimization and improves performance, particularly when proxy reward models are suboptimal.

Conclusion: SBoN is an effective extension of BoN, offering improved performance and robustness with empirical and theoretical support for its advantages in generative model alignment.

Abstract: A simple yet effective method for inference-time alignment of generative
models is Best-of-$N$ (BoN), where $N$ outcomes are sampled from a reference
policy, evaluated using a proxy reward model, and the highest-scoring one is
selected. While prior work argues that BoN is almost optimal in reward vs KL
tradeoffs, the effectiveness of BoN depends critically on the quality of the
proxy reward model used for selection. For this purpose, we study BoN through a
smooth version known as Soft Best-of-N (SBoN) and develop a theoretical
framework to address this gap. We analyze the scaling behaviour of BoN by
providing bounds on the KL divergence between the SBoN policy and the reference
policy, offering insights into how performance varies with the number of
samples. We also study the regret gap, i.e., the gap between the expected true
reward under the optimal policy and the SBoN policy. Our theoretical and
empirical findings show that smoothing helps SBoN mitigate reward
overoptimization, especially when the quality of the proxy reward is low.

</details>


### [319] [Online Regularized Learning Algorithms in RKHS with $β$- and $φ$-Mixing Sequences](https://arxiv.org/abs/2507.05929)
*Priyanka Roy,Susanne Saminger-Platz*

Main category: stat.ML

TL;DR: The paper introduces and evaluates an online learning algorithm in RKHS for dependent processes, focusing on mixing coefficients to quantify dependence.


<details>
  <summary>Details</summary>
Motivation: The study aims to address learning in RKHS settings where data exhibits dependencies, represented by mixing coefficients.

Method: An online regularized learning algorithm is examined using mixing coefficient-based dependent processes, with a focus on stationary Markov chains.

Result: Probabilistic bounds and convergence rates are derived for scenarios with varying decay rates of mixing coefficients.

Conclusion: The paper establishes theoretical foundations for learning with dependent data in RKHS, providing insights into how dependence affects convergence rates.

Abstract: In this paper, we study an online regularized learning algorithm in a
reproducing kernel Hilbert spaces (RKHS) based on a class of dependent
processes. We choose such a process where the degree of dependence is measured
by mixing coefficients. As a representative example, we analyze a strictly
stationary Markov chain, where the dependence structure is characterized by the
\(\phi\)- and \(\beta\)-mixing coefficients. Under these assumptions, we derive
probabilistic upper bounds as well as convergence rates for both the
exponential and polynomial decay of the mixing coefficients.

</details>


### [320] [Kernel Trace Distance: Quantum Statistical Metric between Measures through RKHS Density Operators](https://arxiv.org/abs/2507.06055)
*Arturo Castellanos,Anna Korba,Pavlo Mozharovskyi,Hicham Janati*

Main category: stat.ML

TL;DR: The paper introduces a novel distance measure between probability distributions based on the Schatten norm of kernel covariance operators, aiming to improve statistical machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: The work aims to address limitations of existing distance metrics like MMD and provide a metric that is more discriminative and robust, especially in high-dimensional settings.

Method: The authors propose a kernel-based distance metric leveraging Schatten norms, develop an extension of kernel matrices for measuring distribution differences, and implement an algorithm for practical computation.

Result: The proposed distance metric demonstrates improved properties like robustness and better discriminative capacity when applied in scenarios like Bayesian computation under contamination and particle flow simulations.

Conclusion: The distance metric integrates advantages of both Maximum Mean Discrepancy and Wasserstein distances while leveraging kernel methods for scalability and reliability in complex distributions.

Abstract: Distances between probability distributions are a key component of many
statistical machine learning tasks, from two-sample testing to generative
modeling, among others. We introduce a novel distance between measures that
compares them through a Schatten norm of their kernel covariance operators. We
show that this new distance is an integral probability metric that can be
framed between a Maximum Mean Discrepancy (MMD) and a Wasserstein distance. In
particular, we show that it avoids some pitfalls of MMD, by being more
discriminative and robust to the choice of hyperparameters. Moreover, it
benefits from some compelling properties of kernel methods, that can avoid the
curse of dimensionality for their sample complexity. We provide an algorithm to
compute the distance in practice by introducing an extension of kernel matrix
for difference of distributions that could be of independent interest. Those
advantages are illustrated by robust approximate Bayesian computation under
contamination as well as particle flow simulations.

</details>


### [321] [Estimating prevalence with precision and accuracy](https://arxiv.org/abs/2507.06061)
*Aime Bienfait Igiraneza,Christophe Fraser,Robert Hinch*

Main category: stat.ML

TL;DR: The paper introduces the Precise Quantifier (PQ), a Bayesian method for improved prevalence estimation tasks, focusing on precision and well-calibrated coverage, validated through theory and experiments.


<details>
  <summary>Details</summary>
Motivation: Current methods for prevalence estimation lack clear superiority in terms of precision and confidence interval coverage, necessitating improved techniques for accurate and reliable class distribution estimation.

Method: The authors propose and develop a Bayesian quantifier called Precise Quantifier (PQ), supported by theoretical foundations and experimental validation on both simulated and real-world datasets.

Result: PQ was demonstrated to be more precise and well-calibrated than existing methods, with experiments identifying key factors affecting precision: classifier discriminatory power, labeled dataset size, and unlabeled dataset size.

Conclusion: Precise Quantifier (PQ) significantly advances uncertainty quantification for prevalence estimation tasks by achieving high precision and well-calibrated confidence intervals, providing new insights into the domain.

Abstract: Unlike classification, whose goal is to estimate the class of each data point
in a dataset, prevalence estimation or quantification is a task that aims to
estimate the distribution of classes in a dataset. The two main tasks in
prevalence estimation are to adjust for bias, due to the prevalence in the
training dataset, and to quantify the uncertainty in the estimate. The standard
methods used to quantify uncertainty in prevalence estimates are bootstrapping
and Bayesian quantification methods. It is not clear which approach is ideal in
terms of precision (i.e. the width of confidence intervals) and coverage (i.e.
the confidence intervals being well-calibrated). Here, we propose Precise
Quantifier (PQ), a Bayesian quantifier that is more precise than existing
quantifiers and with well-calibrated coverage. We discuss the theory behind PQ
and present experiments based on simulated and real-world datasets. Through
these experiments, we establish the factors which influence quantification
precision: the discriminatory power of the underlying classifier; the size of
the labeled dataset used to train the quantifier; and the size of the unlabeled
dataset for which prevalence is estimated. Our analysis provides deep insights
into uncertainty quantification for quantification learning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [322] [Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes](https://arxiv.org/abs/2507.05304)
*Saqib Nazir,Olivier Lézoray,Sébastien Bougleux*

Main category: cs.GR

TL;DR: The paper introduces 3DGeoMeshNet, a GCN-based framework designed for accurate reconstruction of irregular 3D meshes, leveraging anisotropic convolution layers.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture both local and global features in irregular 3D meshes due to reliance on isotropic filters or spectral decomposition, as well as intermediate representations like voxel grids.

Method: The proposed 3DGeoMeshNet uses anisotropic convolution layers and a multi-scale encoder-decoder structure with separate global and local pathways to process the original mesh format directly.

Result: Extensive experiments on the COMA dataset showed that 3DGeoMeshNet outperforms in reconstruction accuracy, especially for human face datasets.

Conclusion: 3DGeoMeshNet effectively addresses limitations in previous methods by learning both local and global features while preserving the original mesh format, achieving enhanced reconstruction performance.

Abstract: 3D meshes are fundamental data representations for capturing complex
geometric shapes in computer vision and graphics applications. While
Convolutional Neural Networks (CNNs) have excelled in structured data like
images, extending them to irregular 3D meshes is challenging due to the
non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a
solution by applying convolutions to graph-structured data, but many existing
methods rely on isotropic filters or spectral decomposition, limiting their
ability to capture both local and global mesh features. In this paper, we
introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework
that uses anisotropic convolution layers to effectively learn both global and
local features directly in the spatial domain. Unlike previous approaches that
convert meshes into intermediate representations like voxel grids or point
clouds, our method preserves the original polygonal mesh format throughout the
reconstruction process, enabling more accurate shape reconstruction. Our
architecture features a multi-scale encoder-decoder structure, where separate
global and local pathways capture both large-scale geometric structures and
fine-grained local details. Extensive experiments on the COMA dataset
containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of
reconstruction accuracy.

</details>


### [323] [LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures](https://arxiv.org/abs/2507.06109)
*Seungoh Han,Jaehoon Jang,Hyunsu Kim,Jaeheung Surh,Junhyung Kwak,Hyowon Ha,Kyungdon Joo*

Main category: cs.GR

TL;DR: The paper introduces LighthouseGS, an NVS framework optimized for handheld cameras with panoramic motion to create photorealistic 3D scene reconstructions, addressing challenges posed by narrow baselines and motion drift.


<details>
  <summary>Details</summary>
Motivation: To make high-quality 3D Gaussian Splatting-based novel view synthesis accessible and practical for general users, especially when using handheld cameras with simpler panorama-style motion.

Method: LighthouseGS framework uses rough geometric priors (e.g., monocular depth estimation, camera poses) and leverages indoor planar structures to enable stable and consistent 3D point generation. Innovations include 'plane scaffold assembly,' stable pruning, and geometric/photometric corrections to handle camera motion challenges.

Result: LighthouseGS achieves superior photorealistic rendering compared to state-of-the-art methods for both real and synthetic indoor scenes, tested under challenging rotation-dominant motion and textureless conditions.

Conclusion: LighthouseGS demonstrates the feasibility of creating high-quality panoramic 3D reconstructions with handheld-device-friendly approaches, making view synthesis and object placement more accessible for general users.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [324] [OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning](https://arxiv.org/abs/2507.05876)
*Nehal Baganal Krishna,Anam Tahir,Firas Khamis,Mina Tahmasbi Arashloo,Michael Zink,Amr Rizk*

Main category: cs.NI

TL;DR: The paper presents a solution to mitigate staleness and congestion in Asynchronous Distributed Reinforcement Learning (DRL) by enabling inline processing of model updates and introducing a queueing mechanism and lightweight control measures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of degraded convergence in asynchronous DRL caused by stale model updates resulting from network congestion in large-scale training setups.

Method: The approach includes a network accelerator architecture with inline update processing, a novel queueing mechanism for combining updates, feedback-driven transmission control at worker nodes, and a new metric (Age-of-Model) for assessing staleness.

Result: The proposed architectural solution reduces update staleness and network congestion, leading to improved convergence rates in asynchronous DRL training.

Conclusion: Network acceleration with inline update processing and transmission control proves effective in addressing stale updates and congestion, ultimately enhancing the performance of asynchronous DRL systems.

Abstract: Asynchronous Distributed Reinforcement Learning (DRL) can suffer from
degraded convergence when model updates become stale, often the result of
network congestion and packet loss during large-scale training. This work
introduces a network data-plane acceleration architecture that mitigates such
staleness by enabling inline processing of DRL model updates as they traverse
the accelerator engine. To this end, we design and prototype a novel queueing
mechanism that opportunistically combines compatible updates sharing a network
element, reducing redundant traffic and preserving update utility.
Complementing this we provide a lightweight transmission control mechanism at
the worker nodes that is guided by feedback from the in-network accelerator. To
assess model utility at line rate, we introduce the Age-of-Model (AoM) metric
as a proxy for staleness and verify global fairness and responsiveness
properties using a formal verification method. Our evaluations demonstrate that
this architecture significantly reduces update staleness and congestion,
ultimately improving the convergence rate in asynchronous DRL workloads.

</details>


### [325] [A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation](https://arxiv.org/abs/2507.05731)
*Yuxin Zhang,Jiahao Yang,Zhe Chen,Wenjun Zhu,Jin Zhao,Yue Gao*

Main category: cs.NI

TL;DR: The paper introduces SpaceVerse, a system to efficiently deploy large vision-language models (LVLMs) in low Earth orbit satellite networks for near real-time Earth observation, addressing challenges in image data transmission.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs excel at analyzing LEO satellite Earth observation images but face challenges in fast image data transmission due to satellite motion and limited contact windows. The need for real-time applications like disaster monitoring motivates this work.

Method: The system deploys compact LVLMs in satellites for lightweight tasks and regular LVLMs in ground stations for intensive tasks. A co-design framework with progressive confidence networks and attention-based multi-scale preprocessing reduces data redundancy and transmission needs.

Result: SpaceVerse achieves a significant improvement, including a 31.2% accuracy gain and a 51.2% latency reduction compared to current methods, when evaluated on real-world LEO satellite networks.

Conclusion: The SpaceVerse system offers an innovative solution enabling efficient and near real-time Earth observation by optimizing LVLM deployment and reducing communication overhead.

Abstract: Recently, large vision-language models (LVLMs) unleash powerful analysis
capabilities for low Earth orbit (LEO) satellite Earth observation images in
the data center. However, fast satellite motion, brief satellite-ground station
(GS) contact windows, and large size of the images pose a data download
challenge. To enable near real-time Earth observation applications (e.g.,
disaster and extreme weather monitoring), we should explore how to deploy LVLM
in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground
synergistic LVLM inference system. To this end, firstly, we deploy compact
LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs
to handle computationally intensive tasks. Then, we propose a computing and
communication co-design framework comprised of a progressive confidence network
and an attention-based multi-scale preprocessing, used to identify on-satellite
inferring data, and reduce data redundancy before satellite-GS transmission,
separately. We implement and evaluate SpaceVerse on real-world LEO satellite
constellations and datasets, achieving a 31.2% average gain in accuracy and a
51.2% reduction in latency compared to state-of-the-art baselines.

</details>


### [326] [Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing](https://arxiv.org/abs/2507.05829)
*Zekai Sun,Xiuxian Guan,Zheng Lin,Zihan Fang,Xiangming Cai,Zhe Chen,Fangming Liu,Heming Cui,Jie Xiong,Wei Ni,Chau Yuen*

Main category: cs.NI

TL;DR: Intra-DP is a system designed to optimize deep neural network inference on mobile edge computing by mitigating transmission bottlenecks through parallel computation of sub-operations, achieving faster and more energy-efficient performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Mobile devices often struggle to deploy DNNs due to limited resources and battery life. MEC, with GPU servers, could assist, but current solutions have inefficiencies in transmission due to sequential execution.

Method: Intra-DP introduces a parallel computing technique that divides operations into sub-operations and overlaps computation and transmission to enhance performance for DNN inference.

Result: Experimental results show that Intra-DP reduces inference latency by up to 50% and energy consumption by 75%, all while maintaining accuracy.

Conclusion: Intra-DP is a significant improvement over existing approaches, offering a fast, energy-efficient, and accurate solution for DNN inference on resource-constrained devices.

Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices
presents significant challenges, particularly in achieving real-time
performance while simultaneously coping with limited computational resources
and battery life. While Mobile Edge Computing (MEC) offers collaborative
inference with GPU servers as a promising solution, existing approaches
primarily rely on layer-wise model partitioning and undergo significant
transmission bottlenecks caused by the sequential execution of DNN operations.
To address this challenge, we present Intra-DP, a high-performance
collaborative inference system optimized for DNN inference on MEC. Intra DP
employs a novel parallel computing technique based on local operators (i.e.,
operators whose minimum unit input is not the entire input tensor, such as the
convolution kernel). By decomposing their computations (operations) into
several independent sub-operations and overlapping the computation and
transmission of different sub-operations through parallel execution, Intra-DP
mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient
inference. The evaluation demonstrates that Intra-DP reduces per-inference
latency by up to 50% and energy consumption by up to 75% compared to
state-of-the-art baselines, without sacrificing accuracy.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [327] [Fredholm Neural Networks for forward and inverse problems in elliptic PDEs](https://arxiv.org/abs/2507.06038)
*Kyriakos Georgiou,Constantinos Siettos,Athanasios N. Yannacopoulos*

Main category: math.NA

TL;DR: This paper extends the Fredholm Neural Networks framework to solve forward and inverse problems of elliptic PDEs using a boundary integral method and deep neural networks.


<details>
  <summary>Details</summary>
Motivation: To create an explainable and accurate approach for solving elliptic PDEs, addressing both forward and inverse problems in a structured manner.

Method: The Potential Fredholm Neural Network (PFNN) uses a deep neural network structured on fixed-point iteration principles and potential theory to ensure explainable architecture and iterative accuracy.

Result: The scheme accurately solves elliptic PDEs with small errors inside the domain and near-machine precision on the boundary, supported by consistency proofs and explicit error bounds.

Conclusion: PFNN is an accurate and explainable approach for solving elliptic PDEs, effectively integrating theoretical consistency into its neural network design.

Abstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm
NNs/ FNNs) for solving integral equations, we extend the framework to tackle
forward and inverse problems for linear and semi-linear elliptic partial
differential equations. The proposed scheme consists of a deep neural network
(DNN) which is designed to represent the iterative process of fixed-point
iterations for the solution of elliptic PDEs using the boundary integral method
within the framework of potential theory. The number of layers, weights, biases
and hyperparameters are computed in an explainable manner based on the
iterative scheme, and we therefore refer to this as the Potential Fredholm
Neural Network (PFNN). We show that this approach ensures both accuracy and
explainability, achieving small errors in the interior of the domain, and near
machine-precision on the boundary. We provide a constructive proof for the
consistency of the scheme and provide explicit error bounds for both the
interior and boundary of the domain, reflected in the layers of the PFNN. These
error bounds depend on the approximation of the boundary function and the
integral discretization scheme, both of which directly correspond to components
of the Fredholm NN architecture. In this way, we provide an explainable scheme
that explicitly respects the boundary conditions. We assess the performance of
the proposed scheme for the solution of both the forward and inverse problem
for linear and semi-linear elliptic PDEs in two dimensions.

</details>


### [328] [Conservative approximation-based feedforward neural network for WENO schemes](https://arxiv.org/abs/2507.06190)
*Kwanghyuk Park,Jiaxi Gu,Jae-Hun Jung*

Main category: math.NA

TL;DR: The paper introduces a new method replacing the classical weighting procedure in WENO schemes with a neural network for solving hyperbolic conservation laws, resulting in enhanced performance.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and robustness of WENO schemes for solving hyperbolic conservation laws by introducing a neural network-based approach.

Method: The paper uses a supervised learning-based feedforward neural network to replace the classical nonlinear weighting procedure in WENO schemes, trained on a dataset constructed for one-dimensional conservative approximation.

Result: The newly introduced WENO3-CADNNs showcase robust generalization in several benchmark cases and outperform WENO3-Z in accuracy, achieving results comparable to WENO5-JS.

Conclusion: Integrating neural networks into WENO schemes improves their performance and provides high-order accuracy for approximating derivatives in hyperbolic conservation law solutions.

Abstract: In this work, we present the feedforward neural network based on the
conservative approximation to the derivative from point values, for the
weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic
conservation laws. The feedforward neural network, whose inputs are point
values from the three-point stencil and outputs are two nonlinear weights,
takes the place of the classical WENO weighting procedure. For the training
phase, we employ the supervised learning and create a new labeled dataset for
one-dimensional conservative approximation, where we construct a numerical flux
function from the given point values such that the flux difference approximates
the derivative to high-order accuracy. The symmetric-balancing term is
introduced for the loss function so that it propels the neural network to match
the conservative approximation to the derivative and satisfy the symmetric
property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes,
WENO3-CADNNs, demonstrate robust generalization across various benchmark
scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy
comparable to WENO5-JS.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [329] [AI-Reporter: A Path to a New Genre of Scientific Communication](https://arxiv.org/abs/2507.05903)
*Gerd Graßhoff*

Main category: cs.DL

TL;DR: The paper introduces AI-Reporter, a system that converts academic presentations into publication-ready chapters within minutes.


<details>
  <summary>Details</summary>
Motivation: To address the gap between temporary academic presentations and permanent scientific documentation.

Method: Using a specific case study of Arno Simons' lecture on Large Language Models, demonstrating the effectiveness of converting ephemeral content into documented chapters.

Result: AI-Reporter successfully transformed the lecture into publication-ready material in under three minutes.

Conclusion: AI-Reporter revolutionizes scientific publication practice by enabling rapid and efficient documentation of academic presentations.

Abstract: The AI-Reporter represents a paradigmatic shift in scientific publication
practice. This document demonstrates through a concrete case study how our
system transforms academic presentations into publication-ready chapters -- in
less than three minutes. Using Arno Simons' lecture on Large Language Models
from the ``Large Language Models for the History, Philosophy, and Sociology of
Science'' workshop (NEPI) as an example, we show how technological innovation
bridges the gap between ephemeral presentation and permanent scientific
documentation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [330] [Optimal structure learning and conditional independence testing](https://arxiv.org/abs/2507.05689)
*Ming Gao,Yuhao Wang,Bryon Aragam*

Main category: math.ST

TL;DR: This paper establishes a link between the complexity of structure learning and conditional independence testing, deriving minimax optimal rates for specific cases and identifying a modified PC algorithm as optimal.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quantifying the statistical complexity of structure learning and connect it to conditional independence testing.

Method: A general reduction is developed between structure learning and conditional independence testing for poly-forests, with derivation of the minimax rates for various models like Bernoulli, Gaussian, and nonparametric.

Result: The authors show that the minimax optimal rate for structure learning aligns with that for conditional independence testing and identify a modified PC algorithm as optimal for these settings.

Conclusion: The work provides a theoretical framework that links structure learning and minimax testing, showcasing optimized methodologies and algorithms for specific probabilistic models.

Abstract: We establish a fundamental connection between optimal structure learning and
optimal conditional independence testing by showing that the minimax optimal
rate for structure learning problems is determined by the minimax rate for
conditional independence testing in these problems. This is accomplished by
establishing a general reduction between these two problems in the case of
poly-forests, and demonstrated by deriving optimal rates for several examples,
including Bernoulli, Gaussian and nonparametric models. Furthermore, we show
that the optimal algorithm in these settings is a suitable modification of the
PC algorithm. This theoretical finding provides a unified framework for
analyzing the statistical complexity of structure learning through the lens of
minimax testing.

</details>


### [331] [Consistency and Inconsistency in $K$-Means Clustering](https://arxiv.org/abs/2507.06226)
*Moïse Blanchard,Adam Quinn Jaffe,Nikita Zhivotovskiy*

Main category: math.ST

TL;DR: This paper investigates the asymptotic consistency of $k$-means clustering under finite expectation, providing a nuanced view with both negative and positive results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore $k$-means clustering under weaker assumptions, particularly when only finite expectation is guaranteed, as opposed to the traditional requirement of finite variance.

Method: The paper analyzes scenarios of failure in $k$-means clustering consistency due to cluster imbalance caused by outliers, followed by proposing methods to recover consistency by enforcing balance among empirical clusters.

Result: Negative results show inconsistency from cluster imbalances due to outliers, while positive results demonstrate that consistency can be achieved with a priori balance in empirical clusters.

Conclusion: Consistency in $k$-means clustering under finite expectation depends on addressing cluster imbalances, highlighting the subtlety of the problem and the need for structured approaches to enforce balance.

Abstract: A celebrated result of Pollard proves asymptotic consistency for $k$-means
clustering when the population distribution has finite variance. In this work,
we point out that the population-level $k$-means clustering problem is, in
fact, well-posed under the weaker assumption of a finite expectation, and we
investigate whether some form of asymptotic consistency holds in this setting.
As we illustrate in a variety of negative results, the complete story is quite
subtle; for example, the empirical $k$-means cluster centers may fail to
converge even if there exists a unique set of population $k$-means cluster
centers. A detailed analysis of our negative results reveals that inconsistency
arises because of an extreme form of cluster imbalance, whereby the presence of
outlying samples leads to some empirical $k$-means clusters possessing very few
points. We then give a collection of positive results which show that some
forms of asymptotic consistency, under only the assumption of finite
expectation, may be recovered by imposing some a priori degree of balance among
the empirical $k$-means clusters.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [332] [HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales](https://arxiv.org/abs/2507.05658)
*Daniel Abdi,Isidora Jankov,Paul Madden,Vanderlei Vargas,Timothy A. Smith,Sergey Frolov,Montgomery Flora,Corey Potvin*

Main category: physics.ao-ph

TL;DR: A machine learning-based emulator (HRRRCast) improves weather forecasting with competitive accuracy compared to the operational HRRR model.


<details>
  <summary>Details</summary>
Motivation: To create a computationally efficient alternative for operational weather forecasting using the HRRR model.

Method: HRRRCast employs deep learning techniques, including ResNet and Graph Neural Network architectures (ResHRRR and GraphHRRR). It focuses on probabilistic forecasting and longer lead times using advanced training strategies.

Result: The ResHRRR architecture provides better performance at light rainfall thresholds and competitive accuracy for moderate rainfall levels compared to HRRR.

Conclusion: HRRRCast offers a promising and efficient method for regional weather forecasting, enhancing accuracy and spatial detail while paving the way for future improvements in graph-based models.

Abstract: The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model
used in operational weather forecasting across the contiguous United States
(CONUS). To provide a computationally efficient alternative, we introduce
HRRRCast, a data-driven emulator built with advanced machine learning
techniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR)
and a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional
neural networks enhanced with squeeze-and-excitation blocks and Feature-wise
Linear Modulation, and supports probabilistic forecasting via the Denoising
Diffusion Implicit Model (DDIM). To better handle longer lead times, we train a
single model to predict multiple lead times (1h, 3h, and 6h), then use a greedy
rollout strategy during inference. When evaluated on composite reflectivity
over the full CONUS domain using ensembles of 3 to 10 members, ResHRRR
outperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves
competitive performance at moderate thresholds (30 dBZ). Our work advances the
StormCast model of Pathak et al. [21] by: a) training on the full CONUS domain,
b) using multiple lead times to improve long-range skill, c) training on
analysis data instead of the +1h post-analysis data inadvertently used in
StormCast, and d) incorporating future GFS states as inputs, enabling
downscaling that improves long-lead accuracy. Grid-, neighborhood-, and
object-based metrics confirm better storm placement, lower frequency bias, and
higher success ratios than HRRR. HRRRCast ensemble forecasts also maintain
sharper spatial detail, with power spectra more closely matching HRRR analysis.
While GraphHRRR underperforms in its current form, it lays groundwork for
future graph-based forecasting. HRRRCast represents a step toward efficient,
data-driven regional weather prediction with competitive accuracy and ensemble
capability.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [333] [GPU-accelerated Modeling of Biological Regulatory Networks](https://arxiv.org/abs/2506.19866)
*Joyce Reimer,Pranta Saha,Chris Chen,Neeraj Dhar,Brook Byrns,Steven Rayan,Gordon Broderick*

Main category: q-bio.MN

TL;DR: The paper explores the acceleration of global optimization algorithms for biological logic model identification using GPU computing, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges in parameter search spaces of biological networks' logic models and make such optimization practical for pharmaceutical research.

Method: Implement global optimization algorithms in a GPU computing environment and assess performance improvements on model biological regulatory systems.

Result: GPU computing showed efficiency gains, with a 33%-43% improvement over multi-thread CPU and a 33%-1866% improvement over serial CPU implementations.

Conclusion: GPU computing makes global optimization for logic model identification more practical and scalable, enhancing its utility for hypothesis generation and experiment design.

Abstract: The complex regulatory dynamics of a biological network can be succinctly
captured using discrete logic models. Given even sparse time-course data from
the system of interest, previous work has shown that global optimization
schemes are suitable for proposing logic models that explain the data and make
predictions about how the system will behave under varying conditions.
Considering the large scale of the parameter search spaces associated with
these regulatory systems, performance optimizations on the level of both
hardware and software are necessary for making this a practical tool for in
silico pharmaceutical research. We show here how the implementation of these
global optimization algorithms in a GPU-computing environment can accelerate
the solution of these parameter search problems considerably. We carry out
parameter searches on two model biological regulatory systems that represent
almost an order of magnitude scale-up in complexity, and we find the gains in
efficiency from GPU to be a 33%-43% improvement compared to multi-thread CPU
implementations and a 33%-1866% increase compared to CPU in serial. These
improvements make global optimization of logic model identification a far more
attractive and feasible method for in silico hypothesis generation and design
of experiments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [334] [Special-Unitary Parameterization for Trainable Variational Quantum Circuits](https://arxiv.org/abs/2507.05535)
*Kuan-Cheng Chen,Huan-Hsin Tseng,Samuel Yen-Chi Chen,Chen-Yu Liu,Kin K. Leung*

Main category: quant-ph

TL;DR: The paper proposes SUN-VQC, a variational quantum circuit architecture that reduces barren plateaus in quantum computing by confining evolution to a symmetry-restricted Lie subgroup. It demonstrates improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenge of barren plateaus in variational quantum algorithms (VQAs), which lead to vanishing gradients and hinder optimization processes, particularly on near-term quantum processors.

Method: The authors introduced SUN-VQC, which confines operations to a symmetry-restricted Lie subgroup SU(2^k) within SU(2^n), significantly reducing the dimensionality of the dynamical Lie algebra. This design also uses a generalized parameter-shift rule for efficient gradient computation without ancillary qubits.

Result: Numerical experiments demonstrated that SUN-VQCs produce larger gradient signals, converge 2-3 times faster, and achieve higher fidelities compared to standard circuit designs like Pauli-rotation or hardware-efficient approaches.

Conclusion: Lie-subalgebra engineering, as implemented in SUN-VQC, offers a scalable and hardware-compatible method for overcoming barren plateaus in VQAs, highlighting its potential for near-term quantum computing applications.

Abstract: We propose SUN-VQC, a variational-circuit architecture whose elementary
layers are single exponentials of a symmetry-restricted Lie subgroup,
$\mathrm{SU}(2^{k}) \subset \mathrm{SU}(2^{n})$ with $k \ll n$. Confining the
evolution to this compact subspace reduces the dynamical Lie-algebra dimension
from $\mathcal{O}(4^{n})$ to $\mathcal{O}(4^{k})$, ensuring only polynomial
suppression of gradient variance and circumventing barren plateaus that plague
hardware-efficient ans\"atze. Exact, hardware-compatible gradients are obtained
using a generalized parameter-shift rule, avoiding ancillary qubits and
finite-difference bias. Numerical experiments on quantum auto-encoding and
classification show that SUN-VQCs sustain order-of-magnitude larger gradient
signals, converge 2--3$\times$ faster, and reach higher final fidelities than
depth-matched Pauli-rotation or hardware-efficient circuits. These results
demonstrate that Lie-subalgebra engineering provides a principled, scalable
route to barren-plateau-resilient VQAs compatible with near-term quantum
processors.

</details>


### [335] [Learnable quantum spectral filters for hybrid graph neural networks](https://arxiv.org/abs/2507.05640)
*Ammar Daskin*

Main category: quant-ph

TL;DR: This paper introduces a quantum circuit modeled as convolutional and pooling layers for graph neural networks, achieving efficient graph signal processing and applying it to graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of approximating Laplacian-based learnable functions in graph neural networks using classical methods like Chebyshev polynomials or Taylor expansions.

Method: The authors propose a parameterized quantum Fourier circuit leveraging the eigenspace of a graph's Laplacian operator, enabling efficient dimensionality reduction and signal compression for graph data.

Result: The proposed quantum circuit, combined with a classical prediction head, achieves competitive performance on benchmark graph classification tasks using few parameters and leveraging geometric structure.

Conclusion: This hybrid quantum-classical approach demonstrates the potential for efficient and effective graph signal processing, particularly when the geometric structure of graphs is crucial.

Abstract: In this paper, we describe a parameterized quantum circuit that can be
considered as convolutional and pooling layers for graph neural networks. The
circuit incorporates the parameterized quantum Fourier circuit where the qubit
connections for the controlled gates derived from the Laplacian operator.
Specifically, we show that the eigenspace of the Laplacian operator of a graph
can be approximated by using QFT based circuit whose connections are determined
from the adjacency matrix. For an $N\times N$ Laplacian, this approach yields
an approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These
types of circuits can eliminate the expensive classical computations for
approximating the learnable functions of the Laplacian through Chebyshev
polynomial or Taylor expansions.
  Using this circuit as a convolutional layer provides an $n-$ dimensional
probability vector that can be considered as the filtered and compressed graph
signal. Therefore, the circuit along with the measurement can be considered a
very efficient convolution plus pooling layer that transforms an
$N$-dimensional signal input into $n-$dimensional signal with an exponential
compression. We then apply a classical neural network prediction head to the
output of the circuit to construct a complete graph neural network. Since the
circuit incorporates geometric structure through its graph connection-based
approach, we present graph classification results for the benchmark datasets
listed in TUDataset library. Using only [1-100] learnable parameters for the
quantum circuit and minimal classical layers (1000-5000 parameters) in a
generic setting, the obtained results are comparable to and in some cases
better than many of the baseline results, particularly for the cases when
geometric structure plays a significant role.

</details>


### [336] [Instance-Optimal Quantum State Certification with Entangled Measurements](https://arxiv.org/abs/2507.06010)
*Ryan O'Donnell,Chirag Wadhwa*

Main category: quant-ph

TL;DR: This study addresses quantum state certification with entangled measurements, establishing nearly instance-optimal bounds and deriving key results using a new quantum adaptation of the Ingster-Suslina method.


<details>
  <summary>Details</summary>
Motivation: To determine instance-optimal bounds for quantum state certification when testers can perform entangled measurements, filling the gap in prior research focused only on unentangled scenarios.

Method: The authors introduce a quantum adaptation of the Ingster-Suslina method to derive lower and upper bounds for certifying a quantum state.

Result: They establish that the copy complexity for certifying a state depends on the fidelity between the hypothesis state and the maximally mixed state, recovering prior lower bounds with simplified proofs.

Conclusion: This work provides insights into the optimality of quantum state certification with entangled measurements, advancing the understanding of certification complexity and introducing methodologies for broader utility in quantum information.

Abstract: We consider the task of quantum state certification: given a description of a
hypothesis state $\sigma$ and multiple copies of an unknown state $\rho$, a
tester aims to determine whether the two states are equal or $\epsilon$-far in
trace distance. It is known that $\Theta(d/\epsilon^2)$ copies of $\rho$ are
necessary and sufficient for this task, assuming the tester can make entangled
measurements over all copies [CHW07,OW15,BOW19]. However, these bounds are for
a worst-case $\sigma$, and it is not known what the optimal copy complexity is
for this problem on an instance-by-instance basis. While such instance-optimal
bounds have previously been shown for quantum state certification when the
tester is limited to measurements unentangled across copies [CLO22,CLHL22],
they remained open when testers are unrestricted in the kind of measurements
they can perform.
  We address this open question by proving nearly instance-optimal bounds for
quantum state certification when the tester can perform fully entangled
measurements. Analogously to the unentangled setting, we show that the optimal
copy complexity for certifying $\sigma$ is given by the worst-case complexity
times the fidelity between $\sigma$ and the maximally mixed state. We prove our
lower bounds using a novel quantum analogue of the Ingster-Suslina method,
which is likely to be of independent interest. This method also allows us to
recover the $\Omega(d/\epsilon^2)$ lower bound for mixedness testing [OW15],
i.e., certification of the maximally mixed state, with a surprisingly simple
proof.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [337] [A Formal Refutation of the Blockchain Trilemma](https://arxiv.org/abs/2507.05809)
*Craig Wright*

Main category: cs.CC

TL;DR: This paper refutes the blockchain trilemma, demonstrating that it's flawed in reasoning and presents a practical protocol design achieving scalability, security, and decentralization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to challenge the widely accepted blockchain trilemma which claims the inherent impossibility of achieving scalability, security, and decentralization in blockchain protocols.

Method: The authors use formal analysis methods such as predicate logic, automata theory, computational complexity, and graph theory to dissect the trilemma, accompanied by a practical counterexample protocol design.

Result: The trilemma is shown to be based on conceptual errors, unproven assumptions, and fallacies. A protocol meeting the three objectives simultaneously is also presented.

Conclusion: The blockchain trilemma is not a fundamental architectural limitation but rather a heuristic misconception, and achieving scalability, security, and decentralization is possible with proper design.

Abstract: The so-called blockchain trilemma asserts the impossibility of simultaneously
achieving scalability, security, and decentralisation within a single
blockchain protocol. In this paper, we formally refute that proposition.
Employing predicate logic, formal automata theory, computational complexity
analysis, and graph-theoretic measures of relay topology--specifically Baran's
model of network path redundancy--we demonstrate that the trilemma constitutes
a category error, conflates distinct analytical domains, and relies upon
unproven causal assumptions. We further expose its reliance on composition
fallacies drawn from flawed system implementations. A constructive
counterexample is presented: a blockchain protocol exhibiting unbounded
transaction throughput, cryptographic security under adversarial load, and
multipath decentralised propagation. This example is not hypothetical but
grounded in protocol design enabled by compact block relay, SPV verification,
and IPv6 multicast. The trilemma is revealed not as a law of protocol
architecture, but as a heuristic fallacy sustained by imprecision and design
defeatism.

</details>


### [338] [Generalized and Unified Equivalences between Hardness and Pseudoentropy](https://arxiv.org/abs/2507.05972)
*Lunjia Hu,Salil Vadhan*

Main category: cs.CC

TL;DR: This paper unifies and strengthens pseudoentropy characterizations for both uniform and non-uniform computational models, extending them to general entropy notions and achieving efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To better understand the relationship between computational hardness and randomness by developing a more general and efficient pseudoentropy characterization that encompasses various entropy notions.

Method: The authors leverage weight-restricted calibration and computational indistinguishability (multiaccuracy) to extend existing frameworks like the Complexity-Theoretic Regularity Lemma and Leakage Simulation Lemma, achieving a general pseudoentropy characterization.

Result: They achieve a generalized pseudoentropy characterization that applies across different entropy notions, using a universal function to reduce complexity dependency on alphabet size exponentially compared to prior works.

Conclusion: Weight-restricted calibration emerges as a powerful tool in understanding computational hardness and randomness, enabling both theoretical insights and practical improvements in entropy-based computational models.

Abstract: Pseudoentropy characterizations provide a quantitatively precise
demonstration of the close relationship between computational hardness and
computational randomness. We prove a unified pseudoentropy characterization
that generalizes and strengthens previous results for both uniform and
non-uniform models of computation. Our characterization holds for a general
family of entropy notions that encompasses the common notions of Shannon
entropy and min entropy as special cases. Moreover, we show that the
characterizations for different entropy notions can be simultaneously achieved
by a single, universal function that simultaneously witnesses computational
hardness and computational randomness. A key technical insight of our work is
that the notion of weight-restricted calibration from the recent literature on
algorithm fairness, along with standard computational indistinguishability
(known as multiaccuracy in the fairness literature), suffices for proving
pseudoentropy characterizations for general entropy notions. This demonstrates
the power of weight-restricted calibration to enhance the classic
Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009)
and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to
achieve an exponential improvement in the complexity dependency on the alphabet
size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and
Vadhan (2024) based on the much stronger notion of multicalibration. We show
that the exponential dependency on the alphabet size is inevitable for
multicalibration as well as for the weaker notion of calibrated multiaccuracy.

</details>


### [339] [Complexity Results of Persuasion](https://arxiv.org/abs/2507.05951)
*Alban Grastien*

Main category: cs.CC

TL;DR: This paper demonstrates that persuasion falls into the class of problems that are NP-complete.


<details>
  <summary>Details</summary>
Motivation: To understand the computational complexity of persuasion and classify it within established complexity classes.

Method: The paper utilizes theoretical computer science frameworks to establish the NP-completeness of persuasion.

Result: Persuasion is proven to be an NP-complete problem.

Conclusion: Persuasion, a computational problem, is as hard as other NP-complete problems, suggesting significant practical computational limitations.

Abstract: We prove that persuasion is an NP-complete problem.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [340] [Predicting mutational effects on protein binding from folding energy](https://arxiv.org/abs/2507.05502)
*Arthur Deng,Karsten Householder,Fang Wu,Sebastian Thrun,K. Christopher Garcia,Brian Trippe*

Main category: q-bio.BM

TL;DR: This paper introduces StaB-ddG, a novel transfer-learning approach for predicting protein-protein binding energy changes due to mutations. It achieves state-of-the-art accuracy comparable to FoldX, while being over 1,000x faster.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of mutational effects on protein-protein binding energies is essential for advancements in structural biology and therapeutic development, yet existing deep learning approaches fall short of empirical force-field methods due to limited data.

Method: The authors propose a transfer-learning approach utilizing pre-trained inverse-folding models to estimate folding and binding energies. Fine-tuning is performed with abundant folding energy data and limited binding energy measurements.

Result: StaB-ddG matches the accuracy of FoldX, a leading empirical force-field method, and surpasses it in computational efficiency, offering significant speed improvements over existing techniques.

Conclusion: This study demonstrates the feasibility of achieving high accuracy in protein binding energy prediction using deep learning models, highlighting the potential for faster and equally accurate alternatives to empirical methods.

Abstract: Accurate estimation of mutational effects on protein-protein binding energies
is an open problem with applications in structural biology and therapeutic
design. Several deep learning predictors for this task have been proposed, but,
presumably due to the scarcity of binding data, these methods underperform
computationally expensive estimates based on empirical force fields. In
response, we propose a transfer-learning approach that leverages advances in
protein sequence modeling and folding stability prediction for this task. The
key idea is to parameterize the binding energy as the difference between the
folding energy of the protein complex and the sum of the folding energies of
its binding partners. We show that using a pre-trained inverse-folding model as
a proxy for folding energy provides strong zero-shot performance, and can be
fine-tuned with (1) copious folding energy measurements and (2) more limited
binding energy measurements. The resulting predictor, StaB-ddG, is the first
deep learning predictor to match the accuracy of the state-of-the-art empirical
force-field method FoldX, while offering an over 1,000x speed-up.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [341] [Exploring LLM Capabilities in Extracting DCAT-Compatible Metadata for Data Cataloging](https://arxiv.org/abs/2507.05282)
*Lennart Busch,Daniel Tebernum,Gissel Velarde*

Main category: cs.IR

TL;DR: This paper explores automating metadata maintenance for text-based data using LLMs, achieving high-quality, DCAT-compatible metadata efficiently.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency in data exploration caused by the need for significant time and expertise to maintain metadata manually.

Method: Various LLMs were tested for metadata generation using zero-shot and few-shot prompting strategies and a fine-tuned model for classification.

Result: LLMs produced high-quality metadata comparable to human efforts, with larger models and fine-tuning enhancing performance significantly.

Conclusion: LLMs can reliably accelerate metadata creation with high quality, but their successful application requires attention to task-specific and domain-specific contexts.

Abstract: Efficient data exploration is crucial as data becomes increasingly important
for accelerating processes, improving forecasts and developing new business
models. Data consumers often spend 25-98 % of their time searching for suitable
data due to the exponential growth, heterogeneity and distribution of data.
Data catalogs can support and accelerate data exploration by using metadata to
answer user queries. However, as metadata creation and maintenance is often a
manual process, it is time-consuming and requires expertise. This study
investigates whether LLMs can automate metadata maintenance of text-based data
and generate high-quality DCAT-compatible metadata. We tested zero-shot and
few-shot prompting strategies with LLMs from different vendors for generating
metadata such as titles and keywords, along with a fine-tuned model for
classification. Our results show that LLMs can generate metadata comparable to
human-created content, particularly on tasks that require advanced semantic
understanding. Larger models outperformed smaller ones, and fine-tuning
significantly improves classification accuracy, while few-shot prompting yields
better results in most cases. Although LLMs offer a faster and reliable way to
create metadata, a successful application requires careful consideration of
task-specific criteria and domain context.

</details>


### [342] [A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models](https://arxiv.org/abs/2507.05288)
*Shuliang Liu,Hongyi Liu,Aiwei Liu,Bingchen Duan,Qi Zheng,Yibo Yan,He Geng,Peijie Jiang,Jia Liu,Xuming Hu*

Main category: cs.IR

TL;DR: The paper presents a proactive defense strategy against LLM-generated misinformation using a Three Pillars framework, showing a significant improvement over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the societal risks posed by LLM-generated misinformation, which is self-reinforcing, highly plausible, and often escapes traditional detection methods.

Method: The paper proposes a proactive defense paradigm with a Three Pillars framework: Knowledge Credibility, Inference Reliability, and Input Robustness. It employs a comprehensive meta-analysis of existing techniques.

Result: Proactive defense strategies show up to a 63% improvement in misinformation mitigation compared to conventional methods, though challenges like computational overhead persist.

Conclusion: Future research should focus on robust knowledge foundations, reasoning certification, and attack-resistant interfaces to counter misinformation effectively in various domains.

Abstract: The widespread deployment of large language models (LLMs) across critical
domains has amplified the societal risks posed by algorithmically generated
misinformation. Unlike traditional false content, LLM-generated misinformation
can be self-reinforcing, highly plausible, and capable of rapid propagation
across multiple languages, which traditional detection methods fail to mitigate
effectively. This paper introduces a proactive defense paradigm, shifting from
passive post hoc detection to anticipatory mitigation strategies. We propose a
Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of
training and deployed data; (2) Inference Reliability, embedding
self-corrective mechanisms during reasoning; and (3) Input Robustness,
enhancing the resilience of model interfaces against adversarial attacks.
Through a comprehensive survey of existing techniques and a comparative
meta-analysis, we demonstrate that proactive defense strategies offer up to
63\% improvement over conventional methods in misinformation prevention,
despite non-trivial computational overhead and generalization challenges. We
argue that future research should focus on co-designing robust knowledge
foundations, reasoning certification, and attack-resistant interfaces to ensure
LLMs can effectively counter misinformation across varied domains.

</details>


### [343] [Enhancing Learning Path Recommendation via Multi-task Learning](https://arxiv.org/abs/2507.05295)
*Afsana Nasrin,Lijun Qian,Pamela Obiomon,Xishuang Dong*

Main category: cs.IR

TL;DR: This paper introduces a multi-task LSTM model for personalized learning path recommendation, transforming the task into a sequence-to-sequence prediction problem and achieving better performance compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve personalized learning path recommendations using advanced deep learning techniques, addressing the challenge of automatically optimally sequencing learning items.

Method: The proposed method involves a multi-task LSTM model, using shared and task-specific layers to jointly perform learning path recommendation and deep knowledge tracing. A non-repeat loss is incorporated to avoid redundant recommendations within generated paths.

Result: Experiments on the ASSIST09 dataset demonstrate that the model significantly outperforms existing baseline methods in learning path recommendation.

Conclusion: The study confirms the effectiveness of reframing learning path recommendation as a Seq2Seq problem and highlights the advantages of the multi-task LSTM model for personalization and efficiency in education systems.

Abstract: Personalized learning is a student-centered educational approach that adapts
content, pace, and assessment to meet each learner's unique needs. As the key
technique to implement the personalized learning, learning path recommendation
sequentially recommends personalized learning items such as lectures and
exercises. Advances in deep learning, particularly deep reinforcement learning,
have made modeling such recommendations more practical and effective. This
paper proposes a multi-task LSTM model that enhances learning path
recommendation by leveraging shared information across tasks. The approach
reframes learning path recommendation as a sequence-to-sequence (Seq2Seq)
prediction problem, generating personalized learning paths from a learner's
historical interactions. The model uses a shared LSTM layer to capture common
features for both learning path recommendation and deep knowledge tracing,
along with task-specific LSTM layers for each objective. To avoid redundant
recommendations, a non-repeat loss penalizes repeated items within the
recommended learning path. Experiments on the ASSIST09 dataset show that the
proposed model significantly outperforms baseline methods for the learning path
recommendation.

</details>


### [344] [News Source Citing Patterns in AI Search Systems](https://arxiv.org/abs/2507.05301)
*Kai-Cheng Yang*

Main category: cs.IR

TL;DR: The study analyzes citation patterns in AI-powered search systems using data from 24,000 conversations and 65,000 responses, highlighting tendencies toward liberal bias and concentration among specific outlets.


<details>
  <summary>Details</summary>
Motivation: To understand how AI-powered search systems cite news sources and explore the implications for user access to information and bias in citation practices.

Method: Analyzed a dataset from the AI Search Arena, comparing citation behavior of AI models from three major providers (OpenAI, Perplexity, Google), focusing on attributes such as source credibility, political leaning, and user satisfaction.

Result: Found shared citation patterns across models, heavy concentration on specific outlets, liberal bias in news citations, and minimal citations to low-credibility sources. User satisfaction was unaffected by political bias or source quality.

Conclusion: Current AI search systems exhibit biases and citation patterns, highlighting challenges for governance and model improvement in ensuring equitable access to diverse and high-quality information sources.

Abstract: AI-powered search systems are emerging as new information gatekeepers,
fundamentally transforming how users access news and information. Despite their
growing influence, the citation patterns of these systems remain poorly
understood. We address this gap by analyzing data from the AI Search Arena, a
head-to-head evaluation platform for AI search systems. The dataset comprises
over 24,000 conversations and 65,000 responses from models across three major
providers: OpenAI, Perplexity, and Google. Among the over 366,000 citations
embedded in these responses, 9% reference news sources. We find that while
models from different providers cite distinct news sources, they exhibit shared
patterns in citation behavior. News citations concentrate heavily among a small
number of outlets and display a pronounced liberal bias, though low-credibility
sources are rarely cited. User preference analysis reveals that neither the
political leaning nor the quality of cited news sources significantly
influences user satisfaction. These findings reveal significant challenges in
current AI search systems and have important implications for their design and
governance.

</details>


### [345] [PLACE: Prompt Learning for Attributed Community Search](https://arxiv.org/abs/2507.05311)
*Shuheng Fang,Kangfei Zhao,Rener Zhang,Yu Rong,Jeffrey Xu Yu*

Main category: cs.IR

TL;DR: PLACE introduces a graph prompt learning framework inspired by NLP, enabling effective attributed community search (ACS) through prompt-augmented graphs and a scalable divide-and-conquer strategy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve attributed community search (ACS) by leveraging ideas from NLP prompt-tuning to address challenges in handling structural cohesiveness and attribute similarity in query-dependent graph analysis.

Method: By integrating structural and learnable prompt tokens into graphs, PLACE creates prompt-augmented graphs optimized via alternating training. A divide-and-conquer scalability strategy enables processing of large graphs.

Result: PLACE significantly outperformed existing methods in attributed community search, achieving an average F1 score improvement of 22% across nine real-world datasets.

Conclusion: PLACE demonstrates the viability of adapting NLP prompt-tuning concepts for graph-based community search, offering notable improvements in both scalability and accuracy for ACS tasks.

Abstract: In this paper, we propose PLACE (Prompt Learning for Attributed Community
Search), an innovative graph prompt learning framework for ACS. Enlightened by
prompt-tuning in Natural Language Processing (NLP), where learnable prompt
tokens are inserted to contextualize NLP queries, PLACE integrates structural
and learnable prompt tokens into the graph as a query-dependent refinement
mechanism, forming a prompt-augmented graph. Within this prompt-augmented graph
structure, the learned prompt tokens serve as a bridge that strengthens
connections between graph nodes for the query, enabling the GNN to more
effectively identify patterns of structural cohesiveness and attribute
similarity related to the specific query. We employ an alternating training
paradigm to optimize both the prompt parameters and the GNN jointly. Moreover,
we design a divide-and-conquer strategy to enhance scalability, supporting the
model to handle million-scale graphs. Extensive experiments on 9 real-world
graphs demonstrate the effectiveness of PLACE for three types of ACS queries,
where PLACE achieves higher F1 scores by 22% compared to the state-of-the-arts
on average.

</details>


### [346] [Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA](https://arxiv.org/abs/2507.05577)
*Shashank Verma,Fengyi Jiang,Xiangning Xue*

Main category: cs.IR

TL;DR: The paper discusses a Retrieval-Augmented Generation (RAG) system designed for biomedical question answering using data from the BioASQ Task13b Challenge. The paper focuses on retrieval and answer generation methods and details the system's competitive performance.


<details>
  <summary>Details</summary>
Motivation: Biomedical semantic question answering can help researchers and users access knowledge from the rapidly evolving biomedical literature, making it essential to develop better information retrieval and answer generation systems.

Method: The authors created a RAG system combining dense embedding retrieval, re-ranking with cross-encoders and large language models (LLMs), and few-shot prompting of instruction-tuned LLMs for answer generation.

Result: The system achieved an MAP@10 of 0.1581 for document retrieval (10th place). It also reached macro-F1 of 0.95 for yes/no questions (12th), MRR of 0.64 for factoid questions (1st), mean-F1 of 0.63 for list questions (5th), and ROUGE-SU4 of 0.29 for ideal answers (11th).

Conclusion: The system demonstrates competitive performance on a variety of benchmarks in biomedical question answering, showcasing strengths in retrieval and specialized answer generation. However, there is room for improvement in certain aspects like document retrieval and ideal answers.

Abstract: Biomedical semantic question answering rooted in information retrieval can
play a crucial role in keeping up to date with vast, rapidly evolving and
ever-growing biomedical literature. A robust system can help researchers,
healthcare professionals and even layman users access relevant knowledge
grounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important
benchmark, offering a competitive platform for advancement of this space. This
paper presents the methodologies and results from our participation in this
challenge where we built a Retrieval-Augmented Generation (RAG) system that can
answer biomedical questions by retrieving relevant PubMed documents and
snippets to generate answers. For the retrieval task, we generated dense
embeddings from biomedical articles for initial retrieval, and applied an
ensemble of finetuned cross-encoders and large language models (LLMs) for
re-ranking to identify top relevant documents. Our solution achieved an MAP@10
of 0.1581, placing 10th on the leaderboard for the retrieval task. For answer
generation, we employed few-shot prompting of instruction-tuned LLMs. Our
system achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean
Reciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of
0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal
answers (rank 11).

</details>


### [347] [Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation](https://arxiv.org/abs/2507.05933)
*Y. Du*

Main category: cs.IR

TL;DR: The paper proposes a framework to predict query-level performance in vector retrieval systems, improving recall by 9.4% with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Current vector retrieval systems face performance variability due to inconsistencies in embedding quality, necessitating a method to predict and address these variations.

Method: The authors introduce a framework combining quantization robustness and neighborhood density metrics to assess embedding stability and improve retrieval predictions.

Result: Experimental evaluation on four datasets shows the method improves Recall@10 by 9.4±1.2% compared to baselines, with computational overhead under 5% of the retrieval time.

Conclusion: The proposed framework enhances retrieval strategies, identifies embedding quality patterns, and suggests directions for data augmentation, providing a practical and efficient solution.

Abstract: Vector retrieval systems exhibit significant performance variance across
queries due to heterogeneous embedding quality. We propose a lightweight
framework for predicting retrieval performance at the query level by combining
quantization robustness and neighborhood density metrics. Our approach is
motivated by the observation that high-quality embeddings occupy geometrically
stable regions in the embedding space and exhibit consistent neighborhood
structures. We evaluate our method on 4 standard retrieval datasets, showing
consistent improvements of 9.4$\pm$1.2\% in Recall@10 over competitive
baselines. The framework requires minimal computational overhead (less than 5\%
of retrieval time) and enables adaptive retrieval strategies. Our analysis
reveals systematic patterns in embedding quality across different query types,
providing insights for targeted training data augmentation.

</details>


### [348] [Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India](https://arxiv.org/abs/2507.06090)
*Swapnil Bhattacharyya,Shrey Ganatra,Harshvivek Kashid,Spandan Anaokar,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.IR

TL;DR: This paper introduces Nyay-Darpan, an AI framework for summarizing consumer case files and retrieving similar judgments, achieving over 75% accuracy in predictions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of AI tools in Indian consumer law and improve decision-making in consumer disputes.

Method: Developed a two-in-one framework for summarization and case retrieval, with innovative evaluation metrics for summary quality.

Result: The system achieved over 75% accuracy in similar case predictions and 70% accuracy in summary evaluations.

Conclusion: Nyay-Darpan effectively aids consumer dispute resolution and its release will foster further research in this domain.

Abstract: AI-based judicial assistance and case prediction have been extensively
studied in criminal and civil domains, but remain largely unexplored in
consumer law, especially in India. In this paper, we present Nyay-Darpan, a
novel two-in-one framework that (i) summarizes consumer case files and (ii)
retrieves similar case judgements to aid decision-making in consumer dispute
resolution. Our methodology not only addresses the gap in consumer law AI tools
but also introduces an innovative approach to evaluate the quality of the
summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',
symbolizing the ability of our tool to reflect the core of consumer disputes
through precise summarization and intelligent case retrieval. Our system
achieves over 75 percent accuracy in similar case prediction and approximately
70 percent accuracy across material summary evaluation metrics, demonstrating
its practical effectiveness. We will publicly release the Nyay-Darpan framework
and dataset to promote reproducibility and facilitate further research in this
underexplored yet impactful domain.

</details>


### [349] [When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs](https://arxiv.org/abs/2507.05733)
*Kechen Liu*

Main category: cs.IR

TL;DR: The paper introduces SASRecLLM, a framework combining SASRec and LLM fine-tuned with LoRA for improved sequential recommendations and hybrid training strategies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in recommendation systems, such as lack of domain-specific knowledge and collaborative signals.

Method: SASRecLLM integrates SASRec as a collaborative encoder with a fine-tuned LLM connected via a mapping layer, optimized with targeted training strategies.

Result: Experiments on multiple datasets show that SASRecLLM consistently outperforms strong baselines in cold-start and warm-start scenarios.

Conclusion: SASRecLLM effectively combines collaborative filtering with the semantic power of fine-tuned LLMs, advancing LLM-based recommendation systems.

Abstract: Self-Attentive Sequential Recommendation (SASRec) effectively captures
long-term user preferences by applying attention mechanisms to historical
interactions. Concurrently, the rise of Large Language Models (LLMs) has
motivated research into LLM-based recommendation, which leverages their
powerful generalization and language understanding capabilities. However, LLMs
often lack the domain-specific knowledge and collaborative signals essential
for high-quality recommendations when relying solely on textual prompts. To
address this limitation, this study proposes SASRecLLM, a novel framework that
integrates SASRec as a collaborative encoder with an LLM fine-tuned using
Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to
align their dimensional spaces, and three targeted training strategies are
designed to optimize the hybrid architecture. Extensive experiments on multiple
datasets demonstrate that SASRecLLM achieves robust and consistent improvements
over strong baselines in both cold-start and warm-start scenarios. This work
advances the field of LLM-based recommendation by presenting a modular and
effective paradigm for fusing structured collaborative filtering with the
semantic power of fine-tuned LLMs. The implementation is available on GitHub:
https://github.com/kechenkristin/RecLLM

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [350] [Solar Flare Prediction Using LSTM and DLSTM with Sliding Window Pattern Recognition](https://arxiv.org/abs/2507.05313)
*Zeinab Hassani,Davud Mohammadpur,Hossein Safari*

Main category: astro-ph.SR

TL;DR: The study investigates using LSTM and Decomposition-LSTM (DLSTM), combined with an ensemble approach, for predicting solar flares using GOES dataset from 2003 to 2023, achieving strong performance metrics (e.g., TSS=0.74, Recall=0.95).


<details>
  <summary>Details</summary>
Motivation: Reliable and accurate solar flare prediction is challenging due to the Sun's complex, self-organized criticality-driven behavior and is essential for understanding solar activity and mitigating related risks.

Method: The paper employs LSTM and DLSTM models on time-series data (peak fluxes and waiting times), combined with resampling for class imbalance and a sliding window approach on regularized series to detect quasi-patterns. DLSTM performs trend and seasonal decomposition to enhance model accuracy.

Result: The DLSTM model, particularly with an ensemble approach for regularized time series, achieves high performance: TSS=0.74, recall=0.95, and AUC=0.87, outperforming models on irregular time series by reducing false errors.

Conclusion: The DLSTM approach, especially with decomposition and ensemble modeling, effectively predicts solar flares and demonstrates the importance of advanced machine learning and resampling strategies for improving long-term forecasts.

Abstract: We investigate the use of Long Short-Term Memory (LSTM) and
Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to
predict solar flare occurrences using time-series data from the GOES catalog.
The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among
approximately possible patterns, 7,552 yearly pattern windows are identified,
highlighting the challenge of long-term forecasting due to the Sun's complex,
self-organized criticality-driven behavior. A sliding window technique is
employed to detect temporal quasi-patterns in both irregular and regularized
flare time series. Regularization reduces complexity, enhances large flare
activity, and captures active days more effectively. To address class
imbalance, resampling methods are applied. LSTM and DLSTM models are trained on
sequences of peak fluxes and waiting times from irregular time series, while
LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding
windows of regularized time series with a 3-hour interval. Performance metrics,
particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87)
in the receiver operating characteristic (ROC), indicate that DLSTM with an
ensemble approach on regularized time series outperforms other models, offering
more accurate large-flare forecasts with fewer false errors compared to models
trained on irregular time series. The superior performance of DLSTM is
attributed to its ability to decompose time series into trend and seasonal
components, effectively isolating random noise. This study underscores the
potential of advanced machine learning techniques for solar flare prediction
and highlights the importance of incorporating various solar cycle phases and
resampling strategies to enhance forecasting reliability.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [351] [The Neural Networks with Tensor Weights and the Corresponding Fermionic Quantum Field Theory](https://arxiv.org/abs/2507.05303)
*Guojun Huang,Kai Zhou*

Main category: hep-th

TL;DR: This paper demonstrates a connection between complex-valued neural networks (CVNNs) and fermionic quantum field theory, extending the neural network quantum field theory (NN-QFT) framework to fermionic systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to bridge the gap in NN-QFT, which thus far primarily focused on linking neural network architectures to bosonic quantum field theories, leaving fermionic systems unaddressed.

Method: The authors employ complex-valued neural networks with tensor-valued weights. By promoting weights to Clifford algebra-valued tensors, they induce anticommutation relations that embody fermionic statistics and analyze the generating functional to derive exact quantum states.

Result: The study reveals a theoretical mapping where input-to-hidden layer parameters link to quantum system eigenvalues, and hidden-to-output tensor weights correspond to fermionic fields. They reproduce free fermion correlators and diagrammatic expansions that confirm anticommutation relations.

Conclusion: This work establishes a groundbreaking connection between neural networks and fermionic QFT, enabling the encoding of fermionic symmetries in machine learning, with potential implications for quantum simulation and lattice field theory.

Abstract: In this paper, we establish a theoretical connection between complex-valued
neural networks (CVNNs) and fermionic quantum field theory (QFT), bridging a
fundamental gap in the emerging framework of neural network quantum field
theory (NN-QFT). While prior NN-QFT works have linked real-valued architectures
to bosonic fields, we demonstrate that CVNNs equipped with tensor-valued
weights intrinsically generate fermionic quantum fields. By promoting
hidden-to-output weights to Clifford algebra-valued tensors, we induce
anticommutation relations essential for fermionic statistics. Through
analytical study of the generating functional, we obtain the exact quantum
state in the infinite-width limit, revealing that the parameters between the
input layer and the last hidden layer correspond to the eigenvalues of the
quantum system, and the tensor weighting parameters in the hidden-to-output
layer map to dynamical fermionic fields. The continuum limit reproduces free
fermion correlators, with diagrammatic expansions confirming anticommutation.
The work provides the first explicit mapping from neural architectures to
fermionic QFT at the level of correlation functions and generating functional.
It extends NN-QFT beyond bosonic theories and opens avenues for encoding
fermionic symmetries into machine learning models, with potential applications
in quantum simulation and lattice field theory.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [352] [Critical Nodes Identification in Complex Networks: A Survey](https://arxiv.org/abs/2507.06164)
*Duxin Chen,Jiawen Chen,Xiaoyu Zhang,Qinghan Jia,Xiaolu Liu,Ye Sun,Linyuan Lv,Wenwu Yu*

Main category: cs.SI

TL;DR: The paper reviews and categorizes techniques for identifying critical nodes in complex networks, highlighting advances and challenges across varied network types.


<details>
  <summary>Details</summary>
Motivation: To address challenges in identifying critical nodes in complex networks due to structural heterogeneity and dynamic, higher-order features.

Method: The study categorizes techniques into seven classes, evaluates them systematically, and highlights gaps based on methodology and applicability.

Result: Techniques are evaluated for their strengths, limitations, and applicability, revealing challenges such as computational efficiency and scalability.

Conclusion: The paper consolidates progress and identifies open questions for future research including real-time evaluation, dynamic modeling, and scalable algorithms.

Abstract: Complex networks have become essential tools for understanding diverse
phenomena in social systems, traffic systems, biomolecular systems, and
financial systems. Identifying critical nodes is a central theme in
contemporary research, serving as a vital bridge between theoretical
foundations and practical applications. Nevertheless, the intrinsic complexity
and structural heterogeneity characterizing real-world networks, with
particular emphasis on dynamic and higher-order networks, present substantial
obstacles to the development of universal frameworks for critical node
identification. This paper provides a comprehensive review of critical node
identification techniques, categorizing them into seven main classes:
centrality, critical nodes deletion problem, influence maximization, network
control, artificial intelligence, higher-order and dynamic methods. Our review
bridges the gaps in existing surveys by systematically classifying methods
based on their methodological foundations and practical implications, and by
highlighting their strengths, limitations, and applicability across different
network types. Our work enhances the understanding of critical node research by
identifying key challenges, such as algorithmic universality, real-time
evaluation in dynamic networks, analysis of higher-order structures, and
computational efficiency in large-scale networks. The structured synthesis
consolidates current progress and highlights open questions, particularly in
modeling temporal dynamics, advancing efficient algorithms, integrating machine
learning approaches, and developing scalable and interpretable metrics for
complex systems.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [353] [Minimal Deterministic Echo State Networks Outperform Random Reservoirs in Learning Chaotic Dynamics](https://arxiv.org/abs/2507.06050)
*Francesco Martinuzzi*

Main category: nlin.CD

TL;DR: This study demonstrates that Echo State Networks (ESNs) with deterministic construction and simple topologies outperform traditional ESNs for chaotic attractor reconstruction, with improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Improve ESN performance by reducing sensitivity to hyperparameter selection and stochastic initialization in order to better model chaotic systems.

Method: The authors benchmarked 10 types of deterministic minimal reservoir initializations (MESNs) across a dataset of over 90 chaotic systems to compare their performance against traditional stochastic ESNs.

Result: MESNs achieved up to 41% error reduction, greater robustness with less variation across runs, and the ability to reuse hyperparameters across systems compared to standard ESNs.

Conclusion: Structured deterministic approaches to ESN design can surpass stochastic methods in learning chaotic dynamics, offering better performance and reliability.

Abstract: Machine learning (ML) is widely used to model chaotic systems. Among ML
approaches, echo state networks (ESNs) have received considerable attention due
to their simple construction and fast training. However, ESN performance is
highly sensitive to hyperparameter choices and to its random initialization. In
this work, we demonstrate that ESNs constructed using deterministic rules and
simple topologies (MESNs) outperform standard ESNs in the task of chaotic
attractor reconstruction. We use a dataset of more than 90 chaotic systems to
benchmark 10 different minimal deterministic reservoir initializations. We find
that MESNs obtain up to a 41% reduction in error compared to standard ESNs.
Furthermore, we show that the MESNs are more robust, exhibiting less inter-run
variation, and have the ability to reuse hyperparameters across different
systems. Our results illustrate how structured simplicity in ESN design can
outperform stochastic complexity in learning chaotic dynamics.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [354] [What ZTF Saw Where Rubin Looked: Anomaly Hunting in DR23](https://arxiv.org/abs/2507.06217)
*Maria V. Pruzhinskaya,Anastasia D. Lavrukhina,Timofey A. Semenikhi,Alina A. Volnova,Sreevarsha Sreejith,Vadim V. Krushinsky,Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Konstantin L. Malanchev*

Main category: astro-ph.IM

TL;DR: The study uses the SNAD anomaly detection pipeline with the PineForest algorithm to analyze ZTF fields also observed by LSSTComCam, discovering six new variable stars and refining details for six known ones.


<details>
  <summary>Details</summary>
Motivation: To systematically search for anomalies in ZTF fields, overlapping with LSSTComCam observations, to demonstrate the capabilities of the SNAD pipeline.

Method: The PineForest active anomaly detection algorithm was applied to four chosen galactic and extragalactic fields, and 400 candidates were visually inspected.

Result: Six new variable stars were discovered, including types like RS CVn, BY Draconis, and solar-type variables; classifications and periods of six known objects were refined.

Conclusion: The SNAD anomaly detection pipeline is effective, showcasing its potential for discoveries in future LSST data.

Abstract: We present results from the SNAD VIII Workshop, during which we conducted the
first systematic anomaly search in the ZTF fields also observed by LSSTComCam
during Rubin Scientific Pipeline commissioning. Using the PineForest active
anomaly detection algorithm, we analysed four selected fields (two galactic and
two extragalactic) and visually inspected 400 candidates. As a result, we
discovered six previously uncatalogued variable stars, including RS~CVn, BY
Draconis, ellipsoidal, and solar-type variables, and refined classifications
and periods for six known objects. These results demonstrate the effectiveness
of the SNAD anomaly detection pipeline and provide a preview of the discovery
potential in the upcoming LSST data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [355] [iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips](https://arxiv.org/abs/2507.05576)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed Badawy,Ahmad Patooghy*

Main category: cs.CR

TL;DR: This paper addresses evasive thermal Trojan attacks on System-on-Chips (SoCs) and introduces an advanced detection technique to counter them.


<details>
  <summary>Details</summary>
Motivation: The paper aims to mitigate the threat posed by increasingly evasive thermal Trojan attacks that compromise the security and reliability of SoCs in mobile devices.

Method: It introduces Intermittent Thermal Trojans (iThermTroj) as a novel attack model and proposes tiny Machine Learning classifiers for runtime anomaly detection.

Result: The proposed approach improves detection rates by up to 29.4% in various scenarios and increases protection resolution to 0.8 degrees Celsius with 100% accuracy for detection.

Conclusion: The study highlights the vulnerabilities of SoCs to novel intermittent attacks and provides an effective ML-based solution that significantly enhances security and reliability.

Abstract: Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.

</details>


### [356] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu,Shravya Kanchi,Sifat Muhammad Abdullah,Nicholas Kong,Daphne Yao,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CR

TL;DR: The paper introduces TuneShield, a framework that mitigates toxicity during chatbot fine-tuning, leveraging LLM-based toxicity classification and synthetic 'healing data'. It preserves conversational quality and withstands various attacks.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of preventing toxicity in chatbots when customizing large language models (LLMs) with potentially untrusted datasets.

Method: TuneShield uses LLM-based toxicity classification to identify toxic data and generates synthetic 'healing data' from it. This data is employed during fine-tuning to diminish toxicity and enhance desirable behavior.

Result: TuneShield effectively reduces toxicity in chatbots while maintaining conversational quality, even when the toxicity classifiers are flawed or biased. It also resists adversarial and jailbreak attacks.

Conclusion: TuneShield is a robust solution for mitigating toxicity during chatbot customization, effectively balancing safety and conversational performance.

Abstract: Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


### [357] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: The paper surveys model extraction attacks on language models and offers a taxonomy of attacks and defenses, alongside recommendations for securing these models.


<details>
  <summary>Details</summary>
Motivation: To address the growing security threats to deployed language models caused by extraction attacks, compromising intellectual property and user privacy.

Method: The paper categorizes attacks into types like functionality, data extraction, and prompt-targeted attacks. It also examines and evaluates defense strategies and proposes metrics for assessing both attacks and protections.

Result: The paper identifies strengths and weaknesses of current strategies and highlights the need for integrated and adaptive techniques to protect language models.

Conclusion: A systematic taxonomy and evaluation of attacks and defenses can guide researchers and professionals in mitigating risks to language models in production.

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [358] [Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice](https://arxiv.org/abs/2507.05512)
*Gehao Zhang,Eugene Bagdasarian,Juan Zhai,Shiqing Ma*

Main category: cs.CR

TL;DR: The paper studies the robustness of N-gram-based watermarking schemes for distinguishing AI-generated code against code obfuscation techniques. The study finds that these watermarking schemes are highly vulnerable to obfuscation attacks, and their detection abilities deteriorate significantly.


<details>
  <summary>Details</summary>
Motivation: Understanding the robustness of watermarking schemes is essential due to their applications in authorship attribution, misuse detection, and intellectual property protection. While N-gram-based watermarking is widely used, its resilience against sophisticated obfuscation techniques is underexplored.

Method: The authors formally model code obfuscation, establish the theoretical limitations of N-gram-based watermarking schemes, and conduct experiments using three SOTA watermarking schemes, two LLMs, two programming languages, multiple code benchmarks, and obfuscators.

Result: The study concludes that watermarking detectors' performance significantly drops under obfuscation attacks, with AUROC values approximating 0.5, equivalent to random guessing. Both programming languages used in the experiments show obfuscators achieving strong attack effects.

Conclusion: The paper demonstrates the inherent vulnerability of N-gram-based watermarking to obfuscation transformations and suggests possible pathways to develop robust watermarking techniques for code.

Abstract: Distinguishing AI-generated code from human-written code is becoming crucial
for tasks such as authorship attribution, content tracking, and misuse
detection. Based on this, N-gram-based watermarking schemes have emerged as
prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated.
Most claims rely solely on defenses against simple code transformations or code
optimizations as a simulation of attack, creating a questionable sense of
robustness. In contrast, more sophisticated schemes already exist in the
software engineering world, e.g., code obfuscation, which significantly alters
code while preserving functionality. Although obfuscation is commonly used to
protect intellectual property or evade software scanners, the robustness of
code watermarking techniques against such transformations remains largely
unexplored.
  In this work, we formally model the code obfuscation and prove the
impossibility of N-gram-based watermarking's robustness with only one intuitive
and experimentally verified assumption, distribution consistency, satisfied.
Given the original false positive rate of the watermarking detection, the ratio
that the detector failed on the watermarked code after obfuscation will
increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two
LLMs, two programming languages, four code benchmarks, and four obfuscators.
Among them, all watermarking detectors show coin-flipping detection abilities
on obfuscated codes (AUROC tightly surrounds 0.5). Among all models,
watermarking schemes, and datasets, both programming languages own obfuscators
that can achieve attack effects with no detection AUROC higher than 0.6 after
the attack. Based on the theoretical and practical observations, we also
proposed a potential path of robust code watermarking.

</details>


### [359] [DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective](https://arxiv.org/abs/2507.05622)
*Shuo Shao,Yiming Li,Mengren Zheng,Zhiyang Hu,Yukun Chen,Boheng Li,Yu He,Junfeng Guo,Tianwei Zhang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: This paper evaluates dataset auditing methods against adversarial attacks using a new benchmark, DATABench, and finds existing methods lack robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency in dataset usage, which raises privacy and copyright concerns, especially under adversarial conditions.

Method: The study introduces a new taxonomy for auditing methods, formulates attack strategies (evasion and forgery), and proposes a comprehensive benchmark, DATABench.

Result: The analysis shows that current dataset auditing methods are neither robust nor distinctive when faced with adversarial attacks.

Conclusion: The findings call for developing more secure and reliable dataset auditing methods to ensure transparency in dataset usage despite adversarial manipulation.

Abstract: The widespread application of Deep Learning across diverse domains hinges
critically on the quality and composition of training datasets. However, the
common lack of disclosure regarding their usage raises significant privacy and
copyright concerns. Dataset auditing techniques, which aim to determine if a
specific dataset was used to train a given suspicious model, provide promising
solutions to addressing these transparency gaps. While prior work has developed
various auditing methods, their resilience against dedicated adversarial
attacks remains largely unexplored. To bridge the gap, this paper initiates a
comprehensive study evaluating dataset auditing from an adversarial
perspective. We start with introducing a novel taxonomy, classifying existing
methods based on their reliance on internal features (IF) (inherent to the
data) versus external features (EF) (artificially introduced for auditing).
Subsequently, we formulate two primary attack types: evasion attacks, designed
to conceal the use of a dataset, and forgery attacks, intending to falsely
implicate an unused dataset. Building on the understanding of existing methods
and attack objectives, we further propose systematic attack strategies:
decoupling, removal, and detection for evasion; adversarial example-based
methods for forgery. These formulations and strategies lead to our new
benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9
representative auditing methods. Extensive evaluations using DATABench reveal
that none of the evaluated auditing methods are sufficiently robust or
distinctive under adversarial settings. These findings underscore the urgent
need for developing a more secure and reliable dataset auditing method capable
of withstanding sophisticated adversarial manipulation. Code is available at
https://github.com/shaoshuo-ss/DATABench.

</details>


### [360] [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)
*Sarthak Choudhary,Divyam Anshumaan,Nils Palumbo,Somesh Jha*

Main category: cs.CR

TL;DR: The paper identifies a vulnerability in known-answer detection (KAD) frameworks meant to protect LLMs from prompt injection attacks, and proposes an adaptive attack, DataFlip, which evades these defenses effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the security risks of Language Model (LLM)-integrated applications susceptible to prompt injection attacks, and to test the robustness of KAD defenses.

Method: The authors formalized the KAD framework, identified a critical design vulnerability, and developed an adaptive attack named DataFlip to exploit this weakness without white-box LLM access or optimization processes.

Result: The DataFlip attack successfully circumvented KAD detection, with detection rates as low as 1.5% and success rates for malicious behavior up to 88%.

Conclusion: The paper concludes that KAD-based defenses are structurally vulnerable and ineffective against adaptive attacks like DataFlip, highlighting the need for more robust security measures.

Abstract: LLM-integrated applications and agents are vulnerable to prompt injection
attacks, in which adversaries embed malicious instructions within seemingly
benign user inputs to manipulate the LLM's intended behavior. Recent defenses
based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect
performance by using an LLM to classify inputs as clean or contaminated. In
this work, we formally characterize the KAD framework and uncover a structural
vulnerability in its design that invalidates its core security premise. We
design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this
fundamental weakness. It consistently evades KAD defenses with detection rates
as low as $1.5\%$ while reliably inducing malicious behavior with success rates
of up to $88\%$, without needing white-box access to the LLM or any
optimization procedures.

</details>


### [361] [DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning](https://arxiv.org/abs/2507.05649)
*Kaixiang Zhao,Joseph Yousry Attalla,Qian Lou,Yushun Dong*

Main category: cs.CR

TL;DR: DESIGN is a framework for efficient and secure Graph Neural Network (GNN) inference under Fully Homomorphic Encryption (FHE), emphasizing input graph pruning and adaptive computation to reduce overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing Fully Homomorphic Encryption (FHE) methods for GNNs, which struggle with high computational costs and overlook input data redundancy, making real-time and privacy-preserving inference impractical.

Method: DESIGN introduces a hierarchical optimization strategy. It computes node importance scores (from encrypted graph statistics) to guide graph pruning and employs adaptive polynomial activation schemes based on node importance, all performed under FHE.

Result: The proposed DESIGN framework significantly improves the performance of FHE GNN inference, offering substantial speedups over state-of-the-art methods, while preserving competitive accuracy.

Conclusion: DESIGN provides an efficient and practical solution for privacy-preserving GNN inference, overcoming the computational challenges of FHE and enabling secure and real-time graph analytics.

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph-based learning tasks. However, enabling privacy-preserving GNNs
in encrypted domains, such as under Fully Homomorphic Encryption (FHE),
typically incurs substantial computational overhead, rendering real-time and
privacy-preserving inference impractical. In this work, we propose DESIGN
(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel
framework for efficient encrypted GNN inference. DESIGN tackles the critical
efficiency limitations of existing FHE GNN approaches, which often overlook
input data redundancy and apply uniform computational strategies. Our framework
achieves significant performance gains through a hierarchical optimization
strategy executed entirely on the server: first, FHE-compatible node importance
scores (based on encrypted degree statistics) are computed from the encrypted
graph. These scores then guide a homomorphic partitioning process, generating
multi-level importance masks directly under FHE. This dynamically generated
mask facilitates both input graph pruning (by logically removing unimportant
elements) and a novel adaptive polynomial activation scheme, where activation
complexity is tailored to node importance levels. Empirical evaluations
demonstrate that DESIGN substantially accelerates FHE GNN inference compared to
state-of-the-art methods while maintaining competitive model accuracy,
presenting a robust solution for secure graph analytics.

</details>


### [362] [AI Agent Smart Contract Exploit Generation](https://arxiv.org/abs/2507.05558)
*Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: The paper introduces A1, a system leveraging LLMs for autonomous vulnerability detection and exploitation in smart contracts, achieving high success rates and profitability in both analysis and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop an autonomous system that uses AI capabilities to efficiently discover and exploit vulnerabilities in smart contracts on blockchain platforms.

Method: A1 integrates six domain-specific tools with LLMs to autonomously analyze, test, and exploit vulnerabilities in smart contracts. The process involves iterative execution and refinement based on feedback.

Result: A1 achieved a 62.96% success rate on the VERITE benchmark, discovered additional vulnerabilities post-training cutoff, and extracted up to $9.33 million USD across cases. Experimental results show diminishing returns in iterative performance improvement, while demonstrating profitability for attackers at lower thresholds compared to defenders.

Conclusion: A1 reveals an asymmetry in favor of attackers regarding the cost-effectiveness and success of exploit generation versus defensive measures, sparking concerns about AI's potential imbalance between offense and defense in blockchain security.

Abstract: We present A1, an agentic execution driven system that transforms any LLM
into an end-to-end exploit generator. A1 has no hand-crafted heuristics and
provides the agent with six domain-specific tools that enable autonomous
vulnerability discovery. The agent can flexibly leverage these tools to
understand smart contract behavior, generate exploit strategies, test them on
blockchain states, and refine approaches based on execution feedback. All
outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and
Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the
VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional
vulnerable contracts, with 5 cases occurring after the strongest model's
training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59
million USD per case and 9.33 million USD total. Through 432 experiments across
six LLMs, we analyze iteration-wise performance showing diminishing returns
with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations
2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo
analysis of 19 historical attacks shows success probabilities of 85.9%-88.8%
without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying
A1 as a continuous on-chain scanning system. Our model shows that OpenAI's
o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%
vulnerability incidence rates, while faster models require >=1.000% rates to
break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability
rates, attackers achieve an on-chain scanning profitability at a $6000 exploit
value, while defenders require $60000, raising fundamental questions about
whether AI agents inevitably favor exploitation over defense.

</details>


### [363] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked,Nan Messe*

Main category: cs.CR

TL;DR: Proposes a formal automated reasoning tool to systematize vulnerability management and security control design in systems.


<details>
  <summary>Details</summary>
Motivation: Vulnerability management is essential for securing systems, but current approaches lack systematic reasoning about vulnerabilities in system designs.

Method: Develops a formally grounded automated reasoning mechanism integrated into an existing security design tool.

Result: The mechanism helps system designers identify vulnerabilities, specify mitigation measures, declare controls, and manage vulnerability postures systematically.

Conclusion: The proposed reasoning tool enhances the systematic management of vulnerabilities and supports proactive security control design in system architectures.

Abstract: For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [364] [Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI](https://arxiv.org/abs/2507.06092)
*Shravya Kanchi,Neal Mangaokar,Aravind Cheruvu,Sifat Muhammad Abdullah,Shirin Nilizadeh,Atul Prakash,Bimal Viswanath*

Main category: cs.CR

TL;DR: The paper explores using Generative AI (GenAI) to augment training datasets for security classifiers, achieving up to 32.6% improvement in performance even with limited data, though some challenges remain.


<details>
  <summary>Details</summary>
Motivation: Supervised classifiers for security tasks suffer from data-related challenges, impacting performance, and the paper investigates whether GenAI can address these issues.

Method: The study augmented training datasets with synthetic data generated using 6 GenAI methods and introduced a novel scheme, Nimai, to evaluate classifier performance across various security tasks.

Result: GenAI techniques led to performance improvements of up to 32.6%, even with limited training data (around 180 samples), and helped in adapting to concept drift with minimal labeling.

Conclusion: The approach shows promise for improving security classifiers and adapting to evolving data contexts, despite challenges such as noisy labels and overlapping class distributions.

Abstract: Machine learning-based supervised classifiers are widely used for security
tasks, and their improvement has been largely focused on algorithmic
advancements. We argue that data challenges that negatively impact the
performance of these classifiers have received limited attention. We address
the following research question: Can developments in Generative AI (GenAI)
address these data challenges and improve classifier performance? We propose
augmenting training datasets with synthetic data generated using GenAI
techniques to improve classifier generalization. We evaluate this approach
across 7 diverse security tasks using 6 state-of-the-art GenAI methods and
introduce a novel GenAI scheme called Nimai that enables highly controlled data
synthesis. We find that GenAI techniques can significantly improve the
performance of security classifiers, achieving improvements of up to 32.6% even
in severely data-constrained settings (only ~180 training samples).
Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to
concept drift post-deployment, requiring minimal labeling in the adjustment
process. Despite successes, our study finds that some GenAI schemes struggle to
initialize (train and produce data) on certain security tasks. We also identify
characteristics of specific tasks, such as noisy labels, overlapping class
distributions, and sparse feature vectors, which hinder performance boost using
GenAI. We believe that our study will drive the development of future GenAI
tools designed for security tasks.

</details>


### [365] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Jan Mendling,Minseok Song*

Main category: cs.CR

TL;DR: This paper proposes a method to balance privacy and utility in event log anonymization by combining anonymization with event data partitioning.


<details>
  <summary>Details</summary>
Motivation: Event logs in information systems contain sensitive data, necessitating anonymization while maintaining their utility for process discovery.

Method: The pipeline uses event abstraction to segment event logs into sub-logs, which are anonymized separately to manage the trade-off between privacy and utility.

Result: The proposed method improves process discovery utility for directly-follows-based anonymization techniques, validated via experiments on real-world event logs and process discovery approaches.

Conclusion: Event partitioning coupled with anonymization is effective in enhancing privacy while preserving the utility of event logs for process discovery.

Abstract: Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


### [366] [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043)
*Xiaohu Li,Yunfeng Ning,Zepeng Bao,Mayi Xu,Jianhao Chen,Tieyun Qian*

Main category: cs.CR

TL;DR: This paper introduces a combined framework using GANs to analyze and improve LLM security by addressing both jailbreak attacks and defenses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vulnerabilities in LLM security mechanisms exposed by jailbreak attack methods and enhance model security.

Method: The method involves leveraging the linearly separable property of intermediate LLM embeddings and employing GANs to learn the internal security judgment boundaries for attacks and defenses.

Result: The method achieved an average jailbreak success rate of 88.85% and defense success rate of 84.17% across three popular LLMs and state-of-the-art datasets.

Conclusion: The findings validate the approach and provide insights into LLM security mechanisms, offering novel strategies for improving security against malicious queries.

Abstract: Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [367] [Assessing Linear Control Strategies for Zero-Speed Fin Roll Damping](https://arxiv.org/abs/2507.05867)
*Nikita Savin,Elena Ambrosovskaya,Dmitry Romaev,Anton Proskurnikov*

Main category: eess.SY

TL;DR: This paper presents a roll damping system using active zero-speed fins for effective stabilization at low or zero-speed, tested via simulations on a vessel model.


<details>
  <summary>Details</summary>
Motivation: To address the ineffectiveness of traditional hydrodynamic fins in roll stabilization for vessels at low-speed or zero-speed conditions.

Method: The paper proposes a linear control architecture accounting for nonlinear drag forces and actuator constraints, implemented on actively controlled zero-speed fins.

Result: Simulation results on a high-fidelity vessel model show that the proposed approach effectively stabilizes roll motion.

Conclusion: Active zero-speed fins with the linear control system are a viable solution for roll stabilization, even under challenging conditions like zero vessel speed.

Abstract: Roll stabilization is a critical aspect of ship motion control, particularly
for vessels operating in low-speed or zero-speed conditions, where traditional
hydrodynamic fins lose their effectiveness. In this paper, we consider a roll
damping system, developed by Navis JSC, based on two actively controlled
zero-speed fins. Unlike conventional fin stabilizers, zero-speed fins employ a
drag-based mechanism and active oscillations to generate stabilizing forces
even when the vessel is stationary. We propose a simple linear control
architecture that, however, accounts for nonlinear drag forces and actuator
limitations. Simulation results on a high-fidelity vessel model used for HIL
testing demonstrate the effectiveness of the proposed approach.

</details>


### [368] [Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning](https://arxiv.org/abs/2507.05785)
*Jian Kai,Tianwei Zhang,Zihan Ling,Yang Cao,Can Shen*

Main category: eess.SY

TL;DR: RBWE is a robust bandwidth estimation framework using offline reinforcement learning techniques to enhance real-time communication (RTC).


<details>
  <summary>Details</summary>
Motivation: Improve bandwidth estimation in RTC applications where traditional and online RL methods face adaptability and cost challenges.

Method: RBWE integrates Q-ensembles and Gaussian mixture policies to address out-of-distribution risks, with a fallback mechanism to maintain system stability.

Result: RBWE reduces overestimation errors by 18% and improves the 10th percentile QoE by 18.6%, showcasing better performance.

Conclusion: RBWE proves effective for bandwidth estimation in dynamic networks, leveraging offline RL for real-world RTC applications.

Abstract: Accurate bandwidth estimation (BWE) is critical for real-time communication
(RTC) systems. Traditional heuristic approaches offer limited adaptability
under dynamic networks, while online reinforcement learning (RL) suffers from
high exploration costs and potential service disruptions. Offline RL, which
leverages high-quality data collected from real-world environments, offers a
promising alternative. However, challenges such as out-of-distribution (OOD)
actions, policy extraction from behaviorally diverse datasets, and reliable
deployment in production systems remain unsolved. We propose RBWE, a robust
bandwidth estimation framework based on offline RL that integrates Q-ensemble
(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD
risks and enhance policy learning. A fallback mechanism ensures deployment
stability by switching to heuristic methods under high uncertainty.
Experimental results show that RBWE reduces overestimation errors by 18% and
improves the 10th percentile Quality of Experience (QoE) by 18.6%,
demonstrating its practical effectiveness in real-world RTC applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [369] [Towards Serverless Processing of Spatiotemporal Big Data Queries](https://arxiv.org/abs/2507.06005)
*Diana Baumann,Tim C. Rese,David Bermbach*

Main category: cs.DB

TL;DR: The paper proposes a serverless data processing approach for improved scalability in big spatiotemporal data analysis.


<details>
  <summary>Details</summary>
Motivation: Existing systems for spatiotemporal data analysis struggle with scalability despite many queries being highly parallelizable.

Method: The authors suggest leveraging Function-as-a-Service platforms to break down queries into smaller subqueries, enabling rapid and parallel execution.

Result: Their approach partially addresses the scalability challenges in big spatiotemporal data processing.

Conclusion: Serverless data processing offers a promising solution for scaling spatiotemporal data analysis effectively.

Abstract: Spatiotemporal data are being produced in continuously growing volumes by a
variety of data sources and a variety of application fields rely on rapid
analysis of such data. Existing systems such as PostGIS or MobilityDB usually
build on relational database systems, thus, inheriting their scale-out
characteristics. As a consequence, big spatiotemporal data scenarios still have
limited support even though many query types can easily be parallelized. In
this paper, we propose our vision of a native serverless data processing
approach for spatiotemporal data: We break down queries into small subqueries
which then leverage the near-instant scaling of Function-as-a-Service platforms
to execute them in parallel. With this, we partially solve the scalability
needs of big spatiotemporal data processing.

</details>


### [370] [Prompt Migration: Stabilizing GenAI Applications with Evolving Large Language Models](https://arxiv.org/abs/2507.05573)
*Shivani Tripathi,Pushpanjali Nema,Aditya Halder,Shi Qiao,Alekh Jindal*

Main category: cs.DB

TL;DR: The paper introduces a systematic approach called "prompt migration" to stabilize business applications using generative AI amid changes in large language models (LLMs), ensuring reliability through redesign and testing.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of maintaining consistency and reliability in generative AI business applications affected by rapid LLM evolution.

Method: The authors proposed a "prompt migration" framework, incorporating prompt redesign and a migration testbed, and validated this approach using the Tursio enterprise search application as a case study.

Result: The study demonstrated that prompt migration techniques successfully restored application consistency and reliability after disruptions caused by changes in LLMs.

Conclusion: The paper recommends adopting structured prompt lifecycle management and robust testing to ensure the dependable operation of GenAI-based applications.

Abstract: Generative AI is transforming business applications by enabling natural
language interfaces and intelligent automation. However, the underlying large
language models (LLMs) are evolving rapidly and so prompting them consistently
is a challenge. This leads to inconsistent and unpredictable application
behavior, undermining the reliability that businesses require for
mission-critical workflows. In this paper, we introduce the concept of prompt
migration as a systematic approach to stabilizing GenAI applications amid
changing LLMs. Using the Tursio enterprise search application as a case study,
we analyze the impact of successive GPT model upgrades, detail our migration
framework including prompt redesign and a migration testbed, and demonstrate
how these techniques restore application consistency. Our results show that
structured prompt migration can fully recover the application reliability that
was lost due to model drift. We conclude with practical lessons learned,
emphasizing the need for prompt lifecycle management and robust testing to
ensure dependable GenAI-powered business applications.

</details>


### [371] [SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads](https://arxiv.org/abs/2507.06192)
*Jiale Lao,Immanuel Trummer*

Main category: cs.DB

TL;DR: SQLBarber utilizes Large Language Models (LLMs) to generate realistic and customized SQL workloads tailored to user-defined specifications, overcoming privacy constraints and inefficiencies of prior methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in acquiring real-world SQL queries due to privacy concerns and the limitations of existing generation methods in customization and constraints satisfaction.

Method: SQLBarber employs a declarative interface, an LLM-powered pipeline with self-correction capabilities, and a Bayesian Optimizer to generate SQL templates based on real-world execution stats and user-defined cost distributions.

Result: SQLBarber significantly reduces query generation time (by one to three orders of magnitude) and improves alignment with target cost distributions, outperforming existing methods.

Conclusion: SQLBarber effectively generates customized and realistic SQL benchmarks, making it a practical solution for database research and development tasks.

Abstract: Database research and development often require a large number of SQL queries
for benchmarking purposes. However, acquiring real-world SQL queries is
challenging due to privacy concerns, and existing SQL generation methods are
limited in customization and in satisfying realistic constraints. To address
this issue, we present SQLBarber, a system based on Large Language Models
(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)
eliminates the need for users to manually craft SQL templates in advance, while
providing the flexibility to accept natural language specifications to
constrain SQL templates, (ii) scales efficiently to generate large volumes of
queries matching any user-defined cost distribution (e.g., cardinality and
execution plan cost), and (iii) uses execution statistics from Amazon Redshift
and Snowflake to derive SQL template specifications and query cost
distributions that reflect real-world query characteristics. SQLBarber
introduces (i) a declarative interface for users to effortlessly generate
customized SQL templates, (ii) an LLM-powered pipeline augmented with a
self-correction module that profiles, refines, and prunes SQL templates based
on query costs, and (iii) a Bayesian Optimizer to efficiently explore different
predicate values and identify a set of queries that satisfy the target cost
distribution. We construct and open-source ten benchmarks of varying difficulty
levels and target query cost distributions based on real-world statistics from
Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show
that SQLBarber is the only system that can generate customized SQL templates.
It reduces query generation time by one to three orders of magnitude, and
significantly improves alignment with the target cost distribution, compared
with existing methods.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [372] [seMCD: Sequentially implemented Monte Carlo depth computation with statistical guarantees](https://arxiv.org/abs/2507.06227)
*Felix Gnettner,Claudia Kirch,Alicia Nieto-Reyes*

Main category: stat.ME

TL;DR: This paper introduces seMCD, a sequential Monte Carlo method for efficiently computing statistical depth functions, offering finite-sample and asymptotic guarantees.


<details>
  <summary>Details</summary>
Motivation: The computation of statistical depth functions for spaces with multiple dimensions can be computationally expensive. The authors aimed to address this limitation by proposing a more efficient methodology.

Method: The paper introduces seMCD, a sequentially implemented Monte Carlo method that computes depth functions by outputting intervals (seMCD-buckets) with high-probability guarantees. The number of samples required is random but typically smaller than conventional methods.

Result: The seMCD method was shown to reliably compute depth functions efficiently, with application demonstrated in tasks like anomaly detection and classification. It utilizes finite-sample and asymptotic guarantees.

Conclusion: The seMCD method efficiently computes accurate depth approximations with fewer Monte Carlo samples, providing strong statistical guarantees for multivariate and functional spaces.

Abstract: Statistical depth functions provide center-outward orderings in spaces of
dimension larger than one, where a natural ordering does not exist. The
numerical evaluation of such depth functions can be computationally
prohibitive, even for relatively low dimensions. We present a novel
sequentially implemented Monte Carlo methodology for the computation of,
theoretical and empirical, depth functions and related quantities (seMCD), that
outputs an interval, a so-called seMCD-bucket, to which the quantity of
interest belongs with a high probability prespecified by the user. For specific
classes of depth functions, we adapt algorithms from sequential testing,
providing finite-sample guarantees. For depth functions dependent on unknown
distributions, we offer asymptotic guarantees using non-parametric statistical
methods. In contrast to plain-vanilla Monte Carlo methodology the number of
samples required in the algorithm is random but typically much smaller than
standard choices suggested in the literature. The seMCD method can be applied
to various depth functions, covering multivariate and functional spaces. We
demonstrate the efficiency and reliability of our approach through empirical
studies, highlighting its applicability in outlier or anomaly detection,
classification, and depth region computation. In conclusion, the
seMCD-algorithm can achieve accurate depth approximations with few Monte Carlo
samples while maintaining rigorous statistical guarantees.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [373] [Dynamical Archetype Analysis: Autonomous Computation](https://arxiv.org/abs/2507.05505)
*Abel Sagodi,Il Memming Park*

Main category: math.DS

TL;DR: The authors propose a method using abstract dynamical archetypes to understand neural system computations, emphasizing dissimilarity measures based on effective behavior.


<details>
  <summary>Details</summary>
Motivation: The complexity of neural systems requires automated, principled tools to abstract details, aiding our understanding of their information-processing functions.

Method: The authors define archetypes based on asymptotic dynamical structures and introduce a new dissimilarity measure considering topological conjugacy and diffeomorphisms. They perform numerical experiments to validate the method's robustness.

Result: The method successfully overcomes fragility in existing measures for high-dimensional recurrent neural networks and approximate continuous attractors.

Conclusion: Abstract dynamical archetypes provide a more practical framework for interpreting recurrent dynamics in both biological and artificial neural systems.

Abstract: The study of neural computation aims to understand the function of a neural
system as an information processing machine. Neural systems are undoubtedly
complex, necessitating principled and automated tools to abstract away details
to organize and incrementally build intuition. We argue that systems with the
same effective behavior should be abstracted by their ideal representative,
i.e., archetype, defined by its asymptotic dynamical structure. We propose a
library of archetypical computations and a new measure of dissimilarity that
allows us to group systems based on their effective behavior by explicitly
considering both deformations that break topological conjugacy as well as
diffeomorphisms that preserve it. The proposed dissimilarity can be estimated
from observed trajectories. Numerical experiments demonstrate our method's
ability to overcome previously reported fragility of existing (dis)similarity
measures for approximate continuous attractors and high-dimensional recurrent
neural networks. Although our experiments focus on working memory systems, our
theoretical approach naturally extends to general mechanistic interpretation of
recurrent dynamics in both biological and artificial neural systems. We argue
that abstract dynamical archetypes, rather than detailed dynamical systems,
offer a more useful vocabulary for describing neural computation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [374] [Practical design and performance of physical reservoir computing using hysteresis](https://arxiv.org/abs/2507.06063)
*Yuhei Yamada*

Main category: cs.ET

TL;DR: This paper explores how reservoirs composed of independent hysteretic systems can improve physical reservoir computing while being easy to implement experimentally.


<details>
  <summary>Details</summary>
Motivation: To achieve an effective yet experimentally simple design for physical reservoir computing that is practical for real-world applications.

Method: The study focuses on a reservoir comprising independent hysteretic systems, analyzing its design, performance, and limitations.

Result: The paper identifies appropriate guidelines for creating such hysteresis-based reservoirs, showcasing their practicality in physical reservoir computing.

Conclusion: The study provides practical insights for constructing and utilizing hysteresis-based reservoirs, advancing their experimental applicability.

Abstract: Physical reservoir computing is an innovative idea for using physical
phenomena as computational resources. Recent research has revealed that
information processing techniques can improve the performance, but for
practical applications, it is equally important to study the level of
performance with a simple design that is easy to construct experimentally. We
focus on a reservoir composed of independent hysteretic systems as a model
suitable for the practical implementation of physical reservoir computing. In
this paper, we discuss the appropriate design of this reservoir, its
performance, and its limitations. This research will serve as a practical
guideline for constructing hysteresis-based reservoirs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [375] [Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning](https://arxiv.org/abs/2507.05900)
*Zengjing Chen,Lu Wang,Chengzhi Xing*

Main category: cs.SD

TL;DR: The paper introduces a laser chaos-based method for stable relay assignment in underwater acoustic networks, focusing on classical and ambiguous stable arrangements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stable acoustic relay assignment in underwater networks considering environmental changes and ambiguous cognitions.

Method: The study uses a Laser Chaos-based Multi-Processing Learning (LC-ML) method leveraging chaotic random numbers for relay decision-making to enhance stability and throughput.

Result: The proposed method showed improved throughput, adaptability to environmental changes, and less volatility in configurations compared to traditional methods.

Conclusion: The LC-ML method effectively supports relay selection in underwater acoustic environments, presenting a practical approach for stable operations.

Abstract: This study addresses the problem of stable acoustic relay assignment in an
underwater acoustic network. Unlike the objectives of most existing literature,
two distinct objectives, namely classical stable arrangement and ambiguous
stable arrangement, are considered. To achieve these stable arrangements, a
laser chaos-based multi-processing learning (LC-ML) method is introduced to
efficiently obtain high throughput and rapidly attain stability. In order to
sufficiently explore the relay's decision-making, this method uses random
numbers generated by laser chaos to learn the assignment of relays to multiple
source nodes. This study finds that the laser chaos-based random number and
multi-processing in the exchange process have a positive effect on higher
throughput and strong adaptability with environmental changing over time.
Meanwhile, ambiguous cognitions result in the stable configuration with less
volatility compared to accurate ones. This provides a practical and useful
method and can be the basis for relay selection in complex underwater
environments.

</details>


### [376] [Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol](https://arxiv.org/abs/2507.06070)
*Christos Nikou,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: The paper evaluates song identification models in noisy real-world conditions and improves performance using enhanced data augmentation and a transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Current methods for song identification struggle in real-world settings, specifically with audio captured on mobile devices amidst noise.

Method: The authors introduced a new evaluation protocol with varying noise levels, improved augmentation techniques using low/high-pass filters, and developed a transformer-based model with a tailored projection module.

Result: CNN-based models underperformed under noisy conditions, while the proposed transformer-based model achieved higher accuracy, outperforming previous models significantly across all noise levels and query durations.

Conclusion: The transformer-based model is more robust than CNN-based counterparts, aided by improved augmentations, and showcases superior performance in retrieving songs in challenging conditions.

Abstract: Recent advances in song identification leverage deep neural networks to learn
compact audio fingerprints directly from raw waveforms. While these methods
perform well under controlled conditions, their accuracy drops significantly in
real-world scenarios where the audio is captured via mobile devices in noisy
environments. In this paper, we introduce a novel evaluation protocol designed
to better reflect such real-world conditions. We generate three recordings of
the same audio, each with increasing levels of noise, captured using a mobile
device's microphone. Our results reveal a substantial performance drop for two
state-of-the-art CNN-based models under this protocol, compared to previously
reported benchmarks. Additionally, we highlight the critical role of the
augmentation pipeline during training with contrastive loss. By introduction
low pass and high pass filters in the augmentation pipeline we significantly
increase the performance of both systems in our proposed evaluation.
Furthermore, we develop a transformer-based model with a tailored projection
module and demonstrate that transferring knowledge from a semantically relevant
domain yields a more robust solution. The transformer architecture outperforms
CNN-based models across all noise levels, and query durations. In low noise
conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in
finding the correct song, surpassing by 14%, and by 18.5% the second-best
performing model, respectively, Under heavy noise levels, we achieve a
detection rate 56.5% for 15-second query duration. All experiments are
conducted on public large-scale dataset of over 100K songs, with queries
matched against a database of 56 million vectors.

</details>


### [377] [Differentiable Reward Optimization for LLM based TTS system](https://arxiv.org/abs/2507.05911)
*Changfeng Gao,Zhihao Du,Shiliang Zhang*

Main category: cs.SD

TL;DR: The paper introduces Differentiable Reward Optimization (DiffRO) for improving neural codec-based text-to-speech systems, achieving state-of-the-art pronunciation accuracy and enhanced control over emotional and quality attributes.


<details>
  <summary>Details</summary>
Motivation: Enhance text-to-speech systems by overcoming limitations of conventional reinforcement learning approaches that rely on synthesized audio for feedback.

Method: Proposes DiffRO, leveraging Gumbel-Softmax for differentiable reward computation and a multi-task reward model for diverse feedback.

Result: DiffRO yields significant accuracy improvements in TTS pronunciation with SOTA WER results on benchmark datasets, and enables emotion & quality control in zero-shot scenarios.

Conclusion: DiffRO streamlines reward-based RLHF training and proves effective in advancing TTS capabilities, offering better pronunciation accuracy and flexible attribute control.

Abstract: This paper proposes a novel Differentiable Reward Optimization (DiffRO)
method aimed at enhancing the performance of neural codec language models based
text-to-speech (TTS) systems. In contrast to conventional reinforcement
learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly
compute the rewards based on neural codec tokens, rather than relying on
synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to
render the reward function differentiable, thereby streamlining the RLHF
training process. Additionally, we introduce a multi-task reward (MTR) model
which can provide feedback from different perspectives and find that it can
augment the system's capability to follow instructions effectively.Experimental
results indicate that DiffRO significantly improves the pronunciation accuracy
of the TTS system, achieving state-of-the-art (SOTA) WER results on the
seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we
demonstrate the ability to control emotional and quality attributes in a
zero-shot manner.

</details>


### [378] [Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis](https://arxiv.org/abs/2507.06116)
*Xintong Hu,Yixuan Chen,Rui Yang,Wenxiang Guo,Changhao Pan*

Main category: cs.SD

TL;DR: The paper introduces a MOS prediction system using self-supervised models and a Mixture of Experts architecture but identifies challenges in sentence-level tasks.


<details>
  <summary>Details</summary>
Motivation: Current models for automatic speech quality assessment underperform when dealing with varying granularities of prediction tasks, needing better mechanisms.

Method: The study leverages self-supervised models like wav2vec2, incorporates Mixture of Experts (MoE) classification, and uses a large-scale synthetic speech dataset for task improvement.

Result: The proposed method improved some aspects of speech quality prediction but struggled to enhance performance in sentence-level predictions.

Conclusion: The work highlights the limitations of existing methods for sentence-level quality assessments and proposes directions for future advancements in the field.

Abstract: Automatic speech quality assessment plays a crucial role in the development
of speech synthesis systems, but existing models exhibit significant
performance variations across different granularity levels of prediction tasks.
This paper proposes an enhanced MOS prediction system based on self-supervised
learning speech models, incorporating a Mixture of Experts (MoE) classification
head and utilizing synthetic data from multiple commercial generation models
for data augmentation. Our method builds upon existing self-supervised models
such as wav2vec2, designing a specialized MoE architecture to address different
types of speech quality assessment tasks. We also collected a large-scale
synthetic speech dataset encompassing the latest text-to-speech, speech
conversion, and speech enhancement systems. However, despite the adoption of
the MoE architecture and expanded dataset, the model's performance improvements
in sentence-level prediction tasks remain limited. Our work reveals the
limitations of current methods in handling sentence-level quality assessment,
provides new technical pathways for the field of automatic speech quality
assessment, and also delves into the fundamental causes of performance
differences across different assessment granularities.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [379] [Adaptive Communication Through Exploiting RIS, SSK, and CIM for Improved Reliability and Efficiency](https://arxiv.org/abs/2507.05813)
*Ferhat Bayar,Onur Salan,Erdogan Aydin,Haci Ilhan*

Main category: cs.IT

TL;DR: The paper introduces RIS-CIM-TSSK, a new model combining RIS, SSK, and CIM for energy-efficient and reliable wireless communication.


<details>
  <summary>Details</summary>
Motivation: To enhance reliability, energy efficiency, and adaptability in modern wireless networks while ensuring signal obfuscation and reducing RF chain complexity.

Method: The system exploits reconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and Hadamard-coded code index modulation (CIM). It includes simulations across varied antenna and RIS configurations and uses a suboptimal, low-complexity detector for phase adjustment.

Result: Simulation results demonstrate the performance improvements of the proposed system under various configurations, showcasing adaptability and error rate enhancement.

Conclusion: RIS-CIM-TSSK is presented as a robust, energy-efficient solution for modern wireless communication, leveraging novel combinations of RIS, SSK, and CIM technologies.

Abstract: In this paper, we present a novel communication system model that integrates
reconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and code
index modulation (CIM) based on Hadamard coding called RIS based transmit
SSK-CIM (RIS-CIM-TSSK). By leveraging RIS, the system adapts rapidly to dynamic
environments, enhancing error rates and overall reliability. SSK facilitates
the transmission of additional passive information while eliminating the need
for multiple radio frequency (RF) chains, thereby reducing complexity. CIM
enhances passive information transmission through frequency domain spreading,
which may increase signal obfuscation. This proposed scheme not only improves
energy efficiency but also offers a robust solution for reliable communication
in modern wireless networks, paving the way for smarter and more adaptable
implementations. We consider a suboptimal, low-complexity detector for the
proposed scheme and also address the blind case for phase adjustment of the
RIS. Finally, we present the simulation results for the proposed system model
across various configurations, including different numbers of receive and
transmit antennas, varying reflecting elements of the RIS, and different code
lengths.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [380] [evortran: a modern Fortran package for genetic algorithms with applications from LHC data fitting to LISA signal reconstruction](https://arxiv.org/abs/2507.06082)
*Thomas Biekötter*

Main category: hep-ph

TL;DR: evortran is a high-performance Fortran library for evolutionary optimization and genetic algorithms, applicable in physics and beyond.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance optimization techniques for complex problems in physics and other domains where traditional approaches might be ineffective or inefficient.

Method: evortran employs evolutionary algorithms, including selection, crossover, mutation, and elitism strategies, to optimize parameters and solve complex tasks. It supports migration between populations for convergence safety.

Result: The library showcases flexibility and efficiency through benchmark applications and comparison with existing frameworks. Use cases include fitting experimental data and reconstructing physical parameters in physics research.

Conclusion: evortran extends traditional genetic algorithm capabilities with high performance and flexibility while addressing complex physics-motivated problems effectively.

Abstract: evortran is a modern Fortran library designed for high-performance genetic
algorithms and evolutionary optimization. evortran can be used to tackle a wide
range of problems in high-energy physics and beyond, such as derivative-free
parameter optimization, complex search taks, parameter scans and fitting
experimental data under the presence of instrumental noise. The library is
built as an fpm package with flexibility and efficiency in mind, while also
offering a simple installation process, user interface and integration into
existing Fortran programs. evortran offers a variety of selection, crossover,
mutation and elitism strategies, with which users can tailor an evolutionary
algorithm to their specific needs. evortran supports different abstraction
levels: from operating directly on individuals and populations, to running full
evolutionary cycles, and even enabling migration between independently evolving
populations to enhance convergence and maintain diversity. In this paper, we
present the functionality of the evortran library, demonstrate its capabilities
with example benchmark applications, and compare its performance with existing
genetic algorithm frameworks. As physics-motivated applications, we use
evortran to confront extended Higgs sectors with LHC data and to reconstruct
gravitational wave spectra and the underlying physical parameters from LISA
mock data, demonstrating its effectiveness in realistic, data-driven scenarios.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [381] [Beating the Best Constant Rebalancing Portfolio in Long-Term Investment: A Generalization of the Kelly Criterion and Universal Learning Algorithm for Markets with Serial Dependence](https://arxiv.org/abs/2507.05994)
*Duy Khanh Lam*

Main category: q-fin.PM

TL;DR: The paper proposes a novel algorithm for online portfolio optimization that leverages serial dependence in assets' returns, achieving superior cumulative wealth compared to the best constant rebalancing portfolio.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing learning algorithms in online portfolio optimization that struggle due to feature selection complexities and high-dimensional settings.

Method: Develop an algorithm that gradually learns the serial dependence patterns in assets' returns using revealed data, without assuming their distribution.

Result: The proposed algorithm matches the growth rate of the optimal strategy under a generalized Kelly criterion and performs well experimentally as long as serial dependence is significant.

Conclusion: The algorithm demonstrates strong theoretical guarantees and broad applicability, overcoming issues faced by traditional models in stochastic markets with serial dependence.

Abstract: In the online portfolio optimization framework, existing learning algorithms
generate strategies that yield significantly poorer cumulative wealth compared
to the best constant rebalancing portfolio in hindsight, despite being
consistent in asymptotic growth rate. While this unappealing performance can be
improved by incorporating more side information, it raises difficulties in
feature selection and high-dimensional settings. Instead, the inherent serial
dependence of assets' returns, such as day-of-the-week and other calendar
effects, can be leveraged. Although latent serial dependence patterns are
commonly detected using large training datasets, this paper proposes an
algorithm that learns such dependence using only gradually revealed data,
without any assumption on their distribution, to form a strategy that
eventually exceeds the cumulative wealth of the best constant rebalancing
portfolio.
  Moreover, the classical Kelly criterion, which requires independent assets'
returns, is generalized to accommodate serial dependence in a market modeled as
an independent and identically distributed process of random matrices. In such
a stochastic market, where existing learning algorithms designed for stationary
processes fail to apply, the proposed learning algorithm still generates a
strategy that asymptotically grows to the highest rate among all strategies,
matching that of the optimal strategy constructed under the generalized Kelly
criterion. The experimental results with real market data demonstrate the
theoretical guarantees of the algorithm and its performance as expected, as
long as serial dependence is significant, regardless of the validity of the
generalized Kelly criterion in the experimental market. This further affirms
the broad applicability of the algorithm in general contexts.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [382] [BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects](https://arxiv.org/abs/2507.05265)
*Hongyang Li,Sanjoy Dey,Bum Chul Kwon,Michael Danziger,Michal Rosen-Tzvi,Jianying Hu,James Kozloski,Ching-Huei Tsou,Bharath Dandala,Pablo Meyer*

Main category: q-bio.GN

TL;DR: This paper proposes new DNA language models (DNALMs) incorporating sequence variations, primarily SNPs, to improve genomic function predictions.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing DNALMs (e.g., DNABERT, GENA-LM) that do not effectively encode biological functions in the presence of sequence variations, critical for understanding genomic functions.

Method: The paper pre-trains two models using ModernBERT: BMFM-DNA-REF (trained with varying-length sequences and reverse complements from a reference genome) and BMFM-DNA-SNP (trained with sequences utilizing a novel encoding scheme for sequence variations).

Result: Integrating sequence variations leads to improved performance on all fine-tuning tasks, demonstrating better capture of biological functions. Additionally, they explored SNP imputation strategies for promoter detection tasks.

Conclusion: Incorporating sequence variations enhances DNALM performance, but current benchmarks are limited, requiring further evaluation tools. Models and code are openly shared for community use and future development.

Abstract: Large language models (LLMs) trained on text demonstrated remarkable results
on natural language processing (NLP) tasks. These models have been adapted to
decipher the language of DNA, where sequences of nucleotides act as "words"
that encode genomic functions. However, the genome differs fundamentally from
natural language, as it lacks clearly defined words or a consistent grammar.
Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved
high level of performance on genome-related biological tasks, these models do
not encode biological functions in the presence of sequence variations. To
address this problem, we pre-train foundation models that effectively integrate
sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as
they underlie important biological functions. Specifically, we use ModernBERT
to pre-train two different Biomedical Foundation Models (BMFM), namely,
BMFM-DNA-REF in which the model is trained with sequences of varying lengths
along with their reverse complements derived from the reference genome and
BMFM-DNA-SNP in which the model is trained with sequences created using a novel
representation scheme that encodes sequence variations. Our findings indicate
that integrating sequence variations into DNALMs helps capture the biological
functions as seen in improvements on all fine-tuning tasks. To explore the
model's practical utility, we experimented with various strategies for SNP
imputation on promoter detection task introduced in DNABERT-2. However, we
acknowledge that the current benchmarks are limited in their ability to fully
evaluate these models. To enable more comprehensive assessment in the future
and encourage community contributions, we release our models through
HuggingFace and the code to reproduce the results at
https://github.com/BiomedSciAI/biomed-multi-omic

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [383] [Inaugural MOASEI Competition at AAMAS'2025: A Technical Report](https://arxiv.org/abs/2507.05469)
*Ceferino Patino,Tyler J. Billings,Alireza Saleh Abadi,Daniel Redder,Adam Eck,Prashant Doshi,Leen-Kiat Soh*

Main category: cs.MA

TL;DR: MOASEI Competition benchmarks multi-agent AI in dynamic open-world conditions, presenting innovative solutions for real-world challenges.


<details>
  <summary>Details</summary>
Motivation: The MOASEI initiative aims to advance open-agent systems research by benchmarking decision-making in dynamic, open-world environments.

Method: The competition utilized the free-range-zoo suite to evaluate AI agent performance across diverse scenarios with dynamic openness and partial observability. Eleven participant teams deployed cutting-edge methods, including neural networks, predictive modeling, and optimization techniques.

Result: Results showcased effective strategies for adaptation, robustness, and generalization, providing insights into agent behavior in open environments.

Conclusion: The competition highlights significant advancements and establishes infrastructure for future investigations into open-agent systems in practical domains.

Abstract: We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI)
Competition, a multi-agent AI benchmarking event designed to evaluate
decision-making under open-world conditions. Built on the free-range-zoo
environment suite, MOASEI introduced dynamic, partially observable domains with
agent and task openness--settings where entities may appear, disappear, or
change behavior over time. The 2025 competition featured three
tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct
dimensions of openness and coordination complexity. Eleven teams from
international institutions participated, with four of those teams submitting
diverse solutions including graph neural networks, convolutional architectures,
predictive modeling, and large language model--driven meta--optimization.
Evaluation metrics centered on expected utility, robustness to perturbations,
and responsiveness to environmental change. The results reveal promising
strategies for generalization and adaptation in open environments, offering
both empirical insight and infrastructure for future research. This report
details the competition's design, findings, and contributions to the open-agent
systems research community.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [384] [PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT](https://arxiv.org/abs/2507.05317)
*Yi Liu,Yiyang Wen,Zekun Zhou,Junqi Ma,Linghang Wang,Yucheng Yao,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: The paper introduces PWD, a diffusion model for fast sampling and high-quality LACT reconstructions, addressing computational challenges while keeping fine structural details intact.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for limited-angle CT achieve high-quality imaging but are computationally slow, leading to a need for a faster model that doesn’t compromise on detail retention.

Method: The paper proposes PWD, a method that maps LACT images to a fully sampled image distribution during training and utilizes prior information and wavelet feature fusion for better detail preservation and reduced sampling steps.

Result: The method demonstrates significant improvement in PSNR (up by at least 1.7 dB) and SSIM (10% gain) on dental datasets, using only 50 sampling steps.

Conclusion: PWD efficiently balances computational speed and reconstruction quality in LACT imaging, outperforming existing methods while preserving structural details through innovative training and multi-scale feature fusion strategies.

Abstract: Generative diffusion models have received increasing attention in medical
imaging, particularly in limited-angle computed tomography (LACT). Standard
diffusion models achieve high-quality image reconstruction but require a large
number of sampling steps during inference, resulting in substantial
computational overhead. Although skip-sampling strategies have been proposed to
improve efficiency, they often lead to loss of fine structural details. To
address this issue, we propose a prior information embedding and wavelet
feature fusion fast sampling diffusion model for LACT reconstruction. The PWD
enables efficient sampling while preserving reconstruction fidelity in LACT,
and effectively mitigates the degradation typically introduced by
skip-sampling. Specifically, during the training phase, PWD maps the
distribution of LACT images to that of fully sampled target images, enabling
the model to learn structural correspondences between them. During inference,
the LACT image serves as an explicit prior to guide the sampling trajectory,
allowing for high-quality reconstruction with significantly fewer steps. In
addition, PWD performs multi-scale feature fusion in the wavelet domain,
effectively enhancing the reconstruction of fine details by leveraging both
low-frequency and high-frequency information. Quantitative and qualitative
evaluations on clinical dental arch CBCT and periapical datasets demonstrate
that PWD outperforms existing methods under the same sampling condition. Using
only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and
10% gain in SSIM.

</details>


### [385] [Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation](https://arxiv.org/abs/2507.05314)
*Daniel Cieślak,Miriam Reca,Olena Onyshchenko,Jacek Rumiński*

Main category: eess.IV

TL;DR: The paper introduces a dual-attention U-Net++ architecture optimized for segmenting wounds and scale markers in clinical images using advanced techniques like SCSE attention, EfficientNet-B7 encoding, Bayesian tuning, and Test Time Augmentation, achieving a high F1-score of 0.8640.


<details>
  <summary>Details</summary>
Motivation: Effective segmentation of wounds and scale markers is essential for wound management and automated assessment, yet remains challenging due to variability and class imbalances in medical images.

Method: The authors developed a dual-attention U-Net++ leveraging channel-wise and spatial attention mechanisms. EfficientNet-B7 was chosen as the encoder backbone through extensive benchmarking. Separate training was performed for two class-specific models using extensive preprocessing, data augmentation, Bayesian hyperparameter tuning, and Test Time Augmentation. Predictions were evaluated on a benchmark competition dataset.

Result: The proposed method achieved a weighted F1-score of 0.8640 on medical segmentation tasks, showcasing its reliability and effectiveness.

Conclusion: The dual-attention U-Net++ architecture demonstrates strong capability for handling complex segmentation tasks in medical images, providing a robust technique for addressing variability and class imbalance issues in clinical datasets.

Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa
significant challenge, crucial for effective wound management and
automatedassessment. In this study, we propose a novel dual-attention U-Net++
archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms
toaddress severe class imbalance and variability in medical images
effectively.Initially, extensive benchmarking across diverse architectures and
encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal
encoder backbone.Subsequently, we independently trained two class-specific
models with tailoredpreprocessing, extensive data augmentation, and Bayesian
hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test
Time Augmentationto further enhance prediction reliability. Our approach was
evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition.
Segmentationperformance was quantified using a weighted F1-score (75% wounds,
25% scalemarkers), calculated externally by competition organizers on
undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640,
underscoring itseffectiveness for complex medical segmentation tasks.

</details>


### [386] [Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging](https://arxiv.org/abs/2507.05451)
*Lijie Huang,Jingyi Yin,Jingke Zhang,U-Wai Lok,Ryan M. DeRuiter,Jieyang Jin,Kate M. Knoll,Kendra E. Petersen,James D. Krier,Xiang-yang Zhu,Gina K. Hesley,Kathryn A. Robinson,Andrew J. Bentall,Thomas D. Atwell,Andrew D. Rule,Lilach O. Lerman,Shigao Chen,Chengwu Huang*

Main category: eess.IV

TL;DR: The paper introduces HA2HA, a self-supervised denoising framework that significantly improves the signal quality in ultrasound microvascular imaging (UMI), particularly under challenging conditions such as contrast-free or deep tissue scenarios.


<details>
  <summary>Details</summary>
Motivation: Low signal-to-noise ratio (SNR) in UMI hinders vascular quantification and disease diagnosis, particularly in contrast-free or deep tissue imaging.

Method: HA2HA leverages self-supervised learning by creating training pairs from complementary angular subsets of beamformed RF blood flow data, maintaining consistency in vascular signals while noise varies.

Result: HA2HA achieved over 15 dB improvement in both contrast-to-noise ratio (CNR) and SNR, validated across datasets including pig kidneys and human liver/kidney data.

Conclusion: This label-free, generalizable framework enhances UMI quality for more accurate vascular imaging, benefiting applications like power and color Doppler imaging, in both contrast-free and contrast-enhanced settings.

Abstract: Ultrasound microvascular imaging (UMI) is often hindered by low
signal-to-noise ratio (SNR), especially in contrast-free or deep tissue
scenarios, which impairs subsequent vascular quantification and reliable
disease diagnosis. To address this challenge, we propose
Half-Angle-to-Half-Angle (HA2HA), a self-supervised denoising framework
specifically designed for UMI. HA2HA constructs training pairs from
complementary angular subsets of beamformed radio-frequency (RF) blood flow
data, across which vascular signals remain consistent while noise varies. HA2HA
was trained using in-vivo contrast-free pig kidney data and validated across
diverse datasets, including contrast-free and contrast-enhanced data from pig
kidneys, as well as human liver and kidney. An improvement exceeding 15 dB in
both contrast-to-noise ratio (CNR) and SNR was observed, indicating a
substantial enhancement in image quality. In addition to power Doppler imaging,
denoising directly in the RF domain is also beneficial for other downstream
processing such as color Doppler imaging (CDI). CDI results of human liver
derived from the HA2HA-denoised signals exhibited improved microvascular flow
visualization, with a suppressed noisy background. HA2HA offers a label-free,
generalizable, and clinically applicable solution for robust vascular imaging
in both contrast-free and contrast-enhanced UMI.

</details>


### [387] [Learning Segmentation from Radiology Reports](https://arxiv.org/abs/2507.05582)
*Pedro R. A. S. Bassi,Wenxuan Li,Jieneng Chen,Zheren Zhu,Tianyu Lin,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: This paper introduces a new method named R-Super to convert radiology reports into voxel-wise supervision for enhancing tumor segmentation in CT scans.


<details>
  <summary>Details</summary>
Motivation: The scarcity of tumor segmentation masks, which are essential for diagnosis and prognosis, motivates leveraging radiology reports from hospitals to scale and improve segmentation.

Method: They designed the R-Super loss function to convert radiology reports into voxel-level training supervision and combined 6,718 CT-Report pairs from UCSF Hospital with public CT-Mask datasets.

Result: The approach significantly improved tumor segmentation, with up to a 16% increase in F1 Score on internal and external validation, regardless of the number of training masks.

Conclusion: By utilizing radiology reports alongside available segmentation masks, the proposed method enhances tumor segmentation both in scenarios with limited and extensive mask availability.

Abstract: Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis,
yet segmentation masks are scarce because their creation requires time and
expertise. Public abdominal CT datasets have from dozens to a couple thousand
tumor masks, but hospitals have hundreds of thousands of tumor CTs with
radiology reports. Thus, leveraging reports to improve segmentation is key for
scaling. In this paper, we propose a report-supervision loss (R-Super) that
converts radiology reports into voxel-wise supervision for tumor segmentation
AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital),
and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our
R-Super to train with these masks and reports, and strongly improved tumor
segmentation in internal and external validation--F1 Score increased by up to
16% with respect to training with masks only. By leveraging readily available
radiology reports to supplement scarce segmentation masks, R-Super strongly
improves AI performance both when very few training masks are available (e.g.,
50), and when many masks were available (e.g., 1.7K).
  Project: https://github.com/MrGiovanni/R-Super

</details>


### [388] [Diffusion-Based Limited-Angle CT Reconstruction under Noisy Conditions](https://arxiv.org/abs/2507.05647)
*Jiaqi Guo,Santiago López-Tapia*

Main category: eess.IV

TL;DR: The paper addresses noise robustness in Limited-Angle Computed Tomography by proposing a diffusion-based sinogram inpainting framework enhanced with a noise-aware rectification mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of missing angular projections and the underperformance of existing learning-based techniques in noisy measurement scenarios.

Method: A diffusion-based sinogram completion framework using a Mean-Reverting Stochastic Differential Equation (MR-SDE) and a novel noise-aware rectification mechanism called RNSD$^+$.

Result: Extensive experiments show superior performance in data consistency and visual quality over baselines, with good generalization across varying noise levels and acquisition conditions.

Conclusion: The proposed method demonstrates robustness and effectiveness in Limited-Angle Computed Tomography under realistic noise scenarios, providing improved reconstruction.

Abstract: Limited-Angle Computed Tomography (LACT) is a challenging inverse problem
where missing angular projections lead to incomplete sinograms and severe
artifacts in the reconstructed images. While recent learning-based methods have
demonstrated effectiveness, most of them assume ideal, noise-free measurements
and fail to address the impact of measurement noise. To overcome this
limitation, we treat LACT as a sinogram inpainting task and propose a
diffusion-based framework that completes missing angular views using a
Mean-Reverting Stochastic Differential Equation (MR-SDE) formulation. To
improve robustness under realistic noise, we propose RNSD$^+$, a novel
noise-aware rectification mechanism that explicitly models inference-time
uncertainty, enabling reliable and robust reconstruction. Extensive experiments
demonstrate that our method consistently surpasses baseline models in data
consistency and perceptual quality, and generalizes well across varying noise
intensity and acquisition scenarios.

</details>


### [389] [ADPv2: A Hierarchical Histological Tissue Type-Annotated Dataset for Potential Biomarker Discovery of Colorectal Disease](https://arxiv.org/abs/2507.05656)
*Zhiyuan Yang,Kai Li,Sophia Ghamoshi Ramandi,Patricia Brassard,Hakim Khellaf,Vincent Quoc-Huy Trinh,Jennifer Zhang,Lina Chen,Corwyn Rowsell,Sonal Varma,Kostas Plataniotis,Mahdi S. Hosseini*

Main category: eess.IV

TL;DR: The paper introduces ADPv2, a publicly available dataset focusing on gastrointestinal histopathology, featuring 20,004 patches annotated with a hierarchical taxonomy of 32 histological tissue types. A trained multilabel classification model achieves high precision and helps uncover patterns related to colon cancer.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available datasets with granular histological tissue type annotations for computational pathology limits research depth and diagnostic advancements, especially for specific organ studies.

Method: Researchers created and annotated the ADPv2 dataset with gastrointestinal histopathology images and applied a two-stage training procedure using the VMamba architecture to classify colon histological tissue types.

Result: The model trained on ADPv2 achieved a mean average precision of 0.88 in distinguishing colon histological tissue types and revealed statistically significant pathological pathways of colon cancer.

Conclusion: ADPv2 enables an organ-specific study in computational pathology and biomarker discovery, enhancing insights into colon cancer development with publicly accessible data for broad research use.

Abstract: Computational pathology (CoPath) leverages histopathology images to enhance
diagnostic precision and reproducibility in clinical pathology. However,
publicly available datasets for CoPath that are annotated with extensive
histological tissue type (HTT) taxonomies at a granular level remain scarce due
to the significant expertise and high annotation costs required. Existing
datasets, such as the Atlas of Digital Pathology (ADP), address this by
offering diverse HTT annotations generalized to multiple organs, but limit the
capability for in-depth studies on specific organ diseases. Building upon this
foundation, we introduce ADPv2, a novel dataset focused on gastrointestinal
histopathology. Our dataset comprises 20,004 image patches derived from healthy
colon biopsy slides, annotated according to a hierarchical taxonomy of 32
distinct HTTs of 3 levels. Furthermore, we train a multilabel representation
learning model following a two-stage training procedure on our ADPv2 dataset.
We leverage the VMamba architecture and achieving a mean average precision
(mAP) of 0.88 in multilabel classification of colon HTTs. Finally, we show that
our dataset is capable of an organ-specific in-depth study for potential
biomarker discovery by analyzing the model's prediction behavior on tissues
affected by different colon diseases, which reveals statistical patterns that
confirm the two pathological pathways of colon cancer development. Our dataset
is publicly available here: Part 1 at https://zenodo.org/records/15307021, Part
2 at https://zenodo.org/records/15312384 and Part 3 at
https://zenodo.org/records/15312792

</details>


### [390] [Tissue Concepts v2: a Supervised Foundation Model for whole slide images](https://arxiv.org/abs/2507.05742)
*Till Nicke,Daniela Scharcherer,Jan Raphael Schäfer,Natalia Artysh,Antje Prasse,André Homeyer,Andrea Schenk,Henning Höfener,Johannes Lotz*

Main category: eess.IV

TL;DR: The paper introduces Tissue Concepts v2 (TCv2), a supervised foundation model designed to analyze whole slide histopathology images while requiring fewer training resources compared to self-supervised models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the resource-intensive process (weeks of training on large databases) required to develop foundation models for histopathology image analysis.

Method: The TCv2 model employs supervised, end-to-end multitask learning on slide-level labels and uses less training resources compared to self-supervised training.

Result: TCv2 outperforms self-supervised models in cancer subtyping benchmarks and is fully trained using freely available datasets.

Conclusion: TCv2 provides superior performance and enhanced explainability while minimizing resource demands during its training process.

Abstract: Foundation models (FMs) are transforming the field of computational pathology
by offering new approaches to analyzing histopathology images. Typically
relying on weeks of training on large databases, the creation of FMs is a
resource-intensive process in many ways. In this paper, we introduce the
extension of our supervised foundation model, Tissue Concepts, to whole slide
images, called Tissue Concepts v2 (TCv2), a supervised foundation model for
whole slide images to address the issue above. TCv2 uses supervised, end-to-end
multitask learning on slide-level labels. Training TCv2 uses a fraction of the
training resources compared to self-supervised training. The presented model
shows superior performance compared to SSL-trained models in cancer subtyping
benchmarks and is fully trained on freely available data. Furthermore, a shared
trained attention module provides an additional layer of explainability across
different tasks.

</details>


### [391] [A novel framework for fully-automated co-registration of intravascular ultrasound and optical coherence tomography imaging data](https://arxiv.org/abs/2507.05883)
*Xingwei He,Kit Mills Bransby,Ahmet Emir Ulutas,Thamil Kumaran,Nathan Angelo Lecaros Yap,Gonul Zeren,Hesong Zeng,Yaojun Zhang,Andreas Baumbach,James Moon,Anthony Mathur,Jouke Dijkstra,Qianni Zhang,Lorenz Raber,Christos V Bourantas*

Main category: eess.IV

TL;DR: The paper introduces a deep-learning framework for fully automated co-registration of intravascular ultrasound (IVUS) and optical coherence tomography (OCT) images with high accuracy, speed, and reliable performance compared to expert analysts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently and accurately co-registering IVUS and OCT images for multimodal imaging analysis in cardiovascular research.

Method: The framework utilizes deep learning models trained on extensive annotated IVUS and OCT image datasets and employs dynamic time warping and programming for longitudinal and circumferential co-registration.

Result: The method achieved high concordance (>0.99 longitudinally, >0.90 circumferentially) with experts, demonstrated reliable performance metrics (e.g., Williams Index of 0.96 and 0.97), and processed data in under 90 seconds.

Conclusion: The developed DL-based framework demonstrates fast, accurate, and expert-comparable co-registration performance, making it suitable for analyzing large-scale multimodal imaging data in plaque characterization research.

Abstract: Aims: To develop a deep-learning (DL) framework that will allow fully
automated longitudinal and circumferential co-registration of intravascular
ultrasound (IVUS) and optical coherence tomography (OCT) images. Methods and
results: Data from 230 patients (714 vessels) with acute coronary syndrome that
underwent near-infrared spectroscopy (NIRS)-IVUS and OCT imaging in their
non-culprit vessels were included in the present analysis. The lumen borders
annotated by expert analysts in 61,655 NIRS-IVUS and 62,334 OCT frames, and the
side branches and calcific tissue identified in 10,000 NIRS-IVUS frames and
10,000 OCT frames, were used to train DL solutions for the automated extraction
of these features. The trained DL solutions were used to process NIRS-IVUS and
OCT images and their output was used by a dynamic time warping algorithm to
co-register longitudinally the NIRS-IVUS and OCT images, while the
circumferential registration of the IVUS and OCT was optimized through dynamic
programming. On a test set of 77 vessels from 22 patients, the DL method showed
high concordance with the expert analysts for the longitudinal and
circumferential co-registration of the two imaging sets (concordance
correlation coefficient >0.99 for the longitudinal and >0.90 for the
circumferential co-registration). The Williams Index was 0.96 for longitudinal
and 0.97 for circumferential co-registration, indicating a comparable
performance to the analysts. The time needed for the DL pipeline to process
imaging data from a vessel was <90s. Conclusion: The fully automated, DL-based
framework introduced in this study for the co-registration of IVUS and OCT is
fast and provides estimations that compare favorably to the expert analysts.
These features renders it useful in research in the analysis of large-scale
data collected in studies that incorporate multimodality imaging to
characterize plaque composition.

</details>


### [392] [Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration](https://arxiv.org/abs/2507.06067)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: The paper proposes a multimodal approach for generating synthetic CT (sCT) images from CBCT by incorporating a learnable registration module to manage misalignment between intraoperative CBCT and preoperative CT.


<details>
  <summary>Details</summary>
Motivation: CBCT is commonly used for intraoperative imaging but suffers from quality issues compared to CT. Generating synthetic CT images from CBCT can address these limitations, and the paper aims to enhance this process by leveraging multimodal data.

Method: The paper introduces a multimodal learning framework with an end-to-end learnable registration module to account for misalignment issues between CBCT and preoperative CT data during sCT generation.

Result: The proposed method was tested on both synthetic and real-world clinical datasets. It improved sCT quality and outperformed baseline methods in 79 out of 90 settings, particularly in scenarios of low CBCT quality and moderate misalignment.

Conclusion: Integrating registration into multimodal sCT generation significantly enhances image quality, demonstrating robustness and generalizability across various datasets and conditions.

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative
imaging due to its rapid acquisition and low radiation dose. However, CBCT
images typically suffer from artifacts and lower visual quality compared to
conventional Computed Tomography (CT). A promising solution is synthetic CT
(sCT) generation, where CBCT volumes are translated into the CT domain. In this
work, we enhance sCT generation through multimodal learning by jointly
leveraging intraoperative CBCT and preoperative CT data. To overcome the
inherent misalignment between modalities, we introduce an end-to-end learnable
registration module within the sCT pipeline. This model is evaluated on a
controlled synthetic dataset, allowing precise manipulation of data quality and
alignment parameters. Further, we validate its robustness and generalizability
on two real-world clinical datasets. Experimental results demonstrate that
integrating registration in multimodal sCT generation improves sCT quality,
outperforming baseline multimodal methods in 79 out of 90 evaluation settings.
Notably, the improvement is most significant in cases where CBCT quality is low
and the preoperative CT is moderately misaligned.

</details>


### [393] [LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models](https://arxiv.org/abs/2507.06140)
*Zhihao Chen,Tao Chen,Chenhui Wang,Qi Gao,Huidong Xie,Chuang Niu,Ge Wang,Hongming Shan*

Main category: eess.IV

TL;DR: The paper proposes LangMamba, a novel denoising method for low-dose CT (LDCT) that integrates vision-language models (VLMs) to enhance image quality while reducing training costs.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of existing LDCT denoising techniques, which focus primarily on pixel-level mappings and lack semantic guidance, potentially degrading image quality and diagnostic accuracy.

Method: LangMamba employs a two-stage strategy. First, it pre-trains a Language-guided AutoEncoder (LangAE) using VLMs to map normal-dose CT (NDCT) images into semantic spaces enriched with anatomical details. Then, it uses Semantic-Enhanced Efficient Denoiser (SEED) and Language-engaged Dual-space Alignment (LangDA) Loss to ensure high-quality LDCT denoising.

Result: Experimental results on public datasets demonstrate LangMamba's superior performance compared to state-of-the-art methods, showing significant improvements in detail preservation, visual fidelity, and generalizability to unseen datasets.

Conclusion: The study showcases the potential of language-based semantic guidance for advancing LDCT denoising, offering both improved image quality and reduced training costs. It also enhances explainability and provides a modular approach for future applications.

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but often
degrades image quality, potentially compromising diagnostic accuracy. Existing
deep learning-based denoising methods focus primarily on pixel-level mappings,
overlooking the potential benefits of high-level semantic guidance. Recent
advances in vision-language models (VLMs) suggest that language can serve as a
powerful tool for capturing structured semantic information, offering new
opportunities to improve LDCT reconstruction. In this paper, we introduce
LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages
VLM-derived representations to enhance supervision from normal-dose CT (NDCT).
LangMamba follows a two-stage learning strategy. First, we pre-train a
Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT
images into a semantic space enriched with anatomical information. Second, we
synergize LangAE with two key components to guide LDCT denoising:
Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local
semantic while capturing global features with efficient Mamba mechanism, and
Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that
denoised images align with NDCT in both perceptual and semantic spaces.
Extensive experiments on two public datasets demonstrate that LangMamba
outperforms conventional state-of-the-art methods, significantly improving
detail preservation and visual fidelity. Remarkably, LangAE exhibits strong
generalizability to unseen datasets, thereby reducing training costs.
Furthermore, LangDA loss improves explainability by integrating language-guided
insights into image reconstruction and offers a plug-and-play fashion. Our
findings shed new light on the potential of language as a supervisory signal to
advance LDCT denoising. The code is publicly available on
https://github.com/hao1635/LangMamba.

</details>


### [394] [PSAT: Pediatric Segmentation Approaches via Adult Augmentations and Transfer Learning](https://arxiv.org/abs/2507.05764)
*Tristan Kirscher,Sylvain Faisan,Xavier Coubez,Loris Barrier,Philippe Meyer*

Main category: eess.IV

TL;DR: PSAT investigates pediatric medical imaging segmentation challenges using nnU-Net across different strategies, and provides insights for performance improvement.


<details>
  <summary>Details</summary>
Motivation: To address challenges in pediatric medical imaging segmentation, which arise from anatomical differences and limitations of applying adult-trained models to children.

Method: Proposed a framework called PSAT that evaluates different strategies (training sets, data augmentation, transfer learning) using nnU-Net on pediatric CT datasets.

Result: PSAT identifies key issues like mismatched training plans based on adult data and shows the advantages of continual learning strategies for better generalization in pediatric datasets.

Conclusion: PSAT provides practical guidelines to enhance segmentation in pediatric imaging and highlights the importance of tailored strategies for improved accuracy.

Abstract: Pediatric medical imaging presents unique challenges due to significant
anatomical and developmental differences compared to adults. Direct application
of segmentation models trained on adult data often yields suboptimal
performance, particularly for small or rapidly evolving structures. To address
these challenges, several strategies leveraging the nnU-Net framework have been
proposed, differing along four key axes: (i) the fingerprint dataset (adult,
pediatric, or a combination thereof) from which the Training Plan -including
the network architecture-is derived; (ii) the Learning Set (adult, pediatric,
or mixed), (iii) Data Augmentation parameters, and (iv) the Transfer learning
method (finetuning versus continual learning). In this work, we introduce PSAT
(Pediatric Segmentation Approaches via Adult Augmentations and Transfer
learning), a systematic study that investigates the impact of these axes on
segmentation performance. We benchmark the derived strategies on two pediatric
CT datasets and compare them with state-of-theart methods, including a
commercial radiotherapy solution. PSAT highlights key pitfalls and provides
actionable insights for improving pediatric segmentation. Our experiments
reveal that a training plan based on an adult fingerprint dataset is misaligned
with pediatric anatomy-resulting in significant performance degradation,
especially when segmenting fine structures-and that continual learning
strategies mitigate institutional shifts, thus enhancing generalization across
diverse pediatric datasets. The code is available at
https://github.com/ICANS-Strasbourg/PSAT.

</details>


### [395] [Just Say Better or Worse: A Human-AI Collaborative Framework for Medical Image Segmentation Without Manual Annotations](https://arxiv.org/abs/2507.05815)
*Yizhe Zhang*

Main category: eess.IV

TL;DR: The paper proposes a Human-AI collaborative framework to significantly reduce the annotation effort in medical image segmentation, requiring only binary feedback from experts instead of manual pixel-level labeling.


<details>
  <summary>Details</summary>
Motivation: Manually annotating medical images is labor-intensive and slows the development of medical imaging AI systems, necessitating more efficient solutions.

Method: The framework uses preference learning where experts provide binary feedback on segmentation quality, combined with foundation models for feature extraction, label propagation, and a clicking agent to refine segmentation through multiple training rounds.

Result: Experiments on three public datasets show competitive segmentation performance achieved with binary preference feedback, eliminating the need for manual pixel labeling.

Conclusion: The framework shows promise in reducing annotation burdens while maintaining high segmentation performance, paving the way for efficient Human-AI collaboration in medical imaging.

Abstract: Manual annotation of medical images is a labor-intensive and time-consuming
process, posing a significant bottleneck in the development and deployment of
robust medical imaging AI systems. This paper introduces a novel Human-AI
collaborative framework for medical image segmentation that substantially
reduces the annotation burden by eliminating the need for explicit manual
pixel-level labeling. The core innovation lies in a preference learning
paradigm, where human experts provide minimal, intuitive feedback -- simply
indicating whether an AI-generated segmentation is better or worse than a
previous version. The framework comprises four key components: (1) an adaptable
foundation model (FM) for feature extraction, (2) label propagation based on
feature similarity, (3) a clicking agent that learns from human better-or-worse
feedback to decide where to click and with which label, and (4) a multi-round
segmentation learning procedure that trains a state-of-the-art segmentation
network using pseudo-labels generated by the clicking agent and FM-based label
propagation. Experiments on three public datasets demonstrate that the proposed
approach achieves competitive segmentation performance using only binary
preference feedback, without requiring experts to directly manually annotate
the images.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [396] [Exploring Gain-Doped-Waveguide-Synapse for Neuromorphic Applications: A Pulsed Pump-Signal Approach](https://arxiv.org/abs/2507.05931)
*Robert Otupiri,Ripalta Stabile*

Main category: physics.optics

TL;DR: The paper explores the development of new bio-inspired neurons using Doped-Gain-Layer-on-Waveguide Synapses to achieve scalable and energy-efficient neuromorphic computation.


<details>
  <summary>Details</summary>
Motivation: Traditional neuromorphic processors face limitations in flexibility, scalability, and energy efficiency, necessitating innovative solutions.

Method: The study utilizes a pulsed pump-signal mechanism combined with Gain on Waveguide Dynamics to build ultra-efficient artificial synapses.

Result: Key findings include the influence of pulse and material properties on neuronal-like spiking responses, showcasing non-linear pulse transformations for computational logic.

Conclusion: The proposed method successfully mirrors natural neuronal functions, enabling advances in brain-like spiking neural networks with efficient event-driven computation.

Abstract: Neuromorphic computing promises to transform AI systems by enabling them to
perceive, respond to, and adapt swiftly and accurately to dynamic data and user
interactions. However, traditional silicon-based and hybrid electronic
technologies for artificial neurons constrain neuromorphic processors in terms
of flexibility, scalability, and energy efficiency. In this study, we pioneer
the use of Doped-Gain-Layer-on-Waveguide-Synapses for bio-inspired neurons,
utilizing a pulsed pump-signal mechanism to enhance neuromorphic computation.
This approach addresses critical challenges in scalability and energy
efficiency inherent in current technologies.
  We introduce the concept of Gain on Waveguide Dynamics for synapses,
demonstrating how non-linear pulse transformations of input probe signals occur
under various pump-probe configurations. Our findings reveal that primarily
properties of pulse amplitude, period as well material properties such as
doping densities and population dynamics influence strongly the generation of
spiking responses that emulate neuronal behaviour and effectively how
computational logic is. By harnessing the complex interactions of asynchronous
spiking pump techniques and ion densities in excited states, our method
produces event-driven responses that mirror natural neuronal functions. This
gain-enhanced environment supports short-term memory capabilities alongside
essential characteristics like asynchronous spike generation, threshold
operation, and temporal integration, foundational to brain-inspired spiking
neural network paradigms.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [397] [ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](https://arxiv.org/abs/2507.05727)
*He Wang,Linhan Ma,Dake Guo,Xiong Wang,Lei Xie,Jin Xu,Junyang Lin*

Main category: eess.AS

TL;DR: The paper proposes ContextASR-Bench, a large-scale benchmark to evaluate contextual speech recognition using 40,000 data entries across 10+ domains.


<details>
  <summary>Details</summary>
Motivation: Existing ASR models lack sufficient context modeling, memory, and reasoning skills, leading to the need for a benchmark for comprehensive ASR evaluation.

Method: A benchmark named ContextASR-Bench is designed with diverse datasets, testing contextual understanding and entity recognition of ASR systems.

Result: Large Audio Language Models (LALMs) significantly outperform traditional ASR in capturing contexts and recognizing named entities.

Conclusion: ContextASR-Bench is a valuable benchmark for advancing context-aware ASR systems, showcasing the advantages of LALMs for such tasks.

Abstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet
prior evaluative efforts have largely been restricted to contextless paradigms.
This constraint stems from the limited proficiency of conventional ASR models
in context modeling and their deficiency in memory and reasoning based on world
knowledge. Recent breakthroughs in the development of Large Language Models
(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly
enhanced the visibility of general artificial intelligence capabilities.
Consequently, there exists a compelling need for a benchmark that can evaluate
both the generality and intelligence of ASR systems. To address this gap, we
propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to
assess contextual speech recognition. This benchmark encompasses up to 40,000
data entries across over 10 domains, enabling a thorough evaluation of model
performance in scenarios that omit or incorporate coarse-grained or
fine-grained contextual information. Moreover, diverging from conventional ASR
evaluations, our benchmark includes an analysis of model efficacy in
recognizing named entities mentioned within the auditory input. Our extensive
evaluation highlights that LALMs, with strong world knowledge and context
learning capabilities, outperform conventional ASR models by a large margin.
The dataset and evaluation code have been released at
https://github.com/MrSupW/ContextASR-Bench.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [398] [Challenges & Opportunities with LLM-Assisted Visualization Retargeting](https://arxiv.org/abs/2507.01436)
*Luke S. Snyder,Chenglong Wang,Steven M. Drucker*

Main category: cs.HC

TL;DR: The paper studies how Large Language Models (LLMs) can assist in retargeting visualization charts to new datasets and evaluates two methods: direct code generation and structured guidance.


<details>
  <summary>Details</summary>
Motivation: Retargeting existing chart implementations to new datasets is a time-intensive and complex task, requiring technical proficiency. Automating this process with LLMs can reduce the barrier and speed up adaptation.

Method: The authors evaluate two approaches: (1) using LLMs to fully generate and adapt code from text input and (2) a structured synthesis pipeline where LLMs provide guidance on structural properties to adapt code.

Result: Both methods faced challenges when the new data was not prepared or transformed appropriately. The study identified distinct types and severities of failures for these approaches.

Conclusion: LLMs show promise in assisting visualization retargeting, but future systems need better mechanisms for data transformation and structural adaptation to minimize failures.

Abstract: Despite the ubiquity of visualization examples published on the web,
retargeting existing custom chart implementations to new datasets remains
difficult, time-intensive, and tedious. The adaptation process assumes author
familiarity with both the implementation of the example as well as how the new
dataset might need to be transformed to fit into the example code. With recent
advances in Large Language Models (LLMs), automatic adaptation of code can be
achieved from high-level user prompts, reducing the barrier for visualization
retargeting. To better understand how LLMs can assist retargeting and its
potential limitations, we characterize and evaluate the performance of LLM
assistance across multiple datasets and charts of varying complexity,
categorizing failures according to type and severity. In our evaluation, we
compare two approaches: (1) directly instructing the LLM model to fully
generate and adapt code by treating code as text inputs and (2) a more
constrained program synthesis pipeline where the LLM guides the code
construction process by providing structural information (e.g., visual
encodings) based on properties of the example code and data. We find that both
approaches struggle when new data has not been appropriately transformed, and
discuss important design recommendations for future retargeting systems.

</details>


### [399] [NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones](https://arxiv.org/abs/2507.05447)
*Aiur Nanzatov,Lourdes Peña-Castillo,Oscar Meruvia-Pastor*

Main category: cs.HC

TL;DR: The study introduces NRXR-ID, a method for two-factor authentication in extended reality systems using smartphones, which enables users to authenticate without removing their VR headsets. A user study with 30 participants was conducted, showing that a checkers-style challenge was the most effective.


<details>
  <summary>Details</summary>
Motivation: Two-factor authentication is challenging in VR environments because users wear head-mounted displays that limit their interaction with real-world devices.

Method: The researchers introduced NRXR-ID, a system using smartphones for authentication within extended reality. They tested four types of challenges via a user study with a 4X3 within-subjects design, collecting performance data and user feedback.

Result: The study found that the checkers-style visual matching challenge was the best approach, followed by the digital PIN challenge submitted via smartphone in VR.

Conclusion: NRXR-ID successfully addresses the difficulty of implementing 2FA in VR environments, with the checkers-style challenge proving to be the most user-friendly and efficient option.

Abstract: Two-factor authentication (2FA) has become widely adopted as an efficient and
secure way to validate someone's identity online. Two-factor authentication is
difficult in virtual reality (VR) because users are usually wearing a
head-mounted display (HMD) which does not allow them to see their real-world
surroundings. We present NRXR-ID, a technique to implement two-factor
authentication while using extended reality systems and smartphones. The
proposed method allows users to complete an authentication challenge using
their smartphones without removing their HMD. We performed a user study where
we explored four types of challenges for users, including a novel
checkers-style challenge. Users responded to these challenges under three
different configurations, including a technique that uses the smartphone to
support gaze-based selection without the use of VR controllers. A 4X3
within-subjects design allowed us to study all the variations proposed. We
collected performance metrics and performed user experience questionnaires to
collect subjective impressions from 30 participants. Results suggest that the
checkers-style visual matching challenge was the most appropriate option,
followed by entering a digital PIN challenge submitted via the smartphone and
answered within the VR environment.

</details>


### [400] [Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents](https://arxiv.org/abs/2507.05820)
*Syemin Park,Soobin Park,Youn-kyung Lim*

Main category: cs.HC

TL;DR: The paper introduces Constella, a tool powered by large language models (LLMs) designed to assist storywriters in creating interconnected characters and exploring their relational dynamics.


<details>
  <summary>Details</summary>
Motivation: Writers often struggle with envisioning new characters, balancing character similarities and differences, and detailing relationships between characters, especially for long-form stories.

Method: The researchers designed an LLM-based multi-agent tool called Constella with features such as FRIENDS DISCOVERY, JOURNALS, and COMMENTS to support interconnected character creation. It was tested in a deployment study with storywriters.

Result: The deployment study showed that Constella helped writers create expansive communities of related characters, compare thoughts and emotions among characters, and develop a deeper understanding of character relationships.

Conclusion: Multi-agent interactions, as exemplified by Constella, can help distribute writers' attention across a cast of characters, enhancing character development and relational dynamics in storywriting.

Abstract: Creating a cast of characters by attending to their relational dynamics is a
critical aspect of most long-form storywriting. However, our formative study
(N=14) reveals that writers struggle to envision new characters that could
influence existing ones, to balance similarities and differences among
characters, and to intricately flesh out their relationships. Based on these
observations, we designed Constella, an LLM-based multi-agent tool that
supports storywriters' interconnected character creation process. Constella
suggests related characters (FRIENDS DISCOVERY feature), reveals the inner
mindscapes of several characters simultaneously (JOURNALS feature), and
manifests relationships through inter-character responses (COMMENTS feature).
Our 7-8 day deployment study with storywriters (N=11) shows that Constella
enabled the creation of expansive communities composed of related characters,
facilitated the comparison of characters' thoughts and emotions, and deepened
writers' understanding of character relationships. We conclude by discussing
how multi-agent interactions can help distribute writers' attention and effort
across the character cast.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [401] [Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools](https://arxiv.org/abs/2507.05305)
*Lorenzo Lee Solano,Charles Koutcheme,Juho Leinonen,Alexandra Vassar,Jake Renzella*

Main category: cs.CY

TL;DR: The paper explores the potential of smaller, supervised fine-tuned language models as alternatives to larger LLMs for deciphering compiler errors in educational contexts, showing their effectiveness and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: Large LLMs like ChatGPT offer promising capabilities for assisting novice programmers but are hindered by high computational costs, scalability issues, and over-assistance for educational adoption.

Method: The study fine-tuned three smaller open-source models using supervised learning on a dataset of 40,000 C compiler error explanations derived from introductory programming courses.

Result: Fine-tuned smaller models demonstrated comparable performance to larger models in deciphering compiler errors, assessed through expert reviews and automated evaluation of 8,000 responses.

Conclusion: Fine-tuning specialized smaller models on domain-specific data is a viable way to create effective and efficient educational tools, bridging the gap for broader access to generative AI in learning environments.

Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher
cryptic compiler errors for novice programmers, but their computational scale,
cost, and tendency to over-assist make them problematic for widespread
pedagogical adoption. This work demonstrates that smaller, specialised language
models, enhanced via Supervised Fine-Tuning (SFT), present a more viable
alternative for educational tools. We utilise a new dataset of 40,000 C
compiler error explanations, derived from real introductory programming (CS1/2)
student-generated programming errors, which we used to fine-tune three
open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual
evaluation, combining expert human reviews with a large-scale automated
analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our
results show that SFT significantly boosts the pedagogical quality of smaller
models, achieving performance comparable to much larger models. We analyse the
trade-offs between model size and quality, confirming that fine-tuning compact,
efficient models on high-quality, domain-specific data is a potent strategy for
creating specialised models to drive educational tools. We provide a replicable
methodology to foster broader access to generative AI capabilities in
educational contexts.

</details>


### [402] [Integrators at War: Mediating in AI-assisted Resort-to-Force Decisions](https://arxiv.org/abs/2501.06861)
*Dennis Müller,Maurice Chiodo,Mitja Sienknecht*

Main category: cs.CY

TL;DR: This paper investigates challenges in integrating AI systems into military decision-making, emphasizing the role of integrators within the sociotechnical systems.


<details>
  <summary>Details</summary>
Motivation: To study the often-neglected role of integrators in linking developers, users, and machines for effective implementation of AI in military-related decision-making processes.

Method: Three-step approach: conceptualizing the sociotechnical relationship, identifying challenges in human-machine teaming for military decisions, and providing policy recommendations.

Result: Identified challenges include technological issues, integrators' roles, and human-machine dynamics within military decision contexts.

Conclusion: Policy recommendations aim to address the shortcomings in integrating AI systems for resort-to-force decision-making, advocating for improved sociotechnical system dynamics.

Abstract: The integration of AI systems into the military domain is changing the way
war-related decisions are made. It binds together three disparate groups of
actors - developers, integrators, users - and creates a relationship between
these groups and the machine, embedded in the (pre-)existing organisational and
system structures. In this article, we focus on the important, but often
neglected, group of integrators within such a sociotechnical system. In complex
human-machine configurations, integrators carry responsibility for linking the
disparate groups of developers and users in the political and military system.
To act as the mediating group requires a deep understanding of the other
groups' activities, perspectives and norms. We thus ask which challenges and
shortcomings emerge from integrating AI systems into resort-to-force (RTF)
decision-making processes, and how to address them. To answer this, we proceed
in three steps. First, we conceptualise the relationship between different
groups of actors and AI systems as a sociotechnical system. Second, we identify
challenges within such systems for human-machine teaming in RTF decisions. We
focus on challenges that arise a) from the technology itself, b) from the
integrators' role in the sociotechnical system, c) from the human-machine
interaction. Third, we provide policy recommendations to address these
shortcomings when integrating AI systems into RTF decision-making structures.

</details>


### [403] [Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility](https://arxiv.org/abs/2505.10426)
*Maurice Chiodo,Dennis Müller,Paul Siewert,Jean-Luc Wetherall,Zoya Yasmine,John Burden*

Main category: cs.CY

TL;DR: The paper explores the trade-offs in legal responsibility attribution and technical explainability in various Human-in-the-loop (HITL) AI systems, proposing a taxonomy for HITL failure modes and identifying gaps in legal frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the need for better understanding and regulation of Human-in-the-loop (HITL) AI systems for fairer legal responsibility attribution and to avoid human scapegoating.

Method: The authors use oracle machines from computability theory to formalize various HITL setups and devise a taxonomy categorizing HITL failure modes. They also analyze limitations and regulatory oversights in UK and EU legal frameworks.

Result: The paper highlights inherent limitations in HITL setups and demonstrates gaps in legal frameworks focusing on certain HITL setups that may fail to achieve ethical, legal, or sociotechnical outcomes.

Conclusion: The study suggests legal frameworks should adapt to acknowledge the effectiveness of different HITL setups and more accurately assign responsibility. It provides actionable insights for AI developers and lawmakers to design better HITL systems that align with ethical and legal standards.

Abstract: The legal compliance and safety of different Human-in-the-loop (HITL) setups
for AI can vary greatly. This manuscript aims to identify new ways of choosing
between such setups, and shows that there is an unavoidable trade-off between
the attribution of legal responsibility and the technical explainability of AI.
We begin by using the notion of oracle machines from computability theory to
formalise different HITL setups, distinguishing between trivial human
monitoring, single endpoint human action, and highly involved interaction
between the human(s) and the AI. These correspond to total functions, many-one
reductions, and Turing reductions respectively. A taxonomy categorising HITL
failure modes is then presented, highlighting the limitations on what any HITL
setup can actually achieve. Our approach then identifies oversights from UK and
EU legal frameworks, which focus on certain HITL setups which may not always
achieve the desired ethical, legal, and sociotechnical outcomes. We suggest
areas where the law should recognise the effectiveness of different HITL setups
and assign responsibility in these contexts, avoiding unnecessary and
unproductive human "scapegoating". Overall, we show how HITL setups involve
many technical design decisions, and can be prone to failures which are often
out of the humans' control. This opens up a new analytic perspective on the
challenges arising in the creation of HITL setups, helping inform AI developers
and lawmakers on designing HITL to better achieve their desired outcomes.

</details>


### [404] [The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World](https://arxiv.org/abs/2505.20181)
*Maurice Chiodo,Dennis Müller*

Main category: cs.CY

TL;DR: The paper highlights systemic risks arising from interactions among autonomous algorithmic systems and proposes governance measures to mitigate these risks.


<details>
  <summary>Details</summary>
Motivation: The deployment of AI and autonomous algorithms can lead to unexpected negative outcomes due to unregulated interactions, revealing a need for improved systemic governance.

Method: The authors analyze the ecosystem of autonomous systems and propose policy measures like phased system registration, licensing frameworks, and enhanced monitoring capabilities.

Result: The paper identifies inadequacies in current governance frameworks and suggests actionable measures to increase transparency and accountability in algorithmic ecosystems.

Conclusion: Governance frameworks must evolve to address risks from algorithmic system interactions, ensuring safer deployment and reducing catastrophic outcomes.

Abstract: The increasing deployment of Artificial Intelligence (AI) and other
autonomous algorithmic systems presents the world with new systemic risks.
While focus often lies on the function of individual algorithms, a critical and
underestimated danger arises from their interactions, particularly when
algorithmic systems operate without awareness of each other, or when those
deploying them are unaware of the full algorithmic ecosystem deployment is
occurring in. These interactions can lead to unforeseen, rapidly escalating
negative outcomes - from market crashes and energy supply disruptions to
potential physical accidents and erosion of public trust - often exceeding the
human capacity for effective monitoring and the legal capacities for proper
intervention. Current governance frameworks are inadequate as they lack
visibility into this complex ecosystem of interactions. This paper outlines the
nature of this challenge and proposes some initial policy suggestions centered
on increasing transparency and accountability through phased system
registration, a licensing framework for deployment, and enhanced monitoring
capabilities.

</details>


### [405] [A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation](https://arxiv.org/abs/2507.05275)
*Weibing Zheng,Laurah Turner,Jess Kropczynski,Murat Ozer,Seth Overla,Shane Halse*

Main category: cs.CY

TL;DR: The study introduces a Fuzzy Supervisor Agent (FSA) in a medical training platform to improve clinical reasoning by providing adaptive feedback using fuzzy logic.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assisting medical students with clinical reasoning during simulation-based training in medical education.

Method: The proposed FSA employs a Fuzzy Inference System (FIS) to analyze student interactions with clinical agents in real-time, applying fuzzy rule bases for elements like professionalism, medical relevance, and ethical behavior to provide context-aware support.

Result: The FSA demonstrates potential for scalable, adaptive, and human-like feedback during simulation, though empirical validation and broader application integration are still pending.

Conclusion: The FSA framework offers a promising solution to enhance simulation-based clinical reasoning education, paving the way for more natural and effective student supervision.

Abstract: Assisting medical students with clinical reasoning (CR) during clinical
scenario training remains a persistent challenge in medical education. This
paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA),
a novel component for the Multi-Agent Educational Clinical Scenario Simulation
(MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to
continuously interpret student interactions with specialized clinical agents
(e.g., patient, physical exam, diagnostic, intervention) using pre-defined
fuzzy rule bases for professionalism, medical relevance, ethical behavior, and
contextual distraction. By analyzing student decision-making processes in
real-time, the FSA is designed to deliver adaptive, context-aware feedback and
provides assistance precisely when students encounter difficulties. This work
focuses on the technical framework and rationale of the FSA, highlighting its
potential to provide scalable, flexible, and human-like supervision in
simulation-based medical education. Future work will include empirical
evaluation and integration into broader educational settings. More detailed
design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open
sourced here}.

</details>


### [406] [Hungary and AI: efforts and opportunities in comparison with Singapore](https://arxiv.org/abs/2507.05280)
*András Ferenczy*

Main category: cs.CY

TL;DR: The study evaluates Hungary's National AI Strategy in financial, governance, and conceptual aspects, benchmarking results against Singapore's AI strategies.


<details>
  <summary>Details</summary>
Motivation: To assess the efficacy of Hungary's AI strategy implementation and provide targeted recommendations for improvement, drawing comparisons to Singapore's framework.

Method: Analyzed strategic documents, financial records, and conducted expert interviews with key figures involved in Hungary's AI ecosystem.

Result: Total AI-related public investment estimated at EUR 4.65 billion, with shortcomings found in financial data and fragmented execution due to institutional reorganizations.

Conclusion: Hungary faces execution challenges and needs to adapt its AI strategy by leveraging large language models, improving collaboration networks, and positioning itself uniquely in AI automotive development.

Abstract: The study assesses Hungary's National AI Strategy and its implementation
through the analysis of strategic documents, publicly available financial
records, and expert interviews with the Hungarian AI Coalition President and
Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from
Hungary's strategy were evaluated through conceptual, governance, temporal, and
financial dimensions before being benchmarked against Singapore's National AI
Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of
EUR 4.65 billion in AI-related public investment in Hungary. Openly available
financial data was found for only half of the evaluated goals, and just three
projects made up 98\% of all documented funding. The research also reveals
Hungary's implementation challenges, including fragmented execution following
ministerial reorganizations and the absence of designated biennial reviews
since 2020. Furthermore, the paper provides targeted recommendations for
Hungary's forthcoming AI strategy, drawing on Singapore's framework as a
reference point. These include adapting to the era of large language models,
restructuring the existing triple helix network to foster more effective
dialogue and advocacy, and positioning the country as an East-West bridge for
automotive AI experimentation.

</details>


### [407] [Integrating Generative AI in BIM Education: Insights from Classroom Implementation](https://arxiv.org/abs/2507.05296)
*Islem Sahraoui,Kinam Kim,Lu Gao,Zia Din,Ahmed Senouci*

Main category: cs.CY

TL;DR: This study explored using generative AI in a graduate-level BIM course, finding that students met objectives but faced challenges with AI code debugging and performance.


<details>
  <summary>Details</summary>
Motivation: The research aimed to evaluate the application of generative AI to BIM compliance tasks in education, addressing a gap in prior research.

Method: Participants used a generative AI tool and Autodesk Revit to identify code violations. Surveys, interviews, and regression analysis measured effectiveness.

Result: Students met learning goals but struggled with debugging and tool inconsistencies, particularly those with limited programming skills.

Conclusion: Generative AI shows potential in BIM education but requires better instructional support to address student challenges.

Abstract: This study evaluates the implementation of a Generative AI-powered rule
checking workflow within a graduate-level Building Information Modeling (BIM)
course at a U.S. university. Over two semesters, 55 students participated in a
classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an
area with limited prior research. The instructional design included lectures on
prompt engineering and AI-driven rule checking, followed by an assignment where
students used a large language model (LLM) to identify code violations in
designs using Autodesk Revit. Surveys and interviews were conducted to assess
student workload, learning effectiveness, and overall experience, using the
NASA-TLX scale and regression analysis. Findings indicate students generally
achieved learning objectives but faced challenges such as difficulties
debugging AI-generated code and inconsistent tool performance, probably due to
their limited prompt engineering experience. These issues increased cognitive
and emotional strain, especially among students with minimal programming
backgrounds. Despite these challenges, students expressed strong interest in
future GenAI applications, particularly with clear instructional support.

</details>


### [408] [AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts](https://arxiv.org/abs/2507.05321)
*Kwangsuk Park,Jiwoong Yang*

Main category: cs.CY

TL;DR: AGACCI, a multi-agent system, outperforms traditional single-agent GPT-based models in evaluating code-based academic tasks by improving accuracy, depth, and consistency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing vision-language models (VLMs) in handling complex educational tasks, such as evaluating programming assignments that require structured reasoning and clear evaluation criteria.

Method: The authors designed AGACCI, a multi-agent system where specialized agents collaborate to evaluate code-based assignments. The system was tested on 360 graduate-level programming assignments provided by 60 participants with rubric scores and qualitative feedback from domain experts.

Result: AGACCI achieved superior performance over a single GPT-based model in terms of rubric and feedback accuracy, coherence, relevance, and consistency, while maintaining depth and instructional intent.

Conclusion: Multi-agent systems like AGACCI show promise for scalable, accurate, and context-sensitive assessment of complex academic tasks, particularly in code-oriented evaluations.

Abstract: Recent advances in AI-assisted education have encouraged the integration of
vision-language models (VLMs) into academic assessment, particularly for tasks
that require both quantitative and qualitative evaluation. However, existing
VLM based approaches struggle with complex educational artifacts, such as
programming tasks with executable components and measurable outputs, that
require structured reasoning and alignment with clearly defined evaluation
criteria. We introduce AGACCI, a multi-agent system that distributes
specialized evaluation roles across collaborative agents to improve accuracy,
interpretability, and consistency in code-oriented assessment. To evaluate the
framework, we collected 360 graduate-level code-based assignments from 60
participants, each annotated by domain experts with binary rubric scores and
qualitative feedback. Experimental results demonstrate that AGACCI outperforms
a single GPT-based baseline in terms of rubric and feedback accuracy,
relevance, consistency, and coherence, while preserving the instructional
intent and evaluative depth of expert assessments. Although performance varies
across task types, AGACCI highlights the potential of multi-agent systems for
scalable and context-aware educational evaluation.

</details>


### [409] [Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review](https://arxiv.org/abs/2507.06185)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 18 papers on arXiv in 2025 included hidden prompts in white text to manipulate AI-based peer review. The exposed practice revealed vulnerabilities in automated systems processing academic texts, urging for standardized policies and technical safeguards.


<details>
  <summary>Details</summary>
Motivation: To address and analyze the emerging issue of ethical challenges and manipulation in academic peer review systems, specifically through the misuse of generative AI tools.

Method: The paper investigates prompt injection misuse in LLMs, categorizing four types of hidden prompts and analyzing their intent and impact. It examines current policies from major publishers and evaluates the rationale behind such practices.

Result: Prompt injections were found to consistently aim to manipulate AI-based systems, contradicting claims of testing reviewer compliance. This manipulation exploits systematic weaknesses in academic workflows.

Conclusion: Stronger technical measures and harmonized global policies are essential to counter the manipulation of automated academic evaluation systems and foster ethical AI usage.

Abstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were
found to contain hidden instructions known as prompts designed to manipulate
AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY"
were concealed using techniques like white-colored text. Author responses
varied: one planned to withdraw the affected paper, while another defended the
practice as legitimate testing of reviewer compliance. This commentary analyzes
this practice as a novel form of research misconduct. We examine the technique
of prompt injection in large language models (LLMs), revealing four types of
hidden prompts, ranging from simple positive review commands to detailed
evaluation frameworks. The defense that prompts served as "honeypots" to detect
reviewers improperly using AI fails under examination--the consistently
self-serving nature of prompt instructions indicates intent to manipulate.
Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer
review entirely, while Springer Nature permits limited use with disclosure
requirements. The incident exposes systematic vulnerabilities extending beyond
peer review to any automated system processing scholarly texts, including
plagiarism detection and citation indexing. Our analysis underscores the need
for coordinated technical screening at submission portals and harmonized
policies governing generative AI (GenAI) use in academic evaluation.

</details>


### [410] [The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art](https://arxiv.org/abs/2507.05549)
*Prerana Khatiwada,Joshua Washington,Tyler Walsh,Ahmed Saif Hamed,Lokesh Bhatta*

Main category: cs.CY

TL;DR: The paper explores ethical issues surrounding generative AI art, examining its negative impacts and proposing potential solutions, with a focus on regulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexities and ethical challenges posed by generative AI art, which often leads to skepticism alongside its rapid advancements.

Method: The research investigates historical, causal, and consequential aspects of issues such as environmental impacts, misinformation, copyright concerns, and artist displacement. It also proposes solutions and analyzes various viewpoints.

Result: The study identifies significant negative outcomes of generative AI art, such as increased carbon emissions, misinformation, copyright infringement, unauthorized portrayal, and job loss among artists.

Conclusion: The paper concludes that proper legislation and regulation are essential to mitigate the ethical and societal challenges associated with generative AI art.

Abstract: As Artificial Intelligence (AI) continues to grow daily, more exciting (and
somewhat controversial) technology emerges every other day. As we see the
advancements in AI, we see more and more people becoming skeptical of it. This
paper explores the complications and confusion around the ethics of generative
AI art. We delve deep into the ethical side of AI, specifically generative art.
We step back from the excitement and observe the impossible conundrums that
this impressive technology produces. Covering environmental consequences,
celebrity representation, intellectual property, deep fakes, and artist
displacement. Our research found that generative AI art is responsible for
increased carbon emissions, spreading misinformation, copyright infringement,
unlawful depiction, and job displacement. In light of this, we propose multiple
possible solutions for these problems. We address each situation's history,
cause, and consequences and offer different viewpoints. At the root of it all,
though, the central theme is that generative AI Art needs to be correctly
legislated and regulated.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [411] [A Differential Evolution Algorithm with Neighbor-hood Mutation for DOA Estimation](https://arxiv.org/abs/2507.06020)
*Bo Zhou,Kaijie Xu,Yinghui Quan,Mengdao Xing*

Main category: eess.SP

TL;DR: The paper addresses computational inefficiency in the 2D Multiple Signal Classification algorithm for direction-of-arrival (DOA) estimation and proposes a fast, accurate alternative optimization method.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the computational inefficiency of exhaustive 2D grid search in DOA estimation, which hinders real-time application.

Method: The peak-finding process is reformulated as a multimodal optimization problem, and a Differential Evolution algorithm with Neighborhood Mutation (DE-NM) is proposed to avoid dense grid sampling.

Result: Simulation results validate that DE-NM maintains comparable estimation accuracy to traditional grid search methods while greatly reducing computational time.

Conclusion: The proposed DE-NM approach offers an efficient, accurate, and real-time applicable solution for high-resolution DOA estimation, making it suitable for practical implementations.

Abstract: Two-dimensional (2D) Multiple Signal Classification algorithm is a powerful
technique for high-resolution direction-of-arrival (DOA) estimation in array
signal processing. However, the exhaustive search over the 2D an-gular domain
leads to high computa-tional cost, limiting its applicability in real-time
scenarios. In this work, we reformulate the peak-finding process as a
multimodal optimization prob-lem, and propose a Differential Evolu-tion
algorithm with Neighborhood Mutation (DE-NM) to efficiently lo-cate multiple
spectral peaks without requiring dense grid sampling. Simu-lation results
demonstrate that the proposed method achieves comparable estimation accuracy to
the traditional grid search, while significantly reduc-ing computation time.
This strategy presents a promising solution for real-time, high-resolution DOA
estimation in practical applications. The imple-mentation code is available at
https://github.com/zzb-nice/DOA_multimodel_optimize.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [412] [On the Inherent Privacy of Zeroth Order Projected Gradient Descent](https://arxiv.org/abs/2507.05610)
*Devansh Gupta,Meisam Razaviyayn,Vatsal Sharan*

Main category: math.OC

TL;DR: This paper investigates whether the inherent noise in zeroth-order estimators ensures differential privacy and concludes that it is insufficient under certain conditions, requiring additional noise for privatization.


<details>
  <summary>Details</summary>
Motivation: To address the critical question of whether zeroth-order optimization methods, which rely on inherently random estimators, can automatically satisfy differential privacy without added noise.

Method: The study analyzes the privacy dynamics of zeroth-order gradient descent (ZO-GD) under different conditions, including fixed and random initialization, convex objective functions, and intermediate iterations.

Result: It demonstrates that ZO-GD is not inherently differentially private in some scenarios, and the privacy loss can grow superlinearly with the number of iterations for convex objectives.

Conclusion: The inherent randomness of zeroth-order estimators is insufficient to guarantee differential privacy, necessitating explicit mechanisms to ensure privacy-preserving optimization.

Abstract: Differentially private zeroth-order optimization methods have recently gained
popularity in private fine tuning of machine learning models due to their
reduced memory requirements. Current approaches for privatizing zeroth-order
methods rely on adding Gaussian noise to the estimated zeroth-order gradients.
However, since the search direction in the zeroth-order methods is inherently
random, researchers including Tang et al. (2024) and Zhang et al. (2024a) have
raised an important question: is the inherent noise in zeroth-order estimators
sufficient to ensure the overall differential privacy of the algorithm? This
work settles this question for a class of oracle-based optimization algorithms
where the oracle returns zeroth-order gradient estimates. In particular, we
show that for a fixed initialization, there exist strongly convex objective
functions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)
is not differentially private. Furthermore, we show that even with random
initialization and without revealing (initial and) intermediate iterates, the
privacy loss in ZO-GD can grow superlinearly with the number of iterations when
minimizing convex objective functions.

</details>


### [413] [Exact and efficient basis pursuit denoising via differential inclusions and a selection principle](https://arxiv.org/abs/2507.05562)
*Gabriel P. Langlois,Jérôme Darbon*

Main category: math.OC

TL;DR: The paper introduces a highly accurate and efficient algorithm for Basis Pursuit Denoising (BPDN), overcoming limitations of existing methods using differential inclusions.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and inaccuracies in existing algorithms for Basis Pursuit Denoising (BPDN), especially for high-dimensional applications requiring fast, precise solutions.

Method: The paper employs differential inclusions and introduces a selection principle to turn BPDN's dual problem into calculating the trajectory of an integrable projected dynamical system, enabling exact computation.

Result: The proposed algorithm outperforms state-of-the-art algorithms in terms of both accuracy and efficiency, confirmed by numerical experiments. It also introduces homotopy and novel greedy algorithms.

Conclusion: The algorithm provides exact, computationally fast solutions for BPDN and has potential applications in solving a broader class of polyhedral-constrained optimization problems.

Abstract: Basis pursuit denoising (BPDN) is a cornerstone of compressive sensing,
statistics and machine learning. While various algorithms for BPDN have been
proposed, they invariably suffer from drawbacks and must either favor
efficiency at the expense of accuracy or vice versa. As such, state-of-the-art
algorithms remain ineffective for high-dimensional applications that require
accurate solutions within a reasonable amount of computational time. In this
work, we address this issue and propose an exact and efficient algorithm for
BPDN using differential inclusions. Specifically, we prove that a selection
principle from the theory of differential inclusions turns the dual problem of
BPDN into calculating the trajectory of an \emph{integrable} projected
dynamical system, that is, whose trajectory and asymptotic limit can be
computed exactly. Our analysis naturally yields an exact algorithm, numerically
up to machine precision, that is amenable to computing regularization paths and
very fast. Numerical experiments confirm that our algorithm outperforms the
state-of-the-art algorithms in both accuracy and efficiency. Moreover, we show
that the global continuation of solutions (in terms of the hyperparameter and
data) of the projected dynamical system yields a rigorous homotopy algorithm
for BPDN, as well as a novel greedy algorithm for computing feasible solutions
to basis pursuit in strongly polynomial time. Beyond this work, we expect that
our results and analysis can be adapted to compute exact or approximate
solutions to a broader class of polyhedral-constrained optimization problems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [414] [MP-ALOE: An r2SCAN dataset for universal machine learning interatomic potentials](https://arxiv.org/abs/2507.05559)
*Matthew C. Kuner,Aaron D. Kaplan,Kristin A. Persson,Mark Asta,Daryl C. Chrzan*

Main category: cond-mat.mtrl-sci

TL;DR: Introducing MP-ALOE, a dataset with almost 1 million high-accuracy DFT calculations, optimized via active learning to include off-equilibrium structures. Benchmarked machine learning models demonstrate strong predictive capabilities and resilience across thermochemical properties, extreme conditions, and molecular dynamics.


<details>
  <summary>Details</summary>
Motivation: The need for a comprehensive and high-quality dataset for training machine learning interatomic potentials that can accurately predict properties under diverse and extreme structural and dynamic conditions.

Method: Active learning methods were used to generate nearly 1 million DFT calculations based on r2SCAN meta-generalized gradient approximation, focusing on off-equilibrium structures across 89 elements.

Result: The machine learning interatomic potential trained on MP-ALOE performed excellently across multiple benchmarks, including predicting thermochemical properties, handling off-equilibrium structures, performing under extreme deformations, and maintaining stability in molecular dynamics under extreme conditions.

Conclusion: MP-ALOE demonstrates robust predictive performance and is a valuable resource for the scientific community, offering advancements in machine learning-driven material simulations and analyses, particularly under extreme conditions.

Abstract: We present MP-ALOE, a dataset of nearly 1 million DFT calculations using the
accurate r2SCAN meta-generalized gradient approximation. Covering 89 elements,
MP-ALOE was created using active learning and primarily consists of
off-equilibrium structures. We benchmark a machine learning interatomic
potential trained on MP-ALOE, and evaluate its performance on a series of
benchmarks, including predicting the thermochemical properties of equilibrium
structures; predicting forces of far-from-equilibrium structures; maintaining
physical soundness under static extreme deformations; and molecular dynamic
stability under extreme temperatures and pressures. MP-ALOE shows strong
performance on all of these benchmarks, and is made public for the broader
community to utilize.

</details>
