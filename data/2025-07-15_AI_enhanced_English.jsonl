{"id": "2507.08836", "pdf": "https://arxiv.org/pdf/2507.08836", "abs": "https://arxiv.org/abs/2507.08836", "authors": ["Damien Fovet", "Shashank Chamoli", "Sarah Oury", "Srishti Singhal"], "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.", "AI": {"tldr": "The paper evaluates the performance of a compression method (CompactifAI) on the Llama 3.1 8B language model, showing reduced computational resources while maintaining accuracy.", "motivation": "To address the challenge of increasing computational resource demands and costs for large language models while preserving their accuracy.", "method": "The study evaluated CompactifAI by comparing compressed and full-sized Llama 3.1 8B models using Codecarbon for energy consumption analysis and Ragas for accuracy assessment.", "result": "The compressed model significantly reduced computational resource usage while maintaining accuracy.", "conclusion": "CompactifAI enhances efficiency, scalability, and cost-effectiveness for language models without compromising their performance."}}
{"id": "2507.08923", "pdf": "https://arxiv.org/pdf/2507.08923", "abs": "https://arxiv.org/abs/2507.08923", "authors": ["Rub\u00e9n Rodr\u00edguez \u00c1lvarez", "Denisa-Andreea Constantinescu", "Miguel Pe\u00f3n-Quir\u00f3s", "David Atienza"], "title": "CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers", "categories": ["cs.AR", "cs.CY", "cs.PF", "B.8.2; C.0; C.1.4; C.4; C.5.5; J.4; K.1; K.4.1; K.6.4"], "comment": "15 pages, 11 figures, 2 tables", "summary": "The rapid expansion of data centers (DCs) to support large-scale AI and\nscientific workloads is driving unsustainable growth in energy consumption and\ngreenhouse gas emissions. While successive generations of hardware platforms\nhave improved performance and energy efficiency, the question remains whether\nnew, more efficient platforms can realistically offset the rising emissions\nassociated with increasing demand. Prior studies often overlook the complex\ntrade-offs in such transitions by failing to account for both the economic\nincentives and the projected compute demand growth over the operational\nlifetime of the devices. In response, we present CEO-DC, an integrated model\nand decision-making methodology for Carbon and Economy Optimization in Data\nCenters. CEO-DC models the competing forces of cost, carbon, and compute demand\nto guide optimal platform procurement and replacement strategies. We propose\nmetrics to steer procurement, platform design, and policy decisions toward\nsustainable DC technologies. Given current platform trends, our AI case study\nusing CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces\ntotal emissions. However, these upgrades fail to scale with DC demand growth\ntrends without increasing total emissions in over 44% of cases, and require\neconomic incentives for adoption in over 72%. Furthermore, current carbon\nprices are insufficient to motivate upgrades in 9 out of the 14 countries with\nthe highest number of DCs globally. We also find that optimizing platforms for\nenergy efficiency at the expense of latency can increase the carbon price\nrequired to justify their adoption. In summary, CEO-DC provides actionable\ninsights for DC architects, platform designers, and policymakers by timing\nlegacy platform upgrades, constraining DC growth to sustainable levels,\noptimizing platform performance-to-cost ratios, and increasing incentives.", "AI": {"tldr": "This paper introduces CEO-DC, a model to optimize carbon and economic factors in data centers, showing that while timely platform upgrades may reduce emissions, they demand economic incentives and policy support due to persistent growth in compute demand.", "motivation": "The motivation is to address the unsustainable growth in energy consumption and greenhouse gas emissions by data centers driven by increasing demand for AI and scientific workloads, while accounting for economic and operational trade-offs in transitioning platforms.", "method": "The paper proposes CEO-DC, an integrated model and methodology that evaluates cost, carbon emissions, and compute demand to offer optimal strategies for platform procurement, replacement, and design.", "result": "Using CEO-DC, the study finds that upgrading legacy platforms every 4 years can reduce emissions but is not scalable without increasing emissions in 44% of cases, requires economic incentives in 72% of cases, and is hampered by insufficient carbon prices in many leading data center countries.", "conclusion": "CEO-DC provides guidance for achieving sustainable data center operations by strategically timing upgrades, managing data center growth, optimizing cost-performance trade-offs, and advocating for better policies and incentives."}}
{"id": "2507.09790", "pdf": "https://arxiv.org/pdf/2507.09790", "abs": "https://arxiv.org/abs/2507.09790", "authors": ["Helge Spieker", "Th\u00e9o Matricon", "Nassim Belmecheri", "J\u00f8rn Eirik Betten", "Gauthier Le Bartz Lyan", "Heraldo Borges", "Quentin Mazouni", "Dennis Gross", "Arnaud Gotlieb", "Mathieu Acher"], "title": "Prompting for Performance: Exploring LLMs for Configuring Software", "categories": ["cs.SE", "cs.AI", "cs.PF"], "comment": null, "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.", "AI": {"tldr": "Large language models (LLMs) are explored as tools to assist in performance-oriented software configuration tasks like identifying relevant options and recommending configurations. While LLMs show promise in aligning with expert knowledge, they also exhibit limitations such as hallucinations or superficial reasoning.", "motivation": "The motivation is to address the challenge of navigating complex configuration spaces in software systems that influence performance metrics, where current methods require domain expertise or are computationally expensive with machine learning techniques.", "method": "The study investigates the capabilities of LLMs through prompt-based tasks such as identifying relevant options, ranking configurations, and recommending performant configurations on systems like compilers, video encoders, and SAT solvers.", "result": "Preliminary results indicate that LLMs can align well with expert knowledge for some tasks and systems. However, their performance is inconsistent and they may display hallucinations or shallow reasoning in certain scenarios.", "conclusion": "LLMs hold potential for assisting in software configuration tasks, but further systematic evaluation and refinement are required to overcome their limitations and enhance their reliability."}}
{"id": "2507.10367", "pdf": "https://arxiv.org/pdf/2507.10367", "abs": "https://arxiv.org/abs/2507.10367", "authors": ["Jingwei Xu", "Junbin Kang", "Mingkai Dong", "Mingyu Liu", "Lu Zhang", "Shaohong Guo", "Ziyan Qiu", "Mingzhen You", "Ziyi Tian", "Anqi Yu", "Tianhong Ding", "Xinwei Hu", "Haibo Chen"], "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline", "categories": ["cs.DC", "cs.PF"], "comment": "Accepted by NSDI'26", "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.", "AI": {"tldr": "FalconFS introduces a DFS with a stateless-client architecture, replacing client-side caching with server-side path resolution and optimizations, achieving significant throughput improvements in deep learning pipelines.", "motivation": "Existing client-side caching in DFSs is found to be ineffective and memory-intensive in deep learning pipelines.", "method": "FalconFS employs server-side path resolution using hybrid metadata indexing, lazy namespace replication, concurrent request merging, and VFS shortcuts.", "result": "FalconFS demonstrates up to 5.72\u00d7 throughput improvements for small file I/O and up to 12.81\u00d7 throughput for deep learning training, outperforming CephFS and Lustre.", "conclusion": "FalconFS provides a scalable, high-performance solution for deep learning pipelines, validated in Huawei's production environment with 10,000 NPUs for one year."}}
{"id": "2507.09385", "pdf": "https://arxiv.org/pdf/2507.09385", "abs": "https://arxiv.org/abs/2507.09385", "authors": ["Kevin Reyes", "Vasco Cortez"], "title": "Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding", "categories": ["cs.NE", "cs.LG"], "comment": "2025 IEEE Conference on Artificial Intelligence (CAI)", "summary": "Fraud detection is one of the most important challenges that financial\nsystems must address. Detecting fraudulent transactions is critical for payment\ngateway companies like Flow Payment, which process millions of transactions\nmonthly and require robust security measures to mitigate financial risks.\nIncreasing transaction authorization rates while reducing fraud is essential\nfor providing a good user experience and building a sustainable business. For\nthis reason, discovering novel and improved methods to detect fraud requires\ncontinuous research and investment for any company that wants to succeed in\nthis industry. In this work, we introduced a novel method for detecting\ntransactional fraud by incorporating the Relative Distance Rotating Encoding\n(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE\nenhances the characterization of time series data within a Transformer, leading\nto improved fraud detection by better capturing temporal dependencies and event\nrelationships.", "AI": {"tldr": "The paper introduces a novel fraud detection method combining Relative Distance Rotating Encoding (ReDRE) with the RoFormer model to enhance the analysis of time series data and improve fraudulent transaction detection.", "motivation": "Fraud detection is crucial for financial systems, especially for payment companies like Flow Payment, who process millions of transactions and seek to ensure security, improve user experience, and build sustainable operations.", "method": "A new method integrates Relative Distance Rotating Encoding (ReDRE) into the RoFormer model, enabling the Transformer to better capture time series dependencies and relationships via angle rotation.", "result": "The incorporation of ReDRE improves the RoFormer model\u2019s capacity to detect fraudulent transactions by enhancing temporal data characterization.", "conclusion": "The proposed method shows potential in advancing fraud detection capabilities and addressing critical security challenges for financial systems."}}
{"id": "2507.08954", "pdf": "https://arxiv.org/pdf/2507.08954", "abs": "https://arxiv.org/abs/2507.08954", "authors": ["Alexander Fuerst", "Siddharth Anil", "Vishakha Dixit", "Purushottam", "Kulkarni", "Prateek Sharma"], "title": "MQFQ-Sticky: Fair Queueing For Serverless GPU Functions", "categories": ["cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Hardware accelerators like GPUs are now ubiquitous in data centers, but are\nnot fully supported by common cloud abstractions such as Functions as a Service\n(FaaS). Many popular and emerging FaaS applications such as machine learning\nand scientific computing can benefit from GPU acceleration. However, FaaS\nframeworks (such as OpenWhisk) are not capable of providing this acceleration\nbecause of the impedance mismatch between GPUs and the FaaS programming model,\nwhich requires virtualization and sandboxing of each function. The challenges\nare amplified due to the highly dynamic and heterogeneous FaaS workloads. This\npaper presents the design and implementation of a FaaS system for providing GPU\nacceleration in a black-box manner (without modifying function code). Running\nsmall functions in containerized sandboxes is challenging due to limited GPU\nconcurrency and high cold-start overheads, resulting in heavy queueing of\nfunction invocations. We show how principles from I/O scheduling, such as fair\nqueuing and anticipatory scheduling, can be translated to function scheduling\non GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory\nmanagement approach, which balances the tradeoffs between locality, fairness,\nand latency. Empirical evaluation on a range of workloads shows that it reduces\nfunction latency by 2x to 20x compared to existing GPU and CPU queueing\npolicies.", "AI": {"tldr": "The paper addresses integrating GPUs into Functions as a Service (FaaS) systems and introduces a scheduling approach, MQFQ-Sticky, achieving up to 20x reduced latency.", "motivation": "The motivation is to enhance FaaS systems, which lack full GPU support, despite GPUs\u2019 potential to accelerate applications like machine learning and scientific computing.", "method": "The authors propose a system using I/O scheduling principles (fair queuing and anticipatory scheduling) and develop MQFQ-Sticky for function scheduling and GPU memory management within containerized sandboxes.", "result": "The proposed approach demonstrated a 2x to 20x reduction in function latency compared to existing CPU and GPU queueing methods in various workloads.", "conclusion": "Integrating GPUs into FaaS can be efficiently realized by adapting I/O scheduling techniques, making it feasible for dynamic workloads without code modification, with significant performance gains."}}
{"id": "2507.09539", "pdf": "https://arxiv.org/pdf/2507.09539", "abs": "https://arxiv.org/abs/2507.09539", "authors": ["Anna Bolotina", "Christoph M. Kirsch", "Stefanie Muroya Lei", "Matthias Pleschinger"], "title": "Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams", "categories": ["cs.PL"], "comment": null, "summary": "Symbolic execution is a powerful technique for analyzing the behavior of\nsoftware yet scalability remains a challenge due to state explosion in control\nand data flow. Existing tools typically aim at managing control flow\ninternally, often at the expense of completeness, while offloading reasoning\nover data flow to SMT solvers. Moreover, reasoning typically happens on source\ncode or intermediate representation level to leverage structural information,\nmaking machine code generation part of the trust base. We are interested in\nchanging the equation in two non-trivial ways: pushing reasoning down to\nmachine code level, and then offloading reasoning entirely into SMT solvers and\nother, possibly more efficient solver technology. In more abstract terms, we\nare asking if bit-precise reasoning technology can be made scalable on\nsoftware, and not just hardware. For this purpose, we developed two tools\ncalled rotor and bitme for model generation and bounded model checking,\nrespectively. We chose RISC-V restricted to integer arithmetic as modeling\ntarget for rotor since RISC-V integer semantics is essentially equivalent to\nestablished SMT semantics over bitvectors and arrays of bitvectors. While\nstate-of-the-art SMT solvers struggle in our experiments, we have evidence that\nthere is potential for improvement. To show the potential, we have slightly\ngeneralized and then implemented in bitme two types of binary decision diagrams\n(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered\nbinary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input\nthrough models, essentially generalizing constant propagation to domain\npropagation. SMT solvers only get involved when model input cannot be\npropagated, significanly speeding up SMT solving. We then study the impact on\nstate explosion of CFLOBDDs, which are potentially more scalable than ADDs.", "AI": {"tldr": "The paper introduces two tools, rotor and bitme, to improve symbolic execution scalability by reasoning at the machine code level and leveraging more advanced solver techniques.", "motivation": "The authors aim to address scalability challenges in symbolic execution, particularly state explosion in control and data flow.", "method": "They propose pushing reasoning to the machine code level (RISC-V target) and using tools like rotor and bitme, which incorporate advanced techniques like BDDs and CFLOBDDs for efficient model processing.", "result": "Their experiments show state-of-the-art SMT solvers struggle, but their approach demonstrates promise, especially with CFLOBDDs for mitigating state explosion.", "conclusion": "Bit-precise reasoning has the potential to scale for software, and tools like rotor and bitme can contribute to this goal by optimizing symbolic execution and solver interactions."}}
{"id": "2507.08943", "pdf": "https://arxiv.org/pdf/2507.08943", "abs": "https://arxiv.org/abs/2507.08943", "authors": ["Pedro Lopes", "Paola Accioly", "Paulo Borba", "Vitor Menezes"], "title": "Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches", "categories": ["cs.SE", "D.2.7"], "comment": "11 pages with 3 figures", "summary": "Git has become one of the most widely used version control systems today.\nAmong its distinguishing features, its ability to easily and quickly create\nbranches stands out, allowing teams to customize their workflows. In this\ncontext, various formats of collaborative development workflows using Git have\nemerged and gained popularity among software engineers. We can categorize such\nworkflows into two main types: branch-based workflows and trunk-based\nworkflows. Branch-based workflows typically define a set of remote branches\nwith well-defined objectives, such as feature branches, a branch for feature\nintegration, and a main branch. The goal is to migrate changes from the most\nisolated branch to the main one shared by all as the code matures. In this\ncategory, GitFlow stands out as the most popular example. In contrast,\ntrunk-based workflows have a single remote branch where developers integrate\ntheir changes directly. In this range of options, choosing a workflow that\nmaximizes team productivity while promoting software quality becomes a\nnon-trivial task. Despite discussions on forums, social networks, and blogs,\nfew scientific articles have explored this topic. In this work, we provide\nevidence on how Brazilian developers work with Git workflows and what factors\nfavor or hinder the use of each model. To this end, we conducted\nsemi-structured interviews and a survey with software developers. Our results\nindicate that trunk-based development favors fast-paced projects with\nexperienced and smaller teams, while branch-based development suits less\nexperienced and larger teams better, despite posing management challenges.", "AI": {"tldr": "The paper explores Git workflows, focusing on branch-based and trunk-based workflows, and studies their suitability for different team types.", "motivation": "The authors aim to fill the research gap on the effectiveness of Git workflows for different team contexts.", "method": "Conducted semi-structured interviews and surveys with software developers, particularly in Brazil.", "result": "Trunk-based workflows suit fast-paced projects with smaller, experienced teams, while branch-based workflows are better for larger, less experienced teams but face management challenges.", "conclusion": "Different Git workflows have specific advantages depending on the project pace, team size, and experience."}}
{"id": "2507.08865", "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "Spatial ModernBERT leverages text and spatial embeddings to accurately extract tabular data and key-value pairs from financial documents using a token classification approach.", "motivation": "The paper addresses the need for accurate extraction of tables and key-value pairs from financial documents to improve business workflows like auditing and data processing.", "method": "The model Spatial ModernBERT utilizes spatial embeddings and a transformer-based architecture, performing token classification across three heads (Label, Column, Row). It is pretrained on PubTables-1M and fine-tuned with cross-entropy loss on a financial dataset.", "result": "The proposed model demonstrates robustness and high accuracy in extracting tabular layouts and key-value pairs, verified through empirical evaluation.", "conclusion": "Spatial ModernBERT effectively integrates textual and spatial information to enhance automated processing of financial documents."}}
{"id": "2507.09024", "pdf": "https://arxiv.org/pdf/2507.09024", "abs": "https://arxiv.org/abs/2507.09024", "authors": ["Marie St-Laurent", "Basile Pinsard", "Oliver Contier", "Elizabeth DuPre", "Katja Seeliger", "Valentina Borghesani", "Julie A. Boyle", "Lune Bellec", "Martin N. Hebart"], "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience", "categories": ["q-bio.NC", "cs.CV"], "comment": "29 pages manuscript, 5 figures, 12 pages supplementary material", "summary": "Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.\nCNeuroMod-THINGS meets this need by capturing neural representations for a wide\nset of semantic concepts using well-characterized stimuli in a new\ndensely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS\nexploits synergies between two existing projects: the THINGS initiative\n(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has\ndeveloped a common set of thoroughly annotated images broadly sampling natural\nand man-made objects which is used to acquire a growing collection of\nlarge-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring\nhundreds of hours of fMRI data from a core set of participants during\ncontrolled and naturalistic tasks, including visual tasks like movie watching\nand videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each\ncompleted 33-36 sessions of a continuous recognition paradigm using\napproximately 4000 images from the THINGS stimulus set spanning 720 categories.\nWe report behavioural and neuroimaging metrics that showcase the quality of the\ndata. By bridging together large existing resources, CNeuroMod-THINGS expands\nour capacity to model broad slices of the human visual experience.", "AI": {"tldr": "CNeuroMod-THINGS combines two existing projects to create a new large-scale fMRI dataset capturing neural representations of semantic concepts using well-annotated stimuli.", "motivation": "The need for larger, high-quality neuroimaging datasets drives the creation of CNeuroMod-THINGS, facilitating better modeling of neural representations of visual experiences.", "method": "The study integrates the annotated image set from THINGS with extensive fMRI data acquisition from CNeuroMod participants exposed to 4000 images across 33-36 sessions.", "result": "High-quality behavioral and neuroimaging metrics were reported, demonstrating the dataset's utility for modeling human visual experiences.", "conclusion": "CNeuroMod-THINGS enhances the capacity to model the neural basis of human visual experiences by leveraging and uniting existing large-scale resources."}}
{"id": "2507.08851", "pdf": "https://arxiv.org/pdf/2507.08851", "abs": "https://arxiv.org/abs/2507.08851", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried W\u00f6ber", "Gerald Steinbauer-Wagner"], "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "categories": ["cs.RO"], "comment": null, "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "AI": {"tldr": "The paper presents OTAS, a method for open-vocabulary outdoor segmentation, addressing challenges in unstructured environments by clustering and grounding semantic features extracted directly from pretrained vision models.", "motivation": "To address the challenges of understanding open-world semantics in unstructured outdoor environments, where traditional object-centric vision-language mappings fail due to issues like semantic ambiguities and unclear class boundaries.", "method": "OTAS works by clustering semantically similar token structures from pretrained vision models across single and multiple views, grounding them in language. It reconstructs a geometrically consistent feature field for open-vocabulary segmentation. The method operates zero-shot without scene-specific fine-tuning.", "result": "OTAS demonstrated a minor improvement in 2D segmentation IoU on Off-Road Freespace Detection and up to 151% IoU improvement in 3D segmentation on TartanAir. It also runs at ~17 fps and performs well in real-world robotic applications.", "conclusion": "OTAS effectively overcomes limitations of traditional and open-vocabulary segmentation methods. Its zero-shot efficiency, competitive performance, and real-time applicability make it suitable for robotic planning in outdoor environments."}}
{"id": "2507.08806", "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "The paper proposes removing redundant reasoning tokens in large language models by measuring token-level attention scores to improve long-form reasoning accuracy.", "motivation": "To address the redundancy in reasoning chains of large language models, which hampers performance, particularly on reasoning-intensive tasks.", "method": "Systematically identify and prune redundant tokens using token-level attention scores to an end-of-thinking token. Apply structure-aware pruning to remove low-contributing reasoning chunks.", "result": "Redundancy removal improves reasoning accuracy across benchmarks, especially in mathematical competition tasks like AIME and AMC.", "conclusion": "Pruning low-relevant reasoning tokens enhances clear thinking and boosts the performance of large language models in complex reasoning tasks without requiring additional training."}}
{"id": "2507.08831", "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "This paper addresses the sensitivity of Vision-Language Navigation in Continuous Environments (VLNCE) to viewpoint changes, introducing a method called View Invariant Learning (VIL) to enhance robustness.", "motivation": "To improve the ability of AI navigation agents to handle changes in camera viewpoint (height and angle) during Vision-Language Navigation tasks.", "method": "The paper introduces VIL, a post-training strategy using contrastive learning for view-invariant features and a teacher-student framework for enhancing view-invariance in the Waypoint Predictor Module.", "result": "The method outperforms state-of-the-art approaches on Success Rate by 8-15% in the V2-VLNCE setting and achieves state-of-the-art results in the standard VLNCE setting on challenging datasets like RxR-CE.", "conclusion": "VIL enhances robustness to viewpoint variation without reducing performance in standard scenarios, making it a versatile and effective post-training approach."}}
{"id": "2507.08828", "pdf": "https://arxiv.org/pdf/2507.08828", "abs": "https://arxiv.org/abs/2507.08828", "authors": ["Tarek Berghout"], "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm\nthat advances beyond conventional Machine Learning (ML) and Deep Learning (DL).\nWhile DL focuses on learning from static data representations, RE proposes an\nadditional dimension: learning from the evolving behavior of models themselves.\nRE emphasizes multiple mappings of data through identical deep architectures\nand analyzes their internal representations (i.e., feature maps) in conjunction\nwith observed performance signals such as loss. By incorporating these\nbehavioral traces, RE enables iterative self-improvement, allowing each model\nversion to gain insight from its predecessors. The framework is extended\nthrough Multiverse RE (MVRE), which aggregates signals from parallel model\ninstances, and further through Heterogeneous MVRE (HMVRE), where models of\nvarying architectures contribute diverse perspectives. A scalable and adaptive\nvariant, Sc-HMVRE, introduces selective mechanisms and scale diversity for\nreal-world deployment. Altogether, RE presents a shift in DL: from purely\nrepresentational learning to behavior-aware, self-evolving systems. It lays the\ngroundwork for a new class of intelligent models capable of reasoning over\ntheir own learning dynamics, offering a path toward scalable, introspective,\nand adaptive artificial intelligence. A simple code example to support\nbeginners in running their own experiments is provided in Code Availability\nSection of this paper.", "AI": {"tldr": "This paper introduces Recurrent Expansion (RE), a novel learning paradigm shifting from representational learning toward behavior-aware, self-evolving systems.", "motivation": "Current Machine Learning and Deep Learning techniques focus on static data, limiting adaptability and introspection.", "method": "Recurrent Expansion analyzes feature maps via iterative mappings with identical model architectures, incorporating performance signals. Variants like MVRE, HMVRE, and Sc-HMVRE improve scalability and adaptability.", "result": "RE enables iterative self-improvement with model versions drawing insights from predecessors, supported by advanced parallel mechanisms.", "conclusion": "Recurrent Expansion paves the way for scalable, adaptive, and introspective AI systems by leveraging models' evolving behaviors."}}
{"id": "2507.09747", "pdf": "https://arxiv.org/pdf/2507.09747", "abs": "https://arxiv.org/abs/2507.09747", "authors": ["Dongyang Li", "Haoyang Qin", "Mingyang Wu", "Chen Wei", "Quanying Liu"], "title": "BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings", "categories": ["cs.NE"], "comment": "10 pages, ACM MM 2025", "summary": "Understanding how the brain represents visual information is a fundamental\nchallenge in neuroscience and artificial intelligence. While AI-driven decoding\nof neural data has provided insights into the human visual system, integrating\nmultimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical\nhurdle due to their inherent spatiotemporal misalignment. Current approaches\noften analyze these modalities in isolation, limiting a holistic view of neural\nrepresentation. In this study, we introduce BrainFLORA, a unified framework for\nintegrating cross-modal neuroimaging data to construct a shared neural\nrepresentation. Our approach leverages multimodal large language models (MLLMs)\naugmented with modality-specific adapters and task decoders, achieving\nstate-of-the-art performance in joint-subject visual retrieval task and has the\npotential to extend multitasking. Combining neuroimaging analysis methods, we\nfurther reveal how visual concept representations align across neural\nmodalities and with real world object perception. We demonstrate that the\nbrain's structured visual concept representations exhibit an implicit mapping\nto physical-world stimuli, bridging neuroscience and machine learning from\ndifferent modalities of neural imaging. Beyond methodological advancements,\nBrainFLORA offers novel implications for cognitive neuroscience and\nbrain-computer interfaces (BCIs). Our code is available at\nhttps://github.com/ncclab-sustech/BrainFLORA.", "AI": {"tldr": "The paper presents BrainFLORA, a framework aimed at unifying multimodal neuroimaging data to interpret neural visual representations.", "motivation": "To overcome the challenge of spatiotemporal misalignment in integrating neuroimaging modalities (EEG, MEG, fMRI) and enable a holistic understanding of neural visual representation.", "method": "BrainFLORA uses multimodal large language models enhanced with modality-specific adapters and task decoders to merge data from multiple neuroimaging modalities.", "result": "It achieves state-of-the-art results in joint-subject visual retrieval tasks and provides insights into how visual concept representations align with real-world stimuli across neural modalities.", "conclusion": "BrainFLORA links neuroscience with machine learning, offering advancements for cognitive neuroscience and BCIs while enabling unified cross-modal neural understanding."}}
{"id": "2507.09546", "pdf": "https://arxiv.org/pdf/2507.09546", "abs": "https://arxiv.org/abs/2507.09546", "authors": ["Xiangwang Hou", "Jingjing Wang", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato"], "title": "Lightweight Federated Learning over Wireless Edge Networks", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "With the exponential growth of smart devices connected to wireless networks,\ndata production is increasing rapidly, requiring machine learning (ML)\ntechniques to unlock its value. However, the centralized ML paradigm raises\nconcerns over communication overhead and privacy. Federated learning (FL)\noffers an alternative at the network edge, but practical deployment in wireless\nnetworks remains challenging. This paper proposes a lightweight FL (LTFL)\nframework integrating wireless transmission power control, model pruning, and\ngradient quantization. We derive a closed-form expression of the FL convergence\ngap, considering transmission error, model pruning error, and gradient\nquantization error. Based on these insights, we formulate an optimization\nproblem to minimize the convergence gap while meeting delay and energy\nconstraints. To solve the non-convex problem efficiently, we derive closed-form\nsolutions for the optimal model pruning ratio and gradient quantization level,\nand employ Bayesian optimization for transmission power control. Extensive\nexperiments on real-world datasets show that LTFL outperforms state-of-the-art\nschemes.", "AI": {"tldr": "The paper introduces LTFL, a lightweight federated learning framework designed for wireless networks, improving performance through model pruning, gradient quantization, and transmission power control.", "motivation": "The motivation is to address the challenges of deploying federated learning in wireless networks, including communication overhead, privacy concerns, and the need for efficient resource management.", "method": "The method involves integrating techniques like model pruning, gradient quantization, and transmission power control into the federated learning framework. Additionally, a convergence gap expression is derived, and an optimization problem is solved using Bayesian techniques and analytical solutions.", "result": "The proposed LTFL framework demonstrates superior performance over state-of-the-art methods in experiments using real-world datasets.", "conclusion": "LTFL effectively minimizes convergence gaps while meeting delay and energy constraints. It represents a significant improvement for federated learning in wireless networks."}}
{"id": "2507.09010", "pdf": "https://arxiv.org/pdf/2507.09010", "abs": "https://arxiv.org/abs/2507.09010", "authors": ["Chun-Ting Chen", "HanGyeol Mun", "Jian Meng", "Mohamed S. Abdelfattah", "Jae-sun Seo"], "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference", "categories": ["cs.AR", "cs.AI"], "comment": "Accepted as a conference paper at the 2025 IEEE/ACM International\n  Symposium on Low Power Electronics and Design (ISLPED)", "summary": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation.", "AI": {"tldr": "The paper proposes a highly efficient edge inference accelerator for large language models (LLMs) using a hybrid systolic array architecture combined with MXINT4 weight quantization and optimized non-linear operations.", "motivation": "To enable secure, low-latency, and cost-effective edge inference for large language models, addressing memory and compute inefficiencies in the constraints of edge devices.", "method": "The authors designed an edge inference accelerator featuring a hybrid systolic array (HSA) architecture, MXINT4 weight quantization, optimized dataflows, and specialized units for non-linear operations like RMSNorm and RoPE.", "result": "The proposed accelerator achieves up to 247/117 (token/s/mm\u00b2) performance and shows more than 2.45x to 13.5x improvement in efficiency compared to existing approaches, with minimal accuracy loss and high energy efficiency.", "conclusion": "This novel accelerator provides a significant improvement in inference efficiency for LLMs on edge devices, with optimized memory and energy usage, showcasing its value for long-input/long-output scenarios."}}
{"id": "2507.09883", "pdf": "https://arxiv.org/pdf/2507.09883", "abs": "https://arxiv.org/abs/2507.09883", "authors": ["Swarn Priya", "Fr\u00e9d\u00e9ric Besson", "Connor Sughrue", "Tim Steenvoorden", "Jamie Fulford", "Freek Verbeek", "Binoy Ravindran"], "title": "BeePL: Correct-by-compilation kernel extensions", "categories": ["cs.PL"], "comment": "45 pages, 18 figures", "summary": "eBPF is a technology that allows developers to safely extend kernel\nfunctionality without modifying kernel source code or developing loadable\nkernel modules. Since the kernel governs critical system operations and\nenforces isolation boundaries between user space and privileged data, any\nmechanism that modifies its behavior must meet the highest standards of safety\nand correctness. To this end, the eBPF toolchain includes a verifier, which\nstatically checks safety properties such as memory access validity, bounded\nloops, and type correctness before loading the program into the kernel.\nHowever, the existing verifier is both overly conservative in some\ncases-rejecting valid programs-and unsound in others, permitting unsafe\nbehavior that violates the intended semantics of the kernel interface.\n  To address these challenges, we introduce BeePL, a domain-specific language\nfor eBPF with a formally verified type system. The BeePL type system, along\nwith the language design, statically enforces key safety properties such as\ntype-correct memory access, safe pointer usage, absence of unbounded loops, and\nstructured control flow. These guarantees are backed by formal type soundness\nproofs, ensuring that well-typed programs satisfy the safety invariants\nrequired by the eBPF execution environment. BeePL also proves that well-typed\nsource programs meet critical eBPF-specific properties related to memory\nsafety, termination, and control flow, enabling high-level reasoning prior to\ncompilation. For properties not fully enforceable statically-such as dynamic\nbounds and undefined behavior-BeePL inserts semantics-preserving runtime checks\nduring compilation. We develop a verified compilation strategy that extends\nCompCert to generate BPF bytecode from BeePL programs, establishing a\nprincipled foundation for an end-to-end verifiable toolchain for safe kernel\nextensions.", "AI": {"tldr": "BeePL introduces a domain-specific language with a formally verified type system for eBPF, addressing verifier limitations by ensuring critical safety properties and providing a verified compilation approach for safe kernel extensions.", "motivation": "To overcome the safety challenges and limitations of the current eBPF verifier, which is overly conservative and unsound, while supporting safe and extensible kernel functionality.", "method": "Developed BeePL, a domain-specific language for eBPF, featuring a formally verified type system, static enforcement of safety properties, runtime checks, and a verified compilation strategy using CompCert.", "result": "BeePL ensures critical eBPF-specific safety properties like memory safety, termination, and structured control flow, backed by type soundness proofs and runtime checks for dynamic scenarios.", "conclusion": "BeePL establishes a reliable and verifiable toolchain for eBPF, making safe kernel extensions feasible while overcoming verifier shortcomings."}}
{"id": "2507.08992", "pdf": "https://arxiv.org/pdf/2507.08992", "abs": "https://arxiv.org/abs/2507.08992", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "title": "Semantic Source Code Segmentation using Small and Large Language Models", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": "18 pages, 4 figures", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.", "AI": {"tldr": "This paper introduces automated methods for segmenting R code, which aid in navigating and understanding codebases, by using Large and Small Language Models (LLMs/SLMs). Results favor smaller, fine-tuned models.", "motivation": "The research addresses the challenge of segmenting R code in growing repositories, especially for low-resource languages, to aid efficient codebase navigation and knowledge retrieval.", "method": "The study explores two segmentation approaches: context-based line-by-line analysis and range-based segmentation, using both pre-trained and fine-tuned language models on a dataset called StatCodeSeg.", "result": "Context-based line-by-line analysis is more effective, and smaller fine-tuned models like CodeBERT outperform LLMs, even without prior exposure to R code.", "conclusion": "Fine-tuned, smaller models offer a domain-specific, efficient solution for code segmentation, proving advantageous despite their limited exposure during pre-training."}}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "The paper presents SEALGuard, a multilingual safety guardrail system to enhance LLM safety alignment in detecting unsafe and jailbreak inputs in diverse languages.", "motivation": "To address the limitations of current guardrail systems like LlamaGuard in handling multilingual unsafe and jailbreak prompts, particularly in low-resource languages.", "method": "The researchers adapted a general-purpose multilingual language model into a guardrail using low-rank adaptation (LoRA) and introduced a large-scale multilingual dataset, SEALSBench, for evaluation.", "result": "SEALGuard significantly outperformed existing systems like LlamaGuard, improving Defense Success Rate (DSR) by 48% and achieving superior precision and F1-scores.", "conclusion": "SEALGuard effectively closes the multilingual safety alignment gap, demonstrating better performance and robustness in handling unsafe and jailbreak prompts across multiple languages."}}
{"id": "2507.09045", "pdf": "https://arxiv.org/pdf/2507.09045", "abs": "https://arxiv.org/abs/2507.09045", "authors": ["S. Rezaei Afshar", "H. Pouretemad", "G. Reza Jafari"], "title": "Co-evolutionary Balance State of the Autism inter-Brain Network: A Neurofunctional Framework for Biomarker Discovery", "categories": ["q-bio.NC", "physics.bio-ph"], "comment": "27 pages, 5 figures, 9 tables. Preprint of manuscript submitted to\n  Network Neuroscience", "summary": "Autism spectrum disorder (ASD) is a neurodevelopmental condition\ncharacterized by deficits in social communication and repetitive behaviors;\nhowever, objective neurophysiological biomarkers remain lacking. We propose a\ncoevolutionary balance paradigm that quantifies network level energy via a\nHamiltonian integrating regional activity measured by fractional amplitude of\nlow frequency fluctuations (fALFF) and resting state functional connectivity\n(FC). Analysis of resting state fMRI data from 93 adult males with ASD and 93\nmatched controls revealed that empirical networks showed lower energy than 1000\ntopology preserving null models (paired t = -4.12, p less than or equal to\n1e-4). Participants with ASD exhibited more negative whole brain energy (t =\n-3.239, p = 0.0015), driven by increased agreement links and reduced imbalanced\nsame motifs. Subnetwork analysis indicated greater energy in the Default Mode\nNetwork after false discovery rate correction (p less than 0.016) and enhanced\nenergy between the Default Mode, Salience and Dorsal Attention networks (p less\nthan 0.032). Energy metrics and inter network connectivity correlated with\nAutism Diagnostic Interview Revised and Autism Diagnostic Observation Schedule\nseverity scores (absolute correlation greater than or equal to 0.29, p less\nthan 0.02). A k nearest neighbors classifier using nine principal features\nincluding motif proportions, global node link alignment, inter network fALFF\nweighted and FC strengths, subnetwork magnetization and pairwise energy\nachieved an accuracy of 79 percent with balanced sensitivity and specificity.\nThese results demonstrate that coevolutionary energy detects interpretable\nnetwork disruptions and establishes a robust framework for ASD classification.", "AI": {"tldr": "This study introduces a 'coevolutionary balance paradigm' to identify network-level disruptions in ASD using fMRI-derived energy metrics. The method achieves a 79% classification accuracy.", "motivation": "The lack of objective neurophysiological biomarkers for Autism Spectrum Disorder (ASD) motivates the research.", "method": "The study proposes a Hamiltonian-inspired model integrating fALFF and functional connectivity to analyze resting-state fMRI data from adult males with ASD and matched controls. It uses network energy metrics, motif analysis, and classification algorithms.", "result": "Findings show lower energy in ASD networks compared to null models, heightened energy in certain brain networks, and correlations with ASD severity scores. The proposed classifier achieved 79% accuracy.", "conclusion": "The coevolutionary energy model offers an interpretable framework for identifying network disruptions in ASD and delivers promising diagnostic applications."}}
{"id": "2507.08885", "pdf": "https://arxiv.org/pdf/2507.08885", "abs": "https://arxiv.org/abs/2507.08885", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "AI": {"tldr": "This paper introduces AirScape, a world model for aerial agents capable of predicting outcomes of motion intentions in 3D space using visual inputs.", "motivation": "The research aims to address the challenge of enabling robots to anticipate the outcomes of their motion intentions in 3D space, an essential aspect of embodied intelligence.", "method": "The authors develop AirScape, which involves creating a dataset of 11,000 video-intention pairs and training a world model using a two-phase training schedule.", "result": "The resulting model successfully predicts observation sequences in physical 3D environments based on motion intentions and visual cues.", "conclusion": "The study demonstrates that the AirScape model effectively improves spatial imagination capabilities for aerial agents, adhering to spatio-temporal constraints."}}
{"id": "2507.08875", "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "The paper introduces a novel MCA approach combining Virtual Gap Analysis (VGA) models to enhance evaluation efficiency and fairness in decision-making problems.", "motivation": "To address limitations of existing MCA methods influenced by assumptions and subjective judgments, and integrate qualitative and quantitative criteria effectively.", "method": "A novel MCA approach is developed using two Virtual Gap Analysis (VGA) models, leveraging linear programming for improved comprehensive and dependable evaluations.", "result": "Numerical examples validate the transparency and accuracy of the method, showcasing improved efficiency and fairness in multi-criteria assessments.", "conclusion": "The proposed approach is effective for advancing automated and decision-support systems, paving the way for robust progress in evaluation methodologies."}}
{"id": "2507.08917", "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "The paper proposes a novel ML technique to detect deepfake video impersonations using unnatural facial biometrics.", "motivation": "Deepfake videos, combining realistic voice cloning and visual tricks, pose threats like fraud, scams, and political disinformation.", "method": "The method involves machine learning that detects deepfakes by spotting unnatural patterns in facial biometrics. Its performance is tested against a large dataset, video laundering, and unseen deepfake models.", "result": "The forensic ML technique effectively detects deepfake impersonations and demonstrates generalization across various datasets and unseen generators.", "conclusion": "This technique could serve as a robust countermeasure against malicious usage of deepfake technology."}}
{"id": "2507.08829", "pdf": "https://arxiv.org/pdf/2507.08829", "abs": "https://arxiv.org/abs/2507.08829", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "AI": {"tldr": "The paper proposes a method to improve the reliability of DNNs against bit-flip faults using an XAI-based TMR technique, achieving significant reliability improvements.", "motivation": "DNNs are increasingly used in safety-critical applications, but their vulnerability to bit-flip faults poses a challenge. Efficiently applying Triple Modular Redundancy can address this reliability issue, but its effectiveness depends on accurately selecting crucial parameters and components.", "method": "The authors use Layer-wise Relevance Propagation (LRP), a gradient-based XAI technique, to determine the critical weights and parameters in DNNs. These identified critical elements are selectively protected with Triple Modular Redundancy (TMR).", "result": "The proposed method demonstrates over 60% reliability improvement at a bit error rate of 10^-4 for models like AlexNet, comparable to state-of-the-art methods but maintaining the same computational overhead.", "conclusion": "The study introduces an efficient and low-cost TMR approach using XAI techniques to enhance DNN reliability, showing it as an effective alternative to existing methods in safety-critical applications."}}
{"id": "2507.09847", "pdf": "https://arxiv.org/pdf/2507.09847", "abs": "https://arxiv.org/abs/2507.09847", "authors": ["Amin Abdollahi Dehkordi", "Mehdi Neshat", "Nataliia Y. Sergiienko", "Zahra Ghasemi", "Lei Chen", "John Boland", "Hamid Moradkhani", "Amir H. Gandomi"], "title": "Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting", "categories": ["cs.NE"], "comment": null, "summary": "Achieving carbon neutrality, a key focus of UN SDG #13, drives the\nexploration of wave energy, a renewable resource with the potential to generate\n30,000 TWh of clean electricity annually, surpassing global demand. However,\nwave energy remains underdeveloped due to technical and economic challenges,\nparticularly in forecasting wave farm power output, which is vital for grid\nstability and commercial viability. This study proposes a novel predictive\nframework to enhance wave energy integration into power grids. It introduces a\nhybrid sequential learning model combining Self-Attention-enhanced\nConvolutional Bi-LSTM with hyperparameter optimization. The model leverages\nspatial data from Wave Energy Converters (WECs) and is validated using datasets\nfrom wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.\nBenchmarked against ten machine learning algorithms, the model achieves\nsuperior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%\n(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep\nlearning methods, offering robust and scalable predictions for wave energy\noutput across diverse marine environments, supporting reliable integration into\nenergy systems.", "AI": {"tldr": "The study presents a predictive model for wave energy output combining deep learning techniques, achieving high accuracy across Australian wave farms.", "motivation": "Wave energy has immense potential but struggles with forecasting challenges, hindering its adoption for carbon neutrality goals.", "method": "A hybrid sequential learning model using Self-Attention-enhanced Convolutional Bi-LSTM optimized through hyperparameters.", "result": "The model demonstrated high accuracy, outperforming ten other machine learning algorithms, with R2 scores ranging from 82.8% to 91.7%.", "conclusion": "The proposed framework enhances wave energy forecasting, aiding integration into power grids and supporting carbon neutrality efforts."}}
{"id": "2507.09926", "pdf": "https://arxiv.org/pdf/2507.09926", "abs": "https://arxiv.org/abs/2507.09926", "authors": ["Zixuan Song", "Zhishu Shen", "Xiaoyu Zheng", "Qiushi Zheng", "Zheng Lei", "Jiong Jin"], "title": "Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks", "categories": ["cs.DC"], "comment": null, "summary": "As a key complement to terrestrial networks and a fundamental component of\nfuture 6G systems, Low Earth Orbit (LEO) satellite networks are expected to\nprovide high-quality communication services when integrated with ground-based\ninfrastructure, thereby attracting significant research interest. However, the\nlimited satellite onboard resources and the uneven distribution of\ncomputational workloads often result in congestion along inter-satellite links\n(ISLs) that degrades task processing efficiency. Effectively managing the\ndynamic and large-scale topology of LEO networks to ensure balanced task\ndistribution remains a critical challenge. To this end, we propose a dynamic\nmulti-region division framework for intelligent task management in LEO\nsatellite networks. This framework optimizes both intra- and inter-region\nrouting to minimize task delay while balancing the utilization of computational\nand communication resources. Based on this framework, we propose a dynamic\nmulti-region division algorithm based on the Genetic Algorithm (GA), which\nadaptively adjusts the size of each region based on the workload status of\nindividual satellites. Additionally, we incorporate an adaptive routing\nalgorithm and a task splitting and offloading scheme based on Multi-Agent Deep\nDeterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving\ntasks. Simulation results demonstrate that our proposed framework outperforms\ncomparative methods in terms of the task delay, energy consumption per task,\nand task completion rate.", "AI": {"tldr": "The paper develops a framework for managing workloads in LEO satellite networks to enhance efficiency and task distribution.", "motivation": "To address inefficiencies due to limited onboard satellite resources and uneven distributions of computational workloads in LEO satellite networks.", "method": "It introduces a dynamic multi-region division framework using Genetic Algorithm (GA), adaptive routing, and Multi-Agent Deep Deterministic Policy Gradient (MA-DDPG) for task management.", "result": "The proposed framework improves task delay, energy consumption per task, and task completion rate in simulations compared to other methods.", "conclusion": "The framework effectively optimizes resource utilization and task processing in LEO satellite networks, demonstrating significant performance gains."}}
{"id": "2507.09201", "pdf": "https://arxiv.org/pdf/2507.09201", "abs": "https://arxiv.org/abs/2507.09201", "authors": ["Weihong Xu", "Haein Choi", "Po-kai Hsu", "Shimeng Yu", "Tajana Rosing"], "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding", "categories": ["cs.AR", "cs.DC"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments.", "AI": {"tldr": "The paper discusses SLIM, an algorithm-hardware co-design for efficient sparse LLM inference on edge devices, enhancing throughput and energy efficiency while maintaining accuracy.", "motivation": "The motivation lies in addressing the challenge of deploying large language models (LLMs) on resource-constrained embedded devices by tackling inefficiencies in memory and computation due to existing systems' failure to utilize LLM sparsity.", "method": "The proposed method combines an adaptive thresholding algorithm to exploit LLM sparsity with a heterogeneous hardware architecture that integrates near-storage processing (NSP) and processing-in-memory (PIM) to handle different types of operations efficiently.", "result": "The evaluation of SLIM shows significant improvements, achieving 13-18x higher throughput than SSD-GPU systems and 9-10x better energy efficiency than DRAM-GPU systems, all with negligible accuracy loss.", "conclusion": "SLIM makes cost-effective and energy-efficient large language model deployment feasible for edge computing environments by leveraging sparsity in LLM operations and optimizing hardware design."}}
{"id": "2507.10301", "pdf": "https://arxiv.org/pdf/2507.10301", "abs": "https://arxiv.org/abs/2507.10301", "authors": ["Wenhao Tang", "Sam Lindley"], "title": "Rows and Capabilities as Modal Effects", "categories": ["cs.PL"], "comment": null, "summary": "Effect handlers allow programmers to model and compose computational effects\nmodularly. Effect systems statically guarantee that all effects are handled.\nSeveral recent practical effect systems are based on either row polymorphism or\ncapabilities. However, there remains a gap in understanding the precise\nrelationship between effect systems with such disparate foundations. The main\ndifficulty is that in both row-based and capability-based systems, effect\ntracking is typically entangled with other features such as functions.\n  We propose a uniform framework for encoding, analysing, and comparing effect\nsystems. Our framework exploits and generalises modal effect types, a recent\nnovel effect system which decouples effect tracking from functions via\nmodalities. Modalities offer fine-grained control over when and how effects are\ntracked, enabling us to express different strategies for effect tracking. We\ngive encodings as macro translations from existing row-based and\ncapability-based effect systems into our framework and show that these\nencodings preserve types and semantics. Our encodings reveal the essence of\neffect tracking mechanisms in different effect systems, enable a direct\nanalysis on their differences, and provide valuable insights on language\ndesign.", "AI": {"tldr": "The paper introduces a uniform framework using modal effect types to analyze and compare row-based and capability-based effect systems for better understanding of effect tracking mechanisms.", "motivation": "To bridge the gap in understanding the relationship between disparate effect systems based on row polymorphism and capabilities, and how effect tracking is entangled with other features.", "method": "The authors generalize modal effect types into a uniform framework, enabling encodings as macro translations from existing row-based and capability-based systems, preserving types and semantics.", "result": "Encodings reveal the essence of effect tracking mechanisms, facilitate direct analysis of differences, and offer insights for language design.", "conclusion": "The proposed framework provides a systematic approach to decode and understand various effect systems, offering utility for both comparative analysis and programming language innovation."}}
{"id": "2507.09023", "pdf": "https://arxiv.org/pdf/2507.09023", "abs": "https://arxiv.org/abs/2507.09023", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle", "categories": ["cs.SE", "cs.AI", "cs.MA"], "comment": null, "summary": "The pharmaceutical industry faces unprecedented challenges in drug discovery,\nwith traditional approaches struggling to meet modern therapeutic development\ndemands. This paper introduces a novel AI framework, Tippy, that transforms\nlaboratory automation through specialized AI agents operating within the\nDesign-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five\nspecialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with\nSafety Guardrail oversight - each designed to excel in specific phases of the\ndrug discovery pipeline. Tippy represents the first production-ready\nimplementation of specialized AI agents for automating the DMTA cycle,\nproviding a concrete example of how AI can transform laboratory workflows. By\nleveraging autonomous AI agents that reason, plan, and collaborate, we\ndemonstrate how Tippy accelerates DMTA cycles while maintaining scientific\nrigor essential for pharmaceutical research. The system shows significant\nimprovements in workflow efficiency, decision-making speed, and\ncross-disciplinary coordination, offering a new paradigm for AI-assisted drug\ndiscovery.", "AI": {"tldr": "Tippy is an AI framework aiming to revolutionize drug discovery by automating laboratory workflows using specialized AI agents to enhance efficiency and scientific rigor within the DMTA cycle.", "motivation": "The pharmaceutical industry faces difficulties meeting modern therapeutic development demands due to limitations in traditional drug discovery approaches.", "method": "Tippy employs a multi-agent AI system with five specialized agents operating collaboratively within the DMTA cycle, supervised with Safety Guardrail oversight.", "result": "Significant improvements were observed in workflow efficiency, decision-making speed, and cross-disciplinary coordination during drug discovery processes.", "conclusion": "Tippy establishes a new paradigm in AI-assisted drug discovery, emphasizing automation, efficiency, and scientific precision in laboratory workflows."}}
{"id": "2507.08916", "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "categories": ["cs.CL"], "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "The paper assesses the limitations of large language models (LLMs) in medical question answering, especially focusing on inadequacies in evaluation datasets, and underscores the need for better frameworks.", "motivation": "To identify weaknesses in current datasets and evaluation methods of LLMs in providing accurate medical question answering, aiming to improve clinical relevance and rigor.", "method": "Reviewed widely-used benchmark datasets such as MedQA and alternative tools like medical journal challenge questions for their rigor, realism, and potential efficacy.", "result": "Identified significant shortcomings in datasets like lack of clinical realism and robust validation. Challenge questions show promise but are limited in scope and size.", "conclusion": "Calls for a standardized, collaborative framework to improve evaluation datasets for LLMs in medicine, ensuring rigor, transparency, and relevance to clinical practices."}}
{"id": "2507.09360", "pdf": "https://arxiv.org/pdf/2507.09360", "abs": "https://arxiv.org/abs/2507.09360", "authors": ["Gabrielle O'Brien", "Adam L. Weaver", "William H. Barnett", "Dmitry A. Kozhanov", "Gennady S. Cymbalyuk"], "title": "Cellular Mechanisms of Phase Maintenance in a Pyloric Motif of a Central Pattern Generator", "categories": ["q-bio.NC", "nlin.PS"], "comment": "34 pages, 10 figures, 2 tables, 2 supplementary tables", "summary": "In many neural networks, patterns controlling rhythmic behaviors are\nmaintained across a wide range of periods. In the crustacean pyloric central\npattern generator (CPG), a constant bursting pattern is preserved over a\nthree-to-fivefold range of periods. We idescribe how neuromodulation could\nadjust neuronal properties to preserve phase relations as the period changes.\nWe developed a biophysical model implementing a reduced pyloric network motif,\nwhich has a bursting neuron and two follower neurons interconnected through\ninhibitory synaptic coupling. We described cellular mechanisms supporting phase\nmaintenance and investigated possible coordination between these mechanisms in\nfour dynamically distinct ensembles of a pyloric CPG producing a triphasic\npattern. The coordinated variation of the voltages of half-activation for\npotassium (VK2) and hyperpolarization-activated (Vh) currents provides a family\nof three mechanisms for control of burst duration, interburst interval, and\nlatency to spiking. The mechanisms are determined by the Cornerstone\nbifurcation, one of the Shilnikov blue sky catastrophe scenarios. In Mechanism\n1, in a bursting neuron, the burst duration increases as VK2 nears a blue-sky\ncatastrophe bifurcation, while the interburst interval grows as Vh approaches a\nsaddle-node on an invariant circle bifurcation. In Mechanism 2, a silent neuron\nresponds with a single burst to short input; the burst duration grows as VK2\napproaches a saddle-node bifurcation for periodic orbits. In Mechanism 3, a\nspiking neuron responds with a pause to short input; the pause duration grows\nas Vh nears a saddle-node bifurcation for stationary states. In all three\nmechanisms, the measured quantities grow without bound as the bifurcation\nparameter nears its critical value, consistent with an inverse-square-root law.", "AI": {"tldr": "The paper investigates how rhythmic neural patterns in crustacean neural networks are maintained over diverse time periods using neuromodulation mechanisms and biophysical models to preserve phase relations.", "motivation": "The paper aims to understand how neural networks preserve consistent rhythmic behaviors across varying time periods, focusing on crustacean pyloric CPGs and the role of neuromodulation.", "method": "A biophysical model is developed to simulate a reduced pyloric network motif with interconnected neurons. Cellular mechanisms are analyzed using scenarios like Shilnikov catastrophe and bifurcation dynamics.", "result": "Three mechanisms were identified for burst duration, interburst interval, and latency to spiking, with behaviors governed by specific bifurcations such as blue-sky catastrophe and saddle-node scenarios.", "conclusion": "Neural phase relations can be preserved across wide temporal ranges through coordinated variations in neuronal properties governed by bifurcation mechanisms."}}
{"id": "2507.08901", "pdf": "https://arxiv.org/pdf/2507.08901", "abs": "https://arxiv.org/abs/2507.08901", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "categories": ["cs.RO"], "comment": "Accepted by ITSC'25", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "AI": {"tldr": "EGC-VMAP is a new framework that generates high-precision, city-scale vectorized maps by crowdsourcing vehicles' data, solving the shortcomings of traditional LiDAR and single-vehicle methods.", "motivation": "There is a need for accurate and cost-effective mapping for autonomous driving, as traditional LiDAR methods are expensive and slow, and single-vehicle systems lack robustness.", "method": "The paper proposes a Trip-Aware Transformer architecture that fuses multi-vehicle, multi-temporal data directly, alongside hierarchical matching and multi-objective loss for better training and map accuracy.", "result": "The method achieves superior map accuracy and robustness, significantly outperforming single-vehicle baselines and reducing manual annotation costs by 90%.", "conclusion": "EGC-VMAP offers a scalable and efficient solution for city-scale autonomous driving map generation by leveraging crowdsourced data, ensuring both performance and cost efficiency."}}
{"id": "2507.08892", "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "categories": ["cs.AI", "cs.MA"], "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "The paper proposes a flexible framework for using generative AI in multi-actor environments by taking inspiration from tabletop role-playing games (TTRPGs), emphasizing an Entity-Component architecture for modularity and scalability.", "motivation": "The motivation is to address the challenge of supporting diverse use cases of generative AI\u2014such as simulation, narrative generation, and evaluation\u2014in multi-actor environments by proposing a unified and adaptable framework.", "method": "The method involves taking inspiration from TTRPGs by designing a Game Master (GM) as a configurable entity in an Entity-Component architectural pattern. This separates the engineer's role (handling implementation) and the designer's role (creating component configurations).", "result": "The approach fosters modularity, scalability, and rapid iteration. It also demonstrates its effectiveness through the evolution of the Concordia library, which aligns configuration with specific user goals.", "conclusion": "The paper concludes that the Entity-Component architectural pattern inspired by TTRPGs provides an effective framework for configuring generative AI-driven multi-actor scenarios, supporting diverse and scalable use cases."}}
{"id": "2507.08979", "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM is a data-free and task-agnostic solution for reducing bias in vision-language models like CLIP. It involves generating spurious correlations using an LLM and learning a projection to reduce these biases.", "motivation": "VLMs inherit and amplify biases from their training data, leading to skewed predictions. This paper aims to mitigate these biases without relying on predefined bias categories or external data.", "method": "PRISM employs two stages: (1) using LLMs for generating biased scene descriptions and (2) applying a novel contrastive-style debiasing loss to learn a projection that reduces spurious correlations while maintaining image-text alignment.", "result": "Experimental results reveal that PRISM surpasses existing debiasing methods on datasets such as Waterbirds and CelebA.", "conclusion": "PRISM effectively debiases VLMs in a data-free and task-agnostic manner, showing promise for achieving fairer predictions."}}
{"id": "2507.08832", "pdf": "https://arxiv.org/pdf/2507.08832", "abs": "https://arxiv.org/abs/2507.08832", "authors": ["Niranjan Mallikarjun Sindhur", "Pavithra C", "Nivya Muchikel"], "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Farmers in developing regions like Karnataka, India, face a dual challenge:\nnavigating extreme market and climate volatility while being excluded from the\ndigital revolution due to literacy barriers. This paper presents a novel\ndecision support system that addresses both challenges through a unique\nsynthesis of machine learning and human-computer interaction. We propose a\nhybrid recommendation engine that integrates two predictive models: a Random\nForest classifier to assess agronomic suitability based on soil, climate, and\nreal-time weather data, and a Long Short-Term Memory (LSTM) network to forecast\nmarket prices for agronomically viable crops. This integrated approach shifts\nthe paradigm from \"what can grow?\" to \"what is most profitable to grow?\",\nproviding a significant advantage in mitigating economic risk. The system is\ndelivered through an end-to-end, voice-based interface in the local Kannada\nlanguage, leveraging fine-tuned speech recognition and high-fidelity speech\nsynthesis models to ensure accessibility for low-literacy users. Our results\nshow that the Random Forest model achieves 98.5% accuracy in suitability\nprediction, while the LSTM model forecasts harvest-time prices with a low\nmargin of error. By providing data-driven, economically optimized\nrecommendations through an inclusive interface, this work offers a scalable and\nimpactful solution to enhance the financial resilience of marginalized farming\ncommunities.", "AI": {"tldr": "The paper develops a voice-based decision support system for low-literacy farmers, combining predictive models for crop suitability and market prices.", "motivation": "Farmers in regions like Karnataka face market and climate challenges, coupled with digital exclusion due to literacy barriers.", "method": "The system integrates a Random Forest model for crop suitability and an LSTM model for market price forecasting, delivered via a Kannada voice interface.", "result": "The Random Forest model achieved 98.5% accuracy, and the LSTM model showed low error in price prediction.", "conclusion": "The proposed system provides scalable, accessible, and economically optimized recommendations, improving financial resilience of marginalized farmers."}}
{"id": "2507.09782", "pdf": "https://arxiv.org/pdf/2507.09782", "abs": "https://arxiv.org/abs/2507.09782", "authors": ["Muhammad Luthfi Shahab", "Fidya Almira Suheri", "Rudy Kusdiantara", "Hadi Susanto"], "title": "Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices", "categories": ["math.NA", "cs.LG", "cs.NA", "cs.NE", "math.OC"], "comment": "Accepted for publication in Physica D: Nonlinear Phenomena", "summary": "This paper introduces a framework based on physics-informed neural networks\n(PINNs) for addressing key challenges in nonlinear lattices, including solution\napproximation, bifurcation diagram construction, and linear stability analysis.\nWe first employ PINNs to approximate solutions of nonlinear systems arising\nfrom lattice models, using the Levenberg-Marquardt algorithm to optimize\nnetwork weights for greater accuracy. To enhance computational efficiency in\nhigh-dimensional settings, we integrate a stochastic sampling strategy. We then\nextend the method by coupling PINNs with a continuation approach to compute\nsnaking bifurcation diagrams, incorporating an auxiliary equation to\neffectively track successive solution branches. For linear stability analysis,\nwe adapt PINNs to compute eigenvectors, introducing output constraints to\nenforce positivity, in line with Sturm-Liouville theory. Numerical experiments\nare conducted on the discrete Allen-Cahn equation with cubic and quintic\nnonlinearities in one to five spatial dimensions. The results demonstrate that\nthe proposed approach achieves accuracy comparable to, or better than,\ntraditional numerical methods, especially in high-dimensional regimes where\ncomputational resources are a limiting factor. These findings highlight the\npotential of neural networks as scalable and efficient tools for the study of\ncomplex nonlinear lattice systems.", "AI": {"tldr": "The paper proposes using physics-informed neural networks (PINNs) for solving challenges in nonlinear lattice systems, such as solution approximation, bifurcation diagram construction, and stability analysis.", "motivation": "Traditional numerical methods face limitations in addressing high-dimensional nonlinear lattice systems due to computational inefficiencies.", "method": "The framework employs PINNs optimized with the Levenberg-Marquardt algorithm and integrates stochastic sampling for efficiency. PINNs are extended with continuation methods for bifurcation analysis and adapted for stability assessments using eigenvector computations.", "result": "Experiments conducted using the discrete Allen-Cahn equation show that the framework achieves accuracy comparable to or superior to traditional methods, especially in high-dimensional cases.", "conclusion": "The proposed PINN-based framework demonstrates scalability and efficiency, positioning it as a promising tool for analyzing complex nonlinear lattice systems."}}
{"id": "2507.10026", "pdf": "https://arxiv.org/pdf/2507.10026", "abs": "https://arxiv.org/abs/2507.10026", "authors": ["Zhifei Xu", "Zhiqing Tang", "Jiong Lou", "Zhi Yao", "Xuan Xie", "Tian Wang", "Yinglong Wang", "Weijia Jia"], "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning", "categories": ["cs.DC"], "comment": null, "summary": "The growth of Artificial Intelligence (AI) and large language models has\nenabled the use of Generative AI (GenAI) in cloud data centers for diverse\nAI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce\nunavoidable delays and substantial resource overhead, which are unsuitable for\nusers at the network edge with high QoS demands. Deploying AIGC services on\nedge servers reduces transmission times but often leads to underutilized\nresources and fails to optimally balance inference latency and quality. To\naddress these issues, this paper introduces a QoS-aware\n\\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling\n(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to\nvarious edge servers, formulating it as a gang scheduling problem that balances\ninference latency and quality while considering server heterogeneity, such as\ndiffering model distributions and cold start issues. 2) We propose a\nreinforcement learning-based EAT algorithm that uses an attention layer to\nextract load and task queue information from edge servers and employs a\ndiffusion-based policy network for scheduling, efficiently enabling model\nreuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm\nto divide tasks and distribute them across multiple edge servers for\nprocessing. Experimental results based on our system and large-scale\nsimulations show that our EAT algorithm can reduce inference latency by up to\n56\\% compared to baselines. We release our open-source code at\nhttps://github.com/zzf1955/EAT.", "AI": {"tldr": "The study introduces the QoS-aware EAT (Edge-collaborative AIGC Task scheduling) algorithm to efficiently process Generative AI tasks in edge servers, reducing latency by up to 56%.", "motivation": "The need to address delays and inefficiencies in deploying AI-Generated Content (AIGC) services at the network edge due to high QoS demands, underutilized resources, and challenges in balancing inference latency and quality.", "method": "The paper proposes a reinforcement learning-based EAT algorithm leveraging an attention layer for load and queue information extraction, a diffusion-based policy network for scheduling, and formulating the problem as gang scheduling.", "result": "Experimental results show that the EAT algorithm reduces inference latency by up to 56% compared to baseline methods.", "conclusion": "The EAT algorithm effectively improves AIGC task scheduling at edge servers by reducing latency and optimizing resource utilization, making it advantageous for high-QoS environments."}}
{"id": "2507.09660", "pdf": "https://arxiv.org/pdf/2507.09660", "abs": "https://arxiv.org/abs/2507.09660", "authors": ["Shuvra S. Bhattacharyya", "Marilyn Wolf"], "title": "Tools and Methodologies for System-Level Design", "categories": ["cs.AR"], "comment": "This is a preprint of a chapter to appear in the forthcoming volume\n  Handbook on Electronic Design Automation (third edition), published by Taylor\n  & Francis. The final version may differ", "summary": "System-level design, once the province of board designers, has now become a\ncentral concern for chip designers. Because chip design is a less forgiving\ndesign medium -- design cycles are longer and mistakes are harder to correct --\nsystem-on-chip designers need a more extensive tool suite than may be used by\nboard designers and a variety of tools and methodologies have been developed\nfor system-level design of systems-on-chips (SoCs). System-level design is less\namenable to synthesis than are logic or physical design. As a result,\nsystem-level tools concentrate on modeling, simulation, design space\nexploration, and design verification. The goal of modeling is to correctly\ncapture the system's operational semantics, which helps with both\nimplementation and verification. The study of models of computation provides a\nframework for the description of digital systems. Not only do we need to\nunderstand a particular style of computation, such as dataflow, but we also\nneed to understand how different models of computation can reliably communicate\nwith each other. Design space exploration tools, such as hardware/software\nco-design, develop candidate designs to understand trade-offs. Simulation can\nbe used not only to verify functional correctness but also to supply\nperformance and power/energy information for design analysis. This chapter\nemploys two applications -- video and neural networks -- as examples. Both are\nleading-edge applications that illustrate many important aspects of\nsystem-level design.", "AI": {"tldr": "System-level design has become essential for chip designers due to the challenges of system-on-chip (SoC) development. Tools for modeling, simulation, design space exploration, and verification are critical.", "motivation": "Chip design is a less forgiving design medium compared to board design, requiring advanced tools and methods to manage the complexity and risks in SoC development.", "method": "The paper focuses on system-level design tools for modeling, simulation, design space exploration, and verification. Models of computation and communication between these models are highlighted.", "result": "The chapter utilizes video and neural network applications as practical examples to showcase the studied methodologies.", "conclusion": "System-level design tools and methodologies are essential for capturing operational semantics, exploring design spaces, and ensuring functional correctness in advanced high-performance applications."}}
{"id": "2507.10482", "pdf": "https://arxiv.org/pdf/2507.10482", "abs": "https://arxiv.org/abs/2507.10482", "authors": ["Simon Guilloud", "Viktor Kun\u010dak"], "title": "Orthologic Type Systems", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "We propose to use orthologic as the basis for designing type systems\nsupporting intersection, union, and negation types in the presence of subtyping\nassumptions. We show how to extend orthologic to support monotonic and\nantimonotonic functions, supporting the use of type constructors in such type\nsystems. We present a proof system for orthologic with function symbols,\nshowing that it admits partial cut elimination. Using these insights, we\npresent an $\\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation\nunder $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization\nalgorithm, allowing simplification of types to their minimal canonical form.", "AI": {"tldr": "The paper proposes using orthologic as the foundation for type systems handling intersection, union, negation types, and subtyping. It introduces an extended proof system, algorithms for subtyping decisions, and type normalization.", "motivation": "To address challenges in type systems dealing with complex types such as intersection, union, and negation in the presence of subtyping assumptions.", "method": "The authors extend orthologic to include monotonic and antimonotonic functions and develop proof systems with function symbols, enabling partial cut elimination. Algorithms for subtyping relations and type normalization are presented.", "result": "They achieve an $O(n^2(1+m))$ algorithm for subtyping decisions under $m$ assumptions, and an $O(n^2)$ normalization algorithm for canonical type simplification.", "conclusion": "Orthologic serves as a promising framework for creating robust type systems, offering both algorithmic efficiency and theoretical rigor in handling advanced type operations."}}
{"id": "2507.09039", "pdf": "https://arxiv.org/pdf/2507.09039", "abs": "https://arxiv.org/abs/2507.09039", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "Towards Extracting Software Requirements from App Reviews using Seq2seq Framework", "categories": ["cs.SE"], "comment": null, "summary": "Mobile app reviews are a large-scale data source for software improvements. A\nkey task in this context is effectively extracting requirements from app\nreviews to analyze the users' needs and support the software's evolution.\nRecent studies show that existing methods fail at this task since app reviews\nusually contain informal language, grammatical and spelling errors, and a large\namount of irrelevant information that might not have direct practical value for\ndevelopers. To address this, we propose a novel reformulation of requirements\nextraction as a Named Entity Recognition (NER) task based on the\nsequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a\nSeq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced\nwith a self-attention mechanism, GloVe embeddings, and a CRF model. We\nevaluated our framework on two datasets: a manually annotated set of 1,000\nreviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The\nquantitative evaluation of our framework showed that it outperformed existing\nstate-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved\ncomparable performance on Dataset 1 with an F1 score of 0.47.", "AI": {"tldr": "This paper introduces a new method to extract requirements from mobile app reviews by using a Seq2seq Named Entity Recognition (NER) framework, outperforming existing methods on large datasets.", "motivation": "Extracting requirements from app reviews is vital for software evolution, but current methods struggle due to informal language, errors, and irrelevant information in the reviews.", "method": "The paper proposes a Seq2seq framework with a BiLSTM encoder, LSTM decoder, self-attention mechanism, GloVe embeddings, and a CRF model to perform Named Entity Recognition.", "result": "The approach achieved a high F1 score of 0.96 on a large dataset (Dataset 2) and a moderate score of 0.47 on a smaller manually annotated dataset (Dataset 1), exceeding the performance of state-of-the-art methods.", "conclusion": "The proposed framework effectively extracts user requirements from app reviews, addressing challenges of informal language and errors, and proves to be a valuable tool for software improvement."}}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "This paper introduces two Korean expert-level benchmarks designed to evaluate the abilities of Large Language Models (LLMs) in reflecting industrial knowledge.", "motivation": "The paper aims to fill the gap in evaluating LLMs not only in academic fields but also in industrial domains that hold real-world relevance.", "method": "Two benchmarks, KMMLU-Redux and KMMLU-Pro, were constructed using questions from Korean technical qualification and professional licensure exams, ensuring reliability by removing errors.", "result": "The benchmarks successfully represent industrial knowledge in Korea, and experiments validated their capability to assess LLMs on this front.", "conclusion": "These datasets enhance the scope of LLM evaluation and are publically available to advance research in this domain."}}
{"id": "2507.09513", "pdf": "https://arxiv.org/pdf/2507.09513", "abs": "https://arxiv.org/abs/2507.09513", "authors": ["Yanchen Wang", "Han Yu", "Ari Blau", "Yizi Zhang", "The International Brain Laboratory", "Liam Paninski", "Cole Hurwitz", "Matt Whiteway"], "title": "Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "The brain can only be fully understood through the lens of the behavior it\ngenerates -- a guiding principle in modern neuroscience research that\nnevertheless presents significant technical challenges. Many studies capture\nbehavior with cameras, but video analysis approaches typically rely on\nspecialized models requiring extensive labeled data. We address this limitation\nwith BEAST (BEhavioral Analysis via Self-supervised pretraining of\nTransformers), a novel and scalable framework that pretrains\nexperiment-specific vision transformers for diverse neuro-behavior analyses.\nBEAST combines masked autoencoding with temporal contrastive learning to\neffectively leverage unlabeled video data. Through comprehensive evaluation\nacross multiple species, we demonstrate improved performance in three critical\nneuro-behavioral tasks: extracting behavioral features that correlate with\nneural activity, and pose estimation and action segmentation in both the\nsingle- and multi-animal settings. Our method establishes a powerful and\nversatile backbone model that accelerates behavioral analysis in scenarios\nwhere labeled data remains scarce.", "AI": {"tldr": "BEAST is introduced as a self-supervised framework for analyzing behavior videos with vision transformers, aiming to address challenges in neuro-behavioral research while reducing dependence on labeled data.", "motivation": "Understanding brain functionality necessitates analyzing behavior, but current video-based approaches demand significant labeled data, creating scalability challenges.", "method": "BEAST uses masked autoencoding and temporal contrastive learning to pretrain vision transformers on unlabeled video data, enabling diverse neuro-behavioral analyses.", "result": "BEAST enhances behavioral feature extraction tied to neural activity, poses estimation, and action segmentation across species and settings, outperforming existing models.", "conclusion": "The framework provides a scalable solution for behavioral analysis, boosting performance in tasks where labeled data is limited and establishing a versatile model backbone for neuroscience research."}}
{"id": "2507.08903", "pdf": "https://arxiv.org/pdf/2507.08903", "abs": "https://arxiv.org/abs/2507.08903", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by ITSC'25", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "AI": {"tldr": "The paper introduces a camera-LiDAR fusion framework utilizing roadside units for HD semantic mapping, supported by RS-seq, a highly annotated multimodal dataset. Results prove this approach's superior performance over unimodal methods.", "motivation": "To address the challenges of occlusions and limited perspectives in HD semantic mapping at complex intersections using traditional vehicle-based methods.", "method": "A two-stage multimodal fusion framework combining camera texture and LiDAR geometric data for semantic integration. RS-seq dataset supports comprehensive evaluation.", "result": "The multimodal approach surpasses unimodal methods, improving semantic segmentation mIoU by 4% over camera-only techniques and 18% over LiDAR-only techniques.", "conclusion": "The framework establishes a robust methodology for IRU-supported HD semantic mapping, paving the way for infrastructure-aided autonomous driving and future research."}}
{"id": "2507.09080", "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "categories": ["cs.AI"], "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "This paper introduces BioAnalyst, a transformer-based AI Foundation Model tailored for biodiversity analysis and conservation, outperforming existing methods in ecological forecasting, especially in data-scarce scenarios.", "motivation": "The preservation of biodiversity is crucial for ecological balance and sustainability, but it is threatened by issues like habitat loss, climate change, and invasive species. There is a need for advanced tools to monitor, predict, and plan biodiversity conservation efforts effectively.", "method": "BioAnalyst leverages a transformer-based architecture pre-trained on multi-modal datasets (species occurrences, remote sensing, climate, and environmental data). It is adaptably fine-tuned for downstream tasks like species distribution modeling, habitat assessment, and biodiversity forecasting.", "result": "The model was tested on two biodiversity use cases and surpassed existing methods, particularly in data-scarce environments, establishing new accuracy baselines in ecological forecasting.", "conclusion": "BioAnalyst offers a scalable and efficient AI-driven solution for biodiversity conservation. Its open release aims to stimulate scientific collaboration and progress in addressing ecological challenges."}}
{"id": "2507.08981", "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "categories": ["cs.CV"], "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "The paper introduces HMR-ViT, a method for Human Mesh Recovery using both temporal and kinematic information through a Vision Transformer.", "motivation": "Existing methods for Human Mesh Recovery from images lack integration of both temporal and kinematic information, limiting accuracy.", "method": "HMR-ViT constructs a Temporal-kinematic Feature Image using feature vectors from video frames. A Channel Rearranging Matrix (CRM) rearranges kinematic features spatially, which are then encoded via a Vision Transformer to infer pose/shape parameters.", "result": "The method showed competitive performance on the 3DPW and Human3.6M datasets.", "conclusion": "Using both temporal and kinematic information improves HMR performance, demonstrating the effectiveness of the HMR-ViT approach."}}
{"id": "2507.08833", "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "This paper examines the limitations of the Low-Rank Adaptation (LoRA) method for fine-tuning large language models (LLMs) and proposes new methods that achieve similar or better performance with more consistent speed improvements.", "motivation": "The motivation for this study is to address the inconsistent speed improvements observed in LoRA during the fine-tuning of large language models and to identify ways to optimize this process.", "method": "The authors conduct an in-depth analysis of LoRA's speed and performance limitations and propose alternative methods for fine-tuning. These methods are empirically evaluated and compared to LoRA.", "result": "The proposed fine-tuning methods deliver comparable or better performance than LoRA, with consistently faster training speeds across different setups.", "conclusion": "The paper provides practical guidelines and insights for effectively fine-tuning LLMs, helping practitioners better optimize for speed and performance under resource constraints."}}
{"id": "2507.09992", "pdf": "https://arxiv.org/pdf/2507.09992", "abs": "https://arxiv.org/abs/2507.09992", "authors": ["Yuji Kanagawa", "Kenji Doya"], "title": "Evolution of Fear and Social Rewards in Prey-Predator Relationship", "categories": ["q-bio.PE", "cs.AI", "cs.NE"], "comment": "Preprint. Under review", "summary": "Fear is a critical brain function for detecting danger and learning to avoid\nspecific stimuli that can lead to danger. While fear is believed to have\nevolved under pressure from predators, experimentally reproducing the evolution\nis challenging. To investigate the relationship between environmental\nconditions, the evolution of fear, and the evolution of other rewards, such as\nfood reward and social reward, we developed a distributed evolutionary\nsimulation. In our simulation, prey and predator agents co-evolve their innate\nreward functions, including a possibly fear-like term for observing predators,\nand learn behaviors via reinforcement learning. Surprisingly, our simulation\nrevealed that social reward for observing the same species is more important\nfor prey to survive, and fear-like negative reward for observing predators\nevolves only after acquiring social reward. We also found that the predator\nwith increased hunting ability (larger mouth) amplified fear emergence, but\nalso that fear evolution is more stable with non-evolving predators that are\nbad at chasing prey. Additionally, unlike for predators, we found that positive\nrewards evolve in opposition to fear for stationary threats, as areas with\nabundant leftover food develop around them. These findings suggest that fear\nand social reward have had a complex interplay with each other through\nevolution, along with the nature of predators and threats.", "AI": {"tldr": "The study used simulations to explore how fear and other rewards, such as social interaction and food incentives, evolve in prey under different environmental conditions with predators.", "motivation": "To understand why fear evolved in response to predators, alongside other rewards like food and social interaction, in complex natural environments.", "method": "The researchers employed distributed evolutionary simulations where prey and predators co-evolved innate reward functions and learned behaviors using reinforcement learning algorithms.", "result": "Social reward for observing the same species proved more critical for prey survival. Fear-like traits emerged under specific circumstances, connected to social reward and predator characteristics.", "conclusion": "Fear and social rewards are interlinked in prey evolution, influenced by the traits of predators, environmental threats, and food dynamics."}}
{"id": "2507.10069", "pdf": "https://arxiv.org/pdf/2507.10069", "abs": "https://arxiv.org/abs/2507.10069", "authors": ["Zedong Liu", "Shenggan Cheng", "Guangming Tan", "Yang You", "Dingwen Tao"], "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).", "AI": {"tldr": "The paper introduces Elastic Multimodal Parallelism (EMP) and ElasticMM to improve inference efficiency and resource utilization for Multimodal Large Language Models (MLLMs).", "motivation": "Current serving architectures for MLLMs are inefficient, struggling with mixed requests and heterogeneous workloads, leading to high latency and low resource utilization.", "method": "The authors propose Elastic Multimodal Parallelism (EMP) with three main innovations: modality-aware load balancing, elastic partition scheduling, and multimodal prefix caching for efficient inference.", "result": "ElasticMM reduces time-to-first-token (TTFT) latency by up to 4.2x and boosts throughput by 3.2-4.5x compared to state-of-the-art systems.", "conclusion": "ElasticMM successfully addresses serving inefficiencies for MLLMs, enabling scalable, high-performance inference while meeting service-level objectives."}}
{"id": "2507.09730", "pdf": "https://arxiv.org/pdf/2507.09730", "abs": "https://arxiv.org/abs/2507.09730", "authors": ["Jiechen Huang", "Wenjian Yu"], "title": "Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics", "categories": ["cs.AR", "cs.NA", "math.NA"], "comment": "5 pages, 6 figures", "summary": "The accuracy of floating-random-walk (FRW) based capacitance extraction\nstands only when the recursive FRW transitions are sampled unbiasedly according\nto surrounding dielectrics. Advanced technology profiles, featuring complicated\nnon-stratified dielectrics, challenge the accuracy of existing FRW transition\nschemes that approximate dielectrics with stratified or eight-octant patterns.\nIn this work, we propose an algorithm named MicroWalk, enabling accurate FRW\ntransitions for arbitrary dielectrics while keeping high efficiency. It is\nprovably unbiased and equivalent to using transition probabilities solved by\nfinite difference method, but at orders of magnitude lower cost (802$\\times$\nfaster). An enhanced 3-D capacitance solver is developed with a hybrid strategy\nfor complicated dielectrics, combining MicroWalk with the special treatment for\nthe first transition cube and the analytical algorithm for stratified cubes.\nExperiments on real-world structures show that our solver achieves a\nsignificant accuracy advantage over existing FRW solvers, while preserving high\nefficiency.", "AI": {"tldr": "The paper introduces MicroWalk, an algorithm for accurate and efficient floating-random-walk (FRW) transitions in capacitance extraction, overcoming challenges with advanced dielectrics.", "motivation": "Existing FRW methods struggle with advanced dielectric profiles in capacitance extraction due to biased sampling caused by approximations, limiting accuracy.", "method": "The authors propose MicroWalk, an unbiased FRW transition algorithm capable of handling arbitrary dielectrics efficiently. It uses transition probabilities equivalent to finite difference methods but is 802x faster.", "result": "The new algorithm is integrated into a hybrid 3-D capacitance solver, demonstrating superior accuracy and efficiency on real-world structures compared to traditional FRW solvers.", "conclusion": "MicroWalk addresses challenges posed by non-stratified dielectrics and significantly improves the balance between accuracy and efficiency in FRW-based capacitance solvers."}}
{"id": "2507.09049", "pdf": "https://arxiv.org/pdf/2507.09049", "abs": "https://arxiv.org/abs/2507.09049", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews", "categories": ["cs.SE"], "comment": null, "summary": "With the increasing proliferation of mobile applications in our daily lives,\nthe concerns surrounding ethics have surged significantly. Users communicate\ntheir feedback in app reviews, frequently emphasizing ethical concerns, such as\nprivacy and security. Incorporating these reviews has proved to be useful for\nmany areas of software engineering (e.g., requirement engineering, testing,\netc.). However, app reviews related to ethical concerns generally use\ndomain-specific language and are typically overshadowed by more generic\ncategories of user feedback, such as app reliability and usability. Thus,\nmaking automated extraction a challenging and time-consuming effort.\n  This study proposes CMER (A \\underline{C}ontext-Aware Approach for\n\\underline{M}ining \\underline{E}thical Concern-related App\n\\underline{R}eviews), a novel approach that combines Natural Language Inference\n(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract\nethical concern-related app reviews at scale. In CMER, NLI provides\ndomain-specific context awareness by using domain-specific hypotheses, and the\nLlama-like LLM eliminates the need for labeled data in the classification task.\nWe evaluated the validity of CMER by mining privacy and security-related\nreviews (PSRs) from the dataset of more than 382K app reviews of mobile\ninvestment apps. First, we evaluated four NLI models and compared the results\nof domain-specific hypotheses with generic hypotheses. Next, we evaluated three\nLLMs for the classification task. Finally, we combined the best NLI and LLM\nmodels (CMER) and extracted 2,178 additional PSRs overlooked by the previous\nstudy using a keyword-based approach, thus demonstrating the effectiveness of\nCMER. These reviews can be further refined into actionable requirement\nartifacts.", "AI": {"tldr": "The paper presents CMER, a tool that uses Natural Language Inference (NLI) and a Large Language Model (LLM) to efficiently extract ethical concern-related app reviews, such as privacy and security, from large datasets without labeled data.", "motivation": "The authors aim to address the increasing prevalence of ethics-related concerns in app reviews, which are often expressed through domain-specific language and overshadowed by generic feedback categories. They seek an efficient method to extract these critical insights at scale.", "method": "The study combines NLI for domain-specific context awareness and a decoder-only LLM for classification without requiring labeled data. The approach was tested on over 382K app reviews, with evaluations comparing different NLI and LLM models.", "result": "The proposed CMER system identified 2,178 additional privacy and security-related reviews that were overlooked by a previous keyword-based approach, showcasing its enhanced extraction capabilities.", "conclusion": "CMER successfully demonstrated its ability to mine ethical issues like privacy and security from app reviews at scale, offering actionable insights that can aid in software engineering processes."}}
{"id": "2507.08967", "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "SIMS introduces a method for improving LLM alignment during inference without relying on external annotated data by autonomously generating contrastive samples.", "motivation": "Existing model-steering methods depend on external annotated data for aligning LLMs, which limits context adaptability and effectiveness.", "method": "The proposed SIMS framework generates and refines contrastive samples through self-improvement cycles and employs techniques such as prompt ranking and contrast sampling.", "result": "SIMS surpasses traditional methods in steering effectiveness and adaptability across various LLMs and benchmarks.", "conclusion": "Self-improving model steering (SIMS) is a promising approach for inference-time LLM alignment, demonstrating superior adaptability and efficiency."}}
{"id": "2507.10093", "pdf": "https://arxiv.org/pdf/2507.10093", "abs": "https://arxiv.org/abs/2507.10093", "authors": ["Neta Batya Maimon", "Lior Molcho", "Talya Zaimer", "Ofir Chibotero", "Nathan Intrator", "Eliezer Yahalom"], "title": "Dissociating Cognitive Load and Stress Responses Using Single-Channel EEG: Behavioral and Neural Correlates of Anxiety Across Cognitive States", "categories": ["q-bio.NC"], "comment": null, "summary": "Identifying neural markers of stress and cognitive load is key to developing\nscalable tools for mental state assessment. This study evaluated whether a\nsingle-channel high-density EEG (hdrEEG) system could dissociate cognitive and\nstress-related activity during a brief auditory task-based protocol.\nSixty-eight healthy adults completed resting state recordings, cognitively\ndemanding auditory tasks, and exposure to unpredictable literalized startle\nstimuli. Participants also rated their stress and anxiety using a modified\nState-Trait Anxiety Inventory (STAI). EEG analysis focused on frequency bands\n(Theta, Gamma, Delta) and machine-learning-derived features (A0, ST4, VC9, T2).\nA double dissociation emerged: Theta and VC9 increased under cognitive load but\nnot startle, supporting their sensitivity to executive function. In contrast,\nGamma and A0 were elevated by the startle stimulus, consistent with stress\nreactivity. ST4 tracked cognitive effort and worry, while T2 negatively\ncorrelated with self-reported calmness, indicating relevance to emotional\nregulation. These results demonstrate that a short, uniform assessment using\nportable EEG can yield multiple reliable biomarkers of cognitive and affective\nstates. The findings have implications for clinical, occupational, and\neducational settings, and may inform future neurofeedback protocols targeting\nsimultaneous regulation of attention and stress.", "AI": {"tldr": "This paper evaluates the use of a portable high-density EEG system to identify distinct neural markers for cognitive load and stress. Findings suggest the system is effective in distinguishing these states.", "motivation": "The study aims to develop scalable tools for assessing mental states by identifying neural markers linked to stress and cognitive load.", "method": "The study involved 68 participants who completed resting-state EEG recordings, auditory tasks, and exposure to startle stimuli. Frequency bands and machine-learning-derived features were analyzed.", "result": "Theta and VC9 increased under cognitive load, while Gamma and A0 were elevated by startle stimuli. Other features correlated with cognitive effort, worry, and self-reported emotional states.", "conclusion": "The findings show that portable EEG can reliably identify biomarkers for stress and cognitive load and support applications in clinical, educational, and occupational contexts."}}
{"id": "2507.09117", "pdf": "https://arxiv.org/pdf/2507.09117", "abs": "https://arxiv.org/abs/2507.09117", "authors": ["Gagan Khandate"], "title": "Towards Human-level Dexterity via Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": "PhD thesis", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "AI": {"tldr": "The paper addresses challenges in robotic dexterous manipulation, building reinforcement learning frameworks and exploring human demonstration-driven learning methods.", "motivation": "Achieving human-level dexterous intelligence using robotic hands, which is key to general embodied intelligence.", "method": "Developing structured exploration reinforcement learning frameworks and introducing visuo-tactile human demonstration imitation learning techniques.", "result": "Progressively refined reinforcement learning methods, culminating in effective techniques incorporating planning and human demonstration-based methods for dexterity.", "conclusion": "The presented approaches mark significant advancements in computational sensorimotor learning for robotic dexterity, pushing closer to human-level skill in manipulation."}}
{"id": "2507.09089", "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "A randomized controlled trial found that allowing experienced open-source developers to use AI tools actually increased their task completion time by 19%.", "motivation": "The motivation of the paper is to investigate the real-world impact of AI tools on software development productivity, specifically among experienced developers working on mature open-source projects.", "method": "The study involved a randomized controlled trial where 16 experienced open-source developers performed 246 tasks, with tasks randomly assigned to either allow or disallow AI tools.", "result": "The study found that, contrary to expectations, allowing AI tools increased task completion time by 19%. This effect contradicts both developer forecasts and expert predictions.", "conclusion": "The observed slowdown suggests that current AI tools may not always lead to anticipated productivity gains, and highlights the need for further investigations into the complex interactions between AI tools and developer workflows."}}
{"id": "2507.09005", "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "categories": ["cs.CV", "physics.geo-ph"], "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "The paper combines Neural Radiance Fields (NeRF) and Material Point Method (MPM) simulations to infer granular material properties, particularly the friction angle, from visual data, achieving high accuracy.", "motivation": "The need to characterize granular materials like sand in scenarios where direct measurement is impractical, such as real-world environments.", "method": "Synthetic experiments simulate a plow interacting with sand, generating visual observations. NeRF reconstructs 3D geometry from these observations, and MPM simulations with Bayesian optimization estimate material properties by minimizing image loss.", "result": "The approach successfully estimates the friction angle of granular materials with an error margin within 2 degrees.", "conclusion": "The method demonstrates the potential of inverse analysis through visual observations for non-invasive material property estimation, paving the way for real-world applications."}}
{"id": "2507.08834", "pdf": "https://arxiv.org/pdf/2507.08834", "abs": "https://arxiv.org/abs/2507.08834", "authors": ["Karishma Battina", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Physical Informed Neural Networks for modeling ocean pollutant", "categories": ["cs.LG"], "comment": "13 pages, 9 figures, 3 tables", "summary": "Traditional numerical methods often struggle with the complexity and scale of\nmodeling pollutant transport across vast and dynamic oceanic domains. This\npaper introduces a Physics-Informed Neural Network (PINN) framework to simulate\nthe dispersion of pollutants governed by the 2D advection-diffusion equation.\nThe model achieves physically consistent predictions by embedding physical laws\nand fitting to noisy synthetic data, generated via a finite difference method\n(FDM), directly into the neural network training process. This approach\naddresses challenges such as non-linear dynamics and the enforcement of\nboundary and initial conditions. Synthetic data sets, augmented with varying\nnoise levels, are used to capture real-world variability. The training\nincorporates a hybrid loss function including PDE residuals, boundary/initial\ncondition conformity, and a weighted data fit term. The approach takes\nadvantage of the Julia language scientific computing ecosystem for\nhigh-performance simulations, offering a scalable and flexible alternative to\ntraditional solvers", "AI": {"tldr": "This paper introduces a Physics-Informed Neural Network (PINN) framework to model oceanic pollutant transport by simulating the 2D advection-diffusion equation, utilizing a hybrid loss function and noisy synthetic data.", "motivation": "Traditional numerical methods face challenges in efficiently and accurately modeling pollutant transport due to complex dynamics and large-scale oceanic domains.", "method": "The framework uses a Physics-Informed Neural Network embedded with physical laws, trained on noisy synthetic data produced by the finite difference method, and integrates a hybrid loss function for training.", "result": "The PINN approach achieves physically consistent predictions and effectively addresses the enforcement of boundary and initial conditions in simulations.", "conclusion": "This scalable and flexible PINN model provides a promising alternative to traditional numerical solvers for simulating pollutant dispersion in dynamic oceanic environments."}}
{"id": "2507.10005", "pdf": "https://arxiv.org/pdf/2507.10005", "abs": "https://arxiv.org/abs/2507.10005", "authors": ["Yash Arya", "Sang Hoon Lee"], "title": "Effects of structural properties of neural networks on machine learning performance", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.NE", "physics.comp-ph"], "comment": "9 pages, 6 figures", "summary": "In recent years, graph-based machine learning techniques, such as\nreinforcement learning and graph neural networks, have garnered significant\nattention. While some recent studies have started to explore the relationship\nbetween the graph structure of neural networks and their predictive\nperformance, they often limit themselves to a narrow range of model networks,\nparticularly lacking mesoscale structures such as communities. Our work\nadvances this area by conducting a more comprehensive investigation,\nincorporating realistic network structures characterized by heterogeneous\ndegree distributions and community structures, which are typical\ncharacteristics of many real networks. These community structures offer a\nnuanced perspective on network architecture. Our analysis employs model\nnetworks such as random and scale-free networks, alongside a comparison with a\nbiological neural network and its subsets for more detailed analysis. We\nexamine the impact of these structural attributes on the performance of image\nclassification tasks. Our findings reveal that structural properties do affect\nperformance to some extent. Specifically, networks featuring coherent, densely\ninterconnected communities demonstrate enhanced learning capabilities. The\ncomparison with the biological neural network emphasizes the relevance of our\nfindings to real-world structures, suggesting an intriguing connection worth\nfurther exploration. This study contributes meaningfully to network science and\nmachine learning, providing insights that could inspire the design of more\nbiologically informed neural networks.", "AI": {"tldr": "The study investigates how network structures, including realistic community structures and heterogeneous degree distributions, impact the performance of neural networks in machine learning tasks.", "motivation": "The paper aims to address the gap in understanding how the graph structure of neural networks, beyond simplistic designs, influences predictive performance, especially considering mesoscale structures like communities.", "method": "The authors employ randomized and scale-free network models, alongside comparisons with biological neural networks, to analyze performance in image classification tasks. Structured attributes such as degree heterogeneity and community coherence are studied.", "result": "It was observed that networks with coherent and densely interconnected communities exhibit improved learning performance. Comparisons with biological systems reinforce the relevance of structural properties.", "conclusion": "The findings suggest that structural properties of neural networks can affect predictive performance and inspire designs informed by biological networks."}}
{"id": "2507.10139", "pdf": "https://arxiv.org/pdf/2507.10139", "abs": "https://arxiv.org/abs/2507.10139", "authors": ["Filipe Miguel Gon\u00e7alves de Almeida", "CJ Carey", "Hendrik Fichtenberger", "Jonathan Halcrow", "Silvio Lattanzi", "Andr\u00e9 Linhares", "Tao Meng", "Ashkan Norouzi-Fard", "Nikos Parotsidis", "Bryan Perozzi", "David Simcha"], "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.", "AI": {"tldr": "The paper introduces Dynamic Grale Using ScaNN, a system to construct large-scale graphs dynamically and with low latency, suitable for use cases requiring real-time updates.", "motivation": "Existing systems like Grale are efficient for offline graph construction but are unsuitable for dynamic environments requiring low-latency updates.", "method": "The authors developed Dynamic Grale Using ScaNN, a system designed for dynamic settings, inheriting Grale's attributes while achieving tens of milliseconds latency per request.", "result": "Dynamic Grale Using ScaNN has been deployed in over 10 applications at Google, significantly boosting graph construction efficiency and enabling faster detection of harmful apps in Android Security.", "conclusion": "Dynamic Grale Using ScaNN is effective in real-time graph updating, expanding the applicability of graph-learning systems to dynamic industrial scenarios."}}
{"id": "2507.09774", "pdf": "https://arxiv.org/pdf/2507.09774", "abs": "https://arxiv.org/abs/2507.09774", "authors": ["MD Zobaer Hossain Bhuiyan", "Abir Bin Faruque", "Mahtab Newaz", "Mohammad Abdul Qayum"], "title": "Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver", "categories": ["cs.AR"], "comment": null, "summary": "This paper presents the design and development of a low-cost fuel dispensing\nsystem prototype based on the STM32 microcontroller and L298N motor driver. The\nsystem aims to provide an affordable and scalable solution for fuel delivery in\nremote or small-scale environments where conventional, high-cost systems are\nnot feasible. The core control unit is built using an STM32 microcontroller,\nwhich manages user input through a 4x4 matrix keypad and displays operational\ndata on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to\nsimulate the fuel dispensing mechanism, precisely controlled via the dual\nH-bridge L298N motor driver. The system is powered by a 11.1V battery and is\ndesigned for ease of deployment and portability. The keypad allows users to\ninput the desired fuel amount, while the system ensures accurate motor runtime\ncorresponding to the volume to be dispensed. This project demonstrates how\nembedded systems can be leveraged to build cost-effective, user-friendly, and\nenergy-efficient solutions. The proposed design can be further enhanced with\nflow sensors, GSM connectivity, RFID cards, and payment integration for\nreal-world applications in fuel stations or agricultural use.", "AI": {"tldr": "This paper details a low-cost, portable fuel dispensing prototype using an STM32 microcontroller and L298N motor driver, aimed at small-scale and remote environments.", "motivation": "To develop an affordable, scalable fuel dispensing system for remote or small-scale settings where conventional systems are too expensive and impractical.", "method": "The system uses an STM32 microcontroller for control, a 4x4 keypad for user input, a 16x4 LCD for display, and a 12V DC pump motor controlled by an L298N motor driver. It is powered by an 11.1V battery and emphasizes efficiency and portability.", "result": "The prototype successfully simulates a cost-effective and user-friendly fuel dispensing mechanism with accurate motor runtime control.", "conclusion": "The design showcases the potential of embedded systems for building economical and energy-efficient fuel dispensing solutions, with future scope for integrating advanced features like flow sensors, GSM, RFID, and payment modules."}}
{"id": "2507.10099", "pdf": "https://arxiv.org/pdf/2507.10099", "abs": "https://arxiv.org/abs/2507.10099", "authors": ["Jay Lee", "Gyuhyeok Oh", "Joongwon Ahn", "Xiaokang Qiu"], "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI", "categories": ["cs.HC", "cs.PL"], "comment": "Submitted to UIST 2025 Posters", "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs.", "AI": {"tldr": "ReDemon UI allows designers and non-expert programmers to easily build React applications by synthesizing UI behavior from user demonstrations.", "motivation": "The paper aims to simplify the process of UI design for users without extensive coding expertise by leveraging standard prototyping workflows and eliminating technical barriers.", "method": "Users provide static mockup sketches and demonstrate desired runtime behaviors. It employs enumerative synthesis for basic UIs and LLMs (Large Language Models) for complex ones.", "result": "The system identifies reactive data and synthesizes React programs equipped with correct state update logic.", "conclusion": "ReDemon UI bridges the gap for non-programmers and designers, making UI creation more accessible while seamlessly integrating into existing workflows."}}
{"id": "2507.09051", "pdf": "https://arxiv.org/pdf/2507.09051", "abs": "https://arxiv.org/abs/2507.09051", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps", "categories": ["cs.SE"], "comment": null, "summary": "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.", "AI": {"tldr": "This study introduces SAGE, a methodology leveraging GPT and NLI to automatically identify privacy-related reviews in mental health apps, achieving high performance without fine-tuning and extracting actionable insights.", "motivation": "Sensitive data collection practices in MH apps lead to privacy concerns that are often overlooked amidst other user feedback like usability and reliability.", "method": "The paper presents SAGE, which employs Natural Language Inference (NLI) with domain-specific privacy hypotheses and GPT models to identify privacy-relevant reviews, eliminating the need for fine-tuning.", "result": "SAGE achieves an F1 score of 0.85 on a dataset of 204K app reviews, outperforming baseline classifiers, and identifies 748 additional privacy reviews missed by keyword-based methods.", "conclusion": "SAGE is highly effective for mining privacy-relevant user reviews, offering valuable insights for addressing privacy concerns without requiring extensive model modifications."}}
{"id": "2507.08969", "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "categories": ["cs.CL"], "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "The study identifies stigmatizing language in electronic health records and links it to patient demographics and provider types.", "motivation": "To investigate how stigmatization in healthcare manifests through linguistic features in electronic health records, focusing on patient demographics and provider types.", "method": "Analyzed linguistic features using lexicon matching, supervised learning classifiers, and Poisson regression models to determine predictors of stigmatizing language.", "result": "Higher rates of stigmatizing labels were observed among Black/African American patients, patients with government insurance, self-paying patients, and those with stigmatizing health conditions. Nurses and social workers were identified as contributors.", "conclusion": "Stigmatizing language in EHR perpetuates among historically marginalized patient groups and across different healthcare provider types."}}
{"id": "2507.10175", "pdf": "https://arxiv.org/pdf/2507.10175", "abs": "https://arxiv.org/abs/2507.10175", "authors": ["Eliezer Yahalom", "Neta Maimon", "Lior Molcho", "Talya Zeimer", "Ofir Chibotero", "Nathan Intrator"], "title": "The Evaluation of Breathing 5:5 effect on resilience, stress and balance center measured by Single-Channel EEG", "categories": ["q-bio.NC"], "comment": null, "summary": "Slow-paced breathing is a promising intervention for reducing anxiety and\nenhancing emotional regulation through its effects on autonomic and central\nnervous system function. This study examined the neurophysiological and\nsubjective effects of a 5:5 breathing protocol on stress-related EEG biomarkers\nusing a mobile single-channel EEG system. Thirty-eight healthy adults were\nrandomly assigned to either an intervention group (n = 20), which completed two\nsessions spaced two weeks apart with daily breathing practice, or a control\ngroup (n = 18), which completed one session. In each session, participants\nunderwent an auditory EEG assessment with resting, mental load, and startle\nconditions. The intervention group also completed a guided breathing session\nduring the first visit and practiced the technique between sessions. EEG\nbiomarkers (ST4, Alpha, Delta, Gamma, VC0) and subjective anxiety levels (STAI)\nwere assessed before and after the intervention. A significant reduction in\nGamma power was observed in the intervention group immediately following the\nfirst breathing session during mental load (p = .002), indicating acute stress\nreduction. Across sessions, long-term breathing practice led to increased Alpha\nand Delta power and reduced ST4 activity, suggesting cumulative improvements in\nemotional regulation and cognitive efficiency. Correlational analyses revealed\nthat changes in VC0 and Alpha were significantly associated with subjective\nreports of tension, focus difficulty, and calmness. Guided slow-paced breathing\nat a 5:5 rhythm produces both immediate and sustained effects on neural markers\nof stress and cognition, with corresponding improvements in subjective anxiety.\nThese findings support EEG-based monitoring as a scalable method for evaluating\nbreath-based interventions and promoting real-time emotional self-regulation.", "AI": {"tldr": "This study demonstrates that guided slow-paced breathing (5:5 rhythm) improves stress-related neural markers and subjective anxiety levels.", "motivation": "To explore the neurophysiological and subjective effects of slow-paced breathing as a scalable method to reduce anxiety and enhance emotional regulation.", "method": "Thirty-eight participants were randomized into intervention and control groups, with EEG biomarkers and subjective anxiety assessed pre- and post-intervention. The intervention group practiced 5:5 breathing both in sessions and daily.", "result": "Immediate reductions in Gamma power (mental load), and long-term increases in Alpha and Delta power, alongside reduced ST4 activity, correlated with improvements in emotional regulation and subjective calmness.", "conclusion": "Guided slow-paced breathing generates both acute and sustained benefits on stress-related neural and cognitive markers, supporting its efficacy in promoting emotional self-regulation via scalable biofeedback methods."}}
{"id": "2507.09123", "pdf": "https://arxiv.org/pdf/2507.09123", "abs": "https://arxiv.org/abs/2507.09123", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "categories": ["cs.RO"], "comment": null, "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "AI": {"tldr": "The paper addresses limitations in current deep-reinforcement-learning methods for the Online Bin Packing Problem (OBPP) by proposing a framework integrating structural stability validation and heuristic planning to improve bin stability and adaptability.", "motivation": "To address limitations in existing deep-reinforcement-learning methods for OBPP, which fail to ensure structural stability of bins or provide mechanisms to reconfigure them safely when items cannot be directly placed.", "method": "The framework introduces Load Bearable Convex Polygon (LBCP) for stability validation and Stable Rearrangement Planning (SRP) for rearranging items to accommodate new ones while ensuring stability.", "result": "Experiments on OBPP benchmarks show that LBCP effectively validates structural stability, and SRP identifies effort-saving rearrangement strategies, outperforming existing methods.", "conclusion": "The proposed method enhances packing stability and adaptability, offering a robust and practical solution for automated packing in industrial and logistics applications."}}
{"id": "2507.09179", "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "categories": ["cs.AI"], "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "This paper presents a Multi-Agent Reinforcement Learning (MARL) framework for detecting market manipulation in decentralized finance (DeFi) environments, using advanced machine learning and decentralized architectures.", "motivation": "DeFi's rise has enabled innovative financial practices but also left the ecosystem vulnerable to market manipulation tactics like shilling campaigns and pump-and-dump schemes. This paper addresses the need for decentralized solutions to detect and counteract such manipulations.", "method": "The paper introduces a MARL framework with key innovations: Group Relative Policy Optimization (GRPO) for learning stability, a theory-based reward function informed by financial theory, and a multi-modal agent pipeline using LLMs, social graphs, and market data. This framework is built into the Symphony decentralized architecture for peer-to-peer manipulation detection and evaluation.", "result": "The proposed framework, tested on 100,000 real-world data points and adversarial simulations, achieves high accuracy in detecting market manipulation and identifying causal attributions. Symphony enables decentralized, real-time surveillance of DeFi ecosystems.", "conclusion": "The proposed MARL framework bridges the gap between multi-agent systems and financial surveillance. It provides a decentralized, scalable, and effective solution for identifying and addressing market manipulation in DeFi, advancing research in decentralized market intelligence."}}
{"id": "2507.09008", "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "categories": ["cs.CV"], "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "This paper introduces VISTA, a visual analytics framework aimed at enhancing the quality of dataset labels generated by multi-modal foundation models for open-vocabulary image segmentation.", "motivation": "The authors aim to address the challenges posed by the poor quality of labels generated by multi-modal foundation models, as existing research focuses on quantity over quality and struggles with effectively validating large-scale data without ground truth.", "method": "The proposed VISTA framework integrates multi-phased data validation strategies with human expertise, allowing for the identification, understanding, and fixing of errors in FM-generated labels.", "result": "Empirical studies on benchmark datasets and expert reviews showcase the quantitative and qualitative improvements facilitated by VISTA in dataset labeling, leading to better-performing multi-modal models.", "conclusion": "VISTA successfully bridges the gap in validating and improving data quality for multi-modal models, providing a robust tool for refining FM-generated labels and improving downstream model performance."}}
{"id": "2507.08835", "pdf": "https://arxiv.org/pdf/2507.08835", "abs": "https://arxiv.org/abs/2507.08835", "authors": ["Harold Gu\u00e9neau", "Alain Celisse", "Pascal Delange"], "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "comment": null, "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "AI": {"tldr": "The paper proposes a method to detect money laundering using time series data through a transformer neural network, enhancing detection precision while controlling false positives.", "motivation": "The study aims to address the limitations of existing money laundering detection methods, which struggle to effectively balance detecting fraudsters from nonfraudsters while managing false positive rates.", "method": "The paper introduces a two-step procedure using contrastive learning to capture time series representations. Afterward, a scoring model applies a two-threshold approach to ensure controlled false positives.", "result": "The experiments demonstrate that the proposed transformer-based approach outperforms rule-based systems and LSTMs in detecting fraud patterns and distinguishing fraudsters from nonfraudsters while limiting false positives.", "conclusion": "The findings suggest the transformer approach is a promising improvement over traditional methods, providing efficacious money laundering detection with reduced domain supervision and better false positive rate management."}}
{"id": "2507.08906", "pdf": "https://arxiv.org/pdf/2507.08906", "abs": "https://arxiv.org/abs/2507.08906", "authors": ["Nathan Doum\u00e8che"], "title": "Physics-informed machine learning: A mathematical framework with applications to time series forecasting", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G05, 68Q32, 62P30, 35Q62, 68T01", "G.3; G.4; I.2.6; I.5.1"], "comment": "Doctoral thesis, Sorbonne University. 286 pages", "summary": "Physics-informed machine learning (PIML) is an emerging framework that\nintegrates physical knowledge into machine learning models. This physical prior\noften takes the form of a partial differential equation (PDE) system that the\nregression function must satisfy. In the first part of this dissertation, we\nanalyze the statistical properties of PIML methods. In particular, we study the\nproperties of physics-informed neural networks (PINNs) in terms of\napproximation, consistency, overfitting, and convergence. We then show how PIML\nproblems can be framed as kernel methods, making it possible to apply the tools\nof kernel ridge regression to better understand their behavior. In addition, we\nuse this kernel formulation to develop novel physics-informed algorithms and\nimplement them efficiently on GPUs. The second part explores industrial\napplications in forecasting energy signals during atypical periods. We present\nresults from the Smarter Mobility challenge on electric vehicle charging\noccupancy and examine the impact of mobility on electricity demand. Finally, we\nintroduce a physics-constrained framework for designing and enforcing\nconstraints in time series, applying it to load forecasting and tourism\nforecasting in various countries.", "AI": {"tldr": "The paper explores the statistical properties of Physics-Informed Machine Learning (PIML) methods and their industrial applications, including using physical constraints in forecasting energy signals during atypical periods.", "motivation": "To integrate physical knowledge into machine learning models to enhance their effectiveness and reliability. The paper aims to deepen understanding of PIML methods and apply them to industry-relevant forecasting challenges.", "method": "Analyzes statistical properties of PIML methods like Physics-Informed Neural Networks (PINNs), reframes PIML problems using kernel methods for efficient GPU implementation, and explores applications in load forecasting using physical constraints.", "result": "Presented insights into PINNs' approximation, consistency, overfitting, and convergence. Developed novel algorithms with kernel methods. Achieved forecasting improvements in energy signals and mobility-impact analysis.", "conclusion": "PIML methods improved understanding and efficiency through kernel methods and GPU optimization. Industrial applications demonstrated effective use of physical constraints, benefiting forecasting in energy and tourism domains."}}
{"id": "2507.10383", "pdf": "https://arxiv.org/pdf/2507.10383", "abs": "https://arxiv.org/abs/2507.10383", "authors": ["Uri Cohen", "M\u00e1t\u00e9 Lengyel"], "title": "Dynamical stability for dense patterns in discrete attractor neural networks", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "cs.NE", "q-bio.NC"], "comment": null, "summary": "Neural networks storing multiple discrete attractors are canonical models of\nbiological memory. Previously, the dynamical stability of such networks could\nonly be guaranteed under highly restrictive conditions. Here, we derive a\ntheory of the local stability of discrete fixed points in a broad class of\nnetworks with graded neural activities and in the presence of noise. By\ndirectly analyzing the bulk and outliers of the Jacobian spectrum, we show that\nall fixed points are stable below a critical load that is distinct from the\nclassical \\textit{critical capacity} and depends on the statistics of neural\nactivities in the fixed points as well as the single-neuron activation\nfunction. Our analysis highlights the computational benefits of\nthreshold-linear activation and sparse-like patterns.", "AI": {"tldr": "The paper develops a theory for analyzing the stability of fixed points in neural networks modeling biological memory, emphasizing their dependence on neural activity statistics and activation functions.", "motivation": "The paper aims to solve the challenge of guaranteeing the dynamical stability of fixed points in neural networks storing discrete attractors, an essential feature for modeling biological memory.", "method": "The authors derive a new theory to analyze the local stability of fixed points by studying the Jacobian spectrum, including its bulk and outliers, in networks with graded neural activities and noise.", "result": "The study established that fixed points are stable below a critical load, different from classical critical capacity, and identified key influences like neural activity statistics and single-neuron activation functions.", "conclusion": "Threshold-linear activation functions and sparse-like patterns offer computational advantages, enhancing the stability and performance of neural networks storing multiple discrete attractors."}}
{"id": "2507.10150", "pdf": "https://arxiv.org/pdf/2507.10150", "abs": "https://arxiv.org/abs/2507.10150", "authors": ["Ruihao Gong", "Shihao Bai", "Siyu Wu", "Yunqian Fan", "Zaijun Wang", "Xiuhong Li", "Hailong Yang", "Xianglong Liu"], "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees", "categories": ["cs.DC"], "comment": "Accepted to ASPLOS 2025", "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).", "AI": {"tldr": "The paper introduces the Past-Future scheduler to enhance LLM service frameworks by accurately estimating memory requirements and balancing request management, showcased in the open-source framework LightLLM.", "motivation": "Existing memory management in LLM applications struggles with balancing request queuing and evictions, severely impacting throughput and SLA adherence.", "method": "The authors propose the Past-Future scheduler, which uses historical memory usage data and future memory point projections to optimize memory estimates and request handling.", "result": "LightLLM, implementing the Past-Future scheduler, achieves 2-3\u00d7 higher goodput compared to other schedulers under heavy service loads.", "conclusion": "The proposed scheduler consistently outperforms traditional methods in maintaining high service throughput, making it suitable for diverse LLM application scenarios."}}
{"id": "2507.09780", "pdf": "https://arxiv.org/pdf/2507.09780", "abs": "https://arxiv.org/abs/2507.09780", "authors": ["Feilong Qiaoyuan", "Jihe Wang", "Zhiyu Sun", "Linying Wu", "Yuanhua Xiao", "Danghui Wang"], "title": "BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 13 figures, 3 Tables", "summary": "Bit-level sparsity in quantized deep neural networks (DNNs) offers\nsignificant potential for optimizing Multiply-Accumulate (MAC) operations.\nHowever, two key challenges still limit its practical exploitation. First,\nconventional bit-serial approaches cannot simultaneously leverage the sparsity\nof both factors, leading to a complete waste of one factor' s sparsity. Methods\ndesigned to exploit dual-factor sparsity are still in the early stages of\nexploration, facing the challenge of partial product explosion. Second, the\nfluctuation of bit-level sparsity leads to variable cycle counts for MAC\noperations. Existing synchronous scheduling schemes that are suitable for\ndual-factor sparsity exhibit poor flexibility and still result in significant\nunderutilization of MAC units. To address the first challenge, this study\nproposes a MAC unit that leverages dual-factor sparsity through the emerging\nparticlization-based approach. The proposed design addresses the issue of\npartial product explosion through simple control logic, resulting in a more\narea- and energy-efficient MAC unit. In addition, by discarding less\nsignificant intermediate results, the design allows for further hardware\nsimplification at the cost of minor accuracy loss. To address the second\nchallenge, a quasi-synchronous scheme is introduced that adds cycle-level\nelasticity to the MAC array, reducing pipeline stalls and thereby improving MAC\nunit utilization. Evaluation results show that the exact version of the\nproposed MAC array architecture achieves a 29.2% improvement in area efficiency\ncompared to the state-of-the-art bit-sparsity-driven architecture, while\nmaintaining comparable energy efficiency. The approximate variant further\nimproves energy efficiency by 7.5%, compared to the exact version. Index-Terms:\nDNN acceleration, Bit-level sparsity, MAC unit", "AI": {"tldr": "The paper proposes a novel Multiply-Accumulate (MAC) unit and scheduling strategy to efficiently leverage dual-factor bit-level sparsity in quantized Deep Neural Networks (DNNs), addressing challenges like wasted sparsity and pipeline stalls.", "motivation": "The paper seeks to address inefficiencies in Multiply-Accumulate (MAC) operations, particularly wasted dual-factor sparsity and underutilization of MAC units caused by conventional approaches.", "method": "The study introduces a particlization-based MAC unit design to exploit dual-factor sparsity and manages hardware challenges with simple control logic. Additionally, it proposes a quasi-synchronous scheduling scheme to increase the utilization of MAC units.", "result": "The proposed MAC architecture achieves a 29.2% improvement in area efficiency over existing bit-sparsity-driven designs, while its approximate variant offers a further 7.5% energy efficiency boost.", "conclusion": "The work provides a more area-efficient, energy-efficient method for DNN acceleration, addressing key challenges of dual-factor sparsity and irregular MAC operation cycles at the cost of slight accuracy trade-offs."}}
{"id": "2507.10324", "pdf": "https://arxiv.org/pdf/2507.10324", "abs": "https://arxiv.org/abs/2507.10324", "authors": ["Amit K. Chopra", "Samuel H. Christie V", "Munindar P. Singh"], "title": "Toolsuite for Implementing Multiagent Systems Based on Communication Protocols", "categories": ["cs.MA", "cs.AI", "cs.PL", "cs.SE", "I.2.11; I.2.4; I.2.5"], "comment": null, "summary": "Interaction-Oriented Programming (IOP) is an approach to building a\nmultiagent system by modeling the interactions between its roles via a flexible\ninteraction protocol and implementing agents to realize the interactions of the\nroles they play in the protocol.\n  In recent years, we have developed an extensive suite of software that\nenables multiagent system developers to apply IOP. These include tools for\nefficiently verifying protocols for properties such as liveness and safety and\nmiddleware that simplifies the implementation of agents. This paper presents\nsome of that software suite.", "AI": {"tldr": "This paper discusses Interaction-Oriented Programming (IOP) for multiagent systems, introducing a software suite for protocol verification and agent implementation.", "motivation": "To enhance multiagent system development by focusing on interaction modeling through flexible protocols and ease their verification and implementation.", "method": "Development of a comprehensive software suite including tools for protocol verification (liveness, safety) and middleware for agent implementation.", "result": "The software suite facilitates effective and efficient development of multiagent systems, ensuring properties like liveness and safety.", "conclusion": "IOP and the accompanying software support simplify the creation and management of multiagent systems by focusing on interaction protocol efficiency and agent interaction implementation."}}
{"id": "2507.09063", "pdf": "https://arxiv.org/pdf/2507.09063", "abs": "https://arxiv.org/abs/2507.09063", "authors": ["Avi Arora", "Jinu Jang", "Roshanak Zilouchian Moghaddam"], "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern Large Language Model (LLM) agents promise end to end assistance with\nreal-world software tasks, yet existing benchmarks evaluate LLM agents almost\nexclusively in pre-baked environments where every dependency is pre-installed.\nTo fill this gap, we introduce SetupBench, a 93 instance benchmark that\nisolates the environment-bootstrap skill: starting from a bare Linux sandbox,\nan agent must install packages, resolve dependency conflicts, initialize\ndatabases, and configure background services. Our tasks span seven language\necosystems, five database engines, and multi-service orchestration scenarios,\neach accompanies by a natural language problem statement and a deterministic\nsuccess command. Through evaluation of OpenHands, a state-of-the-art coding\nagent, we find low success rates across task categories, with particular\nchallenges in repository setup (38.9-57.4%) and local database configuration\n(20.0-53.3%). Our analysis reveals systematic failure modes including\nincomplete development tooling installation, hallucinated task constraints, and\nnon-persistent environment modifications that break agent-human collaboration\nworkflows. We identify substantial inefficiencies in agent exploration\nstrategies, with 38-89% of actions being unnecessary compared to optimal human\nbehavior. These findings highlight gaps in current agents' practical\nenvironment-bootstrap capabilities. By targeting this critical yet\nunder-evaluated capability, SetupBench provides a rigorous yard-stick for the\nnext generation of software developer agents aiming to solve end to end\nreal-wold tasks.", "AI": {"tldr": "SetupBench evaluates LLM agents on real-world environment setup tasks in a bare Linux sandbox, exposing limitations in current agent capabilities.", "motivation": "To address the gap in benchmarks for evaluating LLM agents\u2019 ability to bootstrap environments in real-world software tasks.", "method": "SetupBench introduces a benchmark composed of 93 tasks across diverse ecosystems, assessing agents\u2019 ability to install dependencies, initialize databases, and configure services.", "result": "OpenHands showcases low success rates, revealing inefficiencies in task execution and systematic flaws like unnecessary actions and hallucinated constraints.", "conclusion": "SetupBench highlights critical deficiencies in current LLM agents for practical environment setup, emphasizing areas for improvement in future agents."}}
{"id": "2507.09011", "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "The study explores how imagery abilities (absent, typical, vivid) influence visual hallucinations during Ganzflicker stimulation, using NLP to analyze hallucination descriptions from over 4,000 participants.", "motivation": "The research aims to investigate how individual differences in visual imagery abilities impact hallucination experiences, as these differences may provide insight into the generative capacity of the visual system.", "method": "The team analyzed free-text descriptions of hallucinations from over 4,000 participants using tools from natural language processing, comparing imagery phenotypes (strong vs. weak imagers) and examining sensorimotor associations in language usage.", "result": "Strong imagers described more complex, naturalistic hallucinations, while weak imagers reported simpler geometric patterns. Vision-language model embeddings captured these distinctions better than text-only language models.", "conclusion": "The findings suggest that variations in imagery abilities reflect differences in coordination between early visual areas and higher-order brain regions, emphasizing the generative diversity in the visual system."}}
{"id": "2507.09160", "pdf": "https://arxiv.org/pdf/2507.09160", "abs": "https://arxiv.org/abs/2507.09160", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "AI": {"tldr": "The paper introduces Tactile-VLA, a framework that integrates vision, language, action, and tactile sensing to enable robots to adapt in fine-grained, contact-rich scenarios using prior knowledge for zero-shot generalization.", "motivation": "To address the challenge of precise grounding of generalist robotic agents into physical interactions, especially in contact-rich scenarios requiring fine-grained force control.", "method": "The framework combines a hybrid position-force controller with a reasoning module, leveraging tactile sensing to connect vision-language models' implicit knowledge with physical action.", "result": "Experiments confirm Tactile-VLA's generalizability and effectiveness in instruction following, commonsense utilization, and adaptive reasoning based on tactile feedback.", "conclusion": "By connecting vision-language models to tactile sensors with few demonstrations, Tactile-VLA activates prior knowledge for zero-shot generalization in physical tasks."}}
{"id": "2507.09329", "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "categories": ["cs.AI", "cs.CR"], "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "This paper performs a comprehensive security evaluation of LLM-based coding agents, revealing significant vulnerabilities and proposing mitigation strategies.", "motivation": "Address the growing deployment of LLM coding agents in software development and understand their security implications.", "method": "Analyzed 12,000 actions from state-of-the-art models on real-world tasks, developed a detection system, and tested mitigation strategies.", "result": "Found 21% agent trajectories contained insecure actions; GPT-4.1 had high success in security mitigation.", "conclusion": "Coding agents require security-aware design to reduce vulnerabilities and promote safer software development practices."}}
{"id": "2507.09036", "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik M\u00f6ller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite is a Python toolkit designed to simplify brain lesion image analysis through modular pipelines and adaptable preprocessing.", "motivation": "To streamline the creation of complex workflows for clinical and scientific applications in brain lesion image analysis.", "method": "It incorporates preprocessing modules (e.g., co-registration, atlas registration, skull-stripping) and tools leveraging BraTS algorithms for synthesis, inpainting, and tumor segmentation, along with performance quantification metrics.", "result": "BrainLesion Suite facilitates versatile image analysis for brain lesions, with functionalities extendable to other biomedical applications.", "conclusion": "The toolkit minimizes cognitive effort and offers accessible resources for efficient and modular biomedical image analysis pipelines."}}
{"id": "2507.08922", "pdf": "https://arxiv.org/pdf/2507.08922", "abs": "https://arxiv.org/abs/2507.08922", "authors": ["Tameem Adel"], "title": "The Bayesian Approach to Continual Learning: An Overview", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Continual learning is an online paradigm where a learner continually\naccumulates knowledge from different tasks encountered over sequential time\nsteps. Importantly, the learner is required to extend and update its knowledge\nwithout forgetting about the learning experience acquired from the past, and\nwhile avoiding the need to retrain from scratch. Given its sequential nature\nand its resemblance to the way humans think, continual learning offers an\nopportunity to address several challenges which currently stand in the way of\nwidening the range of applicability of deep models to further real-world\nproblems. The continual need to update the learner with data arriving\nsequentially strikes inherent congruence between continual learning and\nBayesian inference which provides a principal platform to keep updating the\nprior beliefs of a model given new data, without completely forgetting the\nknowledge acquired from the old data. This survey inspects different settings\nof Bayesian continual learning, namely task-incremental learning and\nclass-incremental learning. We begin by discussing definitions of continual\nlearning along with its Bayesian setting, as well as the links with related\nfields, such as domain adaptation, transfer learning and meta-learning.\nAfterwards, we introduce a taxonomy offering a comprehensive categorization of\nalgorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we\nanalyze the state-of-the-art while zooming in on some of the most prominent\nBayesian continual learning algorithms to date. Furthermore, we shed some light\non links between continual learning and developmental psychology, and\ncorrespondingly introduce analogies between both fields. We follow that with a\ndiscussion of current challenges, and finally conclude with potential areas for\nfuture research on Bayesian continual learning.", "AI": {"tldr": "This paper surveys Bayesian continual learning for handling sequential data updates without losing past knowledge, offering a taxonomy and analysis of state-of-the-art methods, and exploring links to developmental psychology.", "motivation": "To address challenges in real-world applicability of deep models with a framework that mimics human cognitive processes and maintains prior knowledge during sequential learning.", "method": "Classifies and examines Bayesian continual learning algorithms, discusses links to related fields, and provides analogies with developmental psychology.", "result": "Provides a comprehensive taxonomy of Bayesian continual learning methods while analyzing existing algorithms and their performance on task-incremental and class-incremental learning problems.", "conclusion": "Bayesian continual learning has significant potential for real-world applications, though challenges remain, with promising directions for future research highlighted."}}
{"id": "2507.10259", "pdf": "https://arxiv.org/pdf/2507.10259", "abs": "https://arxiv.org/abs/2507.10259", "authors": ["Chengze Du", "Zhiwei Yu", "Heng Xu", "Haojie Wang", "Bo liu", "Jialong Li"], "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning", "categories": ["cs.DC", "cs.NI"], "comment": "17 pages, 12 figures", "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.", "AI": {"tldr": "TORTA improves GPU-based large language model services by introducing a spatiotemporal scheduling framework to handle dynamic workloads efficiently.", "motivation": "Existing GPU scheduling systems fail to account for dynamic changes in task demand and resource availability, leading to inefficient operations and poor performance.", "method": "TORTA uses a two-layer architecture: a macro-level scheduler employing reinforcement learning and optimal transport for task distribution, and a micro-level allocator for task assignment within regions.", "result": "TORTA achieves up to 15% lower inference response times, improves load balance by 4-5%, and reduces operational costs by 10-20% in comparison to existing methods.", "conclusion": "TORTA\u2019s spatiotemporal approach enhances GPU utilization and responsiveness, demonstrating significant advancements over current scheduling strategies."}}
{"id": "2507.10178", "pdf": "https://arxiv.org/pdf/2507.10178", "abs": "https://arxiv.org/abs/2507.10178", "authors": ["Wonung Kim", "Yubin Lee", "Yoonsung Kim", "Jinwoo Hwang", "Seongryong Oh", "Jiyong Jung", "Aziz Huseynov", "Woong Gyu Park", "Chang Hyun Park", "Divya Mahajan", "Jongse Park"], "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.", "AI": {"tldr": "The paper addresses the scalability challenges of long-context inferencing in transformers and explores efficient systems to support both transformers and alternative architectures like SSMs and RNNs. It proposes a novel hardware design, Pimba, for improved token generation throughput.", "motivation": "To address the scalability and efficiency challenges in serving large language models with different architectural needs (transformers and post-transformers) under a unified framework.", "method": "The study analyzes performance characteristics of transformer and post-transformer models, leading to the design of Pimba, a hardware system with shared State-update Processing Units optimized using MX-based quantized arithmetic.", "result": "Pimba delivers up to 3.2x higher token generation throughput compared to GPUs designed for LLMs, and 2.1x higher than GPU+PIM systems.", "conclusion": "Pimba represents a significant breakthrough in hardware acceleration for diverse LLM architectures, bridging scalability gaps while maximizing throughput."}}
{"id": "2507.09108", "pdf": "https://arxiv.org/pdf/2507.09108", "abs": "https://arxiv.org/abs/2507.09108", "authors": ["Aaditya Bhatia", "Gustavo A. Oliva", "Gopi Krishnan Rajbahadur", "Haoxiang Zhang", "Yihao Chen", "Zhilong Chen", "Arthur Leung", "Dayi Lin", "Boyuan Chen", "Ahmed E. Hassan"], "title": "SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "High-quality labeled datasets are crucial for training and evaluating\nfoundation models in software engineering, but creating them is often\nprohibitively expensive and labor-intensive. We introduce SPICE, a scalable,\nautomated pipeline for labeling SWE-bench-style datasets with annotations for\nissue clarity, test coverage, and effort estimation. SPICE combines\ncontext-aware code navigation, rationale-driven prompting, and multi-pass\nconsensus to produce labels that closely approximate expert annotations.\nSPICE's design was informed by our own experience and frustration in labeling\nmore than 800 instances from SWE-Gym. SPICE achieves strong agreement with\nhuman-labeled SWE-bench Verified data while reducing the cost of labeling 1,000\ninstances from around $100,000 (manual annotation) to just $5.10. These results\ndemonstrate SPICE's potential to enable cost-effective, large-scale dataset\ncreation for SE-focused FMs. To support the community, we release both SPICE\ntool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated\nfrom 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench\nVerified).", "AI": {"tldr": "The paper introduces SPICE, a scalable automated pipeline to label datasets for foundation models in software engineering, significantly reducing costs while maintaining quality.", "motivation": "Creating high-quality labeled datasets for software engineering foundation models is expensive and time-consuming.", "method": "SPICE uses automated context-aware code navigation, rationale-driven prompting, and multi-pass consensus to approximate expert annotations efficiently.", "result": "SPICE achieved strong agreement with human-labeled data while reducing costs to $5.10 for 1,000 instances, down from $100,000 using manual annotations.", "conclusion": "SPICE demonstrates potential for cost-effective dataset creation at scale and provides a new large dataset and tool for the SE community."}}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard introduces a method to make Transformer-based large language models more efficient for infinite-context tasks using subquadratic attention with a hybrid mechanism of global and local interaction handling.", "motivation": "The authors seek to address computational inefficiencies and memory bottlenecks in Transformer-based LLMs caused by quadratic softmax attention and growing key-value caches as context lengths increase.", "method": "They implement a subquadratic attention mechanism approximating softmax attention, incorporate a gating module inspired by state-of-the-art models, and combine global and local attention techniques to optimize memory control and sequence length generalization.", "result": "Lizard outperforms previous linearization methods, achieves near-lossless recovery of the teacher model's quality, improves the 5-shot MMLU benchmark by 18 points, and excels in associative recall tasks.", "conclusion": "Lizard enables more efficient, flexible, and scalable Transformer-based LLMs while maintaining high performance, offering promising improvements in long-context modeling."}}
{"id": "2507.09897", "pdf": "https://arxiv.org/pdf/2507.09897", "abs": "https://arxiv.org/abs/2507.09897", "authors": ["Loek van Rossem", "Andrew M. Saxe"], "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task", "categories": ["cs.LG", "q-bio.NC"], "comment": "28 pages, 20 figures", "summary": "Even when massively overparameterized, deep neural networks show a remarkable\nability to generalize. Research on this phenomenon has focused on\ngeneralization within distribution, via smooth interpolation. Yet in some\nsettings neural networks also learn to extrapolate to data far beyond the\nbounds of the original training set, sometimes even allowing for infinite\ngeneralization, implying that an algorithm capable of solving the task has been\nlearned. Here we undertake a case study of the learning dynamics of recurrent\nneural networks (RNNs) trained on the streaming parity task in order to develop\nan effective theory of algorithm development. The streaming parity task is a\nsimple but nonlinear task defined on sequences up to arbitrary length. We show\nthat, with sufficient finite training experience, RNNs exhibit a phase\ntransition to perfect infinite generalization. Using an effective theory for\nthe representational dynamics, we find an implicit representational merger\neffect which can be interpreted as the construction of a finite automaton that\nreproduces the task. Overall, our results disclose one mechanism by which\nneural networks can generalize infinitely from finite training experience.", "AI": {"tldr": "The paper studies how recurrent neural networks (RNNs) trained on the streaming parity task can generalize infinitely after finite training, explaining this through representational dynamics.", "motivation": "To understand how neural networks, particularly RNNs, are capable of extrapolating far beyond training data and achieving infinite generalization in certain tasks.", "method": "The authors examine the representational dynamics of RNNs by studying their training on the streaming parity task, a nonlinear task defined over sequences of arbitrary lengths. They propose an effective theory and analyze phase transitions in learning behavior.", "result": "RNNs, with sufficient finite training, demonstrate a phase transition that leads to perfect infinite generalization. The phenomenon is explained by an implicit representational merger effect, interpreted as building a finite automaton for the task.", "conclusion": "Neural networks, like RNNs, can generalize infinitely from finite training datasets under certain conditions due to a representational merger, effectively learning an algorithm for the task."}}
{"id": "2507.09167", "pdf": "https://arxiv.org/pdf/2507.09167", "abs": "https://arxiv.org/abs/2507.09167", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "title": "PRAG: Procedural Action Generator", "categories": ["cs.RO"], "comment": null, "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "AI": {"tldr": "The paper proposes a generator for procedurally creating multi-step solvable manipulation tasks in robotics, ensuring logical and physical validation.", "motivation": "Robotics often requires solvable and efficient manipulation tasks that are realistic and tailored to the robotic environment.", "method": "The generator uses symbolic validation for logical consistency and physical validation for solvability within the robotic setup, ensuring only functional tasks are retained.", "result": "The generator successfully produced millions of unique, multi-step solvable tasks, with up to 15 actions per sequence.", "conclusion": "The approach offers a scalable way to generate rich, solvable robotic tasks, enhancing datasets and training frameworks for manipulation tasks using dense rewards."}}
{"id": "2507.09369", "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "The paper discusses potential AI scenarios leading to human extinction and advocates preventive measures.", "motivation": "To raise awareness and support preventive measures against the potential catastrophic risks posed by AI.", "method": "Develop a taxonomy of potential omnicidal AI scenarios and provide illustrative examples.", "result": "The taxonomy and examples reveal various hypothetical ways AI could pose catastrophic existential risks to humanity.", "conclusion": "Publicizing possible omnicidal AI scenarios may garner public and institutional support to prevent such risks."}}
{"id": "2507.09052", "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "The paper addresses the challenge of class imbalance in training data for class-conditional image synthesis using diffusion models, proposing innovative contrastive loss functions.", "motivation": "Class-conditional image generation often suffers from a lack of diversity for tail classes due to imbalanced datasets, leading to mode collapse and reduced tail class image quality.", "method": "Introduces two contrastive loss functions: an unsupervised InfoNCE loss to increase dissimilarity among synthetic images, and an MSE loss to enrich tail class diversity by aligning conditional generation with unconditional outputs during initial denoising steps.", "result": "The proposed framework improves the diversity of tail class images without compromising head class fidelity across datasets like CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.", "conclusion": "This easy-to-implement contrastive learning approach significantly enhances long-tailed class-conditional diffusion models and adapts conditional-unconditional alignment effectively for diffusion models."}}
{"id": "2507.08838", "pdf": "https://arxiv.org/pdf/2507.08838", "abs": "https://arxiv.org/abs/2507.08838", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Preprint", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "AI": {"tldr": "This paper proposes $\texttt{wd1}$, a new policy optimization method for improving reasoning in diffusion-based large language models (dLLMs) using reinforcement learning, achieving higher accuracy and computational efficiency.", "motivation": "Current reinforcement learning approaches for dLLMs suffer from computational overhead and bias due to reliance on multiple likelihood approximations, especially during policy optimization steps.", "method": "The paper introduces $\texttt{wd1}$, a weighted likelihood reformulation requiring a single likelihood approximation, eliminating the need for supervised fine-tuning or supervised data while optimizing policies more efficiently.", "result": "Experiments on reasoning benchmarks show that $\texttt{wd1}$ achieves up to 16% higher accuracy compared to existing RL methods, along with reduced training time and fewer function evaluations per gradient step.", "conclusion": "$\texttt{wd1}$ provides a computationally efficient and simpler approach to improving reasoning in dLLMs through reinforcement learning, outperforming existing methods without relying on supervised data."}}
{"id": "2507.08994", "pdf": "https://arxiv.org/pdf/2507.08994", "abs": "https://arxiv.org/abs/2507.08994", "authors": ["Joseph Lazzaro", "Ciara Pike-Burke"], "title": "Fixed-Confidence Multiple Change Point Identification under Bandit Feedback", "categories": ["stat.ML", "cs.LG"], "comment": "ICML 2025", "summary": "Piecewise constant functions describe a variety of real-world phenomena in\ndomains ranging from chemistry to manufacturing. In practice, it is often\nrequired to confidently identify the locations of the abrupt changes in these\nfunctions as quickly as possible. For this, we introduce a fixed-confidence\npiecewise constant bandit problem. Here, we sequentially query points in the\ndomain and receive noisy evaluations of the function under bandit feedback. We\nprovide instance-dependent lower bounds for the complexity of change point\nidentification in this problem. These lower bounds illustrate that an optimal\nmethod should focus its sampling efforts adjacent to each of the change points,\nand the number of samples around each change point should be inversely\nproportional to the magnitude of the change. Building on this, we devise a\nsimple and computationally efficient variant of Track-and-Stop and prove that\nit is asymptotically optimal in many regimes. We support our theoretical\nfindings with experimental results in synthetic environments demonstrating the\nefficiency of our method.", "AI": {"tldr": "The paper introduces a problem and method for identifying abrupt changes in piecewise constant functions using bandit feedback. It provides theoretical insights, such as optimal sampling strategies, proposes an efficient algorithm, and presents experimental validation.", "motivation": "Rapid identification of change points in piecewise constant functions is vital for applications across various domains like chemistry and manufacturing.", "method": "The proposed method uses bandit feedback to sequentially query points, focuses sampling near change points based on magnitude, and introduces a variant of Track-and-Stop algorithm. It provides instance-dependent complexity bounds.", "result": "Theoretical findings indicate asymptotic optimality of the proposed method in multiple regimes. Experimental results validate its computational efficiency in synthetic settings.", "conclusion": "The study successfully devises an optimal and efficient method for change point identification in piecewise constant functions, aligning theoretical insights with practical validation."}}
{"id": "2507.09776", "pdf": "https://arxiv.org/pdf/2507.09776", "abs": "https://arxiv.org/abs/2507.09776", "authors": ["Mihir Kavishwar", "Naresh Shanbhag"], "title": "Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing", "categories": ["eess.SP", "cs.AR"], "comment": "Code available at: https://github.com/mihirvk2/CSNR-optimal-ADC", "summary": "Analog in-memory computing (AIMC) is an energy-efficient alternative to\ndigital architectures for accelerating machine learning and signal processing\nworkloads. However, its energy efficiency is limited by the high energy cost of\nthe column analog-to-digital converters (ADCs). Reducing the ADC precision is\nan effective approach to lowering its energy cost. However, doing so also\nreduces the AIMC's computational accuracy thereby making it critical to\nidentify the minimum precision required to meet a target accuracy. Prior works\noverestimate the ADC precision requirements by modeling quantization error as\ninput-independent noise, maximizing the signal-to-quantization-noise ratio\n(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address\nthese limitations by developing analytical expressions for estimating the\ncompute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and\npropose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a\ncircuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we\nshow that for a 256-dimensional binary dot product, CACTUS reduces the ADC\nprecision requirements by 3b while achieving 6dB higher CSNR over prior\nmethods. We also delineate operating conditions under which our proposed\nCSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.", "AI": {"tldr": "This paper improves the energy efficiency of analog in-memory computing (AIMC) by proposing a new approach to optimize analog-to-digital converter (ADC) parameters, significantly reducing precision requirements while maintaining computational accuracy.", "motivation": "The high energy cost of column ADCs in AIMC systems limits their energy efficiency, and current methods overestimate ADC precision requirements, leading to inefficiencies.", "method": "The authors developed analytical expressions for compute signal-to-noise ratio (CSNR) and proposed a new algorithm, CACTUS, to determine CSNR-optimal ADC parameters.", "result": "CACTUS reduces ADC precision requirements by 3 bits and achieves 6dB higher CSNR compared to traditional methods using a behavioral model of SRAM-based AIMC in a 28nm CMOS process.", "conclusion": "Adopting CSNR-optimal ADCs instead of SQNR-optimal ADCs can enhance computational accuracy and energy efficiency, making AIMC systems more practical for machine learning and signal processing."}}
{"id": "2507.09135", "pdf": "https://arxiv.org/pdf/2507.09135", "abs": "https://arxiv.org/abs/2507.09135", "authors": ["Yalong Du", "Chaozheng Wang", "Huaijin Wang"], "title": "Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in automated\ncode generation, yet their statistical nature and black-box characteristics\ncreate significant semantic gaps manifested through syntax errors, semantic\nhallucinations, and reliability concerns. This position paper argues that\nprincipled integration of Programming Language (PL) techniques is essential for\nbridging these gaps. Through structured program representations, formal\ncorrectness guarantees, and robust verification mechanisms, PL techniques can\nelevate LLM-generated code from statistical pattern matching to truly reliable\nand trustworthy levels. This integration is crucial for developing systems that\ngenerate code that is not only functionally correct but also interpretable,\nverifiable, and ultimately trustworthy.", "AI": {"tldr": "Large Language Models excel in code generation, but integrating Programming Language techniques is essential to overcome drawbacks like syntax errors and reliability issues.", "motivation": "LLMs face challenges like syntax errors and reliability concerns due to their statistical nature.", "method": "Propose integrating PL techniques such as structured representations, formal correctness, and verification mechanisms.", "result": "Enhancement in the reliability and trustworthiness of code generated by LLMs.", "conclusion": "Integrating PL techniques will make LLM-generated code functionally correct, interpretable, and trustworthy."}}
{"id": "2507.09037", "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "This paper proposes ALIGN, a system for aligning and personalizing large language model (LLM)-based decision-makers to diverse user values/preferences through prompt-based adjustments.", "motivation": "To address the challenge of aligning LLMs to user-specific values and preferences for dynamic and personalized decision-making, moving beyond conventional benchmarking tasks.", "method": "Developed ALIGN, a system incorporating prompt-based alignment to fine-grained attributes, robust configurations, structured outputs, and modular integrations, enabling qualitative and quantitative analysis of LLMs.", "result": "Demonstrated the effectiveness of ALIGN in two domains: demographic alignment for public opinion surveys and value alignment for medical triage, providing a qualitative comparison tool and quantitative insights.", "conclusion": "ALIGN offers an open-source framework for advancing research on personalized, responsible, and reliable LLM-based decision-making systems, accommodating the diversity of user needs."}}
{"id": "2507.09996", "pdf": "https://arxiv.org/pdf/2507.09996", "abs": "https://arxiv.org/abs/2507.09996", "authors": ["Quentin Dessain", "Nicolas Delinte", "Bernard Hanseeuw", "Laurence Dricot", "Beno\u00eet Macq"], "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI", "categories": ["cs.CV", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Objective: This study aims to support early diagnosis of Alzheimer's disease\nand detection of amyloid accumulation by leveraging the microstructural\ninformation available in multi-shell diffusion MRI (dMRI) data, using a vision\ntransformer-based deep learning framework.\n  Methods: We present a classification pipeline that employs the Swin\nTransformer, a hierarchical vision transformer model, on multi-shell dMRI data\nfor the classification of Alzheimer's disease and amyloid presence. Key metrics\nfrom DTI and NODDI were extracted and projected onto 2D planes to enable\ntransfer learning with ImageNet-pretrained models. To efficiently adapt the\ntransformer to limited labeled neuroimaging data, we integrated Low-Rank\nAdaptation. We assessed the framework on diagnostic group prediction\n(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)\nand amyloid status classification.\n  Results: The framework achieved competitive classification results within the\nscope of multi-shell dMRI-based features, with the best balanced accuracy of\n95.2% for distinguishing cognitively normal individuals from those with\nAlzheimer's disease dementia using NODDI metrics. For amyloid detection, it\nreached 77.2% balanced accuracy in distinguishing amyloid-positive mild\ncognitive impairment/Alzheimer's disease dementia subjects from\namyloid-negative cognitively normal subjects, and 67.9% for identifying\namyloid-positive individuals among cognitively normal subjects. Grad-CAM-based\nexplainability analysis identified clinically relevant brain regions, including\nthe parahippocampal gyrus and hippocampus, as key contributors to model\npredictions.\n  Conclusion: This study demonstrates the promise of diffusion MRI and\ntransformer-based architectures for early detection of Alzheimer's disease and\namyloid pathology, supporting biomarker-driven diagnostics in data-limited\nbiomedical settings.", "AI": {"tldr": "This paper utilizes a vision transformer-based deep learning framework integrating multi-shell diffusion MRI (dMRI) data to diagnose Alzheimer's disease and identify amyloid accumulation effectively.", "motivation": "The motivation of this research was to enable early detection of Alzheimer's disease and amyloid accumulation, leveraging the potential of microstructural neuroimaging data combined with advanced deep learning techniques, especially in data-limited scenarios.", "method": "The study employed the Swin Transformer on multi-shell dMRI data, extracted critical diffusion metrics, and projected them into 2D representations, allowing the use of ImageNet transfer learning. Additionally, Low-Rank Adaptation was used to optimize performance on neuroimaging data with limited labeling.", "result": "The proposed framework demonstrated a balanced accuracy of 95.2% for classifying cognitively normal individuals vs. Alzheimer's patients, 77.2% for amyloid detection in mixed groups, and 67.9% for amyloid detection among cognitively normal individuals. Grad-CAM identified clinically relevant brain areas as explanatory contributors to predictions.", "conclusion": "Diffusion MRI combined with transformer-based models holds significant potential for enhancing early Alzheimer's disease and amyloid pathology detection, underscoring its value in limited biomedical data environments."}}
{"id": "2507.09176", "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "categories": ["cs.RO", "cs.CL"], "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "This paper introduces a new targetless approach for calibrating multiple LiDARs, even with non-overlapping fields of view or imprecise initial setups. It achieves high accuracy and reliability without requiring manual annotation or special patterns.", "motivation": "The motivation behind this paper is to address the challenges of accurately calibrating multi-LiDAR systems, especially in situations where the sensors have non-overlapping fields of view or lack precise initial parameter estimates.", "method": "The authors propose a unified optimization framework that combines LiDAR bundle adjustment (LBA) with robust iterative refinement. This method constructs a reference point cloud map via continuous scanning paired with a sliding-window LBA and formulates the extrinsic calibration as a joint LBA optimization problem.", "result": "Experiments in both simulation (CARLA) and real-world environments show superior performance compared to existing methods, achieving an average translational error of 5 mm and a rotational error of 0.2\u00b0 even with significant initial error tolerances.", "conclusion": "The proposed framework provides precise, outlier-resistant calibration for multi-LiDAR systems without requiring manual intervention or specialized infrastructure, making it a valuable contribution to 3D map reconstruction systems."}}
{"id": "2507.09374", "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "categories": ["cs.AI", "I.2.6; I.2.10"], "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "AI": {"tldr": "EduFlow addresses scientific reasoning shortcomings in multimodal large language models by introducing end-to-end frameworks for educational reasoning, including data selection, MCTS-based reasoning, and self-correction mechanisms.", "motivation": "Current multimodal large language models struggle with scientific tasks requiring multi-step reasoning and reliability, making them insufficient for structured educational contexts.", "method": "EduFlow introduces frameworks like EduPRM for process-aware critique and iterative refinement and EduMCTS for search optimization leveraging self-reflection mechanisms and bootstrapping actions.", "result": "Experimental results show that EduFlow significantly improves the consistency and coherence of reasoning in scientific tasks.", "conclusion": "The EduFlow framework is effective in enhancing structured and interpretable reasoning in scientific contexts, with improved reliability and depth."}}
{"id": "2507.09068", "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "The paper explores the ambitious goal of Infinite Video Understanding (IVU), emphasizing the importance of continuous, seamless video comprehension over infinite durations.", "motivation": "To address limitations in current state-of-the-art video understanding technologies that struggle with computational, memory, and reasoning challenges for long-duration video content.", "method": "This is a position paper that outlines core challenges and key research directions, inspired by advancements in ultra-long video models, streaming architectures, and persistent memory systems.", "result": "No experimental results are presented as this is a position paper; it highlights research gaps and frameworks for tackling infinite video understanding.", "conclusion": "Framing IVU as a long-term research objective could stimulate innovation across AI domains, paving the way for new techniques in video comprehension and reasoning."}}
{"id": "2507.08839", "pdf": "https://arxiv.org/pdf/2507.08839", "abs": "https://arxiv.org/abs/2507.08839", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": "MICCAI 2025", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "AI": {"tldr": "The study introduces the Transferability Aware Transformer (TAT) to adapt Alzheimer's disease (AD) knowledge for improved diagnosis of Lewy Body Disease (LBD), addressing data scarcity and domain shifts.", "motivation": "To overcome challenges in Lewy Body Disease (LBD) diagnosis caused by data scarcity and domain shifts, leveraging the comparatively rich datasets of Alzheimer's disease (AD).", "method": "The proposed Transferability Aware Transformer (TAT) uses the attention mechanism to emphasize disease-transferable features and suppress domain-specific ones. Structural connectivity (SC) from structural MRI is utilized as input data.", "result": "Experimental results show that TAT effectively mitigates domain shift and enhances diagnostic accuracy, even with limited LBD data.", "conclusion": "This is the first study to explore domain adaptation from AD to LBD, offering a promising framework for diagnosing rare diseases under challenging conditions of data scarcity and domain variability."}}
{"id": "2507.09093", "pdf": "https://arxiv.org/pdf/2507.09093", "abs": "https://arxiv.org/abs/2507.09093", "authors": ["Aleksandar Armacki", "Dragana Bajovic", "Dusan Jakovetic", "Soummya Kar"], "title": "Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": "38 pages, 1 figure", "summary": "We study convergence in high-probability of SGD-type methods in non-convex\noptimization and the presence of heavy-tailed noise. To combat the heavy-tailed\nnoise, a general black-box nonlinear framework is considered, subsuming\nnonlinearities like sign, clipping, normalization and their smooth\ncounterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the\nrate $\\widetilde{\\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments\nand a symmetric probability density function (PDF). Crucially, N-SGD has\nexponentially decaying tails, matching the performance of linear SGD under\nlight-tailed noise. To handle non-symmetric noise, we propose two novel\nestimators, based on the idea of noise symmetrization. The first, dubbed\nSymmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any\nreference point is available at the start of training, while the second, dubbed\nMini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.\nCombined with the nonlinear framework, we get N-SGE and N-MSGE methods,\nrespectively, both achieving the same convergence rate and exponentially\ndecaying tails as N-SGD, while allowing for non-symmetric noise with unbounded\nmoments and PDF satisfying a mild technical condition, with N-MSGE additionally\nrequiring bounded noise moment of order $p \\in (1,2]$. Compared to works\nassuming noise with bounded $p$-th moment, our results: 1) are based on a novel\nsymmetrization approach; 2) provide a unified framework and relaxed moment\nconditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly\nbetter than existing works when $p < 2$, while the complexity of N-MSGE is\nclose to existing works. Compared to works assuming symmetric noise with\nunbounded moments, we: 1) provide a sharper analysis and improved rates; 2)\nfacilitate state-dependent symmetric noise; 3) extend the strong guarantees to\nnon-symmetric noise.", "AI": {"tldr": "This paper investigates convergence of SGD-type methods in non-convex optimization under heavy-tailed noise by introducing nonlinear SGD (N-SGD) and two novel estimators to address non-symmetric noise.", "motivation": "The authors aim to address the limitations of existing optimization techniques that are hampered by heavy-tailed and non-symmetric noise in stochastic gradients, which affect convergence rates and tail behaviors.", "method": "The paper introduces N-SGD, complemented by two novel noise-symmetrization techniques: Symmetrized Gradient Estimator (SGE) and Mini-batch SGE (MSGE). These approaches allow for improved handling of noise with unbounded moments.", "result": "N-SGD achieves $\nWidETilde{\\mathcal{O}}(t^{-1/2})$ convergence and exponentially decaying tails. With SGE and MSGE, the methods extend to handle non-symmetric noise while maintaining strong convergence and optimal or close-to-optimal oracle complexities.", "conclusion": "The proposed methods outperform prior approaches under relaxed noise assumptions, offering a unified framework, improved analysis, and better handling of both symmetric and non-symmetric noise with heavy tails."}}
{"id": "2507.10392", "pdf": "https://arxiv.org/pdf/2507.10392", "abs": "https://arxiv.org/abs/2507.10392", "authors": ["Runsheng Benson Guo", "Utkarsh Anand", "Khuzaima Daudjee", "Rathijit Sen"], "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters", "categories": ["cs.DC"], "comment": null, "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.", "AI": {"tldr": "The paper introduces Zorse, a system addressing challenges of training large language models on heterogeneous GPU clusters through improved pipeline and data parallelism.", "motivation": "The motivation is to efficiently utilize heterogeneous GPU clusters, overcoming difficulties in load balancing, memory optimization, and communication over diverse interconnects.", "method": "The method involves integrating pipeline and data parallelism in a flexible way, allowing asymmetric pipeline stages and mixed GPU types in the same group, supported by an automatic configuration planner.", "result": "Zorse achieves significant performance improvements compared to state-of-the-art systems in heterogeneous training scenarios.", "conclusion": "Efficient training on heterogeneous GPU clusters is feasible through Zorse, which unifies advanced parallelism strategies and an automated planner to optimize workloads."}}
{"id": "2507.09948", "pdf": "https://arxiv.org/pdf/2507.09948", "abs": "https://arxiv.org/abs/2507.09948", "authors": ["Zijian Ding", "Tung Nguyen", "Weikai Li", "Aditya Grover", "Yizhou Sun", "Jason Cong"], "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data", "categories": ["cs.LG", "cs.AR"], "comment": "9 pages. accepted to ICLAD'25", "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}", "AI": {"tldr": "This paper introduces Iceberg, a synthetic data augmentation methodology that enhances deep learning-based prediction models for High-Level Synthesis (HLS) hardware designs, addressing their generalization issues.", "motivation": "To improve the generalization abilities of deep learning models for hardware design prediction via synthetic data augmentation and pretraining.", "method": "The paper employs pretraining on LLM-generated programs and integrates an in-context architecture for meta-learning through weak label generation.", "result": "Iceberg boosts modeling accuracy by 86.4% in few-shot examples across six applications and achieves significantly better offline DSE performance in two test datasets.", "conclusion": "The proposed Iceberg framework substantially enhances generalizability, accuracy, and dataset adaptation, establishing itself as an effective tool for HLS prediction models."}}
{"id": "2507.09186", "pdf": "https://arxiv.org/pdf/2507.09186", "abs": "https://arxiv.org/abs/2507.09186", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "title": "OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research", "categories": ["cs.SE"], "comment": null, "summary": "We introduce OpenCAMS (Open-Source Connected and Automated Mobility\nCo-Simulation Platform), an open-source, synchronized, and extensible\nco-simulation framework that tightly couples three best-in-class simulation\ntools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support\nadvanced research in transportation safety, mobility, and cybersecurity by\ncombining the strengths of each simulation domain. Specifically, SUMO provides\nlarge-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D\nperception, vehicle dynamics, and control simulation; and OMNeT++ enables\nmodular, event-driven network communication, such as cellular\nvehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,\nbidirectional coupling architecture that ensures coherent simulation\nprogression across traffic, perception, and communication domains while\npreserving modularity and reproducibility. For example, CARLA can simulate and\nrender a subset of vehicles that require detailed sensor emulation and control\nlogic; SUMO orchestrates network-wide traffic flow, vehicle routing, and\ntraffic signal management; and OMNeT++ dynamically maps communication nodes to\nboth mobile entities (e.g., vehicles) and static entities (e.g., roadside\nunits) to enable C-V2X communication. While these three simulators form the\nfoundational core of OpenCAMS, the platform is designed to be expandable and\nfuture-proof, allowing additional simulators to be integrated on top of this\ncore without requiring fundamental changes to the system architecture. The\nOpenCAMS platform is fully open-source and publicly available through its\nGitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,\nproviding the research community with an accessible, flexible, and\ncollaborative environment for advancing next-generation intelligent\ntransportation systems.", "AI": {"tldr": "OpenCAMS is an open-source co-simulation framework integrating SUMO, CARLA, and OMNeT++ for transportation research, focusing on safety, mobility, and cybersecurity.", "motivation": "The paper addresses the need for a comprehensive simulation platform that integrates traffic modeling, 3D perception, and network communication to advance research in intelligent transportation systems.", "method": "The authors constructed a time-synchronized, bidirectional coupling architecture between SUMO, CARLA, and OMNeT++ to ensure modularity and coherent simulation progression across different domains.", "result": "OpenCAMS successfully combines large-scale traffic modeling, high-fidelity 3D vehicle simulation, and event-driven network communication, demonstrating seamless co-simulation across these domains.", "conclusion": "The platform is open-source, expandable, and future-proof, providing a flexible tool for collaborative transportation research and innovation."}}
{"id": "2507.09075", "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "This paper introduces OpenCodeReasoning-II, a large-scale dataset featuring 2.5M question-solution-critique triples for code reasoning, alongside fine-tuned models achieving superior code generation and critique performance.", "motivation": "Large-scale, high-quality datasets are crucial for progress in distillation for code generation and critique, especially given advancements in reasoning-based Large Language Models.", "method": "The study employs a two-stage supervised fine-tuning approach: first for code generation and then for joint training for both code generation and critique, using the OpenCodeReasoning-II dataset to train Qwen2.5-Instruct models.", "result": "Fine-tuned models achieve performance in code generation exceeding or matching prior open-weight models. Integration of code generation and critique models boosts competitive coding performance.", "conclusion": "The proposed dataset and methods significantly advance code reasoning and evaluation capabilities, supporting robust development and assessment of LLMs in programming tasks."}}
{"id": "2507.10145", "pdf": "https://arxiv.org/pdf/2507.10145", "abs": "https://arxiv.org/abs/2507.10145", "authors": ["Ryohei Fukuma", "Yoshinobu Kawahara", "Okito Yamashita", "Kei Majima", "Haruhiko Kishima", "Takufumi Yanagisawa"], "title": "Intrinsic frequency distribution characterises neural dynamics", "categories": ["eess.SP", "q-bio.NC"], "comment": null, "summary": "Decomposing multivariate time series with certain basic dynamics is crucial\nfor understanding, predicting and controlling nonlinear spatiotemporally\ndynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method\nfor decomposing nonlinear spatiotemporal dynamics into several basic dynamics\n(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,\nunlike Fourier transform-based methods, which are used to decompose a\nsingle-channel signal into the amplitudes of sinusoidal waves with discrete\nfrequencies at a regular interval, DMD can derive the intrinsic frequencies of\na multichannel signal on the basis of the available data; furthermore, it can\ncapture nonstationary components such as alternations between states with\ndifferent intrinsic frequencies. Here, we propose the use of the distribution\nof intrinsic frequencies derived from DMDs (DM frequencies) to characterise\nneural activities. The distributions of DM frequencies in the\nelectroencephalograms of healthy subjects and patients with dementia or\nParkinson's disease in a resting state were evaluated. By using the\ndistributions, these patients were distinguished from healthy subjects with\nsignificantly greater accuracy than when using amplitude spectra derived by\ndiscrete Fourier transform. This finding suggests that the distribution of DM\nfrequencies exhibits distinct behaviour from amplitude spectra, and therefore,\nthe distribution may serve as a new biomarker by characterising the nonlinear\nspatiotemporal dynamics of electrophysiological signals.", "AI": {"tldr": "The paper introduces a novel application of dynamic mode decomposition (DMD) to characterize neural activities using distributions of intrinsic frequencies.", "motivation": "Understanding and distinguishing nonlinear spatiotemporal dynamics in multivariate time series, such as brain activity, is essential for medical and scientific applications.", "method": "Dynamic mode decomposition (DMD) was applied to electroencephalograms (EEGs) of healthy subjects and patients, deriving the distribution of intrinsic frequencies and comparing them to discrete Fourier transform (DFT) amplitude spectra.", "result": "The distribution of intrinsic frequencies obtained through DMD distinguished patients with dementia and Parkinson's from healthy subjects with higher accuracy than traditional DFT methods.", "conclusion": "The distribution of DMD frequencies is a promising biomarker for neural activity characterization, offering advantages over traditional Fourier-based approaches."}}
{"id": "2507.09309", "pdf": "https://arxiv.org/pdf/2507.09309", "abs": "https://arxiv.org/abs/2507.09309", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "categories": ["cs.RO"], "comment": null, "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "AI": {"tldr": "This paper introduces HZ-MP, a novel motion planner utilizing hybrid zonotopes and ellipsotope heuristics for solving challenging path planning in nonconvex free spaces.", "motivation": "Path planning in nonconvex free spaces is difficult due to NP-hard complexity when modeled as mixed-integer linear programs (MILPs). Existing planners struggle in narrow gaps or boxed-goal environments.", "method": "The approach involves space decomposition paired with low-dimensional face sampling informed by ellipsotope heuristics, aiming to focus exploration on promising regions while reducing excessive and unreachable sampling.", "result": "HZ-MP is proven probabilistically complete, asymptotically optimal, converges to near-optimal trajectories in finite time, and scales effectively to cluttered, high-dimensional scenarios.", "conclusion": "HZ-MP improves upon informed motion planners by achieving efficient and focused exploration in complex nonconvex spaces, offering better scalability and optimality guarantees."}}
{"id": "2507.09389", "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "categories": ["cs.AI", "cs.CY", "cs.IR"], "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "AI": {"tldr": "The paper evaluates how structure and complexity of knowledge impact large language models' performance in querying a triplestore within neurosymbolic AI systems.", "motivation": "To investigate the integration of explainability, interpretability, and adaptability in neurosymbolic AI systems to optimize their performance.", "method": "The research systematically evaluates the effects of different conceptualizations and representations of knowledge in an AI agent's querying process using a triplestore.", "result": "The findings reveal that both the structure and complexity of knowledge representations influence the effectiveness of AI agents.", "conclusion": "These insights could guide the design of future Agentic Retrieval-Augmented Generation systems with improved transferability and interpretability."}}
{"id": "2507.09071", "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "The study introduces BlindSight, a method to optimize Vision-Language Models (VLMs) by leveraging sparse attention patterns, resulting in significant computation reduction while maintaining accuracy.", "motivation": "The paper aims to address inefficiencies caused by the extended prompt length and the high computational complexity of attention in Vision-Language Models (VLMs).", "method": "The authors analyzed attention patterns in VLMs to identify sparsity in cross-image attention and proposed a training-free attention sparsity mask, BlindSight, based on categorized sparse patterns.", "result": "BlindSight achieved a 32%-41% reduction in FLOPs on average with an accuracy variation of -2% to +2% compared to the original models in benchmarks.", "conclusion": "BlindSight effectively reduces computational requirements for VLMs without significantly impacting accuracy, making it suitable for optimizing multi-image understanding tasks."}}
{"id": "2507.08841", "pdf": "https://arxiv.org/pdf/2507.08841", "abs": "https://arxiv.org/abs/2507.08841", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "AI": {"tldr": "The paper introduces a novel zero-shot NAS method called Weighted Response Correlation (WRCor) for efficient architecture evaluation without the need for training, achieving superior performance compared to prior methods.", "motivation": "Neural Architecture Search (NAS) often requires significant computational resources and time due to the need to train numerous architectures from scratch. The goal is to develop a zero-shot, training-free method that is efficient, stable, and generalizable for architecture evaluation.", "method": "The paper proposes the Weighted Response Correlation (WRCor) proxy, which uses correlation coefficient matrices of responses across different input samples to determine proxy scores for architectures, reflecting their expressivity and generalizability.", "result": "WRCor demonstrated stronger efficiency as an estimation strategy compared to existing methods, and when applied in architecture searches across varying search spaces, it outperformed many existing NAS algorithms. It discovered architectures with high performance, such as achieving a 22.1% test error on ImageNet-1k within just 4 GPU hours.", "conclusion": "The research establishes WRCor as an innovative and practical zero-shot NAS proxy that significantly reduces the computational demands of architecture search while maintaining high effectiveness and generality."}}
{"id": "2507.09103", "pdf": "https://arxiv.org/pdf/2507.09103", "abs": "https://arxiv.org/abs/2507.09103", "authors": ["Gianluigi Silvestri", "Luca Ambrogioni"], "title": "CoVAE: Consistency Training of Variational Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Current state-of-the-art generative approaches frequently rely on a two-stage\ntraining procedure, where an autoencoder (often a VAE) first performs\ndimensionality reduction, followed by training a generative model on the\nlearned latent space. While effective, this introduces computational overhead\nand increased sampling times. We challenge this paradigm by proposing\nConsistency Training of Variational AutoEncoders (CoVAE), a novel single-stage\ngenerative autoencoding framework that adopts techniques from consistency\nmodels to train a VAE architecture. The CoVAE encoder learns a progressive\nseries of latent representations with increasing encoding noise levels,\nmirroring the forward processes of diffusion and flow matching models. This\nsequence of representations is regulated by a time dependent $\\beta$ parameter\nthat scales the KL loss. The decoder is trained using a consistency loss with\nvariational regularization, which reduces to a conventional VAE loss at the\nearliest latent time. We show that CoVAE can generate high-quality samples in\none or few steps without the use of a learned prior, significantly\noutperforming equivalent VAEs and other single-stage VAEs methods. Our approach\nprovides a unified framework for autoencoding and diffusion-style generative\nmodeling and provides a viable route for one-step generative high-performance\nautoencoding. Our code is publicly available at\nhttps://github.com/gisilvs/covae.", "AI": {"tldr": "This paper introduces CoVAE, a single-stage generative autoencoding framework that combines VAEs with consistency training, simplifying the generative modeling process while enhancing sample quality.", "motivation": "To address the inefficiencies of the two-stage generative modeling approach, such as computational overhead and increased sampling times, by creating a single-stage model with improved performance.", "method": "CoVAE integrates VAE architecture with techniques from consistency models. It uses a progressive series of noisy latent representations controlled by a time-dependent parameter for KL loss scaling and trains the decoder with a combination of consistency and variational regularization losses.", "result": "CoVAE generates high-quality samples in a few steps, significantly outperforming traditional VAEs and other single-stage methods without requiring a learned prior.", "conclusion": "CoVAE serves as a unified, efficient, and high-performing framework for generative modeling, providing a new pathway for one-step generative high-quality autoencoding."}}
{"id": "2507.10413", "pdf": "https://arxiv.org/pdf/2507.10413", "abs": "https://arxiv.org/abs/2507.10413", "authors": ["Gabriel Rocha"], "title": "Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?", "categories": ["cs.DC", "cs.CC", "cs.IT", "cs.LO", "math.IT"], "comment": "10 pages", "summary": "The consensus problem, briefly stated, consists of having processes in an\nasynchronous distributed system agree on a value. It is widely known that the\nconsensus problem does not have a deterministic solution that ensures both\ntermination and consistency, if there is at least one faulty process in the\nsystem. This result, known as the FLP impossibility theorem, led to several\ngeneralizations and developments in theoretical distributed computing. This\npaper argues that the FLP impossibility theorem holds even under a generalized\ndefinition of computation through oracles. Furthermore, using a theoretical\nmachinery from complex systems, this paper also posits that inconsistency may\nbe an emergent feature of consensus over distributed systems by examining how a\nsystem transitions phases. Under the same complex systems framework, this paper\nexamines paraconsistent logics, arguing that while inconsistency is not an\nemergent feature for these logics, triviality may be. Lastly, some attention is\ngiven to the possibility of developing consensus algorithms capable of\nparaconsistent reasoning.", "AI": {"tldr": "This paper revisits the FLP impossibility theorem and explores its persistence under generalized computational frameworks, alongside phase transitions in distributed systems using complex systems theory.", "motivation": "The paper aims to investigate whether the FLP impossibility theorem holds under broader computational models and analyze emergent features, such as inconsistency, in distributed systems using a theoretical framework.", "method": "The study adopts complex systems theory and theoretical analysis of phase transitions in distributed systems coupled with paraconsistent logics.", "result": "It demonstrates that inconsistency might emerge during phase transitions in consensus processes. However, while inconsistency is not inherent in paraconsistent logics, triviality could be an emergent concern.", "conclusion": "The FLP impossibility theorem remains valid under generalized computation settings. Insights into phase transitions in distributed systems reveal limitations, and exploration of paraconsistent reasoning opens avenues for future consensus algorithm design."}}
{"id": "2507.09965", "pdf": "https://arxiv.org/pdf/2507.09965", "abs": "https://arxiv.org/abs/2507.09965", "authors": ["Weiyu Chen", "Chengjie Liu", "Wenhao Huang", "Jinyang Lyu", "Mingqian Yang", "Yuan Du", "Li Du", "Jun Yang"], "title": "AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design", "categories": ["cs.MA", "cs.AR"], "comment": "accepted by ISEDA 2025", "summary": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation.", "AI": {"tldr": "AnalogTester leverages large language models (LLMs) to automate testbench generation for analog circuits, addressing a key manual bottleneck in circuit design.", "motivation": "To overcome the time-intensive and manual nature of testbench construction, especially in replicating designs from academic papers, and to advance automation in analog circuit design.", "method": "Utilization of an LLM-powered pipeline consisting of domain knowledge integration, paper information extraction, simulation scheme synthesis, and testbench code generation.", "result": "Automatic testbench generation was successfully demonstrated for operational amplifiers (op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs).", "conclusion": "AnalogTester streamlines automation in analog circuit design and creates foundational datasets for LLM specialization in this domain."}}
{"id": "2507.09199", "pdf": "https://arxiv.org/pdf/2507.09199", "abs": "https://arxiv.org/abs/2507.09199", "authors": ["Huihui Huang", "Ratnadira Widyasari", "Ting Zhang", "Ivana Clairine Irsan", "Jieke Shi", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "Hong Jin Kang", "David Lo"], "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval", "categories": ["cs.SE"], "comment": null, "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\n  Inspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.91%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.", "AI": {"tldr": "The paper addresses the limitations of existing issue-commit linking methods by proposing a more realistic evaluation setting called Realistic Distribution Setting (RDS) and introducing a method, EasyLink, which significantly outperforms state-of-the-art techniques.", "motivation": "Existing issue-commit linking evaluations fail to account for challenges posed by repositories with numerous irrelevant commits, leading to unrealistic performance results.", "method": "The paper introduces the Realistic Distribution Setting (RDS) to develop a more realistic dataset and proposes EasyLink, leveraging a vector database and large language model for issue-commit linking.", "result": "The evaluation demonstrates that state-of-the-art methods perform poorly under RDS, while EasyLink achieves a Precision@1 of 75.91%, significantly higher than existing methods.", "conclusion": "EasyLink provides a robust improvement in issue-commit linking, emphasizing the need for realistic evaluation methodologies. The paper offers practical research guidance to enhance this domain further."}}
{"id": "2507.09076", "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "This paper presents a Dynamic Parameter Memory (DPM) mechanism to address the limitations of speech large language models (SLLMs) in processing long audio content for speech emotion recognition. Using a novel encoding approach, it achieves state-of-the-art results on the IEMOCAP dataset.", "motivation": "Speech large language models (SLLMs) struggle with processing high-frame-rate speech data, leading to limitations in handling long audio sequences for emotion recognition due to context window constraints.", "method": "The authors propose a Dynamic Parameter Memory (DPM) mechanism that encodes sentence-level information and emotions into a temporary LoRA module during inference. This enables the efficient handling of unlimited-length audio with limited context windows.", "result": "The proposed DPM mechanism, when integrated with an emotion SLLM backbone, improved emotion recognition in long audio sequences, achieving state-of-the-art results on the IEMOCAP dataset.", "conclusion": "Dynamic Parameter Memory (DPM) enhances the capability of speech large language models to process and understand emotions in extended conversational audio, addressing the inherent limitations of SLLMs and advancing emotion recognition performance."}}
{"id": "2507.09340", "pdf": "https://arxiv.org/pdf/2507.09340", "abs": "https://arxiv.org/abs/2507.09340", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "AI": {"tldr": "This paper introduces RMRP and RPATR to tackle challenges in autonomous navigation for UAVs and UGVs. The approach emphasizes efficient mapping and safe navigation.", "motivation": "Current methods for autonomous navigation in mobile robots are computationally demanding, ineffective in large-scale complex environments, and lack perception-aware strategies.", "method": "The authors propose Random Mapping and Random Projection (RMRP) for lightweight mapping, alongside the RPATR framework for navigation. UAV methods unify grid and ESDF maps, while UGV methods focus on terrain characterization.", "result": "RMRP and RPATR significantly improved mapping in terms of memory, execution time, and accuracy, enabling efficient and safe trajectory planning for UAVs and UGVs.", "conclusion": "The proposed method offers a robust, scalable solution for autonomous navigation, with theoretical guarantees and practical validations. Open-source code will enhance community engagement."}}
{"id": "2507.09407", "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "categories": ["cs.AI", "cs.CR", "cs.GT"], "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "AI": {"tldr": "The paper introduces LLM-Stackelberg games using large language models (LLMs) to model decision-making interactions between a leader and a follower, emphasizing probabilistic reasoning, adaptive strategies, and asymmetric information.", "motivation": "The motivation is to extend classical Stackelberg game theory by incorporating LLMs to capture bounded rationality and adapt to modern contexts like cybersecurity and misinformation.", "method": "Agents interact using structured prompts and LLM-generated probabilistic behaviors, with equilibrium concepts established via cognitive reasoning and epistemic uncertainty modeling.", "result": "The framework successfully modeled spearphishing deception games and demonstrated potential for broader applications in adversarial and decision-making domains.", "conclusion": "LLM-Stackelberg games serve as an innovative tool to study modern strategic interactions that involve complex reasoning, asymmetric information, and LLM-mediated behaviors."}}
{"id": "2507.09081", "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "This paper reviews advancements in remote sensing inversion techniques, from traditional physical models to modern machine learning and foundation models, identifying strengths and ongoing challenges.", "motivation": "Remote sensing inversion is critical for estimating surface variables to support applications in ecosystem monitoring, carbon accounting, and land management.", "method": "The authors systematically review methodological evolutions from physical models, through machine learning, to foundation models, discussing pretraining, multi-modal integration, and cross-task adaptation.", "result": "They compare modeling paradigms and analyze their assumptions, scenarios, and limitations while identifying challenges like physical interpretability and uncertainty quantification.", "conclusion": "The paper outlines a vision for next-gen foundation models with emphasis on unified modeling, cross-domain generalization, and improved interpretability in remote sensing inversion."}}
{"id": "2507.08842", "pdf": "https://arxiv.org/pdf/2507.08842", "abs": "https://arxiv.org/abs/2507.08842", "authors": ["Zhufeng Lu", "Chentao Jia", "Ming Hu", "Xiaofei Xie", "Mingsong Chen"], "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by ACM SIGKDD 2025", "summary": "As a promising privacy-aware collaborative model training paradigm, Federated\nLearning (FL) is becoming popular in the design of distributed recommender\nsystems. However, Federated Recommender Systems (FedRecs) greatly suffer from\ntwo major problems: i) extremely high communication overhead due to massive\nitem embeddings involved in recommendation systems, and ii) intolerably low\ntraining efficiency caused by the entanglement of both heterogeneous network\nenvironments and client devices. Although existing methods attempt to employ\nvarious compression techniques to reduce communication overhead, due to the\nparameter errors introduced by model compression, they inevitably suffer from\nmodel performance degradation. To simultaneously address the above problems,\nthis paper presents a communication-efficient FedRec framework named FedRAS,\nwhich adopts an action-sharing strategy to cluster the gradients of item\nembedding into a specific number of model updating actions for communication\nrather than directly compressing the item embeddings. In this way, the cloud\nserver can use the limited actions from clients to update all the items. Since\ngradient values are significantly smaller than item embeddings, constraining\nthe directions of gradients (i.e., the action space) introduces smaller errors\ncompared to compressing the entire item embedding matrix into a reduced space.\nTo accommodate heterogeneous devices and network environments, FedRAS\nincorporates an adaptive clustering mechanism that dynamically adjusts the\nnumber of actions. Comprehensive experiments on well-known datasets demonstrate\nthat FedRAS can reduce the size of communication payloads by up to 96.88%,\nwhile not sacrificing recommendation performance within various heterogeneous\nscenarios. We have open-sourced FedRAS at\nhttps://github.com/mastlab-T3S/FedRAS.", "AI": {"tldr": "FedRAS is a novel Federated Recommender System framework that reduces communication overhead by clustering item embedding gradients instead of compressing embeddings, maintaining performance without introducing significant errors.", "motivation": "To address the communication overhead and low training efficiency in Federated Recommender Systems caused by large item embeddings and heterogeneous client environments.", "method": "The method involves an action-sharing strategy where gradients of item embeddings are clustered into updating actions for communication, paired with an adaptive clustering mechanism to cater to device and network variations.", "result": "FedRAS achieves communication payload size reduction by up to 96.88% without sacrificing recommendation performance, verified through experiments on well-known datasets.", "conclusion": "FedRAS provides a communication-efficient solution for Federated Recommender Systems, addressing both communication overhead and training inefficiency while maintaining model performance. Its implementation is publicly available for further research."}}
{"id": "2507.09128", "pdf": "https://arxiv.org/pdf/2507.09128", "abs": "https://arxiv.org/abs/2507.09128", "authors": ["Ronak Mehta", "Zaid Harchaoui"], "title": "A Generalization Theory for Zero-Shot Prediction", "categories": ["stat.ML", "cs.LG"], "comment": "Published at ICML '25 (Oral)", "summary": "A modern paradigm for generalization in machine learning and AI consists of\npre-training a task-agnostic foundation model, generally obtained using\nself-supervised and multimodal contrastive learning. The resulting\nrepresentations can be used for prediction on a downstream task for which no\nlabeled data is available. We present a theoretical framework to better\nunderstand this approach, called zero-shot prediction. We identify the target\nquantities that zero-shot prediction aims to learn, or learns in passing, and\nthe key conditional independence relationships that enable its generalization\nability.", "AI": {"tldr": "The paper theoretically analyzes zero-shot prediction using pre-trained models for downstream tasks.", "motivation": "The motivation is to theoretically understand how zero-shot prediction works and its generalization ability.", "method": "A theoretical framework is developed to identify target quantities and conditional independence relationships in zero-shot prediction.", "result": "Key insights were provided on the target quantities and conditions enabling zero-shot generalization.", "conclusion": "The paper offers a foundational understanding of zero-shot prediction that may inform advancements in its application and theory."}}
{"id": "2507.10430", "pdf": "https://arxiv.org/pdf/2507.10430", "abs": "https://arxiv.org/abs/2507.10430", "authors": ["Ji Liu", "Beichen Ma", "Yang Zhou", "Jingbo Zhou", "Ruoming Jin", "Dejing Dou", "Huaiyu Dai", "Haixun Wang", "Patrick Valduriez"], "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "29 pages, to appear in ACM Transactions on Knowledge Discovery from\n  Data (TKDD)", "summary": "Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller).", "AI": {"tldr": "The paper introduces FedDHAD, a federated learning framework with two methods\u2014FedDH and FedAD\u2014to tackle challenges of data heterogeneity and device constraints, improving accuracy, efficiency, and reducing computation cost.", "motivation": "Federated learning suffers from performance drops due to data heterogeneity among edge devices and limited capabilities of these devices, leading to slower model convergence and reduced accuracy.", "method": "FedDH dynamically adjusts local model aggregation weights based on data heterogeneity (non-IID), while FedAD performs adaptive neuron operations to optimize accuracy and efficiency for heterogeneous devices.", "result": "FedDHAD improves accuracy by up to 6.7%, speeds up training by up to 2.02 times, and cuts computation costs by up to 15% compared to state-of-the-art methods.", "conclusion": "FedDHAD proves effective in addressing federated learning challenges, offering higher accuracy, faster convergence, and lower computation costs, making it an efficient framework for edge-device environments."}}
{"id": "2507.10338", "pdf": "https://arxiv.org/pdf/2507.10338", "abs": "https://arxiv.org/abs/2507.10338", "authors": ["Enyuan Tian", "Yiwei Ci", "Qiusong Yang", "Yufeng Li", "Zhichao Lyu"], "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction", "categories": ["cs.SE", "cs.AR", "cs.LO"], "comment": "7 pages, 3 figures", "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.", "AI": {"tldr": "The paper introduces AssertCoder, a framework for automatically generating hardware verification assertions from multimodal specifications, enhancing functional correctness and error detection over existing methods.", "motivation": "Writing SystemVerilog Assertions (SVAs) for hardware verification is labor-intensive, error-prone, and inefficient. A solution is needed to automate and improve this process.", "method": "AssertCoder uses a custom pipeline with modality-sensitive preprocessing to handle diverse input formats. It employs semantic analyzers for structured data extraction, CoT prompting for assertion synthesis, and a mutation-based evaluation for refinement.", "result": "AssertCoder showed improved results in hardware verification, achieving an 8.4% increase in functional correctness and a 5.8% gain in mutation detection compared to state-of-the-art approaches, based on tests across three real-world RTL designs.", "conclusion": "AssertCoder provides a significant advancement in hardware design verification by improving the automation and quality of assertion generation, offering a more effective alternative to manual or other automated methods."}}
{"id": "2507.09220", "pdf": "https://arxiv.org/pdf/2507.09220", "abs": "https://arxiv.org/abs/2507.09220", "authors": ["Syed Tauhid Ullah Shah", "Mohammad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "title": "Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation", "categories": ["cs.SE"], "comment": null, "summary": "Artificial Intelligence (AI) tools for automating design artifact generation\nare increasingly used in Requirements Engineering (RE) to transform textual\nrequirements into structured diagrams and models. While these AI tools,\nparticularly those based on Natural Language Processing (NLP), promise to\nimprove efficiency, their adoption remains limited in regulated industries\nwhere transparency and traceability are essential. In this paper, we\ninvestigate the explainability gap in AI-driven design artifact generation\nthrough semi-structured interviews with ten practitioners from safety-critical\nindustries. We examine how current AI-based tools are integrated into workflows\nand the challenges arising from their lack of explainability. We also explore\nmitigation strategies, their impact on project outcomes, and features needed to\nimprove usability. Our findings reveal that non-explainable AI outputs\nnecessitate extensive manual validation, reduce stakeholder trust, struggle to\nhandle domain-specific terminology, disrupt team collaboration, and introduce\nregulatory compliance risks, often negating the anticipated efficiency\nbenefits. To address these issues, we identify key improvements, including\nsource tracing, providing clear justifications for tool-generated decisions,\nsupporting domain-specific adaptation, and enabling compliance validation. This\nstudy outlines a practical roadmap for improving the transparency, reliability,\nand applicability of AI tools in requirements engineering workflows,\nparticularly in regulated and safety-critical environments where explainability\nis crucial for adoption and certification.", "AI": {"tldr": "The paper examines challenges posed by non-explainable AI tools in requirements engineering, particularly in safety-critical industries, and proposes practical improvements to enhance transparency, usability, and compliance.", "motivation": "The paper aims to address the limited adoption of AI tools in regulated industries due to challenges related to explainability, transparency, and traceability.", "method": "Through semi-structured interviews with ten practitioners from safety-critical industries, the authors studied workflow integrations and challenges related to AI's lack of explainability.", "result": "Findings show that non-explainable AI tools lead to reduced trust, manual validation, difficulties with domain-specific terminology, disrupted collaboration, and compliance risks, undermining efficiency.", "conclusion": "Key improvements identified include source tracing, decision justification, domain-specific support, and compliance validation, offering a roadmap for improving adoption in regulated industries."}}
{"id": "2507.09104", "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "The paper introduces CompassJudger-2, a new generalist judge model for evaluating large language models, addressing the limitations of current evaluation methods.", "motivation": "Current judge models for assessing large language models are narrow in specialization and lack robustness, which hinders their effectiveness in comprehensive evaluations.", "method": "The authors employed a task-driven, multi-domain data curation strategy supervised with verifiable rewards and utilized rejection sampling for intrinsic critical reasoning. They also developed a margin policy gradient loss to refine the learning objective.", "result": "CompassJudger-2 showed superior judgment accuracy across multiple benchmarks and performed competitively with significantly larger models, such as DeepSeek-V3 and Qwen3-235B-A22B.", "conclusion": "The research advances scalable and robust LLM judgment while proposing new performance benchmarks like JudgerBenchV2 to standardize model evaluation."}}
{"id": "2507.09344", "pdf": "https://arxiv.org/pdf/2507.09344", "abs": "https://arxiv.org/abs/2507.09344", "authors": ["Daniel Engelsman", "Itzik Klein"], "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "14 Pages, 16 Figures, 9 Tables", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "AI": {"tldr": "This paper introduces a novel Controlled Zero-Velocity Update (C-ZUPT) approach to enhance navigation and control in aerial systems by mitigating inertial drift and improving energy efficiency.", "motivation": "To address the challenges of drift-prone state estimation in environments where satellite-based and camera systems are unavailable, and inertial sensors suffer from noise and biases.", "method": "The paper presents C-ZUPT, which leverages quasi-static equilibria to identify uncertainty thresholds and provide velocity updates for aerial systems, without depending on surface contact.", "result": "The C-ZUPT method reduced inertial drift, decreased control effort, mitigated filter divergence, and improved energy efficiency, all of which extend the sustained flight capability of aerial systems.", "conclusion": "C-ZUPT offers a significant improvement in drift resilience and navigation stability, posing a valuable advancement for autonomous aerial systems operating in resource-constrained environments."}}
{"id": "2507.09495", "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "AI": {"tldr": "The paper proposes using generative AI to shift multi-agent reinforcement learning (RL) from reactive to proactive intelligence, addressing challenges like joint action spaces, non-stationary environments, and partial observability.", "motivation": "Current multi-agent RL methods are limited by reactive mechanisms, failing to perform effectively in complex, novel scenarios. There is a need to overcome coordination and adaptability barriers.", "method": "The authors propose agents designed as generative models that predict environmental evolution, anticipate interactions, and enable strategic reasoning, leveraging generative AI's pattern recognition and decision-making abilities.", "result": "Generative-RL agents are anticipated to achieve seamless coordination, enhanced communication, and dynamic adaptation to evolving scenarios.", "conclusion": "This paradigm shift to proactive intelligence promises breakthroughs in distributed intelligence for autonomous systems, robotics, and human-AI collaboration, addressing coordination challenges beyond reactive approaches."}}
{"id": "2507.09082", "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "This paper explores extracting optical flow using frozen generative video models prompted by perturbations, achieving state-of-the-art results without fine-tuning.", "motivation": "The scarcity of flow labels and challenges with synthetic datasets motivate exploring zero-shot methods using generative models instead of fine-tuning.", "method": "The method, KL-tracing, introduces perturbations into the first frame and tracks their propagation using generative video models with specific properties, leveraging the LRAS architecture.", "result": "KL-tracing outperforms state-of-the-art models in real-world and synthetic datasets, showing significant improvements in endpoint error.", "conclusion": "Counterfactual prompting of generative video models offers a scalable, effective route for extracting optical flow, avoiding supervised or photometric-loss-dependent techniques."}}
{"id": "2507.08843", "pdf": "https://arxiv.org/pdf/2507.08843", "abs": "https://arxiv.org/abs/2507.08843", "authors": ["Arpita Soni", "Sahil Tripathi", "Gautam Siddharth Kashyap", "Manaswi Kulahara", "Mohammad Anas Azeez", "Zohaib Hasan Siddiqui", "Nipun Joshi", "Jiechao Gao"], "title": "Can We Predict Your Next Move Without Breaking Your Privacy?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in the 17th International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025\n  in Ontario, Canada", "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.", "AI": {"tldr": "The paper introduces FLLL3M, a framework for privacy-preserving Next-Location Prediction using Federated Learning and Large Language Models, achieving high accuracy while reducing resource requirements.", "motivation": "To develop an effective framework for accurate Next-Location Prediction without compromising user privacy and while using fewer computational resources.", "method": "FLLL3M uses Federated Learning to retain user data locally and applies a tailored outer product mechanism to leverage Large Language Models.", "result": "The framework demonstrates state-of-the-art accuracy on multiple datasets (e.g., Gowalla, WeePlace, Brightkite, FourSquare) while reducing model parameters by up to 45.6% and memory usage by 52.7%.", "conclusion": "FLLL3M successfully achieves efficient and privacy-preserving Next-Location Prediction with significant improvements in accuracy and resource optimization."}}
{"id": "2507.09148", "pdf": "https://arxiv.org/pdf/2507.09148", "abs": "https://arxiv.org/abs/2507.09148", "authors": ["Alberto Del Pia", "Dekun Zhou"], "title": "A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": "29 pages, 2 figures", "summary": "Sparse Principal Component Analysis (SPCA) is a fundamental technique for\ndimensionality reduction, and is NP-hard. In this paper, we introduce a\nrandomized approximation algorithm for SPCA, which is based on the basic SDP\nrelaxation. Our algorithm has an approximation ratio of at most the sparsity\nconstant with high probability, if called enough times. Under a technical\nassumption, which is consistently satisfied in our numerical tests, the average\napproximation ratio is also bounded by $\\mathcal{O}(\\log{d})$, where $d$ is the\nnumber of features. We show that this technical assumption is satisfied if the\nSDP solution is low-rank, or has exponentially decaying eigenvalues. We then\npresent a broad class of instances for which this technical assumption holds.\nWe also demonstrate that in a covariance model, which generalizes the spiked\nWishart model, our proposed algorithm achieves a near-optimal approximation\nratio. We demonstrate the efficacy of our algorithm through numerical results\non real-world datasets.", "AI": {"tldr": "The paper introduces a randomized algorithm for Sparse Principal Component Analysis (SPCA) based on SDP relaxation, achieving a bounded approximation ratio under specific assumptions, which is validated numerically.", "motivation": "SPCA is a vital but computationally hard technique for dimensionality reduction, requiring efficient approximation methods.", "method": "A randomized approximation algorithm is developed using the basic SDP relaxation. The algorithm's performance is analyzed under specific assumptions regarding SDP solutions.", "result": "The algorithm achieves an approximation ratio related to sparsity with high probability and performs near-optimally in a generalized spiked Wishart model. Validation is provided using real-world datasets.", "conclusion": "The proposed SPCA algorithm is effective, with bounded average approximation ratios under favorable conditions and practical efficacy demonstrated on real data."}}
{"id": "2507.08846", "pdf": "https://arxiv.org/pdf/2507.08846", "abs": "https://arxiv.org/abs/2507.08846", "authors": ["Serdar Metin"], "title": "Precomputed Dominant Resource Fairness", "categories": ["cs.GT", "cs.DC", "cs.DS"], "comment": "9 pages", "summary": "Although resource allocation is a well studied problem in computer science,\nuntil the prevalence of distributed systems, such as computing clouds and data\ncentres, the question had been addressed predominantly for single resource type\nscenarios. At the beginning of the last decade, with the introuction of\nDominant Resource Fairness, the studies of the resource allocation problem has\nfinally extended to the multiple resource type scenarios. Dominant Resource\nFairness is a solution, addressing the problem of fair allocation of multiple\nresource types, among users with heterogeneous demands. Based on Max-min\nFairness, which is a well established algorithm in the literature for\nallocating resources in the single resource type scenarios, Dominant Resource\nFairness generalises the scheme to the multiple resource case. It has a number\nof desirable properties that makes it preferable over alternatives, such as\nSharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,\nand as such, it is widely adopted in distributed systems. In the present study,\nwe revisit the original study, and analyse the structure of the algorithm in\ncloser view, to come up with an alternative algorithm, which approximates the\nDominant Resource Fairness allocation in fewer steps. We name the new algorithm\nPrecomputed Dominant Resource Fairness, after its main working principle.", "AI": {"tldr": "The paper revisits Dominant Resource Fairness (DRF) for multiple resource allocation and proposes a streamlined algorithm called Precomputed Dominant Resource Fairness (PDRF).", "motivation": "The motivation stems from the need for efficient resource allocation in distributed systems with multiple resource types and heterogeneous user demands, focusing on improving the Dominant Resource Fairness algorithm.", "method": "The paper provides a detailed analysis of DRF and formulates an alternative algorithm, PDRF, that aims to approximate DRF with fewer computational steps.", "result": "The resulting algorithm, PDRF, is proposed as a more computationally efficient method for achieving similar fairness properties as DRF in resource allocation.", "conclusion": "Precomputed Dominant Resource Fairness enhances DRF by reducing computational complexity while maintaining desirable fairness properties in distributed systems."}}
{"id": "2507.10463", "pdf": "https://arxiv.org/pdf/2507.10463", "abs": "https://arxiv.org/abs/2507.10463", "authors": ["Maxwell Aifer", "Zach Belateche", "Suraj Bramhavar", "Kerem Y. Camsari", "Patrick J. Coles", "Gavin Crooks", "Douglas J. Durian", "Andrea J. Liu", "Anastasia Marchenkova", "Antonio J. Martinez", "Peter L. McMahon", "Faris Sbahi", "Benjamin Weiner", "Logan G. Wright"], "title": "Solving the compute crisis with physics-based ASICs", "categories": ["cs.ET", "cs.AR"], "comment": "16 pages, 5 figures", "summary": "Escalating artificial intelligence (AI) demands expose a critical \"compute\ncrisis\" characterized by unsustainable energy consumption, prohibitive training\ncosts, and the approaching limits of conventional CMOS scaling. Physics-based\nApplication-Specific Integrated Circuits (ASICs) present a transformative\nparadigm by directly harnessing intrinsic physical dynamics for computation\nrather than expending resources to enforce idealized digital abstractions. By\nrelaxing the constraints needed for traditional ASICs, like enforced\nstatelessness, unidirectionality, determinism, and synchronization, these\ndevices aim to operate as exact realizations of physical processes, offering\nsubstantial gains in energy efficiency and computational throughput. This\napproach enables novel co-design strategies, aligning algorithmic requirements\nwith the inherent computational primitives of physical systems. Physics-based\nASICs could accelerate critical AI applications like diffusion models,\nsampling, optimization, and neural network inference as well as traditional\ncomputational workloads like scientific simulation of materials and molecules.\nUltimately, this vision points towards a future of heterogeneous,\nhighly-specialized computing platforms capable of overcoming current scaling\nbottlenecks and unlocking new frontiers in computational power and efficiency.", "AI": {"tldr": "Physics-based ASICs leverage intrinsic physical dynamics to solve AI's compute crisis, achieving energy efficiency and high computational throughput.", "motivation": "Current AI demands lead to unsustainable energy usage and CMOS scaling limits, necessitating transformative computing approaches.", "method": "Focuses on designing ASICs that capitalize on physical processes instead of enforcing conventional digital abstractions.", "result": "Physics-based ASICs demonstrate improved computational efficiency, aligning algorithmic needs with physical system capabilities.", "conclusion": "These systems promise a future of specialized, scalable computing platforms for advancing AI and other computational tasks."}}
{"id": "2507.09315", "pdf": "https://arxiv.org/pdf/2507.09315", "abs": "https://arxiv.org/abs/2507.09315", "authors": ["Yongqian Sun", "Weihua Kuang", "Chao Shen", "Xidao Wen", "Tinghua Zheng", "Heng Liu", "Shenglin Zhang", "Bo Wu", "Dan Pei"], "title": "Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning", "categories": ["cs.SE", "cs.AI"], "comment": "22 pages, 19 figures", "summary": "In modern online services, frequent software changes introduce significant\nrisks. To tackle this challenge, we propose SCELM (Software Change Evaluation\nand Lifecycle Management), an end-to-end automated framework for software\nchange management. SCELM aims to manage software changes efficiently and\nprecisely, significantly reducing service failures and economic losses.", "AI": {"tldr": "SCELM is an automated framework addressing challenges in managing software changes efficiently to minimize risks and failures in modern online services.", "motivation": "Mitigate risks associated with frequent software changes in online services and reduce service failures and economic losses.", "method": "Introduces SCELM, an automated and comprehensive framework for managing software changes.", "result": "The framework improves software change management, contributing to fewer service failures and reduced economic losses.", "conclusion": "SCELM demonstrates the importance of efficient and precise software change management to enhance service reliability and economic outcomes."}}
{"id": "2507.09155", "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD utilizes GPT-4.5 to generate compact, domain-specific references for improving crystallography question answering in smaller AI models.", "motivation": "Address the challenge of copyright issues and the lack of domain-specific training in smaller AI models for X-ray diffraction (XRD).", "method": "It integrates GPT-4.5-generated summaries as open-book references and compares the performance of various vision-language models in answering XRD questions under open-book and closed-book conditions.", "result": "Models demonstrated significant accuracy improvements with GPT-4.5 summaries, especially those with limited crystallography training.", "conclusion": "OPENXRD highlights that AI-generated domain-specific texts can enhance reasoning in scientific tasks, laying groundwork for future NLP tools in specialized fields."}}
{"id": "2507.09371", "pdf": "https://arxiv.org/pdf/2507.09371", "abs": "https://arxiv.org/abs/2507.09371", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "categories": ["cs.RO"], "comment": "This paper is under review", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "AI": {"tldr": "The paper proposes using constrained Markov Decision Processes (CMDP) for balancing stylistic imitation and task performance in robots, validated by experiments across platforms including ANYmal-D hardware.", "motivation": "Learning from demonstrations for robotics often struggles with balancing style and task performance, especially when demonstrations are incomplete or unrealistic.", "method": "Formulated as a CMDP, the method optimizes a style imitation goal while enforcing constraints for task performance using an adaptive Lagrangian multiplier.", "result": "Experiments demonstrate improved task performance and successfully captured stylistic nuances. Results include efficient energy usage and agile gait patterns on ANYmal-D hardware.", "conclusion": "The approach ensures robust task performance while preserving high-quality stylistic behaviors, validated across diverse robots and tasks."}}
{"id": "2507.09534", "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "AI": {"tldr": "This paper proposes Consistency Trajectory Planning (CTP), an offline model-based reinforcement learning approach offering fast single-step trajectory optimization.", "motivation": "Existing diffusion-based planning methods demonstrate strong results but come with high computational costs due to iterative sampling procedures.", "method": "CTP uses the Consistency Trajectory Model (CTM) to enable single-step trajectory generation, avoiding the need for iterative procedures.", "result": "CTP outperforms diffusion-based planning methods on the D4RL benchmark in terms of returns and computation time, achieving over 120x speedup while maintaining high performance.", "conclusion": "CTP is a practical and effective solution for high-performance, low-latency offline reinforcement learning planning."}}
{"id": "2507.09092", "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "The paper presents MI CAM, a novel machine vision technique using activation mapping and mutual information for explainable AI, showing superior qualitative and quantitative results.", "motivation": "Machine vision in critical areas like healthcare and automated systems requires better interpretability of convolutional neural networks (CNNs) to understand their decision-making processes.", "method": "The proposed MI CAM method uses activation mapping, weighting each feature map based on mutual information with the input image and combines them linearly to generate saliency visualizations, validated through counterfactual analysis.", "result": "MI CAM performs on par or better than state-of-the-art methods, particularly excelling in qualitative and quantitative evaluations.", "conclusion": "MI CAM enhances visual explanation capabilities of CNNs, offering unbiased and interpretable model inferencing, with publicly available implementation for reproducibility."}}
{"id": "2507.08845", "pdf": "https://arxiv.org/pdf/2507.08845", "abs": "https://arxiv.org/abs/2507.08845", "authors": ["Irfan Ullah", "Young-Koo Lee"], "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from\ngraph-structured data, however uniform neighbor sampling and static fanout\nsettings frequently limit GNNs' scalability and efficiency. In this paper, we\npropose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel\napproach that dynamically adjusts the fanout based on model performance and\nprioritizes important nodes during training. Our approach leverages node\nscoring based on node degree to focus computational resources on structurally\nimportant nodes, incrementing the fanout as the model training progresses.\nDAFOS also integrates an early stopping mechanism to halt training when\nperformance gains diminish. Experiments conducted on three benchmark datasets,\nogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach\nsignificantly improves training speed and accuracy compared to a\nstate-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv\ndataset and a 12.6x speedup on the Reddit dataset while improving the F1 score\nfrom 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the\nogbn-products dataset, respectively. These results highlight the potential of\nDAFOS as an efficient and scalable solution for large-scale GNN training.", "AI": {"tldr": "This paper introduces the DAFOS method to dynamically optimize fanout in graph neural network training, resulting in significant speed and accuracy enhancements.", "motivation": "Static fanout settings limit the scalability and efficiency of Graph Neural Networks (GNNs). Addressing this challenge, this paper proposes an adaptive approach to prioritize computational resources.", "method": "DAFOS dynamically adjusts fanout during training based on node importance (using node degree scoring), model performance, and employs early stopping to improve efficiency.", "result": "DAFOS demonstrated significant speedup of 3.57x on ogbn-arxiv and 12.6x on Reddit datasets while enhancing F1 scores from 68.5% to 71.21% (ogbn-arxiv) and from 73.78% to 76.88% (ogbn-products).", "conclusion": "DAFOS is an effective and scalable solution that optimizes GNN training, enhancing both computational speed and prediction accuracy."}}
{"id": "2507.09317", "pdf": "https://arxiv.org/pdf/2507.09317", "abs": "https://arxiv.org/abs/2507.09317", "authors": ["Sara Si-Moussi", "Esther Galbrun", "Mickael Hedde", "Giovanni Poggiato", "Matthias Rohr", "Wilfried Thuiller"], "title": "Uncovering symmetric and asymmetric species associations from community and environmental data", "categories": ["stat.ML", "cs.LG", "q-bio.PE", "68T07, 62H22, 92D40", "I.2.3; I.2.6; I.5.1"], "comment": null, "summary": "There is no much doubt that biotic interactions shape community assembly and\nultimately the spatial co-variations between species. There is a hope that the\nsignal of these biotic interactions can be observed and retrieved by\ninvestigating the spatial associations between species while accounting for the\ndirect effects of the environment. By definition, biotic interactions can be\nboth symmetric and asymmetric. Yet, most models that attempt to retrieve\nspecies associations from co-occurrence or co-abundance data internally assume\nsymmetric relationships between species. Here, we propose and validate a\nmachine-learning framework able to retrieve bidirectional associations by\nanalyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences\nfrom a source to a target species, parameterized with two species-specific\nlatent embeddings: the effect of the source species on the community, and the\nresponse of the target species to the community; and (2) jointly fits these\nassociations within a multi-species conditional generative model with different\nmodes of interactions between environmental drivers and biotic associations.\nUsing both simulated and empirical data, we demonstrate the ability of our\nframework to recover known asymmetric and symmetric associations and highlight\nthe properties of the learned association networks. By comparing our approach\nto other existing models such as joint species distribution models and\nprobabilistic graphical models, we show its superior capacity at retrieving\nsymmetric and asymmetric interactions. The framework is intuitive, modular and\nbroadly applicable across various taxonomic groups.", "AI": {"tldr": "The paper introduces a machine-learning framework to analyze spatial species associations by modeling both symmetric and asymmetric biotic interactions using species and environmental data. It outperforms existing methods and is versatile in application.", "motivation": "The motivation of this paper is to address the limitation of existing models that assume symmetric interactions when studying species associations from spatial data, despite the inherently asymmetric nature of biotic interactions.", "method": "The proposed framework models directed influences between species using species-specific latent embeddings to parameterize the effects of source species and responses of target species. It integrates these associations into a multi-species conditional generative model to handle both environmental drivers and biotic interactions.", "result": "Using both simulated and real-world datasets, the framework is shown to accurately recover both symmetric and asymmetric associations between species. It outperforms existing approaches like joint species distribution models and probabilistic graphical models in retrieving these interactions.", "conclusion": "The paper concludes that the presented framework is intuitive, modular, and capable of broadly applicable across various taxonomic groups, making it a versatile tool for studying species associations."}}
{"id": "2507.08868", "pdf": "https://arxiv.org/pdf/2507.08868", "abs": "https://arxiv.org/abs/2507.08868", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "title": "A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies", "categories": ["cs.GT", "cs.DC", "91B26", "J.1; J.4"], "comment": "Preprint", "summary": "Today, static cloud markets where consumers purchase services directly from\nproviders are dominating. Thus, consumers neither negotiate the price nor the\ncharacteristics of the service. In recent years, providers have adopted more\ndynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to\nthe reservation marketspace and the on-demand marketspace, Amazon offers a spot\nmarketspace where consumers can bid for virtual machines. This spot marketspace\nwas extended with spot blocks, and recently Amazon reworked the bidding\noptions. In addition, other cloud providers, such as Virtustream, adopt dynamic\ntrading mechanisms. The scientific community envisions autonomous multi-round\nnegotiations for realizing future cloud marketspaces. Consequently, consumers\nand providers exchange offers and counteroffers to reach an agreement. This\nhelps providers increase the utilization of their datacenters, while consumers\ncan purchase highly customized cloud services.\n  In the paper at hand, we present a survey on multi-round bilateral\nnegotiation strategies for trading cloud resources. Thus, we analyzed\npeer-reviewed articles in order to identify trends, gaps, similarities, and the\nscope of such negotiation strategies. In addition, we surveyed the formalism\nthat the scientific community uses to describe such strategies. Based on these\nfindings, we derived recommendations for creating and documenting bilateral\nmulti-round negotiation strategies to foster their implementation in the\nindustry.", "AI": {"tldr": "The paper surveys and analyzes bilateral multi-round negotiation strategies for trading cloud resources, highlighting trends, gaps, and recommendations.", "motivation": "To address the limitations of static cloud markets and explore dynamic trading mechanisms with negotiation to provide tailored services and enhance datacenter utilization.", "method": "Analyzed peer-reviewed articles to identify trends, gaps, and describe formalism used for bilateral multi-round negotiation strategies in cloud resource trading.", "result": "Identified trends, gaps, and similarities in negotiation strategies; reviewed formalism used; derived recommendations for industry implementation.", "conclusion": "The paper provides insights and actionable recommendations to foster the integration of negotiation strategies in dynamic cloud marketplaces."}}
{"id": "2507.09414", "pdf": "https://arxiv.org/pdf/2507.09414", "abs": "https://arxiv.org/abs/2507.09414", "authors": ["Khizra Sohail", "Atif Aftab Ahmed Jilani", "Nigar Azhar Butt"], "title": "Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs", "categories": ["cs.SE"], "comment": null, "summary": "Automated test generation for game-like programs presents unique challenges\ndue to their non-deterministic behavior and complex control structures. The\nNEATEST framework has been used for automated testing in Scratch games,\nemploying neuroevolution-based test generation optimized for statement\ncoverage. However, statement coverage alone is often insufficient for fault\ndetection, as it does not guarantee execution of all logical branches. This\npaper introduces a branch coverage-based fitness function to enhance test\neffectiveness in automated game testing. We extend NEATEST by integrating a\nbranch fitness function that prioritizes control-dependent branches, guiding\nthe neuroevolution process to maximize branch exploration. To evaluate the\neffectiveness of this approach, empirical experiments were conducted on 25\nScratch games, comparing Neatest with Statement Coverage (NSC) against Neatest\nwith Branch Coverage (NBC). A mutation analysis was also performed to assess\nthe fault detection capabilities of both techniques. The results demonstrate\nthat NBC achieves higher branch coverage than NSC in 13 out of 25 games,\nparticularly in programs with complex conditional structures. Moreover, NBC\nachieves a lower false positive rate in mutation testing, making it a more\nreliable approach for identifying faulty behavior in game programs. These\nfindings confirm that branch coverage-based test generation improves test\ncoverage and fault detection in Scratch programs.", "AI": {"tldr": "This paper improves automated test generation in Scratch games by enhancing NEATEST with a branch coverage fitness function, which outperforms statement coverage in fault detection and complex branch exploration.", "motivation": "The need for improved fault detection and control structure exploration in game-like programs, where existing methods relying only on statement coverage are inadequate.", "method": "Introduce a branch coverage-based fitness function into the NEATEST framework; evaluate the approach empirically on 25 Scratch games and use mutation analysis to compare performance against statement coverage.", "result": "Branch coverage-based testing achieved higher coverage in 13 of 25 games and showed better fault detection, particularly in programs with complex conditions, with fewer false positives.", "conclusion": "Branch coverage testing enhances the effectiveness of automated game testing by improving branch exploration and fault detection compared to statement coverage methods."}}
{"id": "2507.09157", "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "The paper proposes PU-Lie, a model for detecting deception in dialogues, using frozen BERT embeddings, linguistic/game features, and a PU learning approach to address severe class imbalance.", "motivation": "Deception detection is challenging in strategic dialogues, particularly given the subtle nature of deceptive communication and the imbalance between truthful and deceptive messages.", "method": "The authors combine frozen BERT embeddings, linguistic features, game-specific features, and a Positive-Unlabeled (PU) learning objective to build PU-Lie, a lightweight deception-detection model.", "result": "PU-Lie achieves a macro F1 score of 0.60, sets a new benchmark, and reduces trainable parameters by over 650x compared to other models.", "conclusion": "The framework focuses on prioritizing the detection of deceptive messages due to their critical importance, leveraging PU learning for scenarios with limited labeled deceptive data."}}
{"id": "2507.09383", "pdf": "https://arxiv.org/pdf/2507.09383", "abs": "https://arxiv.org/abs/2507.09383", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted to IEEE RA-L 2025", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "AI": {"tldr": "This paper introduces a motion planning system combining diffusion models and artificial potential fields for robust trajectory generation in complex environments, focusing on pursuit-evasion situations.", "motivation": "The study is motivated by the challenge of pursuit-evasion problems where robust, real-time trajectory planning is needed in complex and dynamic environments.", "method": "The method integrates energy-based diffusion models with classifier-free guidance training and local artificial potential fields. It processes obstacle information from point clouds and iteratively refines trajectories in dynamic situations.", "result": "The system successfully generates initial trajectories and effectively refines them in pursuit-evasion scenarios, even under partial observability.", "conclusion": "The framework demonstrates its utility for real-time and robust motion planning, offering efficient obstacle avoidance and adaptability in pursuit-evasion contexts."}}
{"id": "2507.09540", "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "This paper introduces an MH sampling-based framework to train Spiking Neural Networks for reinforcement learning tasks, focusing on energy-efficient solutions and demonstrating superior performance over traditional methods.", "motivation": "The paper targets the challenge of training Spiking Neural Networks, especially for reinforcement learning tasks, given the non-differentiable nature of their spike-based communication.", "method": "The authors utilize Metropolis-Hastings sampling, a Bayesian inference technique, to propose and accept parameter updates probabilistically based on accumulated reward signals rather than gradient methods.", "result": "The MH-based approach surpasses Deep Q-Learning and prior SNN RL methodologies in control benchmarks like AcroBot and CartPole, optimizing reward accumulation and minimizing network resources and required training episodes.", "conclusion": "The framework provides a novel pathway to train SNNs for RL dynamically, proving both efficient and effective without reliance on gradient-based techniques."}}
{"id": "2507.09097", "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "RadEyeVideo integrates radiologists' eye-fixation data as video sequences to enhance chest X-ray (CXR) analysis, improving report generation and disease diagnosis using vision-language models.", "motivation": "The study aims to leverage radiologists' sequential eye-gaze data to improve interaction and insights in chest X-ray analysis, addressing the limitations of current methods that fail to capture the order of eye movements.", "method": "RadEyeVideo incorporates radiologists' eye-fixation data as a video input, capturing spatial and temporal dynamics, and evaluates its impact using open-domain LVLMs alongside scaled metrics.", "result": "RadEyeVideo improves report generation performance by up to 24.6% and enhances diagnostic capabilities by an average of 15.2%, outperforming specialized medical LVLMs.", "conclusion": "Integrating domain expertise like eye-gaze information into LVLMs significantly boosts performance in clinical tasks, showcasing a scalable approach for advancing medical image analysis."}}
{"id": "2507.08848", "pdf": "https://arxiv.org/pdf/2507.08848", "abs": "https://arxiv.org/abs/2507.08848", "authors": ["Calum Corrie Imrie", "Ioannis Stefanakos", "Sepeedeh Shahbeigi", "Richard Hawkins", "Simon Burton"], "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SE"], "comment": null, "summary": "The rapid advancement of machine learning (ML) has led to its increasing\nintegration into cyber-physical systems (CPS) across diverse domains. While CPS\noffer powerful capabilities, incorporating ML components introduces significant\nsafety and assurance challenges. Among ML techniques, reinforcement learning\n(RL) is particularly suited for CPS due to its capacity to handle complex,\ndynamic environments where explicit models of interaction between system and\nenvironment are unavailable or difficult to construct. However, in\nsafety-critical applications, this learning process must not only be effective\nbut demonstrably safe. Safe-RL methods aim to address this by incorporating\nsafety constraints during learning, yet they fall short in providing systematic\nassurance across the RL lifecycle. The AMLAS methodology offers structured\nguidance for assuring the safety of supervised learning components, but it does\nnot directly apply to the unique challenges posed by RL. In this paper, we\nadapt AMLAS to provide a framework for generating assurance arguments for an\nRL-enabled system through an iterative process; AMLAS-RL. We demonstrate\nAMLAS-RL using a running example of a wheeled vehicle tasked with reaching a\ntarget goal without collision.", "AI": {"tldr": "This paper introduces AMLAS-RL, an assurance framework adapted from AMLAS for safe reinforcement learning in cyber-physical systems, demonstrated through a vehicle collision-avoidance example.", "motivation": "The integration of machine learning, particularly reinforcement learning, into cyber-physical systems presents safety challenges, especially in critical applications where learning must be both effective and demonstrably safe.", "method": "AMLAS methodology is adapted to design AMLAS-RL, a framework that structures assurance arguments for RL systems using an iterative process.", "result": "AMLAS-RL is applied to a practical example involving a wheeled vehicle tasked with reaching a target without collision, showcasing its functionality in ensuring RL system safety.", "conclusion": "AMLAS-RL presents a systematic approach to address assurance gaps in reinforcement learning within cyber-physical systems, enhancing safety-critical applications."}}
{"id": "2507.09494", "pdf": "https://arxiv.org/pdf/2507.09494", "abs": "https://arxiv.org/abs/2507.09494", "authors": ["Albert Chiu"], "title": "An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "comment": null, "summary": "We introduce an algorithm for identifying interpretable subgroups with\nelevated treatment effects, given an estimate of individual or conditional\naverage treatment effects (CATE). Subgroups are characterized by ``rule sets''\n-- easy-to-understand statements of the form (Condition A AND Condition B) OR\n(Condition C) -- which can capture high-order interactions while retaining\ninterpretability. Our method complements existing approaches for estimating the\nCATE, which often produce high dimensional and uninterpretable results, by\nsummarizing and extracting critical information from fitted models to aid\ndecision making, policy implementation, and scientific understanding. We\npropose an objective function that trades-off subgroup size and effect size,\nand varying the hyperparameter that controls this trade-off results in a\n``frontier'' of Pareto optimal rule sets, none of which dominates the others\nacross all criteria. Valid inference is achievable through sample splitting. We\ndemonstrate the utility and limitations of our method using simulated and\nempirical examples.", "AI": {"tldr": "The paper introduces an algorithm to identify interpretable rule-based subgroups with elevated treatment effects.", "motivation": "Current models for estimating CATE often result in results that are high-dimensional or unintelligible, creating demand for simpler, interpretable solutions.", "method": "The method uses rule sets to summarize complex CATE models, guided by an objective function balancing subgroup size and effect size, producing Pareto optimal results.", "result": "The proposed framework provides interpretable subgroups and is validated through both simulations and empirical examples.", "conclusion": "This approach enhances decision-making and policy implementation by offering interpretable insights into treatment effects."}}
{"id": "2507.09019", "pdf": "https://arxiv.org/pdf/2507.09019", "abs": "https://arxiv.org/abs/2507.09019", "authors": ["Amey Agrawal", "Nitin Kedia", "Anmol Agarwal", "Jayashree Mohan", "Nipun Kwatra", "Souvik Kundu", "Ramachandran Ramjee", "Alexey Tumanov"], "title": "On Evaluating Performance of LLM Inference Serving Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.", "AI": {"tldr": "The paper critiques flawed evaluation methodologies for Large Language Model (LLM) inference systems and proposes a checklist to avoid such anti-patterns.", "motivation": "Current evaluation methods for LLM inference systems often lead to misleading conclusions and hinder scientific progress.", "method": "The authors analyze key evaluation anti-patterns across Baseline Fairness, Evaluation Setup, and Metric Design, and derive a checklist to improve evaluation practices.", "result": "They identify problematic evaluation anti-patterns and demonstrate their framework's applicability through a speculative decoding case study.", "conclusion": "A robust evaluation foundation is established, enabling accurate comparisons and driving progress in LLM inference systems by addressing real-world requirements."}}
{"id": "2507.09481", "pdf": "https://arxiv.org/pdf/2507.09481", "abs": "https://arxiv.org/abs/2507.09481", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.", "AI": {"tldr": "StateGen is an automated framework to test LLM abilities with sequential API interactions by generating diverse tasks and translating them into natural language. It introduces the StateEval benchmark with 120 test cases.", "motivation": "LLMs' capabilities have grown by integrating APIs, but current evaluation methods are insufficient for testing complex sequential API interactions.", "method": "StateGen uses state-machine-based API constraint solving, energy-based sampling, control-flow injection, and LLM cooperation to generate and translate executable programs.", "result": "StateGen created the StateEval benchmark containing 120 test cases across API task scenarios such as Session Service and Tensor Operation.", "conclusion": "StateGen improves the evaluation of LLMs by addressing gaps in testing sequential API interactions, providing a basis to enhance LLM capabilities."}}
{"id": "2507.09174", "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "RAMA is a retrieval-augmented multi-agent framework for verifying multimedia misinformation, excelling in ambiguous claims by leveraging web-based evidence and multi-agent reasoning.", "motivation": "The study addresses the increasing challenge of verifying multimodal misinformation, particularly when claims are ambiguous or lack adequate context.", "method": "The framework deploys strategic query formulation for precise web searches, cross-verifies evidence from diverse sources, and utilizes a multi-agent ensemble architecture with different multimodal large language models and prompt variants.", "result": "RAMA showed superior performance on benchmark datasets, particularly in resolving ambiguous or improbable claims by grounding its verification process in factual evidence.", "conclusion": "The study highlights the importance of web-based evidence integration and multi-agent reasoning, establishing RAMA as a reliable and scalable solution for multimedia misinformation verification."}}
{"id": "2507.09463", "pdf": "https://arxiv.org/pdf/2507.09463", "abs": "https://arxiv.org/abs/2507.09463", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "categories": ["cs.RO"], "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "AI": {"tldr": "This paper extensively studies the downwash effects in multi-quadrotor systems to improve performance and functionality in dense environments.", "motivation": "Current multi-quadrotor systems struggle with aerodynamic disturbances like downwash, leading to the need for large operational spaces and limiting their deployment capabilities.", "method": "The study employs data-driven approaches, measuring forces, torques, and using particle image velocimetry (PIV) to analyze downwash effects on single and multiple quadrotor setups.", "result": "The analysis offers detailed insights into the aerodynamic interactions, providing valuable data for optimizing formations, enhancing coordination, and improving system robustness.", "conclusion": "The findings enable the development of physics-based strategies to address downwash challenges, paving the way for more efficient and compact multi-quadrotor deployments in constrained environments."}}
{"id": "2507.09588", "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "categories": ["cs.AI"], "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "AI": {"tldr": "eSapiens is an AI-as-a-Service platform leveraging proprietary data, operational workflows, and agnostic LLMs to ensure in-house AI knowledge retention and data security. It offers AI Agents for workflow automation and data insights, showcasing high retrieval accuracy and improved factual alignment.", "motivation": "Enable businesses to securely control AI operations in-house, leveraging insights and AI automation to optimize workflows in critical sectors like legal and finance.", "method": "eSapiens integrates document ingestion, hybrid vector retrieval, and LangChain-based no-code orchestration, supporting multiple LLMs. Experiments focus on retrieval precision and generation quality using benchmarks.", "result": "The system achieved 91.3% Top-3 retrieval accuracy with optimal token chunking and up to a 23% improvement in factual alignment across supported LLMs.", "conclusion": "eSapiens provides auditable, trustworthy workflows optimized for domains requiring precision and security, demonstrating its viability in high-stakes industries."}}
{"id": "2507.09102", "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "This paper introduces PointSD, a framework that leverages text-to-image diffusion models like Stable Diffusion for 3D self-supervised learning, enhancing point cloud representations.", "motivation": "The authors aim to overcome the limitations of existing 3D diffusion-based models that are constrained by small 3D datasets.", "method": "The PointSD framework replaces Stable Diffusion's text encoder with a 3D encoder to train a point-to-image diffusion model. This allows point clouds to guide image denoising and align 3D features with Stable Diffusion features for semantic learning.", "result": "Experiments and ablation studies show improved performance on point cloud tasks using the PointSD framework.", "conclusion": "Stable Diffusion, trained on large-scale datasets, effectively enhances 3D self-supervised learning, supporting robust point cloud representations with the proposed framework PointSD."}}
{"id": "2507.08858", "pdf": "https://arxiv.org/pdf/2507.08858", "abs": "https://arxiv.org/abs/2507.08858", "authors": ["Sami Achour", "Yassine Bouher", "Duong Nguyen", "Nicolas Chesneau"], "title": "Foundation models for time series forecasting: Application in conformal prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.", "AI": {"tldr": "The study evaluates Time Series Foundation Models (TSFMs) against traditional approaches in conformal prediction settings, finding TSFMs more reliable in limited-data scenarios.", "motivation": "Investigate the potential of foundation models for time series forecasting in enhancing the reliability of conformal predictions, especially in data-limited settings.", "method": "The study conducts comparative analysis between Time Series Foundation Models (TSFMs) and traditional methods like statistical models and gradient boosting, specifically exploring their performance within conformal prediction frameworks.", "result": "TSFMs offer superior conformalized prediction intervals and better calibration stability, especially with limited data, compared to traditional methods.", "conclusion": "Foundation models are highly promising for improving the reliability of conformal predictions in time series, particularly when data is scarce."}}
{"id": "2507.09717", "pdf": "https://arxiv.org/pdf/2507.09717", "abs": "https://arxiv.org/abs/2507.09717", "authors": ["Abdullah Karaaslanli", "Bisakh Banerjee", "Tapabrata Maiti", "Selin Aviyente"], "title": "Signed Graph Learning: Algorithms and Theory", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "Real-world data is often represented through the relationships between data\nsamples, forming a graph structure. In many applications, it is necessary to\nlearn this graph structure from the observed data. Current graph learning\nresearch has primarily focused on unsigned graphs, which consist only of\npositive edges. However, many biological and social systems are better\ndescribed by signed graphs that account for both positive and negative\ninteractions, capturing similarity and dissimilarity between samples. In this\npaper, we develop a method for learning signed graphs from a set of smooth\nsigned graph signals. Specifically, we employ the net Laplacian as a graph\nshift operator (GSO) to define smooth signed graph signals as the outputs of a\nlow-pass signed graph filter defined by the net Laplacian. The signed graph is\nthen learned by formulating a non-convex optimization problem where the total\nvariation of the observed signals is minimized with respect to the net\nLaplacian. The proposed problem is solved using alternating direction method of\nmultipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration\ncomplexity from quadratic to linear in the number of nodes is introduced.\nFurthermore, theoretical proofs of convergence for the algorithm and a bound on\nthe estimation error of the learned net Laplacian as a function of sample size,\nnumber of nodes, and graph topology are provided. Finally, the proposed method\nis evaluated on simulated data and gene regulatory network inference problem\nand compared to existing signed graph learning methods.", "AI": {"tldr": "The paper presents a method to learn signed graphs from smooth signed graph signals using the net Laplacian as a key operator. Optimization techniques and an efficient algorithm ensure its practical applicability.", "motivation": "Understanding relationships in biological and social systems often requires capturing positive and negative interactions, which unsigned graphs cannot model effectively.", "method": "The approach employs the net Laplacian to define smooth signed graph signals. A non-convex optimization problem minimizes total variation with respect to the net Laplacian, solved via ADMM and a fast algorithm.", "result": "The algorithm demonstrates theoretical convergence, bounded estimation error, and promising empirical performance on simulated and gene regulatory network data.", "conclusion": "The method advances signed graph learning, offering theoretical rigor, computational efficiency, and practical performance."}}
{"id": "2507.09490", "pdf": "https://arxiv.org/pdf/2507.09490", "abs": "https://arxiv.org/abs/2507.09490", "authors": ["Yan Zhao", "Chiwei Tang"], "title": "Towards LLM-Based Automatic Playtest", "categories": ["cs.SE"], "comment": null, "summary": "Playtesting is the process in which people play a video game for testing. It\nis critical for the quality assurance of gaming software. Manual playtesting is\ntime-consuming and expensive. However, automating this process is challenging,\nas playtesting typically requires domain knowledge and problem-solving skills\nthat most conventional testing tools lack. Recent advancements in artificial\nintelligence (AI) have opened up new possibilities for applying Large Language\nModels (LLMs) to playtesting. However, significant challenges remain: current\nLLMs cannot visually perceive game environments, and most existing research\nfocuses on text-based games or games with robust APIs. Many non-text games lack\nAPIs to provide textual descriptions of game states, making it almost\nimpossible to naively apply LLMs for playtesting. This paper introduces Lap,\nour novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to\ntest match-3 games, a category of games where players match three or more\nidentical tiles in a row or column to earn points. Lap encompasses three key\nphases: processing of game environments, prompting-based action generation, and\naction execution. Given a match-3 game, Lap takes a snapshot of the game board\nand converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to\nsuggest moves based on that matrix and tentatively applies the suggested moves\nto earn points and trigger changes in the game board. It repeats the\nabove-mentioned three steps iteratively until timeout. For evaluation, we\nconducted a case study using Lap on an open-source match-3 game, CasseBonbons,\nand empirically compared it with three existing tools. Our results are\npromising: Lap outperformed existing tools by achieving higher code coverage\nand triggering more program crashes. This research sheds light on the future of\nautomatic testing and LLM applications.", "AI": {"tldr": "The paper introduces \"Lap,\" a new approach using ChatGPT for automating playtesting in match-3 games, ensuring higher code coverage and more crashes detected compared to existing tools.", "motivation": "Manual playtesting in video games is costly and time-consuming; automating it is challenging due to the need for domain knowledge and problem-solving, which conventional tools and LLMs currently lack for non-text games.", "method": "Lap automates playtesting by converting game board snapshots into numeric matrices, utilizing ChatGPT for move suggestions, applying those moves iteratively, and testing them on a match-3 game environment.", "result": "In testing with CasseBonbons, an open-source match-3 game, Lap achieved better code coverage and more program crash detections than three other existing tools.", "conclusion": "Lap demonstrates the potential of using LLMs like ChatGPT for effective automatic playtesting, highlighting future possibilities for broader applications in gaming quality assurance."}}
{"id": "2507.09185", "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "The paper introduces a fine-tuning method that prunes neurons specialized to dataset-specific mechanisms in large language models (LLMs) to improve generalization, achieving better performance on novel tasks.", "motivation": "Large language models often rely on dataset-specific correlations that yield high-confidence predictions but hinder performance on novel tasks or distributions. The motivation is to improve the generalization capability of LLMs by addressing this issue.", "method": "The method employs Integrated Gradients to measure each neuron's contribution to high-confidence predictions, identifying neurons that disproportionately drive dataset-specific mechanisms. These neurons are selectively pruned during fine-tuning to encourage reliance on generalizable representations.", "result": "The proposed pruning-based fine-tuning approach demonstrates significant performance improvements across multiple-choice benchmarks, outperforming previous non-pruning adaptation methods.", "conclusion": "Pruning dataset-specific neurons promotes generalizable reasoning in LLMs, enabling better adaptability to novel tasks and increasing overall robustness across various benchmarks."}}
{"id": "2507.09464", "pdf": "https://arxiv.org/pdf/2507.09464", "abs": "https://arxiv.org/abs/2507.09464", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "AI": {"tldr": "This paper presents software for precise UAV modeling using cost-effective sensors, filtering algorithms, and sensor fusion techniques to improve data quality.", "motivation": "The paper aims to address the demand for accurate modeling of UAVs to support efficient developmental testing processes.", "method": "Data from cost-effective sensors are processed using filtering algorithms and sensor fusion techniques. IMU data is used for UAV orientation with Quaternion Representation, and GPS and accelerometer sensors are combined for real-time position updates.", "result": "The proposed software demonstrates high accuracy and fluid rendering of UAV orientation and location during flight operations.", "conclusion": "The software successfully enhances UAV modeling by integrating cost-effective sensors, ensuring reliable orientation and position tracking."}}
{"id": "2507.09611", "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "categories": ["cs.AI", "cs.CY", "68T01"], "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "AI": {"tldr": "The paper reviews AI's overlooked environmental and ethical issues, identifying concerns like energy use, e-waste, and inequality, while advocating for sustainable practices.", "motivation": "To address systemic environmental and ethical challenges arising from AI's rapid expansion, such as energy consumption and global inequalities.", "method": "Reviewing recent studies and institutional reports to identify systemic issues in areas like energy usage, e-waste, and cybersecurity.", "result": "Identified systemic problems including training-related emissions, hardware turnover, and infrastructure inequality; connected these to Responsible AI principles.", "conclusion": "AI's development must prioritize ethical responsibility and environmental sustainability for a fairer and greener technological future."}}
{"id": "2507.09105", "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "categories": ["cs.CV"], "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "Sign Language Production (SLP) models often struggle with error accumulation and real-time efficiency. This paper introduces a hybrid model using both autoregressive and diffusion approaches, coupled with novel techniques to refine pose generation.", "motivation": "Traditional SLP models face challenges such as error accumulation during inference and inefficiencies in real-time tasks. Recent diffusion-based methods improve generation quality but are computationally expensive.", "method": "The authors propose a hybrid model combining autoregressive and diffusion techniques. They introduce a Multi-Scale Pose Representation module for extracting detailed features and a Confidence-Aware Causal Attention mechanism to guide pose generation using joint confidence scores.", "result": "Experiments on PHOENIX14T and How2Sign datasets confirm that their method improves both the quality of generation and real-time streaming efficiency compared to existing approaches.", "conclusion": "The proposed hybrid model effectively balances quality and efficiency in SLP tasks, addressing key limitations of prior methods."}}
{"id": "2507.08860", "pdf": "https://arxiv.org/pdf/2507.08860", "abs": "https://arxiv.org/abs/2507.08860", "authors": ["Awais Manzoor", "M. Atif Qureshi", "Etain Kidney", "Luca Longo"], "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Retention campaigns in customer relationship management often rely on churn\nprediction models evaluated using traditional metrics such as AUC and F1-score.\nHowever, these metrics fail to reflect financial outcomes and may mislead\nstrategic decisions. We introduce e-Profits, a novel business-aligned\nevaluation metric that quantifies model performance based on customer-specific\nvalue, retention probability, and intervention costs. Unlike existing\nprofit-based metrics such as Expected Maximum Profit, which assume fixed\npopulation-level parameters, e-Profits uses Kaplan-Meier survival analysis to\nestimate personalised retention rates and supports granular, per customer\nevaluation. We benchmark six classifiers across two telecom datasets (IBM Telco\nand Maven Telecom) and demonstrate that e-Profits reshapes model rankings\ncompared to traditional metrics, revealing financial advantages in models\npreviously overlooked by AUC or F1-score. The metric also enables segment-level\ninsight into which models maximise return on investment for high-value\ncustomers. e-Profits is designed as an understandable, post hoc tool to support\nmodel evaluation in business contexts, particularly for marketing and analytics\nteams prioritising profit-driven decisions. All source code is available at:\nhttps://github.com/matifq/eprofits.", "AI": {"tldr": "This paper proposes e-Profits, a profit-based evaluation metric to assess churn prediction models, incorporating customer-specific details to optimize financial outcomes.", "motivation": "Traditional metrics like AUC and F1-score fail to align with financial outcomes in retention campaigns, misleading decision-making.", "method": "Introducing e-Profits, which uses Kaplan-Meier survival analysis for personalized retention rates and evaluates per customer for financial metrics.", "result": "Applying the metric to telecom datasets, e-Profits revealed financial benefits in models overlooked by traditional metrics and offered data-driven insights for high-value customer ROI.", "conclusion": "e-Profits serves as a business-aligned tool that better informs profit-driven decisions and model evaluation for marketing and analytics teams."}}
{"id": "2507.09740", "pdf": "https://arxiv.org/pdf/2507.09740", "abs": "https://arxiv.org/abs/2507.09740", "authors": ["Ridwan Olabiyi", "Han Hu", "Ashif Iquebal"], "title": "Discovering Governing Equations in the Presence of Uncertainty", "categories": ["stat.ML", "cs.LG", "math.DS", "physics.data-an"], "comment": "24 pages, 5 figures", "summary": "In the study of complex dynamical systems, understanding and accurately\nmodeling the underlying physical processes is crucial for predicting system\nbehavior and designing effective interventions. Yet real-world systems exhibit\npronounced input (or system) variability and are observed through noisy,\nlimited data conditions that confound traditional discovery methods that assume\nfixed-coefficient deterministic models. In this work, we theorize that\naccounting for system variability together with measurement noise is the key to\nconsistently discover the governing equations underlying dynamical systems. As\nsuch, we introduce a stochastic inverse physics-discovery (SIP) framework that\ntreats the unknown coefficients as random variables and infers their posterior\ndistribution by minimizing the Kullback-Leibler divergence between the\npush-forward of the posterior samples and the empirical data distribution.\nBenchmarks on four canonical problems -- the Lotka-Volterra predator-prey\nsystem (multi- and single-trajectory), the historical Hudson Bay lynx-hare\ndata, the chaotic Lorenz attractor, and fluid infiltration in porous media\nusing low- and high-viscosity liquids -- show that SIP consistently identifies\nthe correct equations and lowers coefficient root-mean-square error by an\naverage of 82\\% relative to the Sparse Identification of Nonlinear Dynamics\n(SINDy) approach and its Bayesian variant. The resulting posterior\ndistributions yield 95\\% credible intervals that closely track the observed\ntrajectories, providing interpretable models with quantified uncertainty. SIP\nthus provides a robust, data-efficient approach for consistent physics\ndiscovery in noisy, variable, and data-limited settings.", "AI": {"tldr": "This paper proposes a stochastic inverse physics-discovery (SIP) framework to identify governing equations of dynamical systems while accounting for system variability and measurement noise.", "motivation": "Traditional discovery methods for dynamical systems struggle with pronounced variability and noisy, limited data, making accurate modeling and prediction challenging.", "method": "The SIP framework treats unknown coefficients as random variables and infers their posterior distribution by minimizing the Kullback-Leibler divergence between the posterior samples' push-forward and empirical data distribution.", "result": "SIP demonstrates superior performance on benchmark problems, improving coefficient root-mean-square error by an average of 82% compared to alternative methods, and yields interpretable models with quantified uncertainty.", "conclusion": "SIP is a robust and data-efficient method that consistently discovers governing equations in noisy, variable, and data-limited environments, providing validated uncertainty estimates."}}
{"id": "2507.09453", "pdf": "https://arxiv.org/pdf/2507.09453", "abs": "https://arxiv.org/abs/2507.09453", "authors": ["Micha\u0142 J\u00f3\u017awik", "Johan Pouwelse"], "title": "SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity", "categories": ["cs.CR", "cs.DC"], "comment": "18 pages, 4 figures", "summary": "The digitization of democratic processes promises greater accessibility but\npresents challenges in terms of security, privacy, and verifiability. Existing\nelectronic voting systems often rely on centralized architectures, creating\nsingle points of failure and forcing too much trust in authorities, which\ncontradicts democratic principles. This research addresses the challenge of\ncreating a secure, private e-voting system with minimized trust dependencies\ndesigned for the most versatile personal device: the smartphone. We introduce\nSmartphoneDemocracy, a novel e-voting protocol that combines three key\ntechnologies: the emerging European Digital Identity (EUDI) Wallet for\nSybil-resistant identity verification, Zero-Knowledge Proofs for\nprivacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a\nresilient, serverless public bulletin board. Our protocol enables voters to\nregister and cast ballots anonymously and verifiably directly from their\nsmartphones. We provide a detailed protocol design, a security analysis against\na defined threat model, and a performance evaluation demonstrating that the\ncomputational and network overhead is feasible for medium- to large-scale\nelections. By developing and prototyping this system, we demonstrate a viable\npath to empower citizens with a trustworthy, accessible, and user-controlled\ndigital voting experience.", "AI": {"tldr": "SmartphoneDemocracy is a secure, privacy-focused e-voting system employing smartphones, using technologies like EUDI Wallet, Zero-Knowledge Proofs, and TrustChain blockchain.", "motivation": "To address the challenges in privacy, security, and trust dependencies in electronic voting systems, often countering democratic principles.", "method": "The paper introduces the SmartphoneDemocracy e-voting protocol combining EUDI Wallet for identity verification, Zero-Knowledge Proofs for privacy, and TrustChain blockchain for a serverless bulletin board.", "result": "Security analysis and performance evaluations show the system is feasible for medium- to large-scale elections without excessive computational or network loads.", "conclusion": "SmartphoneDemocracy offers a viable e-voting solution that emphasizes privacy, security, accessibility, and aligns with democratic principles."}}
{"id": "2507.09529", "pdf": "https://arxiv.org/pdf/2507.09529", "abs": "https://arxiv.org/abs/2507.09529", "authors": ["Yunqian Wang", "Xiaohong Li", "Yao Zhang", "Yuekang Li", "Zhiping Zhou", "Ruitao Feng"], "title": "It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective", "categories": ["cs.SE"], "comment": null, "summary": "With the growing threat of software vulnerabilities, deep learning (DL)-based\ndetectors have gained popularity for vulnerability detection. However, doubts\nremain regarding their consistency within declared CWE ranges, real-world\neffectiveness, and applicability across scenarios. These issues may lead to\nunreliable detection, high false positives/negatives, and poor adaptability to\nemerging vulnerabilities. A comprehensive analysis is needed to uncover\ncritical factors affecting detection and guide improvements in model design and\ndeployment. In this paper, we present VulTegra, a novel evaluation framework\nthat conducts a multidimensional comparison of scratch-trained and\npre-trained-based DL models for vulnerability detection. VulTegra reveals that\nstate-of-the-art (SOTA) detectors still suffer from low consistency, limited\nreal-world capabilities, and scalability challenges. Contrary to common belief,\npre-trained models are not consistently better than scratch-trained models but\nexhibit distinct strengths in specific contexts.Importantly, our study exposes\nthe limitations of relying solely on CWE-based classification and identifies\nkey factors that significantly affect model performance. Experimental results\nshow that adjusting just one such factor consistently improves recall across\nall seven evaluated detectors, with six also achieving better F1 scores. Our\nfindings provide deeper insights into model behavior and emphasize the need to\nconsider both vulnerability types and inherent code features for effective\ndetection.", "AI": {"tldr": "The paper introduces VulTegra, an analysis framework for evaluating deep learning-based models in vulnerability detection, identifying key challenges and proposing improvements.", "motivation": "To address the unresolved issues of consistency, real-world effectiveness, and adaptability in DL-based vulnerability detectors, which impact the reliability and accuracy of detection systems.", "method": "The authors develop and employ VulTegra, a multidimensional evaluation framework, to analyze both scratch-trained and pre-trained DL models. The framework compares these detectors across various performance dimensions and identifies influential factors.", "result": "The study finds low consistency and scalability challenges in current state-of-the-art detectors. Pre-trained models do not consistently outperform scratch-trained models, and key factors influencing performance are uncovered. Adjusting one such factor improves recall and F1 scores significantly.", "conclusion": "The research highlights critical weaknesses in DL-based detectors, the limited utility of CWE-based classification, and emphasizes the need to integrate vulnerability types and code features into effective detection strategies."}}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "The paper introduces Banzhida, a multilingual large language model optimized for Tibetan, using a curated corpus and tailored benchmarks.", "motivation": "Tibetan language is underrepresented in large language models due to limited high-quality data.", "method": "The authors curated the largest Tibetan corpus to date, applied a specialized data processing pipeline, and fine-tuned a multilingual model into Banzhida.", "result": "Banzhida significantly outperforms comparable open-source models and Tibetan-specific models across diverse benchmarks.", "conclusion": "This advancement facilitates improved generative AI capabilities for Tibetan, addressing its low-resource language challenges."}}
{"id": "2507.09469", "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "categories": ["cs.RO"], "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "The paper introduces mmE-Loc, a ground-based system for precise drone landings using event cameras and mmWave radar to achieve superior accuracy and low latency.", "motivation": "Address the system throughput bottlenecks in traditional drone localization setups caused by mismatched sampling frequencies of frame cameras and mmWave radar.", "method": "Leverages event cameras (harmonized sampling frequency with mmWave radar) and proposes two modules: (i) Consistency-instructed Collaborative Tracking for precise measurement extraction and (ii) Graph-informed Adaptive Joint Optimization for efficient sensor fusion and localization.", "result": "Experiments with a drone delivery company demonstrate that mmE-Loc outperforms existing methods in both accuracy and latency during drone landings.", "conclusion": "The proposed mmE-Loc system enhances precise and efficient drone landings, showcasing its potential in real-world applications by addressing conventional limitations."}}
{"id": "2507.09617", "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "AI": {"tldr": "The paper proposes a neurosymbolic framework that combines multimodal language models with Knowledge Graphs and ontologies to enhance interoperability in robotic applications. It evaluates various models and integration strategies for generating ontology-compliant KGs.", "motivation": "Current robot systems lack interoperability due to proprietary, hard-coded implementations. The paper aims to address this by leveraging the strengths of ontologies/Knowledge Graphs and multimodal language models for adaptable robotic behaviors.", "method": "The authors integrate robot perception data, ontologies, and five multimodal models, employing different neural-symbolic interactions to generate ontology-compliant Knowledge Graphs. They assess model consistency and effectiveness through statistical analysis.", "result": "GPT-o1 and LLaMA 4 Maverick outperform other models in generating ontology-compliant Knowledge Graphs consistently. Integration strategy plays a significant role, as newer models do not always yield better results.", "conclusion": "The proposed integration of multimodal language models and structured representations demonstrates potential for platform-independent robotic applications, with results emphasizing the importance of model and strategy selection in neurosymbolic frameworks."}}
{"id": "2507.09111", "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "The paper introduces RoHOI, the first benchmark for robust Human-Object Interaction detection under real-world challenges like environmental variability and occlusion. It proposes a new strategy, SAMPL, to improve model robustness, showing significant performance gains.", "motivation": "Human-Object Interaction detection is essential for enabling context-aware robot-human assistance. However, current models perform poorly under real-world corruptions like occlusion and noise.", "method": "The authors developed RoHOI, a robustness benchmark with 20 corruption types based on HICO-DET and V-COCO datasets, and introduced SAMPL\u2014a Semantic-Aware Masking-based Progressive Learning strategy\u2014to enhance robust feature learning dynamically.", "result": "Systematic analyses showed that existing models suffered performance drops under corruptions, while the proposed SAMPL strategy outperformed state-of-the-art methods in extensive experiments.", "conclusion": "The paper sets a new standard in robust HOI detection, offering benchmarks, datasets, and strategies for improved real-world application resilience, with all resources made publicly available."}}
{"id": "2507.08861", "pdf": "https://arxiv.org/pdf/2507.08861", "abs": "https://arxiv.org/abs/2507.08861", "authors": ["Lucas Tesan", "Mikel M. Iparraguirre", "David Gonzalez", "Pedro Martins", "Elias Cueto"], "title": "On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper proposes sharp lower bounds for the number of message passing\niterations required in graph neural networks (GNNs) when solving partial\ndifferential equations (PDE). This significantly reduces the need for\nexhaustive hyperparameter tuning. Bounds are derived for the three fundamental\nclasses of PDEs (hyperbolic, parabolic and elliptic) by relating the physical\ncharacteristics of the problem in question to the message-passing requirement\nof GNNs. In particular, we investigate the relationship between the physical\nconstants of the equations governing the problem, the spatial and temporal\ndiscretisation and the message passing mechanisms in GNNs.\n  When the number of message passing iterations is below these proposed limits,\ninformation does not propagate efficiently through the network, resulting in\npoor solutions, even for deep GNN architectures. In contrast, when the\nsuggested lower bound is satisfied, the GNN parameterisation allows the model\nto accurately capture the underlying phenomenology, resulting in solvers of\nadequate accuracy.\n  Examples are provided for four different examples of equations that show the\nsharpness of the proposed lower bounds.", "AI": {"tldr": "The paper establishes precise lower bounds on message passing iterations in GNNs to solve different PDE types efficiently, ensuring adequate information propagation and better performance.", "motivation": "To address the challenge of inefficient information propagation and poor solutions in GNNs when solving PDEs due to insufficient message passing iterations, and to reduce the need for hyperparameter tuning.", "method": "Derived sharp lower bounds for message passing iterations in GNNs by correlating PDE physical constants, discretisation, and message passing mechanisms across hyperbolic, parabolic, and elliptic PDEs.", "result": "The proposed lower bounds ensure efficient information propagation and accurate solutions, validated through examples involving four types of PDE equations.", "conclusion": "Adhering to the lower bounds enhances GNNs' ability to solve PDEs with sufficient accuracy, mitigating deep architecture inefficiencies."}}
{"id": "2507.09828", "pdf": "https://arxiv.org/pdf/2507.09828", "abs": "https://arxiv.org/abs/2507.09828", "authors": ["Shion Takeno", "Yu Inatsu", "Masayuki Karasuyama", "Ichiro Takeuchi"], "title": "Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization", "categories": ["stat.ML", "cs.LG"], "comment": "35pages, 5 figures", "summary": "Bayesian optimization is a powerful tool for optimizing an\nexpensive-to-evaluate black-box function. In particular, the effectiveness of\nexpected improvement (EI) has been demonstrated in a wide range of\napplications. However, theoretical analyses of EI are limited compared with\nother theoretically established algorithms. This paper analyzes a randomized\nvariant of EI, which evaluates the EI from the maximum of the posterior sample\npath. We show that this posterior sampling-based random EI achieves the\nsublinear Bayesian cumulative regret bounds under the assumption that the\nblack-box function follows a Gaussian process. Finally, we demonstrate the\neffectiveness of the proposed method through numerical experiments.", "AI": {"tldr": "This paper analyzes a randomized variant of expected improvement (EI) for Bayesian optimization and demonstrates its effectiveness through theoretical analysis and numerical experiments.", "motivation": "Current theoretical analyses of EI in Bayesian optimization are limited, despite its practical success across many applications.", "method": "The paper proposes a randomized version of EI that relies on evaluating the maximum of the posterior sample path. Bayesian cumulative regret bounds are analyzed for cases where the black-box function follows a Gaussian process.", "result": "It is shown that the proposed method achieves sublinear Bayesian cumulative regret bounds. Numerical experiments validate the effectiveness of the approach.", "conclusion": "The randomized EI variant supported by posterior sampling can be used effectively for optimizing expensive-to-evaluate black-box functions with theoretical guarantees and empirical success."}}
{"id": "2507.09579", "pdf": "https://arxiv.org/pdf/2507.09579", "abs": "https://arxiv.org/abs/2507.09579", "authors": ["Marc Bara"], "title": "PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets", "categories": ["cs.CR", "cs.DC", "68M14, 94A60", "H.3.4; K.6.5; H.3.5"], "comment": "14 pages, 6 figures", "summary": "We present PromptChain, a decentralized Web3 architecture that establishes AI\nprompts as first-class digital assets with verifiable ownership, version\ncontrol, and monetization capabilities. Current centralized platforms lack\nmechanisms for proper attribution, quality assurance, or fair compensation for\nprompt creators. PromptChain addresses these limitations through a novel\nintegration of IPFS for immutable storage, smart contracts for governance, and\ntoken incentives for community curation. Our design includes: (1) a\ncomprehensive metadata schema for cross-model compatibility, (2) a\nstake-weighted validation mechanism to align incentives, and (3) a token\neconomy that rewards contributors proportionally to their impact. The proposed\narchitecture demonstrates how decentralized systems could potentially match\ncentralized alternatives in efficiency while providing superior ownership\nguarantees and censorship resistance through blockchain-anchored provenance\ntracking. By decoupling prompts from specific AI models or outputs, this work\nestablishes the foundation for an open ecosystem of human-AI collaboration in\nthe Web3 era, representing the first systematic treatment of prompts as\nstandalone digital assets with dedicated decentralized infrastructure.", "AI": {"tldr": "PromptChain utilizes Web3 technologies like IPFS and smart contracts to establish AI prompts as verifiable and monetizable digital assets, addressing issues like attribution and compensation.", "motivation": "Centralized platforms lack mechanisms to ensure proper attribution, quality assurance, and fair compensation for AI prompt creators.", "method": "The approach combines IPFS for immutable storage, smart contracts for governance, and token incentives for community curation, alongside metadata schemas and stake-weighted validation mechanisms.", "result": "PromptChain achieves decentralized AI prompt ownership, version control, and monetization, while maintaining high efficiency, ownership security, and censorship resistance.", "conclusion": "PromptChain sets a foundation for treating AI prompts as standalone assets in the Web3 ecosystem, improving human-AI collaboration through decentralized infrastructure."}}
{"id": "2507.09583", "pdf": "https://arxiv.org/pdf/2507.09583", "abs": "https://arxiv.org/abs/2507.09583", "authors": ["Taniv Ashraf"], "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study", "categories": ["cs.SE", "cs.AI", "I.2.7; J.1"], "comment": "6 pages. The live application can be viewed at\n  https://codepen.io/tanivashraf/pen/GgpgxBY and the source code is available\n  at https://github.com/TanivAshraf/ai-stock-analyzer", "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.", "AI": {"tldr": "The paper introduces a cost-efficient, serverless system using Google's Gemini API for real-time stock analysis. It emphasizes system design, debugging challenges, and human-AI collaboration.", "motivation": "To democratize financial data analysis using accessible LLMs while minimizing costs.", "method": "Implemented a serverless system leveraging Gemini API, GitHub Actions for automation, and a static frontend for visualizing results. Incrementally refined through debugging and architectural iterations.", "result": "Built a near-zero-cost, robust platform for AI-driven stock analysis that is publicly accessible and open-sourced.", "conclusion": "LLMs can transform financial analysis, and debugging plays a critical role in developing efficient human-AI collaborative systems."}}
{"id": "2507.09225", "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "categories": ["cs.CL", "cs.CY"], "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "Visual metaphors are valuable for climate change communication, offering aesthetic appeal and promoting deeper cognitive engagement despite complexity.", "motivation": "The study aims to address the lack of research on how visual metaphors affect climate change communication and to support future studies with consistent material.", "method": "The researchers created the MetaClimage database containing climate-related visual metaphors, paired with literal images, human ratings, and analyzed semantic/emotion variables using NLP.", "result": "Visual metaphors were harder to understand but more aesthetically pleasing, eliciting higher cognitive and emotional engagement, particularly among individuals with a high Need For Cognition.", "conclusion": "Visual metaphors may not always be more effective or arousing, but they enhance aesthetic appreciation and facilitate positive, abstract cognitive experiences, aiding climate change communication."}}
{"id": "2507.09505", "pdf": "https://arxiv.org/pdf/2507.09505", "abs": "https://arxiv.org/abs/2507.09505", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "title": "TruckV2X: A Truck-Centered Perception Dataset", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "AI": {"tldr": "TruckV2X introduces the first cooperative perception dataset focusing on heavy-duty trucks with multi-modal and multi-agent scenarios, addressing challenges like blind spots and occlusions.", "motivation": "To improve safety and cost-efficiency in autonomous trucking while addressing limitations in perception due to truck size and trailer dynamics.", "method": "Developed TruckV2X, a large-scale dataset with LiDAR and camera sensing data, involving collaboration among tractors, trailers, and other agents.", "result": "TruckV2X provides benchmarks for truck-oriented collaborative perception and insights into improving occlusion handling in autonomous heavy vehicles.", "conclusion": "The dataset lays the groundwork for advancing cooperative perception systems and accelerating autonomous truck development."}}
{"id": "2507.09626", "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "AI": {"tldr": "The paper introduces a PyTorch-based toolkit that uses stochastic control methods to ensure fairness and robustness in AI systems interacting with multiple agents.", "motivation": "AI systems often interact with multiple agents, requiring guarantees for fairness and robustness in their operations.", "method": "The authors developed a toolkit based on PyTorch that applies stochastic control techniques to model AI system interactions and repeated actions of agents.", "result": "The toolkit allows for modeling robustness and fairness in an easy and reliable way, removing complexity in ensuring fairness guarantees in multi-agent settings.", "conclusion": "This PyTorch-based toolkit simplifies the process of ensuring a priori fairness and robustness guarantees in multi-agent AI systems."}}
{"id": "2507.09118", "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "This paper proposes a method, MG-CLIP, to improve the continual learning performance of CLIP by leveraging and preserving the modality gap.", "motivation": "There is a need to enhance the continual learning capabilities of vision-language pre-trained models (like CLIP) while retaining their generalization and adaptability, particularly by addressing the overlooked modality gap.", "method": "The authors analyze modality gap variations during fine-tuning and propose MG-CLIP, which uses two strategies: modality gap preservation to reduce forgetting and modality gap compensation to enhance adaptability to new data.", "result": "MG-CLIP outperforms existing methods on several benchmarks without using additional replay data, as demonstrated through extensive experiments.", "conclusion": "The study highlights the importance of the modality gap in preserving pre-trained knowledge and introduces a new perspective for continual learning, making MG-CLIP an effective approach in vision-language domain."}}
{"id": "2507.08866", "pdf": "https://arxiv.org/pdf/2507.08866", "abs": "https://arxiv.org/abs/2507.08866", "authors": ["Marina Ceccon", "Giandomenico Cornacchia", "Davide Dalle Pezze", "Alessandro Fabris", "Gian Antonio Susto"], "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond", "categories": ["cs.LG", "cs.CY", "stat.ML"], "comment": "Accepted in Expert Systems with Applications", "summary": "Undesirable biases encoded in the data are key drivers of algorithmic\ndiscrimination. Their importance is widely recognized in the algorithmic\nfairness literature, as well as legislation and standards on\nanti-discrimination in AI. Despite this recognition, data biases remain\nunderstudied, hindering the development of computational best practices for\ntheir detection and mitigation. In this work, we present three common data\nbiases and study their individual and joint effect on algorithmic\ndiscrimination across a variety of datasets, models, and fairness measures. We\nfind that underrepresentation of vulnerable populations in training sets is\nless conducive to discrimination than conventionally affirmed, while\ncombinations of proxies and label bias can be far more critical. Consequently,\nwe develop dedicated mechanisms to detect specific types of bias, and combine\nthem into a preliminary construct we refer to as the Data Bias Profile (DBP).\nThis initial formulation serves as a proof of concept for how different bias\nsignals can be systematically documented. Through a case study with popular\nfairness datasets, we demonstrate the effectiveness of the DBP in predicting\nthe risk of discriminatory outcomes and the utility of fairness-enhancing\ninterventions. Overall, this article bridges algorithmic fairness research and\nanti-discrimination policy through a data-centric lens.", "AI": {"tldr": "The paper examines data biases that lead to algorithmic discrimination, introduces mechanisms to detect and document these biases, and proposes a preliminary framework called Data Bias Profile (DBP).", "motivation": "There is insufficient computational research in detecting and mitigating data biases, despite their known impact on algorithmic discrimination and legal implications.", "method": "The authors analyze three types of data biases and their effects on discrimination in algorithms. They propose mechanisms to detect these biases and introduce a framework, DBP, to systematically document them.", "result": "The study finds that underrepresentation of vulnerable groups is less detrimental than expected, while label and proxy biases are more critical. The DBP effectively predicts discriminatory risks and aids fairness interventions.", "conclusion": "The integration of DBP provides a structured way to assess biases and mitigate algorithmic discrimination, bridging research with anti-discrimination policy through data-focused analysis."}}
{"id": "2507.10154", "pdf": "https://arxiv.org/pdf/2507.10154", "abs": "https://arxiv.org/abs/2507.10154", "authors": ["Ricardo In\u00e1cio", "Zafeiris Kokkinogenis", "Vitor Cerqueira", "Carlos Soares"], "title": "Simulating Biases for Interpretable Fairness in Offline and Online Classifiers", "categories": ["stat.ML", "cs.LG"], "comment": "17 pages, 2 figures, 1 equation, 3 tables: 1 in main body and 2 in\n  the appendix. Submitted to the SynDAiTE: Synthetic Data for AI\n  Trustworthiness and Evolution workshop from ECMLPKDD 2025, anonymized", "summary": "Predictive models often reinforce biases which were originally embedded in\ntheir training data, through skewed decisions. In such cases, mitigation\nmethods are critical to ensure that, regardless of the prevailing disparities,\nmodel outcomes are adjusted to be fair. To assess this, datasets could be\nsystematically generated with specific biases, to train machine learning\nclassifiers. Then, predictive outcomes could aid in the understanding of this\nbias embedding process. Hence, an agent-based model (ABM), depicting a loan\napplication process that represents various systemic biases across two\ndemographic groups, was developed to produce synthetic datasets. Then, by\napplying classifiers trained on them to predict loan outcomes, we can assess\nhow biased data leads to unfairness. This highlights a main contribution of\nthis work: a framework for synthetic dataset generation with controllable bias\ninjection. We also contribute with a novel explainability technique, which\nshows how mitigations affect the way classifiers leverage data features, via\nsecond-order Shapley values. In experiments, both offline and online learning\napproaches are employed. Mitigations are applied at different stages of the\nmodelling pipeline, such as during pre-processing and in-processing.", "AI": {"tldr": "The paper introduces a framework for generating biased synthetic datasets to study how predictive models reinforce biases and presents a novel explainability technique using second-order Shapley values.", "motivation": "Biases in training data can lead to unfair outcomes in predictive models, making it imperative to study bias embedding processes and develop mitigation strategies.", "method": "The authors develop an agent-based model (ABM) to simulate a loan application process and generate synthetic datasets with controllable biases. They apply machine learning classifiers to analyze predictive outcomes and use a novel second-order Shapley values technique for explainability.", "result": "The framework demonstrates how biased data embed unfairness into model predictions. Various mitigation strategies are tested at different pipeline stages (e.g., pre-processing, in-processing), validated through offline and online learning experiments.", "conclusion": "This work emphasizes the importance of studying bias in predictive models and offers a useful framework and explainability tool to assess and mitigate unfairness in such systems."}}
{"id": "2507.09942", "pdf": "https://arxiv.org/pdf/2507.09942", "abs": "https://arxiv.org/abs/2507.09942", "authors": ["Jiaming Cheng", "Duong Tung Nguyen"], "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference", "categories": ["cs.NI", "cs.DC", "cs.SY", "eess.SY", "math.OC"], "comment": "5 pages, 11 figures", "summary": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach.", "AI": {"tldr": "This paper develops an optimization model for distributing large language model (LLM) inference workloads across edge data centers to minimize environmental and operational impacts while improving user experience.", "motivation": "There is a growing need to balance the environmental impacts and operational efficiency of distributed LLM inference workloads due to increasing energy consumption and variability in renewable energy availability.", "method": "The authors propose a novel optimization model to allocate LLM workloads across edge data centers while considering renewable energy generation, dynamic electricity prices, spatiotemporal variability, and user experience factors.", "result": "Numerical results demonstrate that the proposed optimization approach effectively reduces operational costs, energy consumption, carbon emissions, and water usage.", "conclusion": "The optimization model helps LLM service providers achieve sustainable operations by minimizing environmental impacts and operational costs, while maintaining high user experience standards."}}
{"id": "2507.09594", "pdf": "https://arxiv.org/pdf/2507.09594", "abs": "https://arxiv.org/abs/2507.09594", "authors": ["Aydin Homay"], "title": "How to Define Design in Industrial Control and Automation Software", "categories": ["cs.SE"], "comment": null, "summary": "Design is a fundamental aspect of engineering, enabling the creation of\nproducts, systems, and organizations to meet societal and/or business needs.\nHowever, the absence of a scientific foundation in design often results in\nsubjective decision-making, reducing both efficiency and innovation. This\nchallenge is particularly evident in the software industry and, by extension,\nin the domain of industrial control and automation systems (iCAS).\n  In this study, first we review the existing design definitions within the\nsoftware industry, challenge prevailing misconceptions about design, review\ndesign definition in the field of design theory and address key questions such\nas: When does design begin? How can design be defined scientifically? What\nconstitutes good design? and the difference between design and design language\nby relying on advancements in the field of design theory. We also evaluate the\ndistinction between ad-hoc and systematic design approaches, and present\narguments on how to balance complementary operational concerns while resolving\nconflicting evolutionary concerns.", "AI": {"tldr": "The paper analyzes existing definitions of design within software and industrial control systems, proposes scientific approaches, and distinguishes between ad-hoc and systematic methodologies.", "motivation": "To tackle inefficiencies and lack of innovation caused by the absence of a scientific foundation in design, particularly in software and industrial control system industries.", "method": "The paper reviews existing design definitions, evaluates misconceptions, incorporates design theory insights, and compares ad-hoc and systematic design approaches.", "result": "Clarified misconceptions about design, identified characteristics of good design, and defined differences between design and design language scientifically.", "conclusion": "Adopting systematic approaches and addressing both operational and evolutionary concerns are essential for achieving efficient and innovative design solutions."}}
{"id": "2507.09245", "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "categories": ["cs.CL"], "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "Introduces the Swa-bhasha Resource Hub, a repository for Romanized Sinhala to Sinhala transliteration data resources and algorithms from 2020 to 2025.", "motivation": "Facilitates research and development in Sinhala NLP by centralizing transliteration resources.", "method": "Provides openly accessible datasets and tools for transliteration, along with a comparative analysis of applications.", "result": "Comprehensive and publicly available transliteration tools and datasets for advancing Romanized Sinhala research.", "conclusion": "The hub significantly contributes to enhancing research and applications in the Sinhala NLP domain."}}
{"id": "2507.09537", "pdf": "https://arxiv.org/pdf/2507.09537", "abs": "https://arxiv.org/abs/2507.09537", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "AI": {"tldr": "The paper introduces Plan-MAE, a pretraining framework using masked autoencoders to improve prediction and planning for automated vehicles.", "motivation": "To address the limitations of existing automated vehicle systems which focus too narrowly on imitation learning against ground truth metrics, disregarding holistic scene understanding.", "method": "Plan-MAE uses three tasks\u2014reconstructing masked road networks, agent trajectories, and navigation routes\u2014along with a local sub-planning task for better contextual understanding and trajectory prediction. It fine-tunes this pretrained model for downstream tasks.", "result": "Plan-MAE significantly outperforms existing methods in key planning metrics in experiments on large-scale datasets.", "conclusion": "Plan-MAE proves to be a crucial pre-training approach for advanced learning-based motion planning in automated vehicles."}}
{"id": "2507.09662", "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 excel in complex reasoning but struggle with generating unnecessarily lengthy reasoning chains for simple queries, leading to inefficiency. The survey reviews recent progress to make reasoning concise and adaptive.", "motivation": "LRMs have shown strong performance in tasks requiring deep reasoning; however, their inefficiencies in generating lengthy reasoning chains for simple tasks hinder their practical application due to wasted resources and increased response times.", "method": "The survey reviews existing methodologies, benchmarks, and challenges related to concise and adaptive reasoning for improving the efficiency of LRMs.", "result": "The paper systematically highlights advancements in concise and adaptive reasoning approaches, providing insights into research progress and identifying challenges.", "conclusion": "Streamlining LRMs to switch between fast and slow thinking adaptively based on query difficulty is crucial for their practical application and efficient usage. Insights from this survey aim to inspire further innovations in adaptive reasoning."}}
{"id": "2507.09122", "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "categories": ["cs.CV"], "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "The paper introduces SnapMoGen, a high-quality text-motion dataset with improved generative modeling using MoMask++.", "motivation": "To address the limitation of existing text-to-motion generation approaches constrained by short or general text prompts.", "method": "SnapMoGen provides a detailed motion dataset paired with expressive annotations, along with MoMask++, a generative masked transformer for better token utilization.", "result": "SnapMoGen dataset includes 20K motion clips and 122K detailed descriptions. MoMask++ achieves state-of-the-art performance on benchmark evaluations.", "conclusion": "SnapMoGen and MoMask++ enhance fine-grained controllability and generalization in text-to-motion generation, paving the way for advancements in this domain."}}
{"id": "2507.08870", "pdf": "https://arxiv.org/pdf/2507.08870", "abs": "https://arxiv.org/abs/2507.08870", "authors": ["Yaowenqi Liu", "BingXu Meng", "Rui Pan", "Jerry Huang", "Tong Zhang"], "title": "GUIDE: Towards Scalable Advising for Research Ideas", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "The field of AI research is advancing at an unprecedented pace, enabling\nautomated hypothesis generation and experimental design across diverse domains\nsuch as biology, mathematics, and artificial intelligence. Despite these\nadvancements, there remains a significant gap in the availability of scalable\nadvising systems capable of providing high-quality, well-reasoned feedback to\nrefine proposed hypotheses and experimental designs. To address this challenge,\nwe explore key factors that underlie the development of robust advising\nsystems, including model size, context length, confidence estimation, and\nstructured reasoning processes. Our findings reveal that a relatively small\nmodel, when equipped with a well-compressed literature database and a\nstructured reasoning framework, can outperform powerful general-purpose\nlanguage models such as Deepseek-R1 in terms of acceptance rates for\nself-ranked top-30% submissions to ICLR 2025. Moreover, when limited to\nhigh-confidence predictions, our system achieves an acceptance rate exceeding\n90% on the ICLR 2025 test set, underscoring its potential to significantly\nenhance the quality and efficiency of hypothesis generation and experimental\ndesign. The code is released at\nhttps://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.", "AI": {"tldr": "The paper introduces an advising system using small AI models and structured reasoning, outperforming larger models in refining hypotheses and experiments.", "motivation": "Currently, there is a lack of scalable systems to provide well-reasoned feedback for hypotheses and experimental designs, despite rapid advancements in AI.", "method": "The study leverages factors like model size, context length, confidence estimation, and structured reasoning, employing a small model with a compressed literature database.", "result": "The proposed system exceeds 90% acceptance rates for high-confidence predictions and outperforms broader models like Deepseek-R1 for ICLR 2025 submissions.", "conclusion": "A well-designed, small-scale advising system can significantly improve hypothesis generation and experimental design, offering a scalable and efficient solution."}}
{"id": "2507.10303", "pdf": "https://arxiv.org/pdf/2507.10303", "abs": "https://arxiv.org/abs/2507.10303", "authors": ["K. Giannoukou", "X. Zhu", "S. Marelli", "B. Sudret"], "title": "MF-GLaM: A multifidelity stochastic emulator using generalized lambda models", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": null, "summary": "Stochastic simulators exhibit intrinsic stochasticity due to unobservable,\nuncontrollable, or unmodeled input variables, resulting in random outputs even\nat fixed input conditions. Such simulators are common across various scientific\ndisciplines; however, emulating their entire conditional probability\ndistribution is challenging, as it is a task traditional deterministic\nsurrogate modeling techniques are not designed for. Additionally, accurately\ncharacterizing the response distribution can require prohibitively large\ndatasets, especially for computationally expensive high-fidelity (HF)\nsimulators. When lower-fidelity (LF) stochastic simulators are available, they\ncan enhance limited HF information within a multifidelity surrogate modeling\n(MFSM) framework. While MFSM techniques are well-established for deterministic\nsettings, constructing multifidelity emulators to predict the full conditional\nresponse distribution of stochastic simulators remains a challenge. In this\npaper, we propose multifidelity generalized lambda models (MF-GLaMs) to\nefficiently emulate the conditional response distribution of HF stochastic\nsimulators by exploiting data from LF stochastic simulators. Our approach\nbuilds upon the generalized lambda model (GLaM), which represents the\nconditional distribution at each input by a flexible, four-parameter\ngeneralized lambda distribution. MF-GLaMs are non-intrusive, requiring no\naccess to the internal stochasticity of the simulators nor multiple\nreplications of the same input values. We demonstrate the efficacy of MF-GLaM\nthrough synthetic examples of increasing complexity and a realistic earthquake\napplication. Results show that MF-GLaMs can achieve improved accuracy at the\nsame cost as single-fidelity GLaMs, or comparable performance at significantly\nreduced cost.", "AI": {"tldr": "This paper introduces multifidelity generalized lambda models (MF-GLaMs) to emulate the conditional response distribution of high-fidelity (HF) stochastic simulators using lower-fidelity (LF) data, achieving accuracy improvements and cost efficiency compared to single-fidelity methods.", "motivation": "The motivation is to address the challenge of accurately emulating the conditional probability distributions generated by stochastic simulators, which traditional deterministic surrogate modeling techniques cannot handle effectively and are resource-intensive for high-fidelity simulators.", "method": "The proposed method, MF-GLaMs, extends the generalized lambda model (GLaM) to a multifidelity setting, integrating lower-fidelity simulator data to predict the full conditional response distributions of HF stochastic simulators non-intrusively.", "result": "The results demonstrate that MF-GLaMs can achieve improved emulation accuracy for stochastic simulators compared to single-fidelity models while significantly reducing computational costs in synthetic and earthquake simulation examples.", "conclusion": "MF-GLaMs offer an effective and efficient solution for predicting the full conditional response distribution of high-fidelity stochastic simulators by leveraging lower-fidelity data, saving computational costs while maintaining or improving accuracy."}}
{"id": "2507.10160", "pdf": "https://arxiv.org/pdf/2507.10160", "abs": "https://arxiv.org/abs/2507.10160", "authors": ["Manuel R\u00f6der", "Christoph Raab", "Frank-Michael Schleif"], "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "Extension of http://dx.doi.org/10.5220/0012351900003654", "summary": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.", "AI": {"tldr": "FedAcross+ is a Federated Learning framework designed for domain adaptation in resource-constrained environments, enabling effective performance under covariate shifts and limited target samples.", "motivation": "Federated Learning faces challenges in decentralized environments: costly human-labeled data, covariate shifts from environmental sensor variations, and limited feasibility of continuous model updates due to resource constraints.", "method": "The paper proposes FedAcross+, a framework using a pre-trained source model with a frozen backbone and classifier during client adaptation. It employs a domain adaptive linear layer for efficient target domain adaptation and supports streaming data for non-stationary setups.", "result": "Experimental results verify that FedAcross+ achieves effective adaptation with competitive accuracy, using limited target data, and operates efficiently within resource-constrained environments.", "conclusion": "FedAcross+ successfully addresses domain shift and resource limitations in Federated Learning, enabling practical deployment in industrial settings with sporadic updates and streaming data processing."}}
{"id": "2507.09596", "pdf": "https://arxiv.org/pdf/2507.09596", "abs": "https://arxiv.org/abs/2507.09596", "authors": ["Aydin Homay"], "title": "The Mythical Good Software", "categories": ["cs.SE"], "comment": null, "summary": "Good software has high cohesion and low coupling is clumsy, obscure, and in\nsome certain cases could be actually a harmful state of being. It is clumsy\nbecause there is no perfect correlation between higher cohesiveness and optimum\ndesign, and it is obscure because it conveys the message that coupling and\ncohesion are two distinct design principles, while there are in principle the\nsame design approaches, and only the time and space differ between them, and it\ncould also be a harmful state of being because we should not always aim for\nhigher cohesiveness without considering its cost.\n  In the course of this study, we aim to elucidate for the readers the meaning\nand underlying philosophy of the aforementioned paragraph.", "AI": {"tldr": "The paper investigates the notion that high cohesion and low coupling in software design may not universally correlate with optimal design, explaining potential flaws in this belief.", "motivation": "To challenge the widely held notion in software engineering that high cohesion and low coupling are universally ideal characteristics of good software design.", "method": "The study critically examines the concepts and relationship between cohesion and coupling, and their trade-offs, through theoretical analysis, rather than treating them as unrelated principles.", "result": "The study provides a deeper understanding of cohesion and coupling, showing they are interconnected principles varying in space and time, and discusses cases where prioritizing cohesion might not be ideal.", "conclusion": "Cohesion and coupling should not be dogmatically pursued without accounting for their costs and context-dependent trade-offs in software design."}}
{"id": "2507.09259", "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "categories": ["cs.CL"], "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "The paper presents a mechanism, inspired by psychology, to improve humour translation in large language models using a Chain-of-Thought approach. It enhances humour, fluency, and coherence significantly.", "motivation": "Translation models often fail in conveying humour effectively due to linguistic interference and the subtleties of humoristic communication.", "method": "This paper introduces a Humour Decomposition Mechanism (HDM) inspired by human psychological processes, integrating humour theory and Chain-of-Thought reasoning to better guide translation models.", "result": "The proposed method achieved notable improvements in humour (7.75%), fluency (2.81%), and coherence (6.13%) on open-source humour datasets.", "conclusion": "The study provides a framework for better humour translation by simulating human thought processes and integrating humour theory, addressing existing limitations in large language models' performance."}}
{"id": "2507.09538", "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "The paper explores the use of spiking neural networks (SNNs) for direct robot navigation and obstacle avoidance using LIDAR data, demonstrating that careful tuning of neuron membrane leakage achieves performance comparable to conventional CNNs.", "motivation": "Spiking Neural Networks (SNNs) can offer high-precision processing with low computational and memory requirements, which is ideal for battery and payload-constrained robotics applications like drones and rovers.", "method": "The study involves setting up a custom robot with LIDAR, collecting a dataset of LIDAR data labeled with human-operated control commands, and examining the impact of tuning the membrane leakage constant in Leaky Integrate-and-Fire (LIF) neurons within the SNN.", "result": "Carefully tuning the membrane potential leakage enables SNNs to match the robot control precision of traditional CNNs for obstacle avoidance tasks.", "conclusion": "SNNs, with tuned neuron properties, are a viable alternative to traditional non-spiking approaches like CNNs for obstacle avoidance in resource-constrained robots. The authors also release an open-source LIDAR dataset to support future research."}}
{"id": "2507.09742", "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "categories": ["cs.AI"], "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "AI": {"tldr": "This paper presents a causality-informed deep Q-network (Causal DQ) for optimizing sensor placement in anomaly detection, achieving quicker anomaly identification in large data streams.", "motivation": "The motivation is to address the challenges of placing sensors optimally for anomaly detection in large-scale systems, considering causal relationships instead of relying solely on variable correlations.", "method": "The method involves integrating causal information into the Q-network training process within a reinforcement learning framework.", "result": "The proposed approach achieves faster convergence, tighter theoretical error bounds, and reduced anomaly detection time in real-world and large-scale data scenarios.", "conclusion": "The causality-informed method advances anomaly detection approaches and has broader implications for reinforcement learning and causality-informed engineering applications."}}
{"id": "2507.09139", "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "PoseLLM is a novel framework for human pose estimation using a nonlinear MLP connector, outperforming prior methods like LocLLM.", "motivation": "Traditional pose estimation methods struggle to generalize to new poses or unseen keypoints, motivating the need for models with better generalization capabilities.", "method": "PoseLLM replaces LocLLM's linear projector with a two-layer MLP vision-language connector with GELU activation to enhance visual-textual feature fusion.", "result": "PoseLLM achieves an AP of 77.8 on the COCO validation set and demonstrates strong zero-shot generalization to other datasets like Human-Art and MPII.", "conclusion": "A simple yet effective nonlinear MLP connector significantly boosts pose localization accuracy while retaining robust generalization."}}
{"id": "2507.08871", "pdf": "https://arxiv.org/pdf/2507.08871", "abs": "https://arxiv.org/abs/2507.08871", "authors": ["Xishun Liao", "Haoxuan Ma", "Yifan Liu", "Yuxiang Wei", "Brian Yueshuai He", "Chris Stanford", "Jiaqi Ma"], "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.", "AI": {"tldr": "The paper introduces a learning-based framework for travel demand modeling that offers greater scalability and adaptability than traditional models, with a Los Angeles case study demonstrating its effectiveness.", "motivation": "Traditional activity-based models (ABMs) for travel demand are costly, rely on simplified assumptions, and are challenging to adapt to different regions, motivating the need for a more scalable, data-driven, and transferable approach.", "method": "The framework integrates population synthesis, activity generation, location assignment, and traffic simulation into a generative, data-driven, and scalable system, validated through an implementation covering Los Angeles' 10 million population.", "result": "The framework replicates real-world mobility patterns closely while significantly reducing cost. It achieves high accuracy in comparisons against benchmarks (e.g., SCAG ABM, Caltrans PeMS) with metrics including cosine similarity of 0.97, JSD of 0.001-0.006, and MAPE of 6.11%-9.8%.", "conclusion": "The proposed generative framework is a scalable, effective alternative to traditional ABMs, performing comparably or better in validation measures while being easier to adapt across regions."}}
{"id": "2507.10443", "pdf": "https://arxiv.org/pdf/2507.10443", "abs": "https://arxiv.org/abs/2507.10443", "authors": ["Xin Li"], "title": "Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present the Context-Content Uncertainty Principle (CCUP), a unified\nframework that models cognition as the directed flow of information between\nhigh-entropy context and low-entropy content. Inference emerges as a cycle of\nbidirectional interactions, bottom-up contextual disambiguation paired with\ntop-down content reconstruction, which resolves the Information Bottleneck in\nOptimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy\nminimization, CCUP steers representations toward minimal joint uncertainty\nwhile preserving inferential directionality. Local cycle completion underpins\ntemporal bootstrapping, chaining simulations to refine memory, and spatial\nbootstrapping, enabling compositional hierarchical inference. We prove a Delta\nConvergence Theorem showing that recursive entropy minimization yields\ndelta-like attractors in latent space, stabilizing perceptual schemas and motor\nplans. Temporal bootstrapping through perception-action loops and sleep-wake\nconsolidation further transforms episodic traces into semantic knowledge.\nExtending CCUP, each hierarchical level performs delta-seeded inference:\nlow-entropy content seeds diffuse outward along goal-constrained paths shaped\nby top-down priors and external context, confining inference to task-relevant\nmanifolds and circumventing the curse of dimensionality. Building on this, we\npropose that language emerges as a symbolic transport system, externalizing\nlatent content to synchronize inference cycles across individuals. Together,\nthese results establish iBOT as a foundational principle of information flow in\nboth individual cognition and collective intelligence, positioning recursive\ninference as the structured conduit through which minds adapt, align, and\nextend.", "AI": {"tldr": "The paper introduces the Context-Content Uncertainty Principle (CCUP), a framework for modeling cognition as the flow of information between context and content using recursive inference processes.", "motivation": "The authors aim to understand how cognition operates as an inferential process that balances uncertainty in context and content while resolving informational bottlenecks.", "method": "The framework employs Rao-Blackwellized variational entropy minimization and formalizes recursive entropy minimization through a Delta Convergence Theorem. This enables hierarchical inference and bootstrapping processes.", "result": "The framework stabilizes perceptual schemas and motor plans, while temporal bootstrapping converts episodic traces into semantic knowledge. It also conceptualizes language as a system for externalizing content and aligning cognition across individuals.", "conclusion": "The CCUP framework presents recursive inference as a foundational mechanism in both individual and collective intelligence, providing a structured model for cognitive adaptation and alignment."}}
{"id": "2507.10325", "pdf": "https://arxiv.org/pdf/2507.10325", "abs": "https://arxiv.org/abs/2507.10325", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "title": "Convergence of Agnostic Federated Averaging", "categories": ["cs.LG", "cs.DC", "eess.SP"], "comment": "5 pages, 2 figurres, CAMSAP conference", "summary": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.", "AI": {"tldr": "The paper addresses challenges in Federated Learning (FL) with intermittent and biased client participation, proposing and analyzing agnostic FedAvg algorithm for general stochastic participation.", "motivation": "Current FL methods often assume uniform or fully known client participation, which is unrealistic in practical deployments.", "method": "The authors characterize the stochastic dynamics of agnostic FedAvg and provide rigorous convergence guarantees for convex losses with non-uniform client participation.", "result": "Agnostic FedAvg achieves convergence at a rate of $\\mathcal{O}(1/\\sqrt{T})$, outperforming weighted aggregation variants even under server-side knowledge.", "conclusion": "The paper establishes agnostic FedAvg as a robust optimization framework for FL with unpredictable client availability."}}
{"id": "2507.09599", "pdf": "https://arxiv.org/pdf/2507.09599", "abs": "https://arxiv.org/abs/2507.09599", "authors": ["Aydin Homay"], "title": "Complexity and Coupling: A Functional Domain Approach", "categories": ["cs.SE"], "comment": null, "summary": "This paper provides a precise and scientific definition of complexity and\ncoupling, grounded in the functional domain, particularly within industrial\ncontrol and automation systems (iCAS). We highlight the widespread ambiguity in\ndefining complexity and coupling, emphasizing that many existing definitions\nrooted in physical attributes lead to confusion and inconsistencies.\nFurthermore, we re-exhibit why coupled design inherently increases complexity\nand how potentially this complexity could be reduced. Drawing on examples from\nvarious disciplines, such as software engineering, industrial automation, and\nmechanical design, we demonstrate that complexity does not necessarily\ncorrelate with system size or the number of components, and coupling, unlike\ncommon belief in software engineering, actually does not occur in the physical\ndomain but in the functional domain. We conclude that effective design\nnecessitates addressing coupling and complexity within the functional domain.", "AI": {"tldr": "The paper refines the concepts of complexity and coupling in industrial control and automation systems (iCAS), emphasizing their interpretation in the functional domain over physical definitions.", "motivation": "Ambiguity in defining complexity and coupling often leads to confusion and inconsistency in various industries, requiring a clearer and functional-domain-based understanding.", "method": "The authors analyze examples across software engineering, industrial automation, and mechanical design to establish the functional-domain perspective, differentiating it from size or component-based metrics.", "result": "The paper shows complexity isn't tied directly to system size, and coupling occurs in the functional rather than physical domain, challenging traditional beliefs.", "conclusion": "To reduce complexity and ensure effective design, coupling and complexity must be addressed within the functional domain rather than the physical one."}}
{"id": "2507.09282", "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "The paper introduces ClaritySpeech, a framework to address dementia-affected speech challenges using ASR, text obfuscation, and zero-shot TTS technologies.", "motivation": "Dementia alters speech patterns, raising communication and privacy challenges, compounded by mainstream ASR technologies' deficiencies in handling dementia-affected speech.", "method": "The ClaritySpeech framework integrates automatic speech transcription, text obfuscation, and zero-shot text-to-speech synthesis, focusing on preserving speaker identity and improving performance in low-data scenarios without fine-tuning.", "result": "Results demonstrate a drop in mean F1 scores under adversarial conditions (16% for ADReSS and 10% for ADReSSo), significant Word Error Rate (WER) improvements (0.73 to 0.08 for ADReSS and 0.15 for ADReSSo), and an increase in speech quality (from 1.65 to ~2.15).", "conclusion": "ClaritySpeech enhances privacy and accessibility for dementia-affected speech, showing effectiveness in improving transcription accuracy, speaker identity preservation, and speech quality."}}
{"id": "2507.09714", "pdf": "https://arxiv.org/pdf/2507.09714", "abs": "https://arxiv.org/abs/2507.09714", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "AI": {"tldr": "The paper introduces IteraOptiRacing, a strategy for autonomous racing that achieves time-optimal and collision-free trajectories by improving traditional i2LQR methods.", "motivation": "To create a robust autonomous racing system capable of optimizing lap time performance and avoiding collisions with dynamic and competitive obstacles.", "method": "The method is based on i2LQR, utilizing the ego car's iterative historical data for obstacle avoidance and time cost optimization. Parallel computing enables real-time operation in dynamic scenarios.", "result": "Simulations demonstrate the strategy's superiority over existing methods, achieving better maneuverability and performance in randomly generated competitive scenarios.", "conclusion": "This unified approach allows autonomous racing vehicles to achieve reliable and efficient performance against dynamic obstacles in competitive environments."}}
{"id": "2507.09751", "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "The paper integrates large language models (LLMs) into the interpretation of paraconsistent logic to address logical inconsistencies in LLM-generated outputs.", "motivation": "LLMs are powerful but lack logical consistency, creating a need for a method to integrate their knowledge into formal reasoning effectively.", "method": "The authors propose embedding LLMs into the interpretation functions of paraconsistent logic's formal semantics and validate this via experiments on factual datasets.", "result": "Demonstrated feasibility of combining LLM knowledge and logic soundness through dataset evaluations.", "conclusion": "LLMs can be utilized for neuro-symbolic reasoning while maintaining logical soundness and completeness."}}
{"id": "2507.09144", "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "This paper introduces $I^{2}$-World, an efficient framework for 4D occupancy forecasting in 3D scenes, aiming to improve forecasting and scenario generation for autonomous driving.", "motivation": "To address challenges in optimizing 3D scene tokenization for forecasting complex scenarios in autonomous driving, which is critical for handling edge cases.", "method": "The framework includes intra-scene and inter-scene tokenizers to compress and aggregate 3D scenes hierarchically and dynamically, paired with an encoder-decoder architecture to facilitate scene understanding and prediction.", "result": "$I^{2}$-World achieves state-of-the-art performance in 4D occupancy forecasting, improving mIoU by 25.1% and IoU by 36.9%, with high computational efficiency requiring only 2.9 GB memory and 37 FPS real-time inference.", "conclusion": "$I^{2}$-World provides an efficient and high-performing solution for 4D occupancy forecasting, showcasing advancements in tokenization and scene modeling for autonomous driving applications."}}
{"id": "2507.08873", "pdf": "https://arxiv.org/pdf/2507.08873", "abs": "https://arxiv.org/abs/2507.08873", "authors": ["Shaoran Yang", "Dongyu Wei", "Hanzhi Yu", "Zhaohui Yang", "Yuchen Liu", "Mingzhe Chen"], "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE GLOBECOM 2025", "summary": "In this paper, a novel contrastive language-image pre-training (CLIP) model\nbased semantic communication framework is designed. Compared to standard neural\nnetwork (e.g.,convolutional neural network) based semantic encoders and\ndecoders that require joint training over a common dataset, our CLIP model\nbased method does not require any training procedures thus enabling a\ntransmitter to extract data meanings of the original data without neural\nnetwork model training, and the receiver to train a neural network for\nfollow-up task implementation without the communications with the transmitter.\nNext, we investigate the deployment of the CLIP model based semantic framework\nover a noisy wireless network. Since the semantic information generated by the\nCLIP model is susceptible to wireless noise and the spectrum used for semantic\ninformation transmission is limited, it is necessary to jointly optimize CLIP\nmodel architecture and spectrum resource block (RB) allocation to maximize\nsemantic communication performance while considering wireless noise, the delay\nand energy used for semantic communication. To achieve this goal, we use a\nproximal policy optimization (PPO) based reinforcement learning (RL) algorithm\nto learn how wireless noise affect the semantic communication performance thus\nfinding optimal CLIP model and RB for each user. Simulation results show that\nour proposed method improves the convergence rate by up to 40%, and the\naccumulated reward by 4x compared to soft actor-critic.", "AI": {"tldr": "The paper introduces a semantic communication framework utilizing a contrastive language-image pre-training (CLIP) model, enabling independent transmitter-receiver operations and optimizing performance in noisy wireless networks through reinforcement learning.", "motivation": "To address limitations in standard semantic communication frameworks that rely on joint neural network training over common datasets, especially in noisy wireless networks.", "method": "The framework is based on a CLIP model for independent data extraction and task training. A reinforcement learning algorithm, specifically proximal policy optimization (PPO), is used to optimize CLIP model architecture and spectrum resource allocation under wireless noise conditions.", "result": "Simulation results indicate an improvement in convergence rate by up to 40% and an increase in accumulated reward by 4x compared to the soft actor-critic approach.", "conclusion": "The proposed CLIP-based framework and optimization method boost semantic communication efficiency and adaptability in wireless networks."}}
{"id": "2507.09637", "pdf": "https://arxiv.org/pdf/2507.09637", "abs": "https://arxiv.org/abs/2507.09637", "authors": ["Lo Gullstrand Heander", "Emma S\u00f6derberg", "Christofer Rydenf\u00e4lt"], "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review", "categories": ["cs.SE", "cs.HC", "D.2.0; D.2.3; K.4.3"], "comment": "39 pages, 14 figures Submitted to Empirical Software Engineering,\n  Springer Nature", "summary": "Code review is a well-established and valued practice in the software\nengineering community contributing to both code quality and interpersonal\nbenefits. However, there are challenges in both tools and processes that give\nrise to misalignments and frustrations. Recent research seeks to address this\nby automating code review entirely, but we believe that this risks losing the\nmajority of the interpersonal benefits such as knowledge transfer and shared\nownership.\n  We believe that by better understanding the cognitive processes involved in\ncode review, it would be possible to improve tool support, with out without AI,\nand make code review both more efficient, more enjoyable, while increasing or\nmaintaining all of its benefits. In this paper, we conduct an ethnographic\nthink-aloud study involving 10 participants and 34 code reviews. We build a\ncognitive model of code review bottom up through thematic, statistical,\ntemporal, and sequential analysis of the transcribed material. Through the\ndata, the similarities between the cognitive process in code review and\ndecision-making processes, especially recognition-primed decision-making,\nbecome apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how\nthe developers move through two phases during the code review; first an\norientation phase to establish context and rationale and then an analytical\nphase to understand, assess, and plan the rest of the review. Throughout the\nprocess several decisions must be taken, on writing comments, finding more\ninformation, voting, running the code locally, verifying continuous integration\nresults, etc.\n  Analysis software and process-coded data publicly available at:\nhttps://doi.org/10.5281/zenodo.15758266", "AI": {"tldr": "The paper presents a cognitive model for code review processes, called CRDM, identifying two key phases (orientation and analytical) and their associated decision-making actions.", "motivation": "To address misalignments and frustrations in traditional code review tools and processes while maintaining interpersonal benefits such as knowledge transfer and shared ownership.", "method": "Conducted an ethnographic think-aloud study with 10 participants performing 34 code reviews, followed by thematic, statistical, temporal, and sequential analysis to build a cognitive model of code review.", "result": "Introduced the CRDM (Code Review as Decision-Making) model, which highlights two primary phases in code reviews: the orientation phase to establish context and an analytical phase for assessment and planning.", "conclusion": "Understanding cognitive processes in code review can help develop better tools and workflows that enhance efficiency and enjoyment while retaining interpersonal benefits."}}
{"id": "2507.09424", "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM introduces a benchmark to evaluate methods quantifying training data's impact on large language models, focusing on tasks like data selection and bias filtering.", "motivation": "With data attribution gaining importance in applications like model interpretability and dataset curation, systematic evaluation tailored to LLMs has been lacking.", "method": "The authors propose a unified benchmark, DATE-LM, for assessing data attribution methods via three tasks and enabling large-scale, configurable evaluations.", "result": "Using the benchmark, evaluation reveals no single attribution method universally outperforms others, highlighting task-specific trade-offs and sensitivity.", "conclusion": "DATE-LM lays groundwork for systematized research in assessing data attribution methods, accompanied by a leaderboard to foster community engagement."}}
{"id": "2507.09725", "pdf": "https://arxiv.org/pdf/2507.09725", "abs": "https://arxiv.org/abs/2507.09725", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "AI": {"tldr": "The paper introduces a biologically-inspired autonomous navigation system mimicking ant behavior, leveraging a Mushroom Body model for visual homing in outdoor environments on a compact robot.", "motivation": "To develop a resource-efficient, biologically inspired system for autonomous visual navigation using minimal sensory input, inspired by ants.", "method": "The authors implemented a lateralized Mushroom Body architecture on a robot, enabling visual encoding during simulated and real-world experiments, including angular path integration and goal-memory bank categorization.", "result": "The system successfully demonstrated robust visual homing behavior during four experiments, including precise goal-stop behavior in natural settings with minimal computational requirements.", "conclusion": "Through biologically grounded mechanisms, the system showcases efficient and accurate visual homing capabilities, paving the way for advancements in autonomous navigation technologies."}}
{"id": "2507.09801", "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "AI": {"tldr": "This paper proposes technical interventions to manage risks posed by AI systems, enabling a coordinated halt in dangerous AI activities.", "motivation": "The paper emphasizes the urgent need for mechanisms that prevent risks such as loss of control, misuse, and geopolitical instability caused by rapidly advancing AI systems.", "method": "The authors outline technical interventions and evaluate their potential to restrict dangerous AI activities while serving as the basis for future AI governance strategies.", "result": "The paper identifies how specific interventions play a role in restricting harmful AI development and deployment.", "conclusion": "The proposed interventions can form a technical foundation to support broader governance measures aimed at reducing risks associated with advanced AI systems."}}
{"id": "2507.09168", "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "This paper introduces Stable Score Distillation (SSD), a streamlined framework enhancing text-guided image and 3D editing, addressing stability, spatial control, and editing precision issues in prior methods.", "motivation": "Prior methods in text-guided editing, such as Delta Denoising Score, struggle with stability, precise edits, and spatial control due to reliance on complex auxiliary structures.", "method": "SSD uses Classifier-Free Guidance (CFG) for alignment across prompts, introduces a constant-term null-text branch for optimization stability, and a prompt enhancement branch for stronger edit capability.", "result": "The framework exhibits state-of-the-art performance in 2D and 3D editing applications, such as NeRF and text-driven style transformations, achieving faster convergence and lower complexity.", "conclusion": "Stable Score Distillation (SSD) offers a robust, efficient solution for text-guided editing, enabling precise, coherent, and localized modifications across 2D and 3D tasks."}}
{"id": "2507.08874", "pdf": "https://arxiv.org/pdf/2507.08874", "abs": "https://arxiv.org/abs/2507.08874", "authors": ["Yulin Sun", "Xiaopeng Si", "Runnan He", "Xiao Hu", "Peter Smielewski", "Wenlong Wang", "Xiaoguang Tong", "Wei Yue", "Meijun Pang", "Kuo Zhang", "Xizi Song", "Dong Ming", "Xiuyun Liu"], "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework", "categories": ["cs.LG"], "comment": null, "summary": "Timely identification of harmful brain activities via electroencephalography\n(EEG) is critical for brain disease diagnosis and treatment, which remains\nlimited application due to inter-rater variability, resource constraints, and\npoor generalizability of existing artificial intelligence (AI) models. In this\nstudy, a convolutional neural network model, VIPEEGNet, was developed and\nvalidated using EEGs recorded from Massachusetts General Hospital/Harvard\nMedical School. The VIPEEGNet was developed and validated using two independent\ndatasets, collected between 2006 and 2020. The development cohort included EEG\nrecordings from 1950 patients, with 106,800 EEG segments annotated by at least\none experts (ranging from 1 to 28). The online testing cohort consisted of EEG\nsegments from a subset of an additional 1,532 patients, each annotated by at\nleast 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved\nhigh accuracy, with an AUROC for binary classification of seizure, LPD, GPD,\nLRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%\nCI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),\n0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi\nclassification, the sensitivity of VIPEEGNET for the six categories ranges from\n36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance\nsimilar to human experts. Notably, the external validation showed\nKullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the\nexisting 2,767 competing algorithms, while we only used 2.8% of the parameters\nof the first-ranked algorithm.", "AI": {"tldr": "The paper introduces VIPEEGNet, a convolutional neural network model for analyzing EEG data to classify harmful brain activities with high accuracy across various categories.", "motivation": "The existing limitations in EEG analysis, such as inter-rater variability and poor generalizability in current AI models, create challenges in diagnosing brain diseases. The paper aims to overcome these issues.", "method": "A convolutional neural network was trained and validated using EEG data from two independent datasets, annotated by multiple experts, including online testing against expert performance for segmentation and classification.", "result": "VIPEEGNet achieved high AUROC scores for binary classification categories like seizures and LPDs, sensitivity between 36.8% to 88.2%, precision from 55.6% to 80.4%, and comparable performance to human experts. It ranked second among 2,767 competing algorithms using minimal computational parameters.", "conclusion": "VIPEEGNet significantly enhances brain disease diagnosis through accurate EEG evaluation while being computationally efficient, showing potential for clinical utility and outperforming existing models on validation metrics."}}
{"id": "2507.09665", "pdf": "https://arxiv.org/pdf/2507.09665", "abs": "https://arxiv.org/abs/2507.09665", "authors": ["Saima Afrin", "Bowen Xu", "Antonio Mastropaolo"], "title": "Is Quantization a Deal-breaker? Empirical Insights from Large Code Models", "categories": ["cs.SE"], "comment": null, "summary": "The growing scale of large language models (LLMs) not only demands extensive\ncomputational resources but also raises environmental concerns due to their\nincreasing carbon footprint. Model quantization emerges as an effective\napproach that can reduce the resource demands of LLMs by decreasing parameter\nprecision without substantially affecting performance (e.g., 16 bit to 4 bit).\nWhile recent studies have established quantization as a promising approach for\noptimizing large code models (LCMs), a specialized subset of LLMs tailored for\nautomated software engineering, their findings offer only limited insights into\nits practical implications. Specifically, current investigations focus only on\nthe functional correctness of the code generated by quantized models,\nneglecting how quantization impacts critical aspects of code quality such as\nreliability, maintainability, and security. To bridge this gap, our study\ninvestigates the effects of quantization on the qualitative aspects of\nautomatically generated code. We apply Activation-aware Weight Quantization\n(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate\nJava and Python code. Using state-of-the-art static analysis tools, we evaluate\nsoftware quality metrics and static features including cyclomatic complexity,\ncognitive complexity, and lines of code. Our findings reveal that quantization\nis a robust technique that not only preserves functional correctness, but also\nretains key qualitative code attributes sought after by developers, such as\nmaintainability and structural simplicity.", "AI": {"tldr": "The paper explores how model quantization, specifically Activation-aware Weight Quantization (AWQ), impacts functional correctness and qualitative aspects of code generated by large language models tailored for software engineering.", "motivation": "Reducing computational resource demands and environmental impact of large language models, while preserving software quality in automatically generated code.", "method": "Use AWQ quantization on two code models (CodeLlama and DeepSeekCoder) to generate Java and Python code, followed by evaluation using metrics like cyclomatic complexity, cognitive complexity, and lines of code.", "result": "Quantized models maintain their functional correctness while retaining essential code quality attributes like maintainability and simplicity.", "conclusion": "Quantization proves to be a viable technique for optimizing code generation models without compromising software quality."}}
{"id": "2507.09470", "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "categories": ["cs.CL", "cs.AI", "68T07"], "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "This paper optimizes the DRAGON Longformer model for binary classification of clinical text, achieving significant accuracy improvements through tailored adjustments.", "motivation": "The study aimed to improve domain-specific NLP capability in understanding medical text, addressing challenges in classifying clinical observations.", "method": "The authors optimized a pre-trained Longformer model using hyperparameter tuning, extending sequence length, adjusting learning rates, adding medical terminology, and validating results statistically.", "result": "Performance metrics significantly improved, including accuracy (72.0% to 85.2%), precision, recall, and F1-score, with statistical significance (p < .001).", "conclusion": "The optimized model exhibits superior ability for medical text classification, advancing capabilities in clinical NLP with implications for healthcare applications."}}
{"id": "2507.09822", "pdf": "https://arxiv.org/pdf/2507.09822", "abs": "https://arxiv.org/abs/2507.09822", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "title": "Active Probing with Multimodal Predictions for Motion Planning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "AI": {"tldr": "The paper presents a unified framework for autonomous navigation in dynamic environments by integrating trajectory planning, multimodal predictions, and active probing to handle uncertainty.", "motivation": "The aim is to enable autonomous systems to navigate dynamic environments effectively by addressing uncertainties in the behavior of other agents.", "method": "The authors develop a novel risk metric compatible with Gaussian mixture models, allowing a closed-form solution. They employ active probing to strategically reduce prediction ambiguities while maintaining multimodal uncertainty handling.", "result": "Extensive evaluations in the MetaDrive simulation environment show the framework's ability to skillfully navigate complex traffic scenarios and perform well across different agent behavior models.", "conclusion": "The proposed framework is robust and broadly applicable, offering enhanced decision-making for autonomous navigation in uncertain and dynamic settings."}}
{"id": "2507.09850", "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "categories": ["cs.AI"], "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "AI": {"tldr": "The study examines methods to induce long chain-of-thought (CoT) reasoning in base language models through minimal tuning and shows significant performance improvement using just 20 high-quality CoT examples.", "motivation": "The paper aims to explore whether long reasoning chains can be induced in base models using minimal tuning or prompting, given previous success with model distillation and reinforcement learning approaches.", "method": "The authors lightly fine-tuned the base model Qwen2.5-32B using only 20 long CoT examples from the reasoning model QwQ-32B-Preview. They also investigated using CoT data from non-reasoning models and human annotators, employing various enhancement techniques.", "result": "The fine-tuned model outperformed a larger model, Qwen2.5-Math-72B-Instruct, proving that a small set of high-quality examples can significantly improve reasoning. However, data from non-reasoning models and human annotators could not match the performance of reasoning model traces.", "conclusion": "Carefully curated, small-scale CoT supervision, particularly using examples from reasoning models, can activate reasoning behaviors in base models. However, expert CoT traces hold latent qualities that are challenging to replicate by other means, necessitating further research."}}
{"id": "2507.09180", "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "The paper proposes a vision transformer-based backbone to fuse RGB and depth data, paired with contrastive unsupervised learning and curriculum learning for sim2real tasks.", "motivation": "Depth data provides robust spatial insights and complements RGB data for generalization, urging exploration of fusion techniques in vision tasks.", "method": "Separate CNN stems process RGB and depth data, fused for a scalable vision transformer. Contrastive learning with masked/unmasked tokens boosts reinforcement learning efficiency, paired with curriculum learning for domain randomization.", "result": "Enhanced sample efficiency and sim2real transfer achieved through fusion techniques, contrastive learning, and flexible curriculum scheduling.", "conclusion": "The integration of RGB-depth fusion and learning innovations advances performance in reinforcement learning and sim2real transfer scenarios."}}
{"id": "2507.08877", "pdf": "https://arxiv.org/pdf/2507.08877", "abs": "https://arxiv.org/abs/2507.08877", "authors": ["Hanlong Zhang", "Jingsheng Yang", "Hao Li", "Yuhao He", "Franck Gong"], "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.", "AI": {"tldr": "The paper proposes Oriented Distillation for Inline Acceleration (ODIA), a method to reduce latency in LLM-based function calling by distilling knowledge into smaller models to handle simple queries.", "motivation": "High latency in LLM-based Function Calling negatively impacts user experience, necessitating an efficient solution.", "method": "The ODIA framework identifies simple queries based on real-world traffic and uses knowledge distillation from larger models to smaller ones while updating itself through automated data collection.", "result": "The method achieved a 45% reduction in average latency and a 78% reduction in median latency, successfully routing 60% of traffic to smaller models with minimal accuracy loss.", "conclusion": "ODIA is a practical and dynamic solution for reducing latency in production environments with negligible trade-offs in accuracy."}}
{"id": "2507.09682", "pdf": "https://arxiv.org/pdf/2507.09682", "abs": "https://arxiv.org/abs/2507.09682", "authors": ["Laura Baird", "Armin Moin"], "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.", "AI": {"tldr": "The paper presents OrQstrator, a modular framework using Deep Reinforcement Learning for quantum circuit optimization tailored to NISQ-era hardware constraints.", "motivation": "Current quantum circuits face challenges such as noise impact and inefficient optimization, especially in the NISQ era, necessitating advanced strategies to enhance their scalability and hardware efficiency.", "method": "OrQstrator employs three optimization modules: a DRL-based circuit rewriter, a domain-specific resynthesizer, and a parameterized circuit optimizer, coordinated by an orchestration engine that integrates hardware-aware features.", "result": "The proposed system intelligently selects optimization strategies, delivering circuits optimized for depth, gate count, fidelity, and backend-specific constraints.", "conclusion": "OrQstrator effectively addresses NISQ-era challenges by leveraging modular optimization and adaptive orchestration, providing a pathway for more efficient quantum circuit design and execution."}}
{"id": "2507.09474", "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "categories": ["cs.CL"], "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "The CoNLL-2013 shared task addressed grammatical error correction and evaluated teams using specific metrics and data sets.", "motivation": "To advance methods for grammatical error correction through a shared task format, enabling participants to compare methods and gather insights.", "method": "The paper defined grammatical error correction, provided annotated data sets, described evaluation metrics, and assessed team performance in shared task participation.", "result": "Various approaches from participating teams were overviewed, and their performance was evaluated using predefined metrics.", "conclusion": "Insights into grammatical error correction methods and evaluations were presented, facilitating progress in computational linguistics error correction technologies."}}
{"id": "2507.09836", "pdf": "https://arxiv.org/pdf/2507.09836", "abs": "https://arxiv.org/abs/2507.09836", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "AI": {"tldr": "This paper introduces Multi-Residual Mixture of Expert Learning (MRMEL), a method to enhance Lagrangian traffic control for autonomous vehicles by combining mixture of experts and residual reinforcement learning, leading to improved eco-driving performance.", "motivation": "The motivation is to address the challenge of designing robust and effective Lagrangian traffic control policies for autonomous vehicles, given the diversity and complexity in real-world traffic scenarios.", "method": "The method is based on a novel framework called MRMEL, which augments a suboptimal nominal policy with a learned residual and dynamically selects the most suitable nominal policy from a pool of policies, modeled as a mixture of experts and conditioned on traffic scenarios.", "result": "Using real-world traffic scenarios and a case study in cooperative eco-driving at signalized intersections, MRMEL achieved 4%-9% additional reductions in aggregate vehicle emissions compared to the strongest baseline policies.", "conclusion": "MRMEL demonstrates a robust solution to manage diverse traffic scenarios, proving its effectiveness in enhancing autonomous vehicle control for sustainable transportation goals while outperforming baseline approaches."}}
{"id": "2507.09854", "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "categories": ["cs.AI"], "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "AI": {"tldr": "The paper reinterprets instruction-tuned large language models as model-grounded symbolic AI systems, exploring novel learning and reasoning approaches within this framework.", "motivation": "To exploit the complementary strengths of neural networks and symbolic AI for enhancing learning efficiency and reasoning reliability, the authors propose reinterpreting large language models as symbolic systems grounded in their internal representation space.", "method": "The authors utilize large language models as symbolic layers integrated with grounding in internal representation spaces. They explore learning and reasoning approaches inspired by traditional paradigms.", "result": "Preliminary evaluations demonstrate improved learning efficiency and reasoning reliability in axiomatic deductive reasoning tasks of varying complexity.", "conclusion": "This work provides promising insights into the potential of instruction-tuned large language models as hybrid systems, advancing neurosymbolic AI's ability to leverage structured reasoning while maintaining robust learning capabilities."}}
{"id": "2507.09183", "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "This paper addresses Few-Shot Class-Incremental Learning (FSCIL) challenges by introducing LGSP-Prompt, a spatial prompting method, avoiding token-dimension saturation and improving knowledge retention as well as incremental learning.", "motivation": "The motivation is to tackle the dual challenges of data scarcity and incremental learning in FSCIL settings, while addressing the limitations of prompt pool methods in such environments.", "method": "The method involves shifting prompt learning from token dimensions to spatial dimensions via LGSP-Prompt, generating dynamic spatial prompts by combining local spatial features and global frequency-domain representations.", "result": "Extensive experiments showed that LGSP-Prompt achieved state-of-the-art performance across multiple FSCIL benchmarks, excelling in both preserving base knowledge and facilitating incremental learning.", "conclusion": "LGSP-Prompt mitigates token-dimension saturation and provides an effective solution to improve FSCIL tasks, enabling better handling of minimal data and session increments."}}
{"id": "2507.08905", "pdf": "https://arxiv.org/pdf/2507.08905", "abs": "https://arxiv.org/abs/2507.08905", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "G\u00f6ran Falkman", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Last Layer Hamiltonian Monte Carlo", "categories": ["cs.LG", "cs.AI", "G.3"], "comment": "25 pages, 15 figures, 6 tables, currently under submission", "summary": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.", "AI": {"tldr": "The paper explores using Last Layer Hamiltonian Monte Carlo (LL-HMC) sampling for uncertainty estimation in deep neural networks, particularly in computationally constrained settings, and evaluates its effectiveness against other methods.", "motivation": "The motivation is to address the computational challenges of using full HMC sampling in large-scale DNNs, specifically by focusing on the last layer of a DNN to make probabilistic methods applicable to data-intensive real-world scenarios.", "method": "The authors implement LL-HMC, a technique that restricts HMC sampling to the final layer of a DNN, and compare it against five other last-layer probabilistic methods on three video datasets. They test each method's performance on classification, calibration, and out-of-distribution detection, with repeated grid searches for better hyperparameter tuning.", "result": "LL-HMC demonstrates competitive in-distribution classification and OOD detection, though additional sampled last-layer parameters improve only OOD detection. Using multiple sampling chains did not consistently enhance the results.", "conclusion": "LL-HMC is a computationally efficient and competitive method for introducing uncertainty into deep neural networks, particularly well-suited for limited-resource scenarios. However, its advantages are more pronounced for OOD detection than classification."}}
{"id": "2507.09477", "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "This survey discusses combining reasoning and retrieval for enhancing factuality and inference in LLMs, with frameworks that iteratively integrate search and reasoning achieving state-of-the-art results.", "motivation": "To address the limitations of RAG in handling multi-step inference and reasoning-focused methods in maintaining factuality.", "method": "The paper categorizes the advancements in reasoning-enhancing RAG frameworks and highlights frameworks interleaving iterative search and reasoning.", "result": "Emerging synergistic RAG-Reasoning frameworks demonstrate superior performance on knowledge-intensive benchmarks.", "conclusion": "Future research could focus on developing more effective, multimodal, trustworthy, and human-centric RAG-Reasoning systems."}}
{"id": "2507.09857", "pdf": "https://arxiv.org/pdf/2507.09857", "abs": "https://arxiv.org/abs/2507.09857", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "categories": ["cs.RO", "cs.CR"], "comment": "IJCAI'2025", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "AI": {"tldr": "The paper introduces AdvGrasp, a framework for physical adversarial attacks on robotic grasping, targeting lift capability and grasp stability by deforming object shapes to compromise grasp performance.", "motivation": "To evaluate and enhance the robustness of robotic grasping systems by addressing physical principles often overlooked in neural network-focused studies.", "method": "AdvGrasp uses object shape deformation to increase gravitational torque and diminish wrench space stability margin, systematically affecting grasp capability and stability.", "result": "Experiments across diverse scenarios and real-world validations demonstrate the effectiveness, robustness, and practical applicability of the AdvGrasp framework.", "conclusion": "AdvGrasp successfully provides insights to evaluate and improve robotic grasping robustness, showcasing a physically-grounded approach to adversarial attacks."}}
{"id": "2507.09884", "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "categories": ["cs.AI"], "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "AI": {"tldr": "This study introduces VerifyBench, a benchmark designed for evaluating verifiers used alongside LLMs in cross-domain tasks, addressing the limitations of specialized and general verifiers.", "motivation": "The paper aims to tackle the challenge of evaluating the consistency of model-generated responses in reinforcement learning processes involving LLMs, especially given the complexity and diversity of responses.", "method": "VerifyBench was designed with 4,000 expert-level cross-domain questions and employs a four-dimensional framework comparing verifier performance across various structured outputs using multidisciplinary expert annotations.", "result": "Specialized verifiers exhibit high accuracy but low recall, while general verifiers perform inclusively but with unstable precision. Verifiers also showcase sensitivity to input structure and limitations in cross-domain generalization.", "conclusion": "The findings highlight fundamental trade-offs in verifier designs and underline bottlenecks in their current technologies, providing directions for improving RLVR systems."}}
{"id": "2507.09184", "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "The paper investigates and addresses the issue of hallucinations in Large Vision Language Models (LVLMs) caused by alignment bias originating from long-term decay in Rotary Position Encoding (RoPE).", "motivation": "The motivation for the research is to tackle the problem of hallucinations in LVLMs caused by misaligned multimodal features, particularly focusing on long-term decay in positional modeling methods.", "method": "The authors introduce MCA-LLaVA, a method based on Manhattan distance that modifies positional modeling by extending long-term decay into a two-dimensional, multi-directional spatial decay, improving the model's perception of image tokens across spatial locations.", "result": "MCA-LLaVA demonstrates significant improvements in mitigating hallucinations and achieves strong performance across various benchmarks related to hallucination and general evaluation of LVLMs.", "conclusion": "The proposed MCA-LLaVA effectively reduces image alignment bias, enhancing multimodal alignment and the LVLMs' robustness against hallucinations. The approach is verified to be effective and generalizable."}}
{"id": "2507.08912", "pdf": "https://arxiv.org/pdf/2507.08912", "abs": "https://arxiv.org/abs/2507.08912", "authors": ["Tomasz Szandala", "Fatima Ezzeddine", "Natalia Rusin", "Silvia Giordano", "Omran Ayoub"], "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox", "AI": {"tldr": "The paper introduces Fair-FLIP, a method to increase fairness in deepfake detection by adjusting final-layer inputs while retaining near-baseline accuracy.", "motivation": "Deepfake detection models often show biases based on demographic attributes such as gender and ethnicity, threatening equitable trust in AI systems.", "method": "Fair-FLIP, a post-processing technique, reweights final-layer inputs of trained models by prioritizing features with low variability and demoting those with high variability.", "result": "Fair-FLIP improved fairness metrics by up to 30% and maintained baseline accuracy with negligible reduction (0.25%) in performance.", "conclusion": "Fair-FLIP effectively reduces demographic biases in deepfake detection models with minimal trade-offs, contributing to more equitable AI-generated content detection methods."}}
{"id": "2507.09820", "pdf": "https://arxiv.org/pdf/2507.09820", "abs": "https://arxiv.org/abs/2507.09820", "authors": ["Jia Yi Goh", "Shaun Khoo", "Nyx Iskandar", "Gabriel Chua", "Leanne Tan", "Jessica Foo"], "title": "Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications", "categories": ["cs.SE", "cs.CY"], "comment": null, "summary": "Most safety testing efforts for large language models (LLMs) today focus on\nevaluating foundation models. However, there is a growing need to evaluate\nsafety at the application level, as components such as system prompts,\nretrieval pipelines, and guardrails introduce additional factors that\nsignificantly influence the overall safety of LLM applications. In this paper,\nwe introduce a practical framework for evaluating application-level safety in\nLLM systems, validated through real-world deployment across multiple use cases\nwithin our organization. The framework consists of two parts: (1) principles\nfor developing customized safety risk taxonomies, and (2) practices for\nevaluating safety risks in LLM applications. We illustrate how the proposed\nframework was applied in our internal pilot, providing a reference point for\norganizations seeking to scale their safety testing efforts. This work aims to\nbridge the gap between theoretical concepts in AI safety and the operational\nrealities of safeguarding LLM applications in practice, offering actionable\nguidance for safe and scalable deployment.", "AI": {"tldr": "The paper introduces a practical framework for evaluating safety at the application-level in large language model (LLM) systems, addressing gaps between theoretical AI safety and operational deployment.", "motivation": "The authors noticed a lack of safety evaluation methods for application-level LLM systems, focusing on real-world deployment factors like prompts and guardrails.", "method": "The framework includes principles for building safety risk taxonomies and practices for evaluating LLM application safety, tested through internal pilot studies in real-world use cases.", "result": "The framework was successfully validated during its internal deployment, proving its utility and offering a guideline for scaling safety efforts.", "conclusion": "This work emphasizes the need for practical safety measures in LLM applications and provides actionable solutions to enhance the safe deployment of such systems."}}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "The paper introduces M2SaG, a multimodal sarcasm generation dataset paired with ViSP, a framework leveraging reward-based and contrastive learning techniques to improve sarcasm text creation outperforming existing models.", "motivation": "Sarcasm research lacks focus on generation capabilities, particularly in leveraging visual cues and handling content-intent mismatch.", "method": "The authors created a novel dataset, M2SaG, and devised ViSP framework combining Proximal Policy Optimization and contrastive learning to fine-tune sarcastic text generation.", "result": "ViSP outperformed baselines, including large language models, producing texts with higher Sarcasm Scores (0.898 vs. 0.770) and higher Factual Incongruity (0.768 vs. 0.739).", "conclusion": "The study highlights multimodal approaches' superiority for sarcastic text generation and public release aims to advance research in this domain."}}
{"id": "2507.09858", "pdf": "https://arxiv.org/pdf/2507.09858", "abs": "https://arxiv.org/abs/2507.09858", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "categories": ["cs.RO"], "comment": "accepted to IEEE RA-L", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "AI": {"tldr": "The paper proposes a novel method to customize the topological properties of harmonic potential fields for robot navigation in complex environments such as forest worlds.", "motivation": "There is a lack of existing approaches that allow customization of harmonic potential fields and the resulting paths' topological properties for safe robot navigation.", "method": "They introduced a hybrid optimization technique that automatically determines homotopy classes of paths, optimizes tree structures, and fine-tunes weight parameters using diffomorphic transformations and gradient descent.", "result": "The proposed method was validated through simulations and hardware experiments, showcasing the capability to customize navigation paths with desired homotopic properties.", "conclusion": "The approach simplifies robot navigation design, ensures safety and convergence, and allows for flexible adaptations to complex environments."}}
{"id": "2507.09955", "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "categories": ["cs.AI"], "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "AI": {"tldr": "DeepSeek has introduced innovative, cost-effective, high-performing open-source AI models, challenging the AI landscape with novel algorithms and engineering advancements.", "motivation": "To address the challenges of building cost-effective and highly efficient large AI models while fostering innovation in the AI field.", "method": "The paper reviews paradigm shifts in AI model development, presents DeepSeek's unique algorithms (e.g., MLA, MoE, etc.), and examines engineering enhancements in scaling, training, and optimization.", "result": "DeepSeek's V3 and R1 models demonstrate competitive advantages over mainstream LLMs and firm their position as industry disruptors across varied AI applications.", "conclusion": "DeepSeek's innovative methodologies and engineering breakthroughs not only reshape the AI landscape but also provide forward-looking insights into the evolution of large AI model development."}}
{"id": "2507.09200", "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "categories": ["cs.CV"], "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "The paper introduces THYME, a novel approach to improve dynamic scene graph generation with a new aerial dataset and achieves superior performance over state-of-the-art methods.", "motivation": "The rapid expansion of video-based applications has created a need for effective methods to understand dynamic scenes, which previous methods fail to address comprehensively.", "method": "THYME combines hierarchical feature aggregation and cyclic temporal refinement to simultaneously capture spatial details and temporal dependencies.", "result": "THYME surpasses state-of-the-art methods in experiments conducted on ASPIRe and the newly introduced AeroEye-v1.0 dataset, demonstrating improved dynamic scene understanding.", "conclusion": "THYME presents a robust solution for dynamic scene graph generation, underpinned by the AeroEye-v1.0 dataset, offering better spatial and temporal coherence than previous approaches."}}
{"id": "2507.08913", "pdf": "https://arxiv.org/pdf/2507.08913", "abs": "https://arxiv.org/abs/2507.08913", "authors": ["Qi He", "Peiran Yu", "Ziyi Chen", "Heng Huang"], "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Shuffling-type gradient methods are favored in practice for their simplicity\nand rapid empirical performance. Despite extensive development of convergence\nguarantees under various assumptions in recent years, most require the\nLipschitz smoothness condition, which is often not met in common machine\nlearning models. We highlight this issue with specific counterexamples. To\naddress this gap, we revisit the convergence rates of shuffling-type gradient\nmethods without assuming Lipschitz smoothness. Using our stepsize strategy, the\nshuffling-type gradient algorithm not only converges under weaker assumptions\nbut also match the current best-known convergence rates, thereby broadening its\napplicability. We prove the convergence rates for nonconvex, strongly convex,\nand non-strongly convex cases, each under both random reshuffling and arbitrary\nshuffling schemes, under a general bounded variance condition. Numerical\nexperiments further validate the performance of our shuffling-type gradient\nalgorithm, underscoring its practical efficacy.", "AI": {"tldr": "The paper focuses on shuffling-type gradient methods, addressing convergence without requiring Lipschitz smoothness, and achieves solid convergence rates for various convexity cases.", "motivation": "Shuffling-type gradient methods are popular for their simplicity and performance, but they require the restrictive Lipschitz smoothness condition, which may not hold for many machine learning models.", "method": "The paper revisits shuffling-type gradient methods by proposing a stepsize strategy to enable convergence without Lipschitz smoothness, analyzing random reshuffling and arbitrary shuffling schemes.", "result": "It demonstrates convergence rates for nonconvex, strongly convex, and non-strongly convex cases under general bounded variance conditions, achieving the best-known rates.", "conclusion": "The study broadens the applicability of shuffling-type gradient methods by relaxing assumptions, supported by theoretical guarantees and numerical experiments validating practical efficacy."}}
{"id": "2507.08867", "pdf": "https://arxiv.org/pdf/2507.08867", "abs": "https://arxiv.org/abs/2507.08867", "authors": ["Malte Algren", "Tobias Golling", "Francesco Armando Di Bello", "Christopher Pollard"], "title": "Mind the Gap: Navigating Inference with Optimal Transport Maps", "categories": ["physics.data-an", "cs.LG", "hep-ex", "stat.ML"], "comment": "23 pages, 13 figures", "summary": "Machine learning (ML) techniques have recently enabled enormous gains in\nsensitivity across the sciences. In particle physics, much of this progress has\nrelied on excellent simulations of a wide range of physical processes. However,\ndue to the sophistication of modern machine learning (ML) algorithms and their\nreliance on high-quality training samples, discrepancies between simulation and\nexperimental data can significantly limit the effectiveness of ML techniques.\nIn this work, we present a solution to this ``mis-specification'' problem: a\ncalibration approach based on optimal transport, which we apply to\nhigh-dimensional simulations for the first time. We demonstrate the performance\nof our approach through jet tagging, using a CMS-inspired dataset. A\n128-dimensional internal jet representation from a powerful general-purpose\nclassifier is studied; after calibrating this internal ``latent''\nrepresentation, we find that a wide variety of quantities derived from it for\ndownstream tasks are also properly calibrated: using this calibrated\nhigh-dimensional representation, powerful new applications of jet flavor\ninformation can be utilized in LHC analyses. This is a key step toward allowing\nproperly-calibrated ``foundation models'' in particle physics. More broadly,\nthis calibration framework has broad applications for correcting\nhigh-dimensional simulations across the sciences.", "AI": {"tldr": "The paper proposes a calibration method using optimal transport to address discrepancies between simulation and experimental data in machine learning applications for particle physics.", "motivation": "The motivation is to address the limitations of machine learning methods caused by discrepancies between high-quality simulations and experimental data in particle physics, which hinder progress.", "method": "The authors develop a calibration method based on optimal transport and apply it to high-dimensional simulations. They demonstrate its effectiveness on latent representations in jet tagging using a CMS-inspired dataset.", "result": "The calibrated latent representation is shown to improve calibration for various derived quantities, enabling new applications of jet flavor information in LHC analyses.", "conclusion": "The method represents a significant step toward properly-calibrated foundation models for particle physics, with potential applications across various scientific domains."}}
{"id": "2507.09866", "pdf": "https://arxiv.org/pdf/2507.09866", "abs": "https://arxiv.org/abs/2507.09866", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "Turning the Tide: Repository-based Code Reflection", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code large language models (LLMs) enhance programming by understanding and\ngenerating code across languages, offering intelligent feedback, bug detection,\nand code updates through reflection, improving development efficiency and\naccessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code\ngeneration and real-world relevance, previous works ignore the scenario of\nmodifying code in repositories. Considering challenges remaining in improving\nreflection capabilities and avoiding data contamination in dynamic benchmarks,\nwe introduce LiveRepoReflection, a challenging benchmark for evaluating code\nunderstanding and generation in multi-file repository contexts, featuring 1,888\nrigorously filtered test cases across $6$ programming languages to ensure\ndiversity, correctness, and high difficulty. Further, we create\nRepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning\ndataset derived from diverse sources, used to train RepoReflectionCoder through\na two-turn dialogue process involving code generation and error-driven repair.\nThe leaderboard evaluates over 40 LLMs to reflect the model performance of\nrepository-based code reflection.", "AI": {"tldr": "This paper introduces LiveRepoReflection, a benchmark for evaluating code understanding and generation in multi-file repository contexts, and RepoReflection-Instruct, a dataset for training models in code reflection tasks.", "motivation": "The motivation is to address the gap in evaluating LLMs for modifying and reflecting on code in multi-file repository contexts, which remains challenging due to a lack of benchmarks and to avoid data contamination.", "method": "The authors propose LiveRepoReflection, a benchmark with 1,888 test cases in multiple programming languages, and RepoReflection-Instruct, an instruction-tuning dataset. They also introduce RepoReflectionCoder, a model trained via two-turn dialogues for code generation and repair.", "result": "The study presents a leaderboard where over 40 LLMs are evaluated, demonstrating their performance on repository-based code reflection tasks.", "conclusion": "The research highlights improved assessment tools for LLMs in repository settings and lays the foundation for enhancing LLM reflection capabilities in complex programming scenarios."}}
{"id": "2507.09485", "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "The paper addresses aspect-based sentiment analysis (ABSA) by proposing a new method using large language models (LLMs) and reinforcement learning to optimize data augmentation techniques, achieving superior results.", "motivation": "Current aspect-based sentiment analysis methods face challenges due to short text contexts, small and imbalanced training datasets, and a dominance of positive sentiment labels. Improved data augmentation is necessary to enhance model performance in these scenarios.", "method": "The proposed method involves prompting large language models (LLMs) to generate augmented training data based on original datasets, ensuring a larger and more balanced label distribution. Reinforcement learning is used to improve the quality of the augmented synthetic data.", "result": "The approach demonstrates superior performance over strong baselines and many existing studies on English benchmark datasets for ABSA.", "conclusion": "The proposed LLM-based approach with optimized data augmentation effectively enhances performance in ABSA tasks by addressing limitations in data size, balance, and context quality."}}
{"id": "2507.09985", "pdf": "https://arxiv.org/pdf/2507.09985", "abs": "https://arxiv.org/abs/2507.09985", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "categories": ["cs.RO", "cs.AI"], "comment": "Published at R:SS 2025", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "AI": {"tldr": "Octopi-1.5, an improved visual-tactile-language model, introduces multi-part tactile signal processing and retrieval-augmented generation to enhance task performance and learning. It includes a user-friendly handheld tactile interface for practical demonstrations.", "motivation": "To advance robotic manipulation and material identification through touch-based models and provide accessible setups for non-robot users.", "method": "Octopi-1.5 uses tactile signals from multiple object parts and incorporates a retrieval-augmented generation module for enhanced adaptability. It can interact via a handheld tactile-enabled interface equipped with GelSight and TAC-02 sensors.", "result": "The model demonstrates improved inference capabilities, learning new objects on-the-fly, and showcases practical interaction through a Guessing Game and teaching new items.", "conclusion": "Octopi-1.5 highlights both progress and current limitations in visual-tactile-language models, aiming to encourage further research in touch-based AI technologies."}}
{"id": "2507.09989", "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "AI": {"tldr": "The paper introduces the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm to resolve conflicts between monotonic improvement and Partial Parameter-sharing (ParPS) in heterogeneous multi-agent reinforcement learning (MARL).", "motivation": "To address the challenges in achieving monotonic improvement and high cooperative performance in heterogeneous MARL while reconciling the limitations of Partial Parameter-sharing (ParPS).", "method": "The paper develops the OMDPG algorithm, which replaces sequential Q-computations with Optimal Marginal Q (OMQ), uses a Generalized Q Critic (GQC) for stable Q-value estimation, and implements a Centralized Critic Grouped Actor (CCGA) architecture for both ParPS-based local policy networks and accurate Q-function computation.", "result": "Experimental evaluations in SMAC and MAMuJoCo environments show that OMDPG surpasses state-of-the-art MARL baselines in performance.", "conclusion": "OMDPG effectively addresses the issue of policy updating baseline drift while achieving enhanced cooperative performance through a novel algorithmic approach and architecture design."}}
{"id": "2507.09207", "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "This paper presents a method to infer a material's thickness and stiffness using video-based wave propagation analysis.", "motivation": "To enable non-invasive, video-based estimation of physical properties such as thickness and stiffness of materials or tissues for potential applications in health monitoring and other fields.", "method": "The method extracts wave dispersion relations from videos and solves a physics-based optimization problem to infer thickness and stiffness parameters.", "result": "Validated on both simulated and real data, the method shows strong agreement with actual measurements, demonstrating its efficacy.", "conclusion": "The proposed technique is a promising step toward non-invasive health monitoring and has potential applications in human-computer interaction as well."}}
{"id": "2507.08956", "pdf": "https://arxiv.org/pdf/2507.08956", "abs": "https://arxiv.org/abs/2507.08956", "authors": ["Zhenghan Fang", "Mateo D\u00edaz", "Sam Buchanan", "Jeremias Sulam"], "title": "Beyond Scores: Proximal Diffusion Models", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Diffusion models have quickly become some of the most popular and powerful\ngenerative models for high-dimensional data. The key insight that enabled their\ndevelopment was the realization that access to the score -- the gradient of the\nlog-density at different noise levels -- allows for sampling from data\ndistributions by solving a reverse-time stochastic differential equation (SDE)\nvia forward discretization, and that popular denoisers allow for unbiased\nestimators of this score. In this paper, we demonstrate that an alternative,\nbackward discretization of these SDEs, using proximal maps in place of the\nscore, leads to theoretical and practical benefits. We leverage recent results\nin proximal matching to learn proximal operators of the log-density and, with\nthem, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that\n$\\widetilde{O}(d/\\sqrt{\\varepsilon})$ steps suffice for the resulting\ndiscretization to generate an $\\varepsilon$-accurate distribution w.r.t. the KL\ndivergence. Empirically, we show that two variants of ProxDM achieve\nsignificantly faster convergence within just a few sampling steps compared to\nconventional score-matching methods.", "AI": {"tldr": "Diffusion models benefit from an alternative backward discretization using proximal maps, improving convergence speed and theoretical guarantees.", "motivation": "Current methods for generative modeling rely on score-based approaches, but these can be improved in sampling efficiency and KL divergence accuracy.", "method": "Introduce Proximal Diffusion Models (ProxDM) by using proximal operators instead of scores for backward SDE discretization, leveraging proximal matching.", "result": "ProxDM achieves $\nO(d/\\sqrt{\\varepsilon})$ steps for $\n\\varepsilon$-accuracy and demonstrates faster empirical convergence in various implementations.", "conclusion": "Using proximal maps provides both theoretical advantages and practical benefits for improving generative modeling with diffusion methods."}}
{"id": "2507.08896", "pdf": "https://arxiv.org/pdf/2507.08896", "abs": "https://arxiv.org/abs/2507.08896", "authors": ["Byunghee Lee", "Hye Yeon Sin", "Joonsung Kang"], "title": "Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "This study introduces an integrated framework for predictive causal inference\ndesigned to overcome limitations inherent in conventional single model\napproaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial\nhealth state estimation with a Multi Task and Multi Graph Convolutional Network\n(MTGCN) for capturing temporal outcome trajectories. The framework\nasymmetrically treats temporal and spatial information regarding them as\nendogenous variables in the outcome regression, and exogenous variables in the\npropensity score model, thereby expanding the standard doubly robust treatment\neffect estimation to jointly enhance bias correction and predictive accuracy.\nTo demonstrate its utility, we focus on clinical domains such as cancer,\ndementia, and Parkinson disease, where treatment effects are challenging to\nobserve directly. Simulation studies are conducted to emulate latent disease\ndynamics and evaluate the model performance under varying conditions. Overall,\nthe proposed framework advances predictive causal inference by structurally\nadapting to spatiotemporal complexities common in biomedical data.", "AI": {"tldr": "This paper introduces an integrated framework combining HMM and MTGCN for predictive causal inference, focusing on spatiotemporal complexities in biomedical data.", "motivation": "The paper addresses the limitations of conventional single-model approaches in predictive causal inference, especially in handling spatiotemporal complexities in clinical data.", "method": "The framework integrates a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for temporal outcome trajectories. It uses an asymmetric treatment of spatial and temporal data for doubly robust treatment effect estimation.", "result": "Simulation studies evaluated the framework's ability to emulate latent disease dynamics and perform under varying conditions.", "conclusion": "The framework enhances predictive causal inference by structurally addressing spatiotemporal complexities, particularly in clinical domains like cancer, dementia, and Parkinson's disease."}}
{"id": "2507.09892", "pdf": "https://arxiv.org/pdf/2507.09892", "abs": "https://arxiv.org/abs/2507.09892", "authors": ["Zimu Chen", "Di Wang"], "title": "PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths", "categories": ["cs.SE", "D.2.5"], "comment": "10 pages, 1 figure", "summary": "Estimating worst-case resource consumption is a critical task in software\ndevelopment. The worst-case analysis (WCA) problem is an optimization-based\nabstraction of this task. Fuzzing and symbolic execution are widely used\ntechniques for addressing the WCA problem. However, improving code coverage in\nfuzzing or managing path explosion in symbolic execution within the context of\nWCA poses significant challenges. In this paper, we propose PathFuzzing, aiming\nto combine the strengths of both techniques to design a WCA method. The key\nidea is to transform a program into a symbolic one that takes an execution path\n(encoded as a binary string) and interprets the bits as branch decisions.\nPathFuzzing then applies evolutionary fuzzing techniques to the transformed\nprogram to search for binary strings that represent satisfiable path conditions\nand lead to high resource consumption. We evaluate the performance of\nPathFuzzing experimentally on a benchmark suite that consists of prior work's\nbenchmarks and some added by us. Results show that PathFuzzing generally\noutperforms a fuzzing and a symbolic-execution baseline.", "AI": {"tldr": "PathFuzzing combines fuzzing and symbolic execution for worst-case resource consumption analysis, achieving better performance compared to baseline methods.", "motivation": "To address the limitations in code coverage and path explosion in the context of Worst-Case Analysis (WCA) using existing fuzzing and symbolic execution techniques.", "method": "Proposed PathFuzzing, which transforms code into a symbolic form interpreted as paths and uses evolutionary fuzzing to find high resource consumption scenarios.", "result": "Experimental evaluation showed PathFuzzing consistently outperforms existing fuzzing and symbolic execution baselines over a tested benchmark suite.", "conclusion": "PathFuzzing is a promising hybrid approach for WCA, effectively combining the strengths of fuzzing and symbolic execution."}}
{"id": "2507.09497", "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "categories": ["cs.CL"], "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax is a multi-agent collaboration framework designed to improve adaptability, coordination, and knowledge reuse in complex tasks.", "motivation": "Overcome limitations of traditional AI systems in coordination, memory reuse, and scalability for complex tasks.", "method": "GoalfyMax employs a protocol-driven framework with A2A communication, structured memory layers, and advanced safety and dialogue features.", "result": "Empirical benchmarks show GoalfyMax's superior performance in adaptability and experience retention compared to alternatives.", "conclusion": "GoalfyMax offers a scalable approach for building advanced multi-agent intelligent systems suitable for dynamic tasks."}}
{"id": "2507.10003", "pdf": "https://arxiv.org/pdf/2507.10003", "abs": "https://arxiv.org/abs/2507.10003", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "categories": ["cs.RO"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "AI": {"tldr": "This study integrates advanced autonomy solutions into Ariel, an underwater robot, using vision-based systems and refraction-aware state estimation methods. Field tests demonstrate successful performance in challenging underwater environments.", "motivation": "The paper aims to address the challenges of underwater exploration and inspection, particularly in visually degraded environments, by equipping robots with more robust vision-driven autonomy capabilities.", "method": "The authors developed and integrated a vision-driven autonomy system into Ariel, using a multi-camera visual-inertial state estimation and a learning-based proprioceptive velocity prediction method. This approach improves functionality even in challenging visual conditions.", "result": "Field tests in Trondheim's submarine dry dock demonstrated the robustness of the system in challenging environments and verified the generalizability of the path planning techniques across different robot designs.", "conclusion": "The integration of refraction-aware visual-inertial state estimation with learning-based predictive models and autonomous exploration techniques successfully provides aerial drone-level autonomy underwater, making it viable for future underwater applications."}}
{"id": "2507.10000", "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "The paper explores a pragmatic interpretation of latent intentionality in texts using Semantic Spacetime and Promise Theory, enabling content analysis with minimal computation.", "motivation": "The researchers aim to address the lack of practical attention given to the concept of intent and intentionality in science and technology, especially after the philosophical work of Searle.", "method": "The method involves analyzing latent intentionality in data by identifying multi-scale anomalies and separating content into 'intended' content and 'ambient context' based on spacetime coherence, requiring minimal computation.", "result": "A practical approach for conceptualizing latent intentionality was achieved with low computational cost, enabling basic agents to process text without extensive training.", "conclusion": "This approach offers an accessible way to assess intentionality in data, even for simple agents, but the level of concept formation is constrained by memory capacity."}}
{"id": "2507.09209", "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "This paper introduces Expert-CFG, a framework to improve medical vision-language models (MedVLMs) without additional training, by leveraging expert guidance and uncertainty estimation.", "motivation": "The motivation lies in addressing inaccuracies and lack of clinical alignment in current medical vision-language models, which are critical limitations for medical applications.", "method": "The proposed method involves an Expert-Controlled Classifier-Free Guidance (Expert-CFG) framework that uses uncertainty estimation, reference retrieval, and expert-highlighted key terms to refine model outputs, ensuring alignment with clinical expertise.", "result": "Expert-CFG, with fewer parameters and limited expert annotations, outperforms larger state-of-the-art models across three medical visual question answering benchmarks.", "conclusion": "The study concludes that Expert-CFG is effective and resource-efficient, making it a viable solution for medical use, especially in resource-constrained settings."}}
{"id": "2507.08959", "pdf": "https://arxiv.org/pdf/2507.08959", "abs": "https://arxiv.org/abs/2507.08959", "authors": ["Xiang Li", "Xinyu Wang", "Yifan Lin"], "title": "Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign", "categories": ["cs.LG"], "comment": null, "summary": "In order to improve the accuracy of cross-platform advertisement\nrecommendation, a graph neural network (GNN)- based advertisement\nrecommendation method is analyzed. Through multi-dimensional modeling, user\nbehavior data (e.g., click frequency, active duration) reveal temporal patterns\nof interest evolution, ad content (e.g., type, tag, duration) influences\nsemantic preferences, and platform features (e.g., device type, usage context)\nshape the environment where interest transitions occur. These factors jointly\nenable the GNN to capture the latent pathways of user interest migration across\nplatforms. The experimental results are based on the datasets of three\nplatforms, and Platform B reaches 0.937 in AUC value, which is the best\nperformance. Platform A and Platform C showed a slight decrease in precision\nand recall with uneven distribution of ad labels. By adjusting the\nhyperparameters such as learning rate, batch size and embedding dimension, the\nadaptability and robustness of the model in heterogeneous data are further\nimproved.", "AI": {"tldr": "A GNN-based method is proposed to enhance cross-platform advertisement recommendations by analyzing user behavior, ad content, and platform features.", "motivation": "To improve the accuracy of advertisement recommendations across multiple platforms by effectively modeling user behavior and interests.", "method": "The paper uses a graph neural network (GNN) with multi-dimensional modeling of user behavior data, ad content, and platform features to capture patterns of interest evolution and user interest migration across platforms.", "result": "Experimental results show the best AUC value of 0.937 was achieved on Platform B. Platforms A and C showed slightly lower precision and recall due to uneven ad label distribution, but hyperparameter tuning improved model adaptability and robustness.", "conclusion": "The proposed GNN-based model effectively enhances cross-platform ad recommendation accuracy by capturing latent pathways of user interest migration and shows improvements with proper hyperparameter tuning."}}
{"id": "2507.09907", "pdf": "https://arxiv.org/pdf/2507.09907", "abs": "https://arxiv.org/abs/2507.09907", "authors": ["Thomas Hansper", "Kevin Phong Pham", "Michael Neumann"], "title": "Modelling Interrelations Between Agile Practices: The Agile Map", "categories": ["cs.SE"], "comment": null, "summary": "Agile methods are defined through guidelines comprising various practices\nintended to enable agile ways of working. These guidelines further comprise a\nspecific set of agile practices aiming to enable teams for an agile way of\nworking. However, due to its wide-spread use in practice we know that agile\npractices are adopted and tailored intensively, which lead to a high variety of\nagile practices in terms of their level of detail. Problem: A high variety of\nagile practices can be challenging as we do not know how different agile\npractices are interrelated with each other. To be more precise, tailoring and\nadopting agile practices may lead to the challenge, that the combinatorial use\nof several agile practices can only be successful to a limited extent, as\npractices support or even require each other for a effective use in practice.\nObjective: Our study aims to provide an enabler for this problem. We want to\nidentify interrelations between agile practices and describe them in a\nsystematic manner. Contribution: The core contribution of this paper is the\nAgile Map, a theoretical model describing relations between agile practices\nfollowing a systematic approach aiming to provide an overview of coherences\nbetween agile practices. The model aims to support practitioners in selecting\nand combining agile practices in a meaningful way.", "AI": {"tldr": "This paper introduces the Agile Map, a theoretical model outlining the relationships between agile practices to assist practitioners in combining them effectively.", "motivation": "The motivation stems from the challenge posed by the wide variety and combinatorial use of agile practices, as their interrelations are not clearly understood, potentially limiting their successful application in practice.", "method": "The authors employ a systematic approach to identify and describe the interrelationships between different agile practices, resulting in the development of the Agile Map model.", "result": "The study delivers the Agile Map, which provides a structured representation of how different agile practices are related and interact.", "conclusion": "The Agile Map offers practical guidance for teams by elucidating connections between agile practices, enabling their meaningful selection and combination for effective agile work."}}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "This paper introduces Ref-Long, a benchmark to evaluate the long-context referencing abilities of LCLMs, revealing gaps in current models' capabilities, including GPT-4.", "motivation": "The motivation is to address the underexplored area of long-context referencing in LCLMs, which is critical for tasks requiring accurate attribution within long context data.", "method": "The authors propose Ref-Long, a benchmark designed to assess referencing by having models identify document indexes associated with a specific key. They create three subsets ranging from synthetic to realistic scenarios to evaluate performance.", "result": "Testing with 13 LCLMs, including GPT-4, showed significant deficiencies in long-context referencing capabilities. Extensive analyses including human evaluation and fine-tuning were performed.", "conclusion": "LCLMs, including advanced models, demonstrate substantial limitations in long-context referencing, and the Ref-Long benchmark reveals areas for improvement in model precision and comprehension of contextual relationships."}}
{"id": "2507.10030", "pdf": "https://arxiv.org/pdf/2507.10030", "abs": "https://arxiv.org/abs/2507.10030", "authors": ["Marco Cal\u00ec", "Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "categories": ["cs.RO"], "comment": null, "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "AI": {"tldr": "The paper combines Deep Reinforcement Learning (RL) with Evolutionary Strategies (ES) to fine-tune robotic control policies, achieving better performance and robustness for underactuated systems.", "motivation": "To improve the performance and robustness of Deep RL policies in controlling underactuated robotic systems, especially for specific task objectives.", "method": "The approach involves initial policy training with Soft-Actor Critic (SAC) using a surrogate reward function, followed by refinement through the Separable Natural Evolution Strategy (SNES), targeting the original scoring metric.", "result": "Experimental tests during the 2nd AI Olympics using RealAIGym showcased that the fine-tuned controllers outperformed baseline approaches and achieved competitive competition scores.", "conclusion": "Combining RL with ES leads to enhanced control performance and robustness, making it a valuable strategy for underactuated robotic tasks."}}
{"id": "2507.10007", "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "AI": {"tldr": "This paper addresses the reliability challenges of Chain of Thought (CoT) reasoning by introducing a method that uses intrinsic model attention activations to evaluate and improve reasoning accuracy through beam search.", "motivation": "Chain of Thought (CoT) reasoning is powerful but prone to intermediate step errors, undermining its reliability.", "method": "The approach leverages truthfulness-sensitive activations in the model's attention heads to train a confidence predictor and dynamically select reasoning paths via beam search.", "result": "The proposed method surpasses existing state-of-the-art approaches across multiple reasoning tasks in both unimodal and multimodal models.", "conclusion": "This work enhances CoT reasoning reliability and accuracy, demonstrating its broad application potential, particularly for large reasoning models."}}
{"id": "2507.09214", "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "categories": ["cs.CV"], "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "The paper addresses the limitations in 3D detection models for rare anomaly categories in autonomous driving systems, proposing a new algorithm and dataset to enhance model generalization and anomaly detection.", "motivation": "To improve the generalization of 3D detection models in autonomous driving, especially for rare anomalies, which are challenging for models trained on closed datasets.", "method": "Introduced the S3AD algorithm that decouples 2D and 3D training, facilitating arbitrary 3D foreground detection and anomaly scoring based on confidence prediction. Additionally, created KITTI-AR and its subsets with augmented reality for richer datasets.", "result": "Experimental validation showed improved performance in anomaly detection when using the S3AD algorithm and KITTI-AR datasets in zero-shot scenarios.", "conclusion": "The proposed approach effectively enhances 3D detection generalization and anomaly detection, presenting a scalable solution for real-world autonomous driving systems."}}
{"id": "2507.08965", "pdf": "https://arxiv.org/pdf/2507.08965", "abs": "https://arxiv.org/abs/2507.08965", "authors": ["Kevin Rojas", "Ye He", "Chieh-Hsin Lai", "Yuta Takida", "Yuki Mitsufuji", "Molei Tao"], "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).", "AI": {"tldr": "This paper explores Classifier-Free Guidance in masked discrete diffusion, identifies challenges with high early guidance and current implementations, and proposes a simple yet effective improvement.", "motivation": "Existing Classifier-Free Guidance mechanisms have issues in conditional generation tasks for discrete diffusion models, particularly when it comes to guidance scheduling and implementation correctness.", "method": "The authors theoretically analyze guidance schedules in masked discrete diffusion, identify flaws in unmasking transitions, and propose a smoothened guidance mechanism achievable with a one-line code change.", "result": "The proposed guidance mechanism significantly improves sample quality, as demonstrated on datasets like ImageNet and QM9.", "conclusion": "The study offers a better theoretical understanding of guidance schedules in discrete diffusion and presents an easy-to-implement solution to improve generation quality."}}
{"id": "2507.09911", "pdf": "https://arxiv.org/pdf/2507.09911", "abs": "https://arxiv.org/abs/2507.09911", "authors": ["Marvin Auf der Landwehr", "Julia Topp", "Michael Neumann"], "title": "When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance", "categories": ["cs.SE"], "comment": null, "summary": "Context: Agile IT organizations, which are characterized by self-organization\nand collaborative social interactions, require motivating, efficient and\nflexible work environments to maximize value creation. Compressed work\nschedules such as the four-day workweek have evolved into multiple facets over\nthe last decades and are associated with various benefits for organizations and\ntheir employees. Objective: Our objective in this study is to deepen our\ncomprehension of the impact of compressed work schedules on the operational\nefficacy of IT enterprises, while concurrently developing a comprehensive\nframework delineating the intricacies of compressed work schedules.Method: We\nconducted a systematic review of available conceptualizations related to\nfour-day workweek schedules and elaborate on their organizational and social\neffects. To cover scientific and practice-oriented literature, our review\ncombined a systematic literature review and a web content analysis. Results:\nBased on the generated insights, we derive a meta-framework that matches\nconceptualizations and effects, finally guiding the adoption of compressed work\nschedules based on individual managerial prerequisites and circumstances.", "AI": {"tldr": "This study analyzes the impact of compressed work schedules, such as the four-day workweek, on IT organizations and provides a framework for adopting these schedules.", "motivation": "Agile IT organizations require adaptive and efficient work environments to enhance value creation; compressed work schedules are seen as a potential solution.", "method": "The study performs a systematic literature review and web content analysis to explore four-day workweek conceptualizations and their organizational and social effects.", "result": "The study produces a meta-framework that aligns compressed work schedule concepts with their impacts, assisting with tailored implementation by managers.", "conclusion": "Adopting compressed work schedules in IT enterprises can provide organizational benefits when adapted to specific managerial circumstances using the proposed meta-framework."}}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H\u00e4mmerl", "Vil\u00e9m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "This paper studies how errors in user prompts affect the performance of large language models (LLMs) in machine translation and evaluation tasks.", "motivation": "To understand the sensitivity of large language models to errors and perturbations in user prompts, impacting their reliability in practical applications.", "method": "A systematic evaluation was conducted with both human-like and synthetic errors in prompts, analyzing their quantitative impact and providing qualitative insights across various noise types.", "result": "Character-level and combined noise degraded performance more than phrasal perturbations. Poor prompt quality impacted instruction following rather than translation quality directly. LLMs still managed translation in cases of extreme noise.", "conclusion": "Prompt quality significantly influences translation performance, and even good prompts can fail with too many errors. Despite challenges, LLMs exhibit resilience to noise."}}
{"id": "2507.10047", "pdf": "https://arxiv.org/pdf/2507.10047", "abs": "https://arxiv.org/abs/2507.10047", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "categories": ["cs.RO"], "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "AI": {"tldr": "The paper introduces MP-RBFN, a new method for generating motion primitives using Radial Basis Function Networks (RBFNs) to improve efficiency and accuracy in autonomous driving motion planning.", "motivation": "Autonomous driving motion planning faces challenges like computational inefficiency in optimization-based methods and geometric constraints in sampling-based approaches. The paper aims to address these limitations.", "method": "MP-RBFN integrates high-fidelity trajectory generation from sampling-based methods with accurate vehicle dynamics modeling using RBFNs. The approach was validated via empirical testing.", "result": "MP-RBFN achieves seven times higher accuracy in optimized motion primitive generation compared to existing semi-analytic methods and offers low inference times, demonstrating superior performance.", "conclusion": "The method effectively combines the strengths of optimization and sampling-based planning and is practically applicable to trajectory planners. The software is open-source for broader use."}}
{"id": "2507.10045", "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "This paper evaluates Large Language Models (LLMs) for SPARQL-to-SPARQL translations between Knowledge Graph schemas, focusing on DBpedia-Wikidata and DBLP-OpenAlex alignments.", "motivation": "The research addresses a gap in Knowledge Graph (KG) interoperability by assessing the capabilities of LLMs in automating SPARQL query translation between popular KG schemas.", "method": "The study uses two benchmarks (DBpedia-Wikidata, DBLP-OpenAlex) to test three open LLMs (Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, Mistral-Large-Instruct-2407) under zero-shot, few-shot, and chain-of-thought prompting. Results are compared with gold standards and error categories are analyzed.", "result": "Performance across models and prompting strategies varies significantly. Translations from Wikidata to DBpedia perform better than the reverse (DBpedia to Wikidata).", "conclusion": "LLMs show potential for SPARQL-to-SPARQL translations, but effectiveness largely depends on the model, the direction of translation, and the prompting strategy employed."}}
{"id": "2507.09216", "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "categories": ["cs.CV"], "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "The paper proposes a spherical sampling method for panoramic images to improve performance by adapting 2D pre-trained models and achieving better segmentation results on datasets like Stanford2D3D.", "motivation": "The authors aim to address the challenges of distortions and discontinuities in panoramic images that negatively impact tasks relying on 2D pre-trained models.", "method": "The method introduces spherical discrete sampling aligned with weights of pre-trained 2D models, reducing distortions and enhancing compatibility for panoramic tasks.", "result": "The approach achieves commendable results in panoramic image segmentation, specifically tested on the Stanford2D3D dataset, showing improved performance.", "conclusion": "The spherical sampling method successfully adapts 2D pre-trained models for panoramic images, advancing their application in image segmentation tasks."}}
{"id": "2507.08966", "pdf": "https://arxiv.org/pdf/2507.08966", "abs": "https://arxiv.org/abs/2507.08966", "authors": ["Meng Liu", "Karl Leswing", "Simon K. S. Chu", "Farhad Ramezanghorbani", "Griffin Young", "Gabriel Marques", "Prerna Das", "Anjali Panikar", "Esther Jamir", "Mohammed Sulaiman Shamsudeen", "K. Shawn Watts", "Ananya Sen", "Hari Priya Devannagari", "Edward B. Miller", "Muyun Lihan", "Howook Hwang", "Janet Paulsen", "Xin Yu", "Kyle Gion", "Timur Rvachov", "Emine Kucukbenli", "Saee Gopal Paliwal"], "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": "Workshop on Generative AI for Biology at ICML 2025", "summary": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.", "AI": {"tldr": "The study introduces ToxBench, a large-scale AB-FEP dataset for ML development focused on Human Estrogen Receptor Alpha (ER\u03b1) binding affinity prediction, and benchmarks ML methods, including the proposed DualBind model.", "motivation": "Predicting protein-ligand binding affinities is crucial for drug discovery but is limited by reliable data availability for ML and computational complexity of physics-based methods.", "method": "ToxBench combines 8,770 ER\u03b1-ligand complexes with binding free energies calculated via AB-FEP, validated against experimental data, and introduces ligand splits for assessing model generalizability. It also benchmarks ML models and proposes a new model, DualBind, with a dual-loss framework.", "result": "DualBind outperforms other benchmarked ML methods and demonstrates the potential of ML to mimic AB-FEP at significantly reduced computational costs.", "conclusion": "ToxBench is a valuable dataset for advancing ML in protein-ligand binding prediction, showcasing the effectiveness of the DualBind model and ML's capacity to replicate physics-based accuracy efficiently."}}
{"id": "2507.08963", "pdf": "https://arxiv.org/pdf/2507.08963", "abs": "https://arxiv.org/abs/2507.08963", "authors": ["Tao Jiang", "Lin Xiao"], "title": "Stochastic Approximation with Block Coordinate Optimal Stepsizes", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "We consider stochastic approximation with block-coordinate stepsizes and\npropose adaptive stepsize rules that aim to minimize the expected distance from\nthe next iterate to an optimal point. These stepsize rules employ online\nestimates of the second moment of the search direction along each block\ncoordinate. The popular Adam algorithm can be interpreted as a particular\nheuristic for such estimation. By leveraging a simple conditional estimator, we\nderive a new method that obtains comparable performance as Adam but requires\nless memory and fewer hyper-parameters. We prove that this family of methods\nconverges almost surely to a small neighborhood of the optimal point, and the\nradius of the neighborhood depends on the bias and variance of the\nsecond-moment estimator. Our analysis relies on a simple aiming condition that\nassumes neither convexity nor smoothness, thus has broad applicability.", "AI": {"tldr": "Proposes adaptive stepsize rules for block-coordinate stochastic approximation, leveraging second-moment online estimates for better optimization.", "motivation": "To improve optimization approaches by addressing the lack of efficient and adaptive stepsize rules for block-coordinate stochastic approximation.", "method": "Developed adaptive stepsize rules using online estimation of second moments, introducing a simplified algorithm similar to Adam but with reduced memory and hyper-parameter requirements.", "result": "The proposed method matches Adam in performance while being resource-efficient and broadly applicable.", "conclusion": "The method converges to a neighborhood of the optimal point under general conditions, enhancing optimization adaptability and resource efficiency."}}
{"id": "2507.10054", "pdf": "https://arxiv.org/pdf/2507.10054", "abs": "https://arxiv.org/abs/2507.10054", "authors": ["Emir Bosnak", "Sahand Moslemi", "Mayasah Lami", "Anil Koyuncu"], "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks", "categories": ["cs.SE"], "comment": "Accepted to ICSME 2025", "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.", "AI": {"tldr": "This study investigates how open-source large language models (LLMs) generate vulnerable code upon direct or indirect prompting, revealing high rates of vulnerability production.", "motivation": "To understand the risks associated with open-source LLMs generating insecure code when explicitly prompted, and to evaluate the effectiveness of safety mechanisms in these models.", "method": "The authors employ a dual experimental design: Dynamic Prompting to test template-based variations in vulnerability type, persona, and directness; and Reverse Prompting to assess reproduction accuracy using real vulnerable code samples.", "result": "All three open-source models (Qwen2, Mistral, Gemma) frequently produced vulnerable code, with Qwen2 generating the most type-correct outputs. Student personas were more likely to induce vulnerabilities, and vulnerability reproduction peaked at moderate cyclomatic complexity levels.", "conclusion": "Open-source LLMs lack effective safety mechanisms to prevent vulnerable code generation, even in benign scenarios like educational use, highlighting significant security risks."}}
{"id": "2507.09536", "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "categories": ["cs.CL"], "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "The paper discusses the development and assessment of a definition modeling system tailored for the Belarusian language using a new dataset of 43,150 definitions.", "motivation": "To explore ways to adapt and improve definition modeling systems for supporting languages and lects that lack sufficient documentation or resources.", "method": "The authors created a dataset with 43,150 Belarusian definitions and conducted experiments to adapt pre-existing definition modeling systems to this language.", "result": "The experiments revealed that adapting definition modeling systems to Belarusian requires minimal data, although current automatic evaluation metrics have limitations.", "conclusion": "Adapting language models to unsupported languages like Belarusian is feasible with minimal data, but advancements are needed in automatic evaluation metrics to fully assess model performance."}}
{"id": "2507.10055", "pdf": "https://arxiv.org/pdf/2507.10055", "abs": "https://arxiv.org/abs/2507.10055", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "categories": ["cs.RO"], "comment": null, "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "AI": {"tldr": "A lightweight deep learning model was developed for hand gesture recognition in human-robot collaboration, achieving high accuracy while being optimized for edge devices.", "motivation": "Simplify and enhance human-robot collaboration by enabling intuitive control without relying on extra devices.", "method": "Developed an extremely compact neural network model with 1,103 parameters, optimized using TensorFlow Lite quantization and pruning, and tested on the Universal Robot UR5 utilizing ROS2.", "result": "The system achieved 93.5% gesture recognition accuracy and reduced the model size to just 7 KB for real-world deployment.", "conclusion": "The study demonstrates that efficient, compact deep learning models can enable natural and precise human-robot interaction, paving the way for real-world implementations."}}
{"id": "2507.10076", "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "categories": ["cs.AI"], "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "AI": {"tldr": "This paper introduces gradual semantics for assumption-based argumentation (ABA) to attribute dialectical strengths to assumptions in ABA frameworks.", "motivation": "Assumption-based argumentation (ABA), a popular structured argumentation framework, lacks gradual semantics that could determine degrees of acceptability, a gap this paper aims to fill.", "method": "The authors generalize modular gradual semantics for quantitative bipolar argumentation frameworks (QBAFs) and adapt it to ABA while ensuring it satisfies accepted properties. They also benchmark their approach against an existing QBAF-based method using synthetic data.", "result": "The proposed gradual ABA semantics adhered to key properties such as balance and monotonicity, and performed competitively when compared with the benchmark argument-based approach.", "conclusion": "The paper successfully extends gradual semantics to assumption-based argumentation, showcasing its viability and utility for analyzing assumptions' dialectical strengths."}}
{"id": "2507.09217", "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["G\u00f6rkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "The paper addresses online long-term point tracking, presenting 'Track-On,' a transformer-based model for real-time video applications.", "motivation": "Current long-term point tracking methods typically rely on offline settings, which are unsuitable for real-world scenarios requiring causal, moment-by-moment predictions.", "method": "The method involves introducing Track-On, a transformer-based model that processes tracked points as queries and sequentially handles video frames, leveraging memory for coherence without future data access.", "result": "Track-On achieves state-of-the-art performance across seven public benchmarks, demonstrating effective online long-term tracking capabilities.", "conclusion": "Visual foundation models enhance spatial features but need integration into dedicated designs for online tracking. Track-On effectively bridges this gap, enabling robust real-time point tracking."}}
{"id": "2507.08972", "pdf": "https://arxiv.org/pdf/2507.08972", "abs": "https://arxiv.org/abs/2507.08972", "authors": ["Sifan Wang", "Shyam Sankaran", "Panos Stinis", "Paris Perdikaris"], "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "physics.flu-dyn"], "comment": "25 pages, 13 figures, 3 tables", "summary": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.", "AI": {"tldr": "This paper explores the use of physics-informed neural networks (PINNs) to simulate turbulent fluid flows in two and three dimensions without relying on traditional computational grids or data.", "motivation": "Simulating turbulent fluid flows requires vast computational resources, especially at high speeds, which limits traditional methods.", "method": "The authors utilize PINNs that are trained directly from physical equations with innovations like adaptive architectures, causal training, and advanced optimization to overcome chaotic dynamics.", "result": "PINNs accurately reproduce critical turbulence metrics such as energy spectra, kinetic energy, enstrophy, and Reynolds stresses through rigorous validation.", "conclusion": "Neural equation solvers, like PINNs, offer a novel approach to handling chaotic systems, enabling continuous turbulence modeling that bypasses conventional computational limitations."}}
{"id": "2507.10062", "pdf": "https://arxiv.org/pdf/2507.10062", "abs": "https://arxiv.org/abs/2507.10062", "authors": ["Erg\u00fcn Batuhan Kaynak", "Mayasah Lami", "Sahand Moslemi", "Anil Koyuncu"], "title": "LLMShot: Reducing snapshot testing maintenance via LLMs", "categories": ["cs.SE"], "comment": "Accepted to ICSME 2025", "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.", "AI": {"tldr": "The paper presents LLMShot, a framework that uses vision-based large language models to automate the analysis of snapshot test failures in UI testing, significantly reducing manual triage effort.", "motivation": "Snapshot testing for UI validation is labor-intensive due to the need for manually distinguishing between regressions and intentional design changes caused by frequent UI updates.", "method": "The authors developed LLMShot, a framework that uses vision-based Large Language Models for hierarchical classification of UI changes in snapshot test failures. They used an iOS application with configurable feature flags to simulate realistic snapshot scenarios, and evaluated its performance using Gemma3 models.", "result": "LLMShot, specifically the 12B model, achieved over 84% recall in classifying failure root causes, while the smaller 4B model performed sufficiently for continuous integration environments.", "conclusion": "LLMShot offers an automated, semantic analysis approach for snapshot test failures, significantly reducing manual triage efforts and laying groundwork for smarter UI testing paradigms despite some limitations in current prompting-based approaches."}}
{"id": "2507.09601", "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "The paper introduces NMIXX, specialized cross-lingual embedding models for the financial domain with a focus on Korean and English, alongside a Korean financial STS benchmark. NMIXX achieves notable performance gains in semantic tasks compared to seven baselines.", "motivation": "General sentence embedding models have difficulty grasping financial semantics, particularly for low-resource languages like Korean, due to domain-specific jargon and bilingual vocabulary challenges.", "method": "The authors fine-tuned cross-lingual embedding models using 18.8K high-confidence triplets, incorporating paraphrases, hard negatives, and Korean-English translations. They also created KorFinSTS, a financial STS benchmark for nuanced evaluation.", "result": "NMIXX, especially its multilingual bge-m3 variant, performed significantly better in semantic tasks on financial data, achieving Spearman's rho improvements of +0.10 on English FinSTS and +0.22 on KorFinSTS compared to baselines.", "conclusion": "NMIXX and KorFinSTS deliver valuable tools for multilingual representation learning in the financial domain, demonstrating the importance of domain-specific adaptation and tokenizer design for low-resource languages."}}
{"id": "2507.10075", "pdf": "https://arxiv.org/pdf/2507.10075", "abs": "https://arxiv.org/abs/2507.10075", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "AI": {"tldr": "The paper presents a trust-aware game-theoretic lane-changing framework for automated vehicles that accounts for human-driver trust levels to improve interaction, predictability, and efficiency.", "motivation": "Automated vehicles struggle to interact cooperatively with human-driven vehicles due to limitations in predicting human driver behavior, stemming from overlooked dynamic trust levels.", "method": "The study introduces a trust-aware lane-changing framework by modeling a multi-vehicle coalition game, implementing real-time trust evaluations, and considering social dynamics in decision-making.", "result": "Human-in-the-loop experiments show AVs dynamically adapt to varying human driver trust levels, improving efficiency, safety, and transparency in lane-changing interactions.", "conclusion": "Incorporating trust levels into lane-changing strategies enhances AV-HV cooperation, ensuring safe, efficient, and predictable integration in mixed traffic settings."}}
{"id": "2507.10106", "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "categories": ["cs.AI"], "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "AI": {"tldr": "The paper introduces BlueGlass, a framework for integrated AI safety analysis, demonstrated on vision-language models.", "motivation": "The growing prevalence of AI systems necessitates integrated safety tools since existing tools are limited in providing comprehensive safety assurances independently.", "method": "The authors introduce BlueGlass, a unified framework integrating diverse safety tools, and conduct three analyses on vision-language models: distributional evaluation, layer dynamics probing, and concept identification via sparse autoencoders.", "result": "BlueGlass enables comprehensive safety analyses, revealing performance trade-offs across distributions, hierarchical learning dynamics, and interpretable AI concepts.", "conclusion": "BlueGlass facilitates robust AI system safety workflows and provides foundational insights for developing reliable AI systems."}}
{"id": "2507.09222", "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "The paper introduces StaRFM, a unified framework to address distribution shifts and confidence misalignment in foundation model applications like CLIP and SAM, offering significant performance improvements in vision and medical domains.", "motivation": "Foundation models have revolutionized vision and medical imaging, but challenges such as distribution shift and confidence misalignment hinder their deployment. Existing solutions are domain-specific and insufficient for broad applicability.", "method": "StaRFM utilizes two main components: a Fisher information penalty (FIP) to address covariate shifts in embeddings and a confidence misalignment penalty (CMP) for uncertainty calibration. These are extended theoretically and practically for both vision and medical segmentation tasks.", "result": "StaRFM demonstrates higher accuracy (+3.5%) and substantially better calibration (28% lower ECE) across 19 vision datasets. In medical segmentation, it achieves 84.7% DSC and 4.8mm HD95, with a 40% reduction in cross-domain performance gaps compared to baselines.", "conclusion": "StaRFM is an effective, plug-and-play framework for aligning foundation models to diverse tasks, offering improved robustness and generalization with minimal integration effort."}}
{"id": "2507.08977", "pdf": "https://arxiv.org/pdf/2507.08977", "abs": "https://arxiv.org/abs/2507.08977", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Marisa Eisenberg"], "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.", "AI": {"tldr": "Simulation-Grounded Neural Networks (SGNNs) combine mechanistic simulations and neural networks to achieve interpretable and accurate predictions and inference tasks across scientific disciplines.", "motivation": "To address the limitation of mechanistic models collapsing under real-world complexity and machine learning models requiring large labeled datasets while operating as black boxes.", "method": "SGNNs use mechanistic simulations as synthetic training data, spanning diverse conditions, to pretrain neural networks for both prediction and inference tasks.", "result": "SGNNs demonstrated state-of-the-art performance across disciplines, such as tripling COVID-19 forecasting skill, reducing chemical yield prediction error by one third, and improving ecological forecasting resilience.", "conclusion": "SGNNs offer a unified modeling paradigm that integrates scientific simulations with neural networks for robust, interpretable, and flexible inference and predictions, enabling process-level insight and overcoming the limitations of existing models."}}
{"id": "2507.10103", "pdf": "https://arxiv.org/pdf/2507.10103", "abs": "https://arxiv.org/abs/2507.10103", "authors": ["Hanyang Guo", "Xiaoheng Xie", "Hong-Ning Dai", "Peng Di", "Yu Zhang", "Bishenghui Tao", "Zibin Zheng"], "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.", "AI": {"tldr": "SelRepair is a novel automated program repair (APR) approach integrating a fine-tuned language model with a dual Retrieval-Augmented-Generation (RAG) module, effectively improving repair performance and efficiency.", "motivation": "Existing Automated Program Repair (APR) methods face challenges in addressing code-specific features, defect types, training data quality, and computational efficiency. There is a need to improve these limitations, especially with increased adoption of Large Language Models (LLMs) in APR tasks.", "method": "SelRepair incorporates a fine-tuned LLM trained on a bug-fix pair dataset and uses a dual RAG module to consider semantic and structural similarities through an RAG selection gate, optimizing information retrieval and inference efficiency.", "result": "Experiments on Java datasets demonstrate SelRepair significantly outperforms existing APR methods with exact match (EM) rates of 26.29% and 17.64% on different datasets. It also reduces inference time by at least 6.42% with controlled input lengths.", "conclusion": "SelRepair successfully addresses previous APR challenges by leveraging code-specific features and achieving improved performance, accuracy, and efficiency in software repair tasks."}}
{"id": "2507.09628", "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "SpreadPy is a Python library for simulating spreading activation in cognitive networks, aiding research in psychology, neuroscience, and education.", "motivation": "To develop a tool that facilitates numerical simulations to test structure-function relationships in cognitive networks and link them to psychological and clinical phenomena.", "method": "SpreadPy enables simulations of spreading activation on cognitive single-layer and multiplex networks, validated through case studies on knowledge association, creativity tasks, and aphasia-related impairments.", "result": "The results include identifying structural differences in conceptual organization linked to math anxiety, showing modulation of cognitive load during tasks, and correlating network activation with error types in aphasia patients.", "conclusion": "SpreadPy provides a flexible, reproducible framework for exploring cognitive processes, linking network dynamics to individual differences and impairments, and advancing research across disciplines."}}
{"id": "2507.10082", "pdf": "https://arxiv.org/pdf/2507.10082", "abs": "https://arxiv.org/abs/2507.10082", "authors": ["Amit Levy", "Itzik Klein"], "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "categories": ["cs.RO", "eess.SP"], "comment": "5 pages, 2 figures", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "AI": {"tldr": "This paper proposes an innovative method for propagating sigma points in Unscented Kalman Filters, enhancing navigation performance.", "motivation": "To address challenges with accurately predicting mean and covariance propagation in nonlinear navigation applications within the Unscented Kalman Filter.", "method": "A novel approach to propagate sigma points according to the nonlinear dynamic model of the navigation error state vector.", "result": "Improved filter accuracy and better navigation performance demonstrated using real sensor data from an autonomous underwater vehicle.", "conclusion": "The proposed method enhances the effectiveness and reliability of Unscented Kalman Filters for complex navigation scenarios."}}
{"id": "2507.10119", "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "AI": {"tldr": "The paper evaluates AI and RL approaches for solving edge-cloud application migration modeled as Towers of Hanoi (ToH) problems.", "motivation": "To enhance QoS and cost-efficiency through application migration in edge-cloud systems, while understanding AI/RL techniques suited for this task.", "method": "Analyzing and comparing state-of-the-art AI planning and RL techniques using a new classification based on state space definition.", "result": "The analysis highlights how different approaches can handle application migration modeled as ToH problems.", "conclusion": "Understanding which AI/RL approaches are effective in orchestrating migration in edge-cloud systems for emerging environments."}}
{"id": "2507.09230", "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay T\u00fcrkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "This paper introduces a new method to reconstruct animatable avatars from egocentric (first-person) inputs using a generative prior-based approach with Stable Diffusion, overcoming challenges like occlusions and distorted body proportions.", "motivation": "Conventional methods for telepresence require multi-view datasets during training or rely on front-view cameras, which can be less practical. The study aims to make telepresence systems more accessible and portable while maintaining accuracy.", "method": "The paper employs a Stable Diffusion generative backbone with ControlNet to convert a single top-down egocentric image into a frontal view representation. This frontal image is then input into an image-to-motion model to enable avatar motion reconstruction.", "result": "The proposed approach reduces training complexity and enhances generalizability by eliminating the need for multi-view datasets during training, making the pipeline more efficient.", "conclusion": "The study introduces a novel pipeline for creating avatar motions from minimal inputs, advancing the development of accurate, portable, and accessible telepresence systems."}}
{"id": "2507.08980", "pdf": "https://arxiv.org/pdf/2507.08980", "abs": "https://arxiv.org/abs/2507.08980", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "title": "Learning Diffusion Models with Flexible Representation Guidance", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.", "AI": {"tldr": "The paper proposes a framework to improve diffusion models' generation quality via enhanced representation guidance, achieving faster training and superior performance across multiple domains.", "motivation": "Existing diffusion models benefit from improved internal representations, but better systematic strategies for representation guidance are needed to enhance generation quality.", "method": "The authors introduce new decomposition methods and associated training criteria to incorporate auxiliary representations. They propose multimodal pairing strategies and optimal training curricula to balance representation learning and data generation.", "result": "Empirical results show enhanced generation performance across tasks like image, protein sequence, and molecule generation, with notable acceleration in training speeds (e.g., 23.3x faster on ImageNet benchmark compared to SiT-XL).", "conclusion": "Incorporating representation guidance systematically into diffusion models improves generation quality and significantly reduces training time, making these models more efficient and versatile for various applications."}}
{"id": "2507.09043", "pdf": "https://arxiv.org/pdf/2507.09043", "abs": "https://arxiv.org/abs/2507.09043", "authors": ["Jingxiang Qu", "Wenhan Gao", "Yi Liu"], "title": "Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Gaussian-based Probabilistic Generative Models (GPGMs) generate data by\nreversing a stochastic process that progressively corrupts samples with\nGaussian noise. While these models have achieved state-of-the-art performance\nacross diverse domains, their practical deployment remains constrained by the\nhigh computational cost of long generative trajectories, which often involve\nhundreds to thousands of steps during training and sampling. In this work, we\nintroduce a theoretically grounded and empirically validated framework that\nimproves generation efficiency without sacrificing training granularity or\ninference fidelity. Our key insight is that for certain data modalities, the\nnoising process causes data to rapidly lose its identity and converge toward a\nGaussian distribution. We analytically identify a characteristic step at which\nthe data has acquired sufficient Gaussianity, and then replace the remaining\ngeneration trajectory with a closed-form Gaussian approximation. Unlike\nexisting acceleration techniques that coarsening the trajectories by skipping\nsteps, our method preserves the full resolution of learning dynamics while\navoiding redundant stochastic perturbations between `Gaussian-like'\ndistributions. Empirical results across multiple data modalities demonstrate\nsubstantial improvements in both sample quality and computational efficiency.", "AI": {"tldr": "The paper proposes a method to accelerate Gaussian-based Probabilistic Generative Models (GPGMs) by replacing parts of the generative trajectory with a Gaussian approximation, improving efficiency without losing fidelity.", "motivation": "Current GPGMs have high computational costs due to lengthy generative steps, limiting their practical deployment despite achieving state-of-the-art performance.", "method": "The authors analytically identify a step in the generative process where data becomes sufficiently Gaussian and use a closed-form Gaussian approximation to replace subsequent steps, preserving training granularity while avoiding redundant perturbations.", "result": "Empirical results demonstrate significant advancements in both sample quality and computational efficiency across various data modalities.", "conclusion": "The proposed framework offers a theoretically sound and empirically validated solution for improving GPGM generation efficiency, making these models more practically deployable."}}
{"id": "2507.10182", "pdf": "https://arxiv.org/pdf/2507.10182", "abs": "https://arxiv.org/abs/2507.10182", "authors": ["Gehao Zhang", "Zhenting Wang", "Juan Zhai"], "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.", "AI": {"tldr": "This paper demonstrates that small, fine-tuned language models can effectively generate postconditions with lower computational demands, rivaling larger models.", "motivation": "Ensuring software correctness requires formal specifications, but manually creating them is tedious and error-prone. Existing large language models are promising but resource-intensive, prompting the need for smaller, efficient models.", "method": "A $7$B-parameter code model is fine-tuned on a specialized dataset containing prompts, reasoning logs, and postconditions. This method handles code dependencies and maintains pre-state information to facilitate correct specification.", "result": "The fine-tuned compact model matches or exceeds the performance of significantly larger language models in syntax correctness, semantic correctness, and bug-distinguishing capabilities when evaluated using the Defects4J benchmark.", "conclusion": "Using targeted fine-tuning on a modest dataset, smaller language models can achieve competitive results, providing a practical alternative to large resource-heavy models for specification generation."}}
{"id": "2507.09629", "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "The paper provides the first study of Knowledge Editing (KE) specifically for Arabic, evaluating different methods on cross-lingual and multilingual settings using Arabic translations of existing benchmarks.", "motivation": "To address the gap in research on Knowledge Editing behaviors in morphologically rich languages like Arabic, which are underexplored compared to English.", "method": "The study evaluates four KE methods (ROME, MEMIT, ICE, and LTE) on Arabic-translated benchmarks and extends the LTE method to a multilingual setting with joint Arabic-English training.", "result": "Parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods show more robust performance. Multilingual training improves both KE editability and cross-lingual transfer.", "conclusion": "The work highlights the challenges of cross-lingual KE and introduces benchmarks and multilingual datasets to support future research in Arabic Knowledge Editing."}}
{"id": "2507.10087", "pdf": "https://arxiv.org/pdf/2507.10087", "abs": "https://arxiv.org/abs/2507.10087", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "title": "Foundation Model Driven Robotics: A Comprehensive Review", "categories": ["cs.RO"], "comment": null, "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "AI": {"tldr": "A critical review of foundation models in robotics, focusing on applications, trends, limitations, and future directions.", "motivation": "Foundation models like LLMs and VLMs are transforming robotics, necessitating a structured review of their applications and challenges.", "method": "The paper categorizes advancements, identifies enabling trends, and discusses bottlenecks while evaluating real-world feasibility.", "result": "Core strengths such as semantic understanding and multimodal reasoning are recognized, but critical limitations like safety risks and computational constraints are highlighted.", "conclusion": "The paper proposes a roadmap for bridging semantic reasoning and physical intelligence, emphasizing robust and interpretable models for advancement."}}
{"id": "2507.10124", "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "categories": ["cs.AI"], "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "AI": {"tldr": "This paper proposes using metacognitive prompts, such as \"Could you be wrong?\", to help large language models (LLMs) identify and address their own biases and errors.", "motivation": "Biases in large language models (LLMs) are an ongoing concern, and current debiasing efforts might not remain effective as models evolve. The author seeks a durable strategy inspired by human decision-making techniques.", "method": "The author uses metacognitive prompts, specifically \"Could you be wrong?\", to engage LLMs in self-reflection, encouraging them to identify errors, biases, contradictory evidence, and alternatives upon responding to queries.", "result": "Introducing the \"Could you be wrong?\" prompt leads LLMs to reveal metacognitive insights, such as potential biases, reasoning pathways, and alignment issues between user intent and model interpretations.", "conclusion": "Incorporating concepts from human decision-making into LLM prompt engineering offers a promising path for enhancing their cognitive capabilities, particularly in identifying and mitigating biases."}}
{"id": "2507.09242", "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "categories": ["cs.CV"], "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "This paper introduces a new dataset and model for assessing painting processes rather than only final images, aiming for better alignment with human judgment.", "motivation": "Existing methods focus exclusively on assessing static final images of paintings, ignoring the dynamic and multi-stage nature of the artistic process.", "method": "The authors propose the Painting Process Assessment Dataset (PPAD) and a Transformer-based model called PPJudge, featuring temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture.", "result": "The proposed method surpasses existing baselines in accuracy, robustness, and consistency with human judgment across multiple experimental benchmarks.", "conclusion": "This research advances artistic image assessment by addressing the overlooked painting process, offering potential applications in computational creativity and art education."}}
{"id": "2507.08983", "pdf": "https://arxiv.org/pdf/2507.08983", "abs": "https://arxiv.org/abs/2507.08983", "authors": ["Anshuman Suri", "Harsh Chaudhari", "Yuefeng Peng", "Ali Naseh", "Amir Houmansadr", "Alina Oprea"], "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "While poisoning attacks on machine learning models have been extensively\nstudied, the mechanisms by which adversaries can distribute poisoned models at\nscale remain largely unexplored. In this paper, we shed light on how model\nleaderboards -- ranked platforms for model discovery and evaluation -- can\nserve as a powerful channel for adversaries for stealthy large-scale\ndistribution of poisoned models. We present TrojanClimb, a general framework\nthat enables injection of malicious behaviors while maintaining competitive\nleaderboard performance. We demonstrate its effectiveness across four diverse\nmodalities: text-embedding, text-generation, text-to-speech and text-to-image,\nshowing that adversaries can successfully achieve high leaderboard rankings\nwhile embedding arbitrary harmful functionalities, from backdoors to bias\ninjection. Our findings reveal a significant vulnerability in the machine\nlearning ecosystem, highlighting the urgent need to redesign leaderboard\nevaluation mechanisms to detect and filter malicious (e.g., poisoned) models,\nwhile exposing broader security implications for the machine learning community\nregarding the risks of adopting models from unverified sources.", "AI": {"tldr": "The paper investigates how machine learning model leaderboards can be exploited to distribute poisoned models at scale and proposes a framework, TrojanClimb, showing its efficacy in embedding malicious functionalities across various model types.", "motivation": "To explore the under-studied vulnerability of machine learning leaderboards as a distribution channel for poisoned models and highlight the potential risks in the adoption of unverified models.", "method": "They introduce TrojanClimb, a framework enabling the injection of malicious behaviors into models while achieving competitive leaderboard rankings, tested across diverse modalities like text and image generation.", "result": "The framework successfully ranks high on leaderboards and embeds harmful functionalities, such as backdoors and bias injections, proving the significant risk posed by malicious models in the current ecosystem.", "conclusion": "Machine learning leaderboards expose a critical vulnerability that can be exploited for large-scale distribution of malicious models, necessitating redesigns in evaluation mechanisms and awareness of the risks of unverified models."}}
{"id": "2507.09061", "pdf": "https://arxiv.org/pdf/2507.09061", "abs": "https://arxiv.org/abs/2507.09061", "authors": ["Thomas T. Zhang", "Daniel Pfrommer", "Nikolai Matni", "Max Simchowitz"], "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "comment": null, "summary": "We study the problem of imitating an expert demonstrator in a continuous\nstate-and-action dynamical system. While imitation learning in discrete\nsettings such as autoregressive language modeling has seen immense success and\npopularity in recent years, imitation in physical settings such as autonomous\ndriving and robot learning has proven comparably more complex due to the\ncompounding errors problem, often requiring elaborate set-ups to perform\nstably. Recent work has demonstrated that even in benign settings, exponential\ncompounding errors are unavoidable when learning solely from expert-controlled\ntrajectories, suggesting the need for more advanced policy parameterizations or\ndata augmentation. To this end, we present minimal interventions that provably\nmitigate compounding errors in continuous state-and-action imitation learning.\nWhen the system is open-loop stable, we prescribe \"action chunking,\" i.e.,\npredicting and playing sequences of actions in open-loop; when the system is\npossibly unstable, we prescribe \"noise injection,\" i.e., adding noise during\nexpert demonstrations. These interventions align with popular choices in modern\nrobot learning, though the benefits we derive are distinct from the effects\nthey were designed to target. Our results draw insights and tools from both\ncontrol theory and reinforcement learning; however, our analysis reveals novel\nconsiderations that do not naturally arise when either literature is considered\nin isolation.", "AI": {"tldr": "This paper addresses challenges in imitation learning for continuous state-action systems, offering approaches like action chunking and noise injection to reduce compounding errors.", "motivation": "Imitation learning in physical settings faces limitations due to compounding errors, necessitating enhanced methods for stable learning in tasks like autonomous driving and robotics.", "method": "The paper proposes two techniques: action chunking for open-loop stable systems and noise injection during expert demonstrations for potentially unstable systems, leveraging insights from control theory and reinforcement learning.", "result": "The proposed minimal interventions effectively mitigate compounding errors in continuous settings, offering new perspectives beyond traditional methods.", "conclusion": "The study introduces practical techniques to improve imitation learning stability and highlights novel considerations not addressed when focusing solely on control theory or reinforcement learning paradigms."}}
{"id": "2507.10228", "pdf": "https://arxiv.org/pdf/2507.10228", "abs": "https://arxiv.org/abs/2507.10228", "authors": ["Hugo Villamizar", "Daniel Mendez", "Marcos Kalinowski"], "title": "Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements", "categories": ["cs.SE"], "comment": "This paper has been accepted for presentation at the 2025 IEEE 33rd\n  International Requirements Engineering Conference Workshops (REW-RETRAI 2025)", "summary": "Growing concerns around the trustworthiness of AI-enabled systems highlight\nthe role of requirements engineering (RE) in addressing emergent,\ncontext-dependent properties that are difficult to specify without structured\napproaches. In this short vision paper, we propose the integration of two\ncomplementary approaches: AMDiRE, an artefact-based approach for RE, and\nPerSpecML, a perspective-based method designed to support the elicitation,\nanalysis, and specification of machine learning (ML)-enabled systems. AMDiRE\nprovides a structured, artefact-centric, process-agnostic methodology and\ntemplates that promote consistency and traceability in the results; however, it\nis primarily oriented toward deterministic systems. PerSpecML, in turn,\nintroduces multi-perspective guidance to uncover concerns arising from the\ndata-driven and non-deterministic behavior of ML-enabled systems. We envision a\npathway to operationalize trustworthiness-related requirements, bridging\nstakeholder-driven concerns and structured artefact models. We conclude by\noutlining key research directions and open challenges to be discussed with the\nRE community.", "AI": {"tldr": "The paper integrates AMDiRE and PerSpecML approaches for improving requirements engineering in AI-enabled systems.", "motivation": "Address challenges in specifying trustworthiness-related requirements for AI-enabled systems.", "method": "Combining AMDiRE for artefact-based RE and PerSpecML for perspective-based ML-specific requirements.", "result": "Proposes a pathway to align stakeholder concerns with structured artefact models for operationalizing trustworthiness.", "conclusion": "Key research directions and challenges in trustworthiness-related requirements are discussed."}}
{"id": "2507.09638", "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "This paper introduces GRPO, a cost-efficient method to improve law citation accuracy and response quality in Thai legal question answering using advanced embeddings and optimizing policies.", "motivation": "Improving the performance of Retrieval-Augmented Generation systems for Thai legal question answering, particularly in tasks requiring complex legal reasoning.", "method": "An approach using Group-Relative Policy Optimization (GRPO) combined with BGE-M3 semantic-similarity embeddings to enhance citation accuracy and response quality, while reducing computational costs by 2.5x.", "result": "Experiments show GRPO gains up to 90% citation-F1 improvement from the base model and a 31% increase in joint quality metrics over instruction tuning. Enhanced robustness in complex legal reasoning tasks is also demonstrated.", "conclusion": "GRPO offers a resource-efficient solution for improving Thai legal LLMs, boosting both citation accuracy and performance in complex reasoning tasks compared to traditional methods."}}
{"id": "2507.10105", "pdf": "https://arxiv.org/pdf/2507.10105", "abs": "https://arxiv.org/abs/2507.10105", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "AI": {"tldr": "The paper introduces a framework for sensorless torque control in humanoid robots using Physics-Informed Neural Networks (PINNs) and Unscented Kalman Filtering (UKF), achieving better torque tracking and robustness.", "motivation": "Current methods rely heavily on joint torque sensors, which can be limiting in terms of cost and complexity, prompting the need for an efficient sensorless approach.", "method": "The framework uses PINNs to model friction and integrates UKF for torque estimation within a real-time control system.", "result": "Experimental validation on the ergoCub robot shows improved torque tracking, energy efficiency, and disturbance rejection over existing methods.", "conclusion": "The proposed method is proven scalable, adaptable, and practical for sensorless torque control, with consistent performance across varying robot hardware configurations."}}
{"id": "2507.10134", "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "categories": ["cs.AI", "53-01", "C.2"], "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "AI": {"tldr": "This paper presents a UAV resource allocation scheme using LLM-In-Context Learning (FRSICL) for real-time wildfire monitoring, outperforming conventional DRL methods.", "motivation": "The need for efficient and time-critical monitoring of wildfires necessitated the optimization of UAV flight controls and sensor data scheduling.", "method": "The authors developed FRSICL, an LLM-In-Context Learning-based approach, for optimizing UAV velocity and data collection in real-time.", "result": "Simulation results demonstrated that FRSICL outperformed DRL-based methods like PPO and simpler baselines.", "conclusion": "FRSICL is a promising alternative to DRL for dynamic and time-critical applications like wildfire monitoring, thanks to its flexibility and effectiveness."}}
{"id": "2507.09248", "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "The paper proposes AGCD-Net, a context-aware emotion recognition model, which uses causal theory and hybrid encoder designs to address biases in recognizing emotions. It achieves state-of-the-art performance.", "motivation": "To tackle the problem of context bias in emotion recognition, where spurious correlations between background context and emotion labels affect model performance.", "method": "AGCD-Net uses a novel convolutional encoder called Hybrid ConvNeXt, combined with an Attention Guided-Causal Intervention Module (AG-CIM) that perturbs context features, isolates biases, and applies attention-driven corrections based on facial features.", "result": "AGCD-Net achieves state-of-the-art performance on the CAER-S dataset, providing strong evidence for the effectiveness of causal debiasing in emotion recognition.", "conclusion": "The proposed AGCD-Net successfully mitigates context bias using attention and causal intervention, offering a robust solution for real-world emotion recognition tasks in complex scenarios."}}
{"id": "2507.09009", "pdf": "https://arxiv.org/pdf/2507.09009", "abs": "https://arxiv.org/abs/2507.09009", "authors": ["Zhengxiao He", "Huayu Li", "Geng Yuan", "William D. S. Killgore", "Stuart F. Quan", "Chen X. Chen", "Ao Li"], "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.", "AI": {"tldr": "The paper introduces a self-supervised deep learning model using EEG, ECG, and respiratory signals to predict cardiovascular disease (CVD) outcomes, achieving robust and clinically meaningful results validated across cohorts.", "motivation": "To enhance cardiovascular disease risk assessment by developing a framework that extracts clinically meaningful patterns directly from multi-modal PSG data, enabling improved individualized predictions.", "method": "The study created a self-supervised deep learning model trained on over 4,000 participants' EEG, ECG, and respiratory signals. Projection scores were generated by contrasting embeddings of individuals with and without CVD outcomes. Validation was performed on an independent cohort of 1,093 participants.", "result": "The model produced distinct and clinically meaningful patterns across modalities. ECG features were highly predictive of cardiac conditions and CVD mortality, while EEG features predicted incident hypertension and mortality. Respiratory signals provided additional value. Combining the projection scores with the Framingham Risk Score improved predictive performance, with AUC values ranging from 0.607 to 0.965.", "conclusion": "The framework offers a novel way to derive individualized CVD risk scores directly from PSG data, demonstrating high validity. This approach could be integrated into clinical settings for enhanced risk assessment and personalized care."}}
{"id": "2507.09077", "pdf": "https://arxiv.org/pdf/2507.09077", "abs": "https://arxiv.org/abs/2507.09077", "authors": ["Eric C. Chi", "Aaron J. Molstad", "Zheming Gao"], "title": "Convex Clustering", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "This survey reviews a clustering method based on solving a convex\noptimization problem. Despite the plethora of existing clustering methods,\nconvex clustering has several uncommon features that distinguish it from prior\nart. The optimization problem is free of spurious local minima, and its unique\nglobal minimizer is stable with respect to all its inputs, including the data,\na tuning parameter, and weight hyperparameters. Its single tuning parameter\ncontrols the number of clusters and can be chosen using standard techniques\nfrom penalized regression. We give intuition into the behavior and theory for\nconvex clustering as well as practical guidance. We highlight important\nalgorithms and give insight into how their computational costs scale with the\nproblem size. Finally, we highlight the breadth of its uses and flexibility to\nbe combined and integrated with other inferential methods.", "AI": {"tldr": "This paper surveys convex clustering, a stable and globally optimal clustering method with unique advantages over existing approaches.", "motivation": "To address limitations in traditional clustering methods, such as spurious local minima and instability in outputs, by exploring a convex optimization-based clustering approach.", "method": "The authors study convex clustering, an approach based on solving a convex optimization problem that ensures global optimality, stability, and flexible parameter tuning.", "result": "Convex clustering is demonstrated to be robust, stable, computationally scalable, and versatile in integration with other inferential methods.", "conclusion": "Convex clustering offers a novel and compelling framework for clustering, combining theoretical strengths with practical applicability across various domains."}}
{"id": "2507.10235", "pdf": "https://arxiv.org/pdf/2507.10235", "abs": "https://arxiv.org/abs/2507.10235", "authors": ["Zhixiang Chen", "Zhuangbin Chen", "Xingjie Cai", "Wei Li", "Zibin Zheng"], "title": "An Empirical Study of Interaction Bugs in ROS-based Software", "categories": ["cs.SE"], "comment": null, "summary": "Modern robotic systems integrate multiple independent software and hardware\ncomponents, each responsible for distinct functionalities such as perception,\ndecision-making, and execution. These components interact extensively to\naccomplish complex end-to-end tasks. As a result, the overall system\nreliability depends not only on the correctness of individual components, but\nalso on the correctness of their interactions. Failures often manifest at the\nboundaries between components, yet interaction-related reliability issues in\nrobotics--referred to here as interaction bugs (iBugs)--remain underexplored.\n  This work presents an empirical study of iBugs within robotic systems built\nusing the Robot Operating System (ROS), a widely adopted open-source robotics\nframework. A total of 121 iBugs were analyzed across ten actively maintained\nand representative ROS projects. The identified iBugs are categorized into\nthree major types: intra-system iBugs, hardware iBugs, and environmental iBugs,\ncovering a broad range of interaction scenarios in robotics. The analysis\nincludes an examination of root causes, fixing strategies, and the impact of\nthese bugs. Several findingsa are derived that shed light on the nature of\niBugs and suggest directions for improving their prevention and detection.\nThese insights aim to inform the design of more robust and safer robotic\nsystems.", "AI": {"tldr": "The paper investigates interaction bugs (iBugs) in robotics systems using the Robot Operating System (ROS), analyzing 121 iBugs across ten ROS projects to categorize, understand causes, and propose improvements for more reliable robotic systems.", "motivation": "Robotic systems require interaction between multiple software and hardware components, but interaction bugs, which often occur at the boundaries of these components, are underexplored. This paper aims to fill this gap.", "method": "The study conducted an empirical analysis of 121 interaction bugs found in ten actively maintained ROS projects. These bugs were categorized into intra-system, hardware, and environmental iBugs, with an analysis of root causes and fixing strategies.", "result": "The categorization and analysis of iBugs revealed their root causes and the impact on robotics systems, providing new insights into their nature. The findings highlight areas for better prevention and detection of these bugs.", "conclusion": "The insights derived from the study aim to guide the design of more robust and safer robotic systems by addressing interaction bugs in the ROS framework."}}
{"id": "2507.09701", "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "The abstract introduces MCEval, a novel framework for evaluating the cultural awareness and bias of large language models across 13 languages and cultures, exposing fairness issues and emphasizing the importance of language-culture alignment.", "motivation": "There is a need to address cultural biases and enhance cross-cultural understanding in large language models to better serve a diverse global audience.", "method": "The MCEval framework combines dynamic cultural question construction and causal analysis techniques like Counterfactual Rephrasing and Confounder Rephrasing to systematically evaluate cultural awareness and bias in language models.", "result": "MCEval evaluates 39,897 instances of cultural awareness and 17,940 instances of cultural bias, identifying disparities in cultural performance based on language-culture alignment and revealing fairness issues in methods focused on English scenarios.", "conclusion": "This paper introduces MCEval as a pioneering and comprehensive evaluation framework for analyzing cultural understanding in large language models, thereby contributing to better addressing global user diversity and fairness challenges."}}
{"id": "2507.10121", "pdf": "https://arxiv.org/pdf/2507.10121", "abs": "https://arxiv.org/abs/2507.10121", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "categories": ["cs.RO"], "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "AI": {"tldr": "This paper introduces a simulation framework for soft continuum arms (SCAs) combined with experimental video-tracking systems to address their non-linear behavior challenges.", "motivation": "Soft continuum arms are highly versatile, but their non-linear behavior makes real-world application and control challenging.", "method": "The framework integrates a simulation model for SCAs, using fiber reinforced elastomeric enclosures (FREEs), with an experimental video-tracking system to analyze and control their behavior.", "result": "The simulation framework and video-tracking integration allow better testing and control design for SCAs.", "conclusion": "This research advances the practical usability of SCAs by combining modular simulation and experimental methodologies to tackle their non-linear dynamics."}}
{"id": "2507.10142", "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "AI": {"tldr": "The paper introduces the concept of adaptability for evaluating Multi-Agent Reinforcement Learning (MARL) algorithms to improve their real-world deployments under dynamic environments.", "motivation": "The motivation for this work is to address the gap between the demonstrated success of MARL in simulated environments and its limited real-world deployment. The challenges are rooted in the dynamic and complex nature of real-world scenarios, where factors such as fluctuating agent populations, task evolution, and inconsistent conditions arise.", "method": "The proposed method involves introducing the concept of adaptability, which is evaluated through three dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. This structured framework aims to assess MARL's capacity to remain effective amid dynamic environmental changes.", "result": "The adaptability framework provides a conceptual lens for assessing MARL performance in dynamic conditions, extending evaluations beyond traditional static benchmarks.", "conclusion": "This work emphasizes the importance of adaptability for MARL systems to better support their real-world applicability, offering a more robust foundation for algorithm development and deployment."}}
{"id": "2507.09256", "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "The paper introduces AAHR, a framework addressing challenges in image-text matching by reducing ambiguities and improving semantic understanding through dynamic clustering, feature extraction, and graph-based interactions.", "motivation": "The motivation is to resolve the challenges of high-order associations and semantic ambiguities in image-text matching, which result in matching uncertainties due to soft positive and soft negative samples, and to better utilize neighborhood relationships among similar instances.", "method": "AAHR employs dynamic clustering prototype contrastive learning to mitigate soft positive problems, introduces global/local feature extraction and adaptive aggregation, uses correlation matrices and GNN for neighborhood relationships, and integrates momentum contrastive learning to handle negative samples.", "result": "AAHR outperforms existing methods on datasets such as Flickr30K, MSCOCO, and ECCV Caption, showcasing superior accuracy and efficiency in image-text matching tasks.", "conclusion": "AAHR effectively addresses high-order associations and semantic ambiguities in image-text matching while enhancing feature discrimination and achieving state-of-the-art performance."}}
{"id": "2507.09016", "pdf": "https://arxiv.org/pdf/2507.09016", "abs": "https://arxiv.org/abs/2507.09016", "authors": ["Karim Galliamov", "Ivan Titov", "Ilya Pershin"], "title": "Enhancing RLHF with Human Gaze Modeling", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.", "AI": {"tldr": "This paper integrates human gaze modeling into RLHF to enhance its efficiency and reduce computational costs.", "motivation": "RLHF is computationally expensive, despite its utility in aligning language models with human preferences. The research seeks to reduce these costs by utilizing human gaze data.", "method": "The paper explores gaze-aware reward models and sparse rewards at the token level, distributed based on human gaze patterns, to integrate gaze data into RLHF.", "result": "Gaze-informed RLHF leads to faster convergence and maintains or slightly improves performance, resulting in reduced computational costs.", "conclusion": "Human gaze provides valuable information that can be used to optimize RLHF methods, offering potential improvements in its efficiency."}}
{"id": "2507.09087", "pdf": "https://arxiv.org/pdf/2507.09087", "abs": "https://arxiv.org/abs/2507.09087", "authors": ["Esraa Elelimy", "Brett Daley", "Andrew Patterson", "Marlos C. Machado", "Adam White", "Martha White"], "title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "AI": {"tldr": "The paper proposes new gradient-based deep reinforcement learning methods using an extended Generalized Projected Bellman Error ($\\GPBE$) objective to handle multistep credit assignment, achieving improved performance over existing algorithms.", "motivation": "Existing deep reinforcement learning methods using semi-gradient TD are simple and efficient but prone to divergence, while GTD methods, though stable, are underutilized in deep RL due to existing limitations.", "method": "The authors extend the $\\GPBE$ objective to support multistep credit assignment via $\\lambda$-return, deriving three gradient-based algorithms compatible with both forward-view (experience replay) and backward-view (streaming) implementations.", "result": "The proposed methods outperform PPO and StreamQ in the MuJoCo and MinAtar environments, demonstrating superior performance and stability.", "conclusion": "The study successfully introduces multistep GTD methods to deep RL, bridging the gap between efficiency and stability in off-policy learning and achieving state-of-the-art results."}}
{"id": "2507.10244", "pdf": "https://arxiv.org/pdf/2507.10244", "abs": "https://arxiv.org/abs/2507.10244", "authors": ["Adam \u0160t\u011bp\u00e1nek", "David Ku\u0165\u00e1k", "Barbora Kozl\u00edkov\u00e1", "Jan By\u0161ka"], "title": "Helveg: Diagrams for Software Documentation", "categories": ["cs.SE", "D.2.2; D.2.11"], "comment": "13 pages, 5 figures, accepted by TVCG", "summary": "Software developers often have to gain an understanding of a codebase. Be it\nprogrammers getting onboarded onto a team project or, for example, developers\nstriving to grasp an external open-source library. In either case, they\nfrequently turn to the project's documentation. However, documentation in its\ntraditional textual form is ill-suited for this kind of high-level exploratory\nanalysis, since it is immutable from the readers' perspective and thus forces\nthem to follow a predefined path. We have designed an approach bringing aspects\nof software architecture visualization to API reference documentation. It\nutilizes a highly interactive node-link diagram with expressive node glyphs and\nflexible filtering capabilities, providing a high-level overview of the\ncodebase as well as details on demand. To test our design, we have implemented\na prototype named Helveg, capable of automatically generating diagrams of C\\#\ncodebases. User testing of Helveg confirmed its potential, but it also revealed\nproblems with the readability, intuitiveness, and user experience of our tool.\nTherefore, in this paper, which is an extended version of our VISSOFT paper\nwith DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems\nthrough major changes to the glyph design, means of interaction, and user\ninterface of the tool. To assess the improvements, this new version of Helveg\nwas evaluated again with the same group of participants as the previous\nversion.", "AI": {"tldr": "The paper introduces \"Helveg,\" a tool for visualizing codebases using interactive diagrams to improve over traditional, static documentation. It highlights improvements made to the tool after initial feedback and re-evaluates it with users.", "motivation": "Developers often face challenges in understanding codebases, especially when relying on static and predefined documentation. There is a need for more interactive and flexible methods to explore and understand software architectures.", "method": "The authors designed a tool named \"Helveg\" featuring interactive node-link diagrams with glyphs and filtering capabilities to give a high-level overview of codebases. After receiving user feedback on an earlier version, they enhanced the tool by improving its user interface, glyphs, and interaction mechanisms.", "result": "The revised version of Helveg was evaluated by the same group as in the initial test. The improvements addressed many issues such as readability, intuitiveness, and overall user experience.", "conclusion": "Helveg presents a promising visualization tool for codebase exploration, but iterative improvements and user-centered design are crucial to refining its utility and usability."}}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "This paper studies the latent geometry of large language models (LLMs), showing that high-level semantic information lies in low-dimensional, linearly separable subspaces.", "motivation": "To better understand how LLMs internally organize semantic representations and improve model alignment for safer outputs.", "method": "The authors conduct a large-scale empirical study of hidden states in 11 transformer-based LLMs across different layers, topics, and structured reasoning prompts.", "result": "Semantic separability is observed, becomes pronounced in deeper layers, and allows causal interventions to capture reasoning patterns and mitigate adversarial content.", "conclusion": "The study highlights the utility of geometry-aware tools for managing latent representations, enabling effective detection and mitigation of harmful content using methods like lightweight MLP classifiers."}}
{"id": "2507.10131", "pdf": "https://arxiv.org/pdf/2507.10131", "abs": "https://arxiv.org/abs/2507.10131", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "AI": {"tldr": "The paper introduces GUIDER, a probabilistic framework for robots to infer human intent in navigation and manipulation tasks, achieving significant performance improvements over baselines.", "motivation": "To enable seamless human-robot collaboration by accurately inferring human intent without constraining human actions or causing conflicts.", "method": "GUIDER tracks navigation and manipulation goals through a dual-phase probabilistic framework, integrating tools like Synergy Map, U2Net saliency, FastSAM, and geometric kinematics-awareness for real-time object probability evolution.", "result": "In evaluations, GUIDER demonstrated superior stability in navigation (93-100%) and manipulation (94-100%), outperforming baselines by 39.5% and 31.4% in specific scenarios while recognizing object intent three times faster.", "conclusion": "The dual-phase framework of GUIDER significantly enhances intent inference in mobile manipulation, validating its approach and presenting potential for effective human-robot collaboration."}}
{"id": "2507.10156", "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "categories": ["cs.AI"], "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "AI": {"tldr": "This paper introduces Swiss Food Knowledge Graph (SwissFKG) to centralize nutrition-related data, utilizes LLMs for enrichment, and proposes tools to enhance dietary assessments.", "motivation": "Existing dietary assessment systems lack integration of non-visual factors such as ingredient substitutions and personal dietary needs, particularly within the Swiss context.", "method": "The authors developed SwissFKG as a comprehensive graph combining recipes, ingredients, nutrient data, and dietary guidelines. They employed an LLM-powered enrichment pipeline and benchmarked several LLMs for food data augmentation.", "result": "LLMs effectively enriched the SwissFKG with nutritional information, and the Graph-RAG application successfully demonstrated answering user-specific nutrition queries based on the graph.", "conclusion": "SwissFKG sets the stage for advanced dietary assessment tools by uniting diverse nutritional data sources and enabling personalized nutrition guidance leveraging LLM-interaction."}}
{"id": "2507.09266", "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "categories": ["cs.CV"], "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "The paper introduces a new method for gloss-free Sign Language Translation that makes use of a segment-aware visual tokenization framework, reducing input sequence length and computational demands while improving performance.", "motivation": "Current gloss-free SLT methods achieve high performance but come with increased model complexity and scalability issues, prompting the need for a more efficient approach.", "method": "A segment-aware visual tokenization framework is proposed to compress video sequences by up to 50%, combined with token-to-token contrastive alignment and dual-level supervision for better cross-modal alignment.", "result": "The proposed method achieves state-of-the-art results on the PHOENIX14T benchmark, with reduced memory usage and better scalability compared to prior methods.", "conclusion": "The new approach enhances the efficiency and scalability of gloss-free SLT without sacrificing performance, showcasing the potential of their tokenization and alignment strategies."}}
{"id": "2507.09091", "pdf": "https://arxiv.org/pdf/2507.09091", "abs": "https://arxiv.org/abs/2507.09091", "authors": ["Shayan K. Azmoodeh", "Krishna Subramani", "Paris Smaragdis"], "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "6 pages, 3 figures, 1 table. MLSP 2025", "summary": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "AI": {"tldr": "The paper extends PCA and ICA decompositions to continuous-time signals using an implicit neural signal representation framework, enabling applications to irregularly sampled data.", "motivation": "To address the limitations of PCA and ICA in handling continuous-time and irregularly sampled signals, particularly for high-dimensional data like point clouds.", "method": "The paper models signals as continuous-time stochastic processes and unifies PCA and ICA through a contrast function in the network loss to ensure statistical properties like decorrelation and independence.", "result": "The method provides a numerical approximation framework that successfully applies low-rank decomposition to previously inaccessible datasets such as irregularly sampled signals and point clouds.", "conclusion": "The proposed approach generalizes PCA and ICA to a continuous domain, significantly broadening their applicability to modern data challenges."}}
{"id": "2507.10305", "pdf": "https://arxiv.org/pdf/2507.10305", "abs": "https://arxiv.org/abs/2507.10305", "authors": ["Linus Ververs", "Trang Linh Lam", "Janina Berger", "Lutz Prechelt"], "title": "A Grounded Theory on the Teacher and Student Roles in Pair Programming", "categories": ["cs.SE"], "comment": null, "summary": "Context: Pair programming is an established (agile) practice and is practiced\nthroughout the industry. Objective: Understand under what circumstances\nknowledge transfer can harm a pair programming session. Method: Grounded Theory\nMethodology based on 17 recorded pair programming sessions with 18 developers\nfrom 5 German software companies accompanied, by 6 interviews with different\ndevelopers from 4 other German companies. Results: We define the student and\nteacher roles to help developers deal with a one-sided knowledge gap. We\ndescribe pitfalls to avoid and develop a grounded theory centered around the\nPower Gap in pair programming. Conclusions: Knowledge transfer can be harmful\nwhen developers don't pay attention to their partners needs and desires. If\ndevelopers don't pay attention to the Power Gap and keep it in check, Defensive\nBehavior may arise that leads to a vicious cycle impacting the knowledge\ntransfer, the Togetherness and the code quality in a negative way.", "AI": {"tldr": "This paper investigates the conditions under which knowledge transfer negatively impacts pair programming sessions and offers insights on avoiding such harm.", "motivation": "To explore when knowledge transfer in pair programming becomes detrimental, specifically focusing on the implications of power imbalances in developer interactions.", "method": "The authors employed Grounded Theory Methodology by analyzing 17 recorded pair programming sessions with developers from 5 German companies and conducting 6 interviews with developers from 4 other German companies.", "result": "The study introduces the concepts of student and teacher roles to address knowledge gaps, identifies pitfalls, and formulates a grounded theory focusing on the \"Power Gap\" in pair programming.", "conclusion": "Knowledge transfer in pair programming can harm the process when developers neglect each other's needs and don't manage the Power Gap appropriately, potentially leading to defensive behaviors detrimental to collaboration and code quality."}}
{"id": "2507.09758", "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "The paper introduces a self-adaptive curriculum learning approach for NLP tasks, leveraging pre-trained language models to predict example difficulty, achieving faster convergence and better performance.", "motivation": "Existing curriculum learning methods rely on manually defined difficulty metrics, which may not accurately represent model perspectives.", "method": "A self-adaptive curriculum where pre-trained language models predict difficulty scores, and these scores guide fine-tuning strategies such as easy-to-hard, hard-to-easy, or mixed sampling.", "result": "The self-adaptive curriculum improves convergence speed and performance in natural language understanding tasks compared to random sampling.", "conclusion": "Self-adaptive curriculum learning, informed by difficulty scores from pre-trained models, offers a more efficient and effective training strategy for NLP applications."}}
{"id": "2507.10164", "pdf": "https://arxiv.org/pdf/2507.10164", "abs": "https://arxiv.org/abs/2507.10164", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "categories": ["cs.RO"], "comment": null, "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "AI": {"tldr": "The paper introduces an RL framework for bipedal robots with closed-chain dynamics, achieving better sim-to-real transfer and stability in locomotion.", "motivation": "Bipedal robots with closed kinematic chains pose unique training challenges due to the simplification of dynamics in RL approaches, hindering sim-to-real transfer.", "method": "An RL framework incorporating closed-chain dynamics with symmetry-aware loss functions, adversarial training, and network regularization was used.", "result": "The proposed method achieved superior locomotion stability across diverse terrains compared to simplified kinematic models.", "conclusion": "Incorporating closed-chain dynamics in RL frameworks improves sim-to-real transfer and robustness in bipedal robot locomotion."}}
{"id": "2507.10174", "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "AI": {"tldr": "The paper compares Decision Transformer (DT) and Filtered Behavior Cloning (FBC) in reinforcement learning tasks, arguing FBC shows better performance in sparse-reward environments.", "motivation": "To evaluate whether Decision Transformer (DT) is a suitable choice for reinforcement learning tasks, especially under sparse-reward and dense-reward settings.", "method": "Conduct experiments using robotic manipulation tasks (Robomimic) and locomotion benchmarks (D4RL), applying FBC which filters low-performing trajectories before performing ordinary behavior cloning.", "result": "Filtered Behavior Cloning (FBC) outperformed or matched DT in sparse-reward environments, requiring less training data and computational resources.", "conclusion": "The findings call into question DT's superiority in reinforcement learning tasks, suggesting FBC is a more efficient alternative in sparse-reward environments."}}
{"id": "2507.09269", "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "The paper presents Cross Knowledge Distillation (CKD) to improve Spiking Neural Networks (SNNs) with RGB data and ANN-based guidance, addressing cross-modality and cross-architecture challenges.", "motivation": "To enhance SNNs' performance using DVS data and overcome their limitations such as immature architectures and limited annotated datasets.", "method": "Introduced CKD, combining semantic similarity and sliding replacement for cross-modality issues and indirect phased knowledge distillation for cross-architecture problems.", "result": "Achieved superior performance on N-Caltech101 and CEP-DVS datasets, outperforming previous State-of-the-Art methods.", "conclusion": "CKD is an effective approach to improve SNNs in accuracy and efficiency by leveraging RGB data and ANN-driven knowledge distillation strategies."}}
{"id": "2507.09029", "pdf": "https://arxiv.org/pdf/2507.09029", "abs": "https://arxiv.org/abs/2507.09029", "authors": ["Vaibhav Singh", "Zafir Khalid", "Edouard Oyallon", "Eugene Belilovsky"], "title": "Model Parallelism With Subnetwork Data Parallelism", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.", "AI": {"tldr": "This paper introduces a novel distributed pre-training method that significantly reduces memory usage without loss in performance by training small structured subnetworks of models across separate workers.", "motivation": "To address the challenges of high memory demands and significant intra-node communication costs in the distributed pre-training of large models.", "method": "They propose training structured subnetworks in a distributed manner, avoiding inter-node activation communication, and evaluate two subnetwork construction strategies for balance and performance.", "result": "The stochastic block dropping technique outperformed width-wise subnetwork construction, enabling a 20-40% reduction in memory usage while maintaining performance.", "conclusion": "The approach effectively reduces memory usage in distributed training systems and shows potential for widespread adoption in optimizing resource usage for large model training."}}
{"id": "2507.09110", "pdf": "https://arxiv.org/pdf/2507.09110", "abs": "https://arxiv.org/abs/2507.09110", "authors": ["Zhiqi Bu", "Jason M. Klusowski", "Cynthia Rush", "Ruijia Wu"], "title": "Sharp Trade-Offs in High-Dimensional Inference via 2-Level SLOPE", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "Among techniques for high-dimensional linear regression, Sorted L-One\nPenalized Estimation (SLOPE) generalizes the LASSO via an adaptive $l_1$\nregularization that applies heavier penalties to larger coefficients in the\nmodel. To achieve such adaptivity, SLOPE requires the specification of a\ncomplex hierarchy of penalties, i.e., a monotone penalty sequence in $R^p$, in\ncontrast to a single penalty scalar for LASSO. Tuning this sequence when $p$ is\nlarge poses a challenge, as brute force search over a grid of values is\ncomputationally prohibitive. In this work, we study the 2-level SLOPE, an\nimportant subclass of SLOPE, with only three hyperparameters. We demonstrate\nboth empirically and analytically that 2-level SLOPE not only preserves the\nadvantages of general SLOPE -- such as improved mean squared error and\novercoming the Donoho-Tanner power limit -- but also exhibits computational\nbenefits by reducing the penalty hyperparameter space. In particular, we prove\nthat 2-level SLOPE admits a sharp, theoretically tight characterization of the\ntrade-off between true positive proportion (TPP) and false discovery proportion\n(FDP), contrasting with general SLOPE where only upper and lower bounds are\nknown. Empirical evaluations further underscore the effectiveness of 2-level\nSLOPE in settings where predictors exhibit high correlation, when the noise is\nlarge, or when the underlying signal is not sparse. Our results suggest that\n2-level SLOPE offers a robust, scalable alternative to both LASSO and general\nSLOPE, making it particularly suited for practical high-dimensional data\nanalysis.", "AI": {"tldr": "2-level SLOPE simplifies the hyperparameter tuning of the SLOPE method for high-dimensional linear regression while retaining its advantages and computational benefits.", "motivation": "The complexity of tuning a monotone penalty sequence in high-dimensional regression settings makes general SLOPE challenging to deploy. There is a need for a simplified yet effective alternative.", "method": "The authors focus on 2-level SLOPE, an important subclass of SLOPE with only three hyperparameters, and analyze its theoretical and empirical performance.", "result": "2-level SLOPE maintains the benefits of general SLOPE while reducing computational complexity. It provides tight theoretical characterizations of trade-offs like true positive proportion and false discovery proportion.", "conclusion": "2-level SLOPE is a robust, scalable alternative to both LASSO and general SLOPE for high-dimensional data analysis, especially in challenging settings like high noise or non-sparse signals."}}
{"id": "2507.10321", "pdf": "https://arxiv.org/pdf/2507.10321", "abs": "https://arxiv.org/abs/2507.10321", "authors": ["Viktor Sinitsyn", "Nils Schlautmann", "Florian Schwaiger", "Florian Holzapfel"], "title": "Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation", "categories": ["cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "The aerospace industry has experienced significant transformations over the\nlast decade, driven by technological advancements and innovative solutions in\ngoods and personal transportation. This evolution has spurred the emergence of\nnumerous start-ups that now face challenges traditionally encountered by\nestablished aerospace companies. Among these challenges is the efficient\nprocessing of digital intra-device communication interfaces for onboard\nequipment - a critical component for ensuring seamless system integration and\nfunctionality. Addressing this challenge requires solutions that emphasize\nclear and consistent interface descriptions, automation of processes, and\nreduced labor-intensive efforts.\n  This paper presents a novel process and toolchain designed to streamline the\ndevelopment of digital interfaces and onboard software, which our team has\nsuccessfully applied in several completed projects. The proposed approach\nfocuses on automation and flexibility while maintaining compliance with design\nassurance requirements.", "AI": {"tldr": "The paper introduces a novel process and toolchain to improve digital interface design and onboard software development in aerospace, addressing challenges like automation and reducing labor efforts.", "motivation": "The transformative changes in the aerospace industry have brought challenges in handling digital intra-device communications efficiently. Start-ups especially need solutions to streamline integration and functionality.", "method": "This research proposes a process and toolchain emphasizing automation, flexibility, and compliance with design assurance standards. It has been validated through practical applications in various projects.", "result": "The toolchain has been successfully implemented in multiple projects, proving its effectiveness in improving interface design efficiency and software development.", "conclusion": "Adopting this novel approach can guide aerospace start-ups and traditional companies in overcoming digital interface challenges, fostering seamless system integration, and reducing manual labor."}}
{"id": "2507.09777", "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr\u00e9 Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "The paper redefines clickbait, introduces a methodology for creating a clickbait detection dataset, and releases the first open-source Spanish dataset for this purpose (TA1C).", "motivation": "To address the lack of consensus on the definition of clickbait and refine the criteria for detecting it, particularly focusing on Spanish-language content.", "method": "They revise the definition of clickbait, propose a framework to minimize subjectivity in annotations, create a dataset of 3,500 Spanish tweets from 18 media sources, ensure high inter-annotator agreement, and provide strong baseline models for detection.", "result": "The resulting dataset (TA1C) achieved a 0.825 Fleiss' K inter-annotator agreement and the detection baseline models reached an F1-score of 0.84.", "conclusion": "The paper advances the study of clickbait by providing a clear definition, a rigorous annotation framework, and a high-quality dataset tailored to Spanish-language media."}}
{"id": "2507.10204", "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "The REACT framework introduces real-time path planning to prevent tether entanglement for underwater vehicles, ensuring safer and faster inspection missions.", "motivation": "To address the challenge of tether entanglement in underwater vehicle inspections, which can hinder the safety and efficiency of operations.", "method": "The REACT framework employs a geometry-based tether model, leveraging the signed distance field (SDF) map to ensure real-time simulation and tether length constraints. It integrates this model with coverage path planning for proactive entanglement avoidance.", "result": "REACT achieves faster mission completion (20% faster in simulations) while maintaining tether constraints. In real-world tests, it successfully avoids entanglement, unlike conventional planners.", "conclusion": "REACT effectively prevents tether entanglement during underwater inspections, ensuring efficient, safe, and complete coverage missions."}}
{"id": "2507.10208", "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "AI": {"tldr": "The paper discusses gaps in XAI research for data analysis and proposes a framework for categorization and reporting using three dimensions (what, why, who), aiming to address design inconsistencies.", "motivation": "The authors aim to address contradictions and lack of clear design recommendations in XAI research, especially due to inadequate task understanding and insufficient context reporting.", "method": "The paper draws from visual analytics, cognition, and dashboard design to create a categorization framework (what, why, who) and suggests reporting guidelines for XAI tasks.", "result": "The authors identified major issues like poor task descriptions and lack of context or target-user testing, and propose guidelines to improve study generalizability and clarity.", "conclusion": "This work provides researchers with tools to better navigate the XAI domain, address research gaps, and reconcile contradictory findings in XAI design."}}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "The paper introduces Prompt4Trust, a reinforcement learning framework to improve confidence calibration and task accuracy in multimodal large language models (MLLMs) for healthcare applications.", "motivation": "MLLMs, while promising in healthcare, are limited by sensitivity to prompts and confidence issues, which are critical in safety-critical domains where clinicians rely on models' confidence for decision-making.", "method": "Prompt4Trust employs reinforcement learning to train a lightweight LLM to generate auxiliary prompts that enhance the calibration and accuracy of downstream MLLMs, aligning confidence with true predictive accuracy.", "result": "The method achieved state-of-the-art performance on the PMC-VQA benchmark for medical visual question answering, improved task accuracy, and demonstrated the ability to generalize to larger MLLMs with minimal computational overhead.", "conclusion": "Prompt4Trust shows potential for scalable, automated prompt engineering that aligns MLLM outputs with human trust requirements, making them more reliable in clinical decision-making scenarios."}}
{"id": "2507.09031", "pdf": "https://arxiv.org/pdf/2507.09031", "abs": "https://arxiv.org/abs/2507.09031", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "AI": {"tldr": "The study proposes the Recursive MDN (R-MDN) layer to address challenges in removing confounder effects during continual learning, achieving equitable predictions by mitigating bias and catastrophic forgetting.", "motivation": "Confounders cause biased predictions due to spurious correlations between input and target variables, particularly challenging in continual learning where systems need to adapt continuously.", "method": "The R-MDN layer uses statistical regression with the recursive least squares algorithm to adapt feature representations by continually updating to account for changing data distributions and confounders.", "result": "Experiments showed that R-MDN promotes equitable predictions for population groups and reduces catastrophic forgetting during both static and continual learning setups.", "conclusion": "R-MDN effectively integrates into various deep learning models to learn confounder-invariant features, improving predictive equity and robustness in dynamic learning environments."}}
{"id": "2507.09119", "pdf": "https://arxiv.org/pdf/2507.09119", "abs": "https://arxiv.org/abs/2507.09119", "authors": ["Stephen Salerno", "Kentaro Hoffman", "Awan Afiaz", "Anna Neufeld", "Tyler H. McCormick", "Jeffrey T. Leek"], "title": "A Moment-Based Generalization to Post-Prediction Inference", "categories": ["stat.ME", "stat.ML"], "comment": "9 pages", "summary": "Artificial intelligence (AI) and machine learning (ML) are increasingly used\nto generate data for downstream analyses, yet naively treating these\npredictions as true observations can lead to biased results and incorrect\ninference. Wang et al. (2020) proposed a method, post-prediction inference,\nwhich calibrates inference by modeling the relationship between AI/ML-predicted\nand observed outcomes in a small, gold-standard sample. Since then, several\nmethods have been developed for inference with predicted data. We revisit Wang\net al. in light of these recent developments. We reflect on their assumptions\nand offer a simple extension of their method which relaxes these assumptions.\nOur extension (1) yields unbiased point estimates under standard conditions and\n(2) incorporates a simple scaling factor to preserve calibration variability.\nIn extensive simulations, we show that our method maintains nominal Type I\nerror rates, reduces bias, and achieves proper coverage.", "AI": {"tldr": "This paper revisits Wang et al.'s (2020) post-prediction inference method, extends it with relaxed assumptions, and demonstrates improved bias reduction and calibration.", "motivation": "To address the biases and errors that arise when AI/ML-predicted data is naively used as true observations for analysis and to build upon Wang et al.\u2019s method for more reliable inference.", "method": "The authors extend Wang et al.'s post-prediction inference method by introducing modifications to relax assumptions and include a scaling factor to better handle variability.", "result": "The proposed extension achieves unbiased point estimates under standard conditions, maintains Type I error rates, reduces bias, and ensures proper coverage in simulations.", "conclusion": "The extended method improves reliability and accuracy in analyses involving ML-predicted data by appropriately calibrating variability and assumptions."}}
{"id": "2507.09875", "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "This paper examines how large language models generalize to unseen tasks, focusing on the off-by-one addition task, using circuit-style interpretability techniques.", "motivation": "To understand the mechanisms inside language models that enable task-level generalization, particularly in unseen tasks via in-context learning.", "method": "The study uses an off-by-one addition task as a case study and employs circuit-style interpretability techniques like path patching to analyze internal computations.", "result": "It identifies a mechanism resembling induction heads at a higher abstraction level, shows parallelized contributions from attention heads for the +1 function, and demonstrates that this mechanism generalizes across other tasks.", "conclusion": "The findings highlight that reusable and composable structures in language models underpin their ability to generalize to new tasks."}}
{"id": "2507.10284", "pdf": "https://arxiv.org/pdf/2507.10284", "abs": "https://arxiv.org/abs/2507.10284", "authors": ["Venkat Margapuri"], "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "AI": {"tldr": "The paper introduces Prompt-Informed Reinforcement Learning (PIRL), combining LLMs like GPT-3.5 with RL for optimizing visual coverage path planning by UAVs.", "motivation": "Traditional RL struggles with adaptability due to fixed, environment-specific reward systems. The study aims to improve RL by leveraging semantic adaptability from LLMs within UAV visual coverage applications.", "method": "Proposed a PIRL framework where feedback from GPT-3.5 shapes rewards dynamically for the PPO agent in UAV motion and camera control, tested across various settings.", "result": "PIRL outperformed baselines, achieving up to 27% higher coverage, 25% better battery efficiency, and 18% lower redundancy in simulated environments.", "conclusion": "LLM-integrated reward shaping, as demonstrated by PIRL, is effective for spatial exploration in robotics and opens new directions for incorporating natural language into RL."}}
{"id": "2507.10281", "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "AI": {"tldr": "The paper surveys LLM-based Table Agents designed to automate workflows involving tables by addressing noise, structural heterogeneity, and semantic complexity challenging real-world data. Five core competencies are defined for evaluation, highlighting a performance gap between academic and practical benchmarks.", "motivation": "The paper aims to tackle real-world challenges in handling noisy and heterogeneous tables, a problem often ignored in academic research that assumes clean datasets. The goal is to enable automated solutions through LLMs for practical, table-centric tasks.", "method": "The authors propose five core competencies to evaluate current LLM approaches to tables: structure understanding, semantic interpretation, data retrieval/compression, reasoning traceability, and domain generalization. They analyze approaches, focusing on Text-to-SQL Agents.", "result": "The study reveals a notable performance discrepancy between open-source Text-to-SQL agents tested on academic benchmarks versus real-world, noisy scenarios. It identifies weaknesses in adapting to domain-specific challenges in practical settings.", "conclusion": "Actionable insights are shared to enhance the efficiency, robustness, and generalization abilities of LLM-based Table Agents, catering specifically to real-world demands."}}
{"id": "2507.09285", "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "The paper proposes using a deep generative model (GAN) to better initialize the blur kernel for blind motion deblurring (BMD), improving performance and robustness.", "motivation": "Existing deep prior-based methods for blind motion deblurring struggle due to high non-convexity and sensitivity to initial blur kernel estimation.", "method": "The authors pretrain a Generative Adversarial Network (GAN)-based kernel generator and kernel initializer to encode the kernel prior and provide a high-quality initialization for BMD.", "result": "Their approach reduces kernel initialization sensitivity, achieves compact latent kernel manifold constraints, extends to non-uniform motion deblurring, and sets new state-of-the-art results on benchmark datasets.", "conclusion": "The proposed framework significantly enhances blind motion deblurring methods, operates in a plug-and-play manner, and opens avenues for practical applications with shared source code."}}
{"id": "2507.09041", "pdf": "https://arxiv.org/pdf/2507.09041", "abs": "https://arxiv.org/abs/2507.09041", "authors": ["Andrew Wagenmaker", "Zhiyuan Zhou", "Sergey Levine"], "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Developing autonomous agents that quickly explore an environment and adapt\ntheir behavior online is a canonical challenge in robotics and machine\nlearning. While humans are able to achieve such fast online exploration and\nadaptation, often acquiring new information and skills in only a handful of\ninteractions, existing algorithmic approaches tend to rely on random\nexploration and slow, gradient-based behavior updates. How can we endow\nautonomous agents with such capabilities on par with humans? Taking inspiration\nfrom recent progress on both in-context learning and large-scale behavioral\ncloning, in this work we propose behavioral exploration: training agents to\ninternalize what it means to explore and adapt in-context over the space of\n``expert'' behaviors. To achieve this, given access to a dataset of expert\ndemonstrations, we train a long-context generative model to predict expert\nactions conditioned on a context of past observations and a measure of how\n``exploratory'' the expert's behaviors are relative to this context. This\nenables the model to not only mimic the behavior of an expert, but also, by\nfeeding its past history of interactions into its context, to select different\nexpert behaviors than what have been previously selected, thereby allowing for\nfast online adaptation and targeted, ``expert-like'' exploration. We\ndemonstrate the effectiveness of our method in both simulated locomotion and\nmanipulation settings, as well as on real-world robotic manipulation tasks,\nillustrating its ability to learn adaptive, exploratory behavior.", "AI": {"tldr": "The paper introduces 'behavioral exploration,' where agents learn adaptive, exploratory behavior using a generative model trained on expert demonstrations.", "motivation": "Current autonomous agents require significant interaction to adapt effectively, unlike humans, who can adapt quickly and purposefully.", "method": "The authors train a long-context generative model using expert actions conditioned on past observations and exploratory measures from expert demonstrations.", "result": "The method shows success in both simulated (locomotion and manipulation) and real-world robotic manipulation tasks, demonstrating adaptive, exploratory behavior.", "conclusion": "Behavioral exploration provides an effective method for enabling fast online adaptation and targeted exploration for autonomous agents."}}
{"id": "2507.09127", "pdf": "https://arxiv.org/pdf/2507.09127", "abs": "https://arxiv.org/abs/2507.09127", "authors": ["Harshil Kotamreddy", "Marlos C. Machado"], "title": "A Study of Value-Aware Eigenoptions", "categories": ["cs.LG", "stat.ML"], "comment": "Presented at the RLC Workshop on Inductive Biases in Reinforcement\n  Learning 2025", "summary": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.", "AI": {"tldr": "This paper explores the role of eigenoptions in improving exploration and credit assignment in reinforcement learning, evaluating their impact in tabular and deep RL contexts.", "motivation": "Options in RL are often handcrafted and not learned, even though they can be a powerful tool for adding temporal and hierarchical structures. This paper seeks to address the underexplored role of eigenoptions in facilitating credit assignment.", "method": "The authors evaluate the impact of pre-specified and online-discovered eigenoptions in both tabular and pixel-based gridworlds, and propose a method for learning option-values with non-linear function approximation in deep RL.", "result": "Pre-specified eigenoptions improve both exploration and credit assignment, while online-discovered eigenoptions may bias the agent's experience counter-productively. Termination conditions in deep RL also significantly impact performance.", "conclusion": "Eigenoptions hold promise for enhancing reinforcement learning, but their complexity, specifically in online discovery and termination conditions, requires careful consideration to balance exploration and credit assignment."}}
{"id": "2507.10422", "pdf": "https://arxiv.org/pdf/2507.10422", "abs": "https://arxiv.org/abs/2507.10422", "authors": ["Tao Xiao", "Youmei Fan", "Fabio Calefato", "Christoph Treude", "Raula Gaikovina Kula", "Hideaki Hata", "Sebastian Baltes"], "title": "Self-Admitted GenAI Usage in Open-Source Software", "categories": ["cs.SE"], "comment": "17 pages, 8 tables, 1 figures, currently under review", "summary": "The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot\nand ChatGPT is transforming software development. Since generated source code\nis virtually impossible to distinguish from manually written code, their\nreal-world usage and impact on open-source software development remain poorly\nunderstood. In this paper, we introduce the concept of self-admitted GenAI\nusage, that is, developers explicitly referring to the use of GenAI tools for\ncontent creation in software artifacts. Using this concept as a lens to study\nhow GenAI tools are integrated into open-source software projects, we analyze a\ncurated sample of more than 250,000 GitHub repositories, identifying 1,292 such\nself-admissions across 156 repositories in commit messages, code comments, and\nproject documentation. Using a mixed methods approach, we derive a taxonomy of\n32 tasks, 10 content types, and 11 purposes associated with GenAI usage based\non 284 qualitatively coded mentions. We then analyze 13 documents with policies\nand usage guidelines for GenAI tools and conduct a developer survey to uncover\nthe ethical, legal, and practical concerns behind them. Our findings reveal\nthat developers actively manage how GenAI is used in their projects,\nhighlighting the need for project-level transparency, attribution, and quality\ncontrol practices in the new era of AI-assisted software development. Finally,\nwe examine the longitudinal impact of GenAI adoption on code churn in 151\nrepositories with self-admitted GenAI usage and find no general increase,\ncontradicting popular narratives on the impact of GenAI on software\ndevelopment.", "AI": {"tldr": "This paper studies the role of generative AI tools in software development through the lens of developers' explicit mentions of using such tools, analyzing their practical, ethical, and legal implications.", "motivation": "To understand how generative AI tools like ChatGPT and GitHub Copilot are being integrated into open-source software development and their real-world impact.", "method": "The study analyzed over 250,000 GitHub repositories for explicit mentions of generative AI usage, developed a taxonomy of tasks, content types, and purposes, and conducted a developer survey. It also studied policies and guidelines and examined code churn trends.", "result": "The paper identified 1,292 explicit mentions of AI usage across 156 repositories, revealing diverse applications. Developers manage AI use with a focus on transparency and quality control. Contrary to popular beliefs, no broad increase in code churn due to AI adoption was observed.", "conclusion": "Developers are actively shaping how generative AI is utilized in software development, emphasizing the need for better project-level governance and guidelines to address transparency, attribution, and quality control challenges."}}
{"id": "2507.09935", "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "The paper proposes a framework to improve Retrieval-Augmented Generation (RAG) systems by using hierarchical text segmentation and clustering for better chunking strategies.", "motivation": "Traditional chunking methods for Retrieval-Augmented Generation fail to create semantically meaningful chunks as they disregard the textual structure.", "method": "The framework integrates hierarchical text segmentation and clustering to produce semantically coherent chunks, and uses both segment-level and cluster-level vector representations during inference for retrieval.", "result": "Evaluations on NarrativeQA, QuALITY, and QASPER datasets show improved performance compared to traditional chunking methods.", "conclusion": "Integrating hierarchical segmentation and clustering into RAG systems leads to retrieving more contextually relevant and precise information, enhancing their overall effectiveness."}}
{"id": "2507.10290", "pdf": "https://arxiv.org/pdf/2507.10290", "abs": "https://arxiv.org/abs/2507.10290", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "categories": ["cs.RO"], "comment": "8 pages, submitted to RA-L", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "AI": {"tldr": "The paper proposes a novel trajectory optimization framework using CADMM-based algorithm tailored for parallel computing, achieving faster and smoother results compared to existing methods, especially for large-scale trajectories.", "motivation": "Traditional trajectory optimization methods struggle with efficiency and scalability, especially for large-scale trajectories. The paper seeks to address these limitations by leveraging parallel computing.", "method": "The authors use a CADMM-based algorithm that decomposes trajectories into multiple segments, solving subproblems concurrently. This includes a closed-form solution for linear and quadratic constraints and numerical solutions for inequality constraints.", "result": "Experiments verify that the proposed method achieves over a tenfold speed-up and improved smoothness for large-scale trajectories compared to SOTA methods. GPU deployment further showcases high performance with thousands of segments.", "conclusion": "CADMM-based parallel trajectory optimization enables scalable and efficient solutions for large-scale trajectories, demonstrating significant advancements in speed and smoothness while leveraging modern computing architectures."}}
{"id": "2507.10397", "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouv\u00eaa", "Nuno Paulos", "Eduardo Uchoa e Mari\u00e1 C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "categories": ["cs.AI"], "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "AI": {"tldr": "This paper introduces Instance Space Analysis (ISA) to analyze the relationships between instance characteristics and metaheuristic performance in CVRP using machine learning methods, dimensionality reduction, and a dataset from DIMACS.", "motivation": "There is a need to understand how instance characteristics influence metaheuristic performance in solving Capacitated Vehicle Routing Problems (CVRP).", "method": "The study employed three stages\u2014PRELIM, SIFTED, and PILOT\u2014using dimensionality reduction and machine learning techniques to create a two-dimensional projection of the instance space.", "result": "The analysis identified 23 relevant instance characteristics and provided a projection matrix for incorporating new instances, enabling deeper insights into the CVRP field.", "conclusion": "This work contributes a new method for analyzing instances in CVRP and offers tools for better understanding and improving metaheuristic approaches."}}
{"id": "2507.09291", "pdf": "https://arxiv.org/pdf/2507.09291", "abs": "https://arxiv.org/abs/2507.09291", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "title": "Supercharging Floorplan Localization with Semantic Rays", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICCV 2025", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "AI": {"tldr": "This paper introduces a semantic-aware localization framework for floorplans to enhance location estimation using both depth and semantic data.", "motivation": "Current floorplan localization methods primarily focus on depth information and neglect the rich semantics present in floorplans.", "method": "The framework jointly predicts depth and semantic rays, consolidating them into a structural-semantic probability volume refined in a coarse-to-fine manner for location and orientation prediction.", "result": "The method outperforms state-of-the-art techniques significantly, demonstrating higher recall metrics and efficient utilization of metadata like room labels.", "conclusion": "The introduced framework effectively enhances floorplan localization by leveraging structural and semantic information, achieving higher accuracy and room for additional metadata incorporation."}}
{"id": "2507.09177", "pdf": "https://arxiv.org/pdf/2507.09177", "abs": "https://arxiv.org/abs/2507.09177", "authors": ["Zichen Liu", "Guoji Fu", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Continual Reinforcement Learning by Planning with Online World Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025 Spotlight", "summary": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "AI": {"tldr": "The paper addresses catastrophic forgetting in continual reinforcement learning (CRL) by using a Follow-The-Leader (FTL) online world model for planning tasks, proving its effectiveness in retaining skills while solving new tasks.", "motivation": "The motivation is to tackle catastrophic forgetting in CRL, where agents fail to retain knowledge from previously learned tasks when adapting to new ones.", "method": "The method involves creating an online Follow-The-Leader (FTL) shallow world model with model predictive control, which avoids forgetting by construction and updates incrementally.", "result": "The proposed FTL Online Agent (OA) outperforms strong baselines in a dedicated evaluation environment, demonstrating the ability to solve new tasks while remembering old ones without relying on deep world models.", "conclusion": "The FTL Online Agent provides a robust solution to CRL's catastrophic forgetting problem, ensuring continual learning and skill retention through its unique approach to planning with online world models."}}
{"id": "2507.09973", "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "categories": ["cs.CL", "cs.AI"], "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "TinyRM introduces small bidirectional masked language models that achieve performance comparable to much larger models on specific tasks, offering cost-efficiency and scalability in reward modeling.", "motivation": "The high inference costs of large decoder-based language models in RLHF pose challenges for deployment during test-time, necessitating more resource-efficient solutions.", "method": "TinyRM employs a combination of FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to train small masked language models for reward modeling.", "result": "Small models like TinyRM, with only 400 million parameters, showcase competitive performance on RewardBench tasks, rivaling models that are significantly larger, particularly in reasoning tasks.", "conclusion": "Lightweight and efficient models, such as TinyRM, are a promising direction for scalable reward modeling, though challenges around generalist and conversational modeling remain."}}
{"id": "2507.10310", "pdf": "https://arxiv.org/pdf/2507.10310", "abs": "https://arxiv.org/abs/2507.10310", "authors": ["Michael Schr\u00f6der", "Eric Sch\u00f6neberg", "Daniel G\u00f6rges", "Hans D. Schotten"], "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "AI": {"tldr": "The paper resolves the incompatibility between spatially discrete cost-maps and model predictive control (MPC) by re-formulating occupancy grids into continuously differentiable functions using fuzzy logic, enabling their use in trajectory planning and beyond.", "motivation": "The need to enable compatibility between spatially discrete occupancy grid maps and model predictive control (MPC), as obstacles defined on grids pose challenges for continuous differentiability required by MPC solvers.", "method": "The method involves redefining obstacles as polygons represented by intersections of half-spaces and using fuzzy logic to turn logical operators into inequality constraints, making them compatible with MPC formulations.", "result": "The approach is tested successfully in simulation for MPC-based trajectory planning, demonstrating its feasibility and effectiveness.", "conclusion": "The work enables obstacle avoidance in MPC by ensuring compatibility via fuzzy logic and extends its applicability to other domains that require incorporation of logical constraints into MPC frameworks."}}
{"id": "2507.10421", "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "AI": {"tldr": "The paper tackles the issue of school dropout in distance learning by presenting a model that integrates sentiment analysis (using BERT) and socio-demographic/behavioral data (processed with XGBoost).", "motivation": "The motivation is to combat the serious problem of school dropout in distance learning, recognizing the need for early detection and effective intervention to promote student perseverance.", "method": "The approach combines sentiment analysis of student comments with socio-demographic and behavioral data using two machine learning tools: fine-tuning BERT for nuanced sentiment analysis and using XGBoost for feature importance and predictions.", "result": "The model achieved an accuracy of 84% on unseen data, outperforming the baseline model (82%) along with better precision and F1-score results.", "conclusion": "The study highlights the efficacy of combining sentiment analysis and diverse educational data sources, demonstrating its potential to aid in developing personalized strategies to mitigate dropout rates."}}
{"id": "2507.09294", "pdf": "https://arxiv.org/pdf/2507.09294", "abs": "https://arxiv.org/abs/2507.09294", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "categories": ["cs.CV", "cs.RO"], "comment": "IEEE ICIA 2025", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "AI": {"tldr": "The paper proposes Geo-RepNet, a framework using RGB images and depth information to improve surgical phase recognition during Endoscopic Submucosal Dissection, achieving state-of-the-art performance.", "motivation": "Improving surgical phase recognition in minimally invasive procedures is essential for intelligent systems, but visual similarity across phases and lack of structural cues in RGB images are challenging.", "method": "The paper introduces Geo-RepNet, which combines RGB and depth data using a RepVGG backbone, a Depth-Guided Geometric Prior Generation (DGPG) module, and Geometry-Enhanced Multi-scale Attention (GEMA) for spatial insight.", "result": "The model, evaluated on a nine-phase ESD dataset, exhibits state-of-the-art accuracy, robustness, and computational efficiency under challenging surgical environments.", "conclusion": "Geo-RepNet effectively enhances surgical scene understanding, showcasing the advantages of integrating depth and RGB information for surgical phase recognition."}}
{"id": "2507.09178", "pdf": "https://arxiv.org/pdf/2507.09178", "abs": "https://arxiv.org/abs/2507.09178", "authors": ["Liang Ding", "Simon Mak", "C. F. Jeff Wu"], "title": "The BdryMat\u00e9rn GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Gaussian processes (GPs) are broadly used as surrogate models for expensive\ncomputer simulators of complex phenomena. However, a key bottleneck is that its\ntraining data are generated from this expensive simulator and thus can be\nhighly limited. A promising solution is to supplement the learning model with\nboundary information from scientific knowledge. However, despite recent work on\nboundary-integrated GPs, such models largely cannot accommodate boundary\ninformation on irregular (i.e., non-hypercube) domains, and do not provide\nsample path smoothness control or approximation error analysis, both of which\nare important for reliable surrogate modeling. We thus propose a novel\nBdryMat\\'ern GP modeling framework, which can reliably integrate Dirichlet,\nNeumann and Robin boundaries on an irregular connected domain with a boundary\nset that is twice-differentiable almost everywhere. Our model leverages a new\nBdryMat\\'ern covariance kernel derived in path integral form via a stochastic\npartial differential equation formulation. Similar to the GP with Mat\\'ern\nkernel, we prove that sample paths from the BdryMat\\'ern GP satisfy the desired\nboundaries with smoothness control on its derivatives. We further present an\nefficient approximation procedure for the BdryMat\\'ern kernel using finite\nelement modeling with rigorous error analysis. Finally, we demonstrate the\neffectiveness of the BdryMat\\'ern GP in a suite of numerical experiments on\nincorporating broad boundaries on irregular domains.", "AI": {"tldr": "The paper introduces a new Gaussian Process (GP) framework, \"BdryMat\u00e9rn GP,\" designed to integrate boundary information on irregular domains with smoothness control and error analysis.", "motivation": "Traditional GP models face difficulties with limited training data due to the computational cost of simulations and cannot effectively incorporate boundary information on non-hypercube domains.", "method": "The BdryMat\u00e9rn GP uses a novel covariance kernel obtained through a stochastic partial differential equation formulation, incorporating boundary conditions like Dirichlet, Neumann, and Robin on irregular domains. It also employs finite element modeling for efficient kernel approximation with error analysis.", "result": "The model successfully integrates complex boundaries while controlling sample smoothness, demonstrated through numerical experiments on various irregular domains.", "conclusion": "BdryMat\u00e9rn GP offers a reliable and computationally efficient solution for integrating boundary information into surrogate modeling on irregular domains, overcoming key limitations of traditional GP frameworks."}}
{"id": "2507.09982", "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "TextOmics introduces a benchmark aligning omics data with molecular textual descriptions, enabling advancements in molecular generation. ToDi, a generative framework, uses this alignment for creating therapeutic molecules with high precision.", "motivation": "The paper aims to address the lack of heterogeneous data and unified frameworks for molecular generation in drug discovery.", "method": "The authors built TextOmics as a benchmark for omics-to-text molecular data alignment. They developed ToDi by leveraging dual encoders (OmicsEn and TextEn) and a conditional diffusion model (DiffGen) to generate biologically and chemically relevant molecules.", "result": "ToDi, based on TextOmics, demonstrated superior performance in generating hit-like therapeutic molecules compared to current methods, excelling even under zero-shot conditions.", "conclusion": "TextOmics and ToDi represent significant advancements, providing a robust foundation for target-specific drug discovery via heterogeneous molecular generation methods."}}
{"id": "2507.10376", "pdf": "https://arxiv.org/pdf/2507.10376", "abs": "https://arxiv.org/abs/2507.10376", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "categories": ["cs.RO", "I.2"], "comment": "8 pages", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "AI": {"tldr": "A deep learning-based motion estimator combining visual, inertial, and radar data improves odometry accuracy in harsh weather conditions like snow and rain.", "motivation": "Existing autonomous driving sensors struggle under harsh weather and environmental challenges, especially visual sensors.", "method": "The study employs advanced deep learning sensor fusion, dynamically balancing sensor contributions (e.g., radar compensating for visual limitations).", "result": "The proposed model demonstrates robust performance in diverse conditions using the Boreas dataset.", "conclusion": "Radar can significantly enhance pose estimation, especially when visual sensors fail, making it a vital tool in autonomous systems."}}
{"id": "2507.10446", "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "AI": {"tldr": "The paper introduces efficient neural architectures for learning priors in data-scarce domains, applying these designs to 3D scene generation, segmentation, and molecular property prediction.", "motivation": "To enhance the ability of neural networks to adapt to novel tasks with limited data, especially in domains like computational chemistry, medical imaging, and computational immunology where large pre-trained models cannot be employed.", "method": "Utilizes neural memory for adaptation to non-stationary distributions with few samples, hypernetworks trained with Model-Agnostic Meta-Learning (MAML), and a repurposed molecular generative method for pre-training.", "result": "Demonstrated efficient prior acquisition for fast text-to-3D generation, 3D segmentation on new scenes, and improved molecular property prediction with limited data.", "conclusion": "The proposed architectures effectively address data limitations by enabling efficient prior transfer and adaptation, offering solutions for various computational and scientific challenges."}}
{"id": "2507.09299", "pdf": "https://arxiv.org/pdf/2507.09299", "abs": "https://arxiv.org/abs/2507.09299", "authors": ["Abdulvahap Mutlu", "\u015eeng\u00fcl Do\u011fan", "T\u00fcrker Tuncer"], "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "AI": {"tldr": "ViT-ProtoNet integrates Vision Transformers (ViTs) with Prototypical Networks for few-shot image classification, achieving superior performance over CNN-based methods.", "motivation": "The paper aims to harness the underutilized representational capabilities of Vision Transformers for few-shot classification tasks, addressing challenges in learning from limited examples.", "method": "ViT-ProtoNet incorporates a ViT-Small backbone into the Prototypical Network framework, using class conditional token embeddings to create prototypes and evaluate them across benchmarks.", "result": "ViT-ProtoNet outperformed CNN-based counterparts in accuracy (up to 3.2% better) and demonstrated superior robustness, feature separability, and competitiveness against other transformer-based models.", "conclusion": "ViT-ProtoNet establishes itself as a powerful approach for few-shot image classification, setting a new baseline for transformer-based meta-learning with code and pretrained weights provided for reproducibility."}}
{"id": "2507.09084", "pdf": "https://arxiv.org/pdf/2507.09084", "abs": "https://arxiv.org/abs/2507.09084", "authors": ["Nnamdi Daniel Aghanya", "Ta Duong Vu", "Ama\u00eblle Diop", "Charlotte Deville", "Nour Imane Kerroumi", "Irene Moulitsas", "Jun Li", "Desmond Bisandu"], "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction", "categories": ["cs.LG", "cs.AI", "68T07, 90B22, 62M10", "I.2.m"], "comment": "3 figures, 20 pages references and appendix included,", "summary": "Flight delays are a significant challenge in the aviation industry, causing\nmajor financial and operational disruptions. To improve passenger experience\nand reduce revenue loss, flight delay prediction models must be both precise\nand generalizable across different networks. This paper introduces a novel\napproach that combines Queue-Theory with a simple attention model, referred to\nas the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from\nthe US Bureau of Transportation Statistics, where our proposed QT-SimAM\n(Bidirectional) model outperformed existing methods with an accuracy of 0.927\nand an F1 score of 0.932. To assess transferability, we tested the model on the\nEUROCONTROL dataset. The results demonstrated strong performance, achieving an\naccuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an\neffective, end-to-end methodology for predicting flight delays. The proposed\nmodel's ability to forecast delays with high accuracy across different networks\ncan help reduce passenger anxiety and improve operational decision-making", "AI": {"tldr": "The paper introduces a new predictive model for flight delays, QT-SimAM, which uses Queue-Theory and an attention mechanism, showing high accuracy on US and EU datasets.", "motivation": "Flight delays cause financial losses and inconvenience. Predicting them accurately can improve passenger experience and operational efficiency.", "method": "The proposed QT-SimAM model combines Queue-Theory and a simple attention mechanism, validated using datasets from the US and the EU.", "result": "QT-SimAM achieved an accuracy of 0.927 and F1 score of 0.932 on US data, and 0.826 accuracy with 0.791 F1 on EU data, outperforming existing methods.", "conclusion": "QT-SimAM provides an effective, transferable way to predict flight delays, aiding in operational decision-making and reducing passenger inconvenience."}}
{"id": "2507.09211", "pdf": "https://arxiv.org/pdf/2507.09211", "abs": "https://arxiv.org/abs/2507.09211", "authors": ["Xinyue Liu", "Xiao Peng", "Shuyue Yan", "Yuntian Chen", "Dongxiao Zhang", "Zhixiao Niu", "Hui-Min Wang", "Xiaogang He"], "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an", "physics.geo-ph", "stat.ML"], "comment": null, "summary": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.", "AI": {"tldr": "This paper introduces DeepX-GAN to model rare, unseen climate extremes beyond historical records, revealing latent risks and informing adaptive policies.", "motivation": "Existing climate records miss unseen extremes and neglect spatial dependencies, undermining risk assessment and adaptation strategies.", "method": "DeepX-GAN is proposed as a deep generative model informed by knowledge to simulate spatially dependent unseen extremes with zero-shot generalizability.", "result": "DeepX-GAN reveals vulnerable regions like MENA, and predicts future warming could shift risk hotspots to Indo-Pakistan and Central Africa.", "conclusion": "Addressing unseen extremes requires spatially adaptive policies to anticipate emerging risk hotspots, rather than relying on historical data extrapolation."}}
{"id": "2507.10008", "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "This paper proposes a framework using Reddit data to dynamically predict suicide risk transitions by jointly learning risk and protective factors' influence.", "motivation": "Current suicide risk prediction models focus only on risk factors and ignore rapid mental state changes and the protective factors that mitigate suicide risk.", "method": "The study develops a Protective Factor-Aware Dataset and a Dynamic Factors Influence Learning method to capture the evolving role of both risk and protective factors over 12 years of Reddit posts.", "result": "Experiments reveal the model\u2019s superiority over state-of-the-art models and large language models, while yielding interpretable insights into suicide risk patterns.", "conclusion": "The framework improves predictive accuracy and provides valuable insights for clinicians, aiding in better understanding and intervention of suicidal behaviors."}}
{"id": "2507.10500", "pdf": "https://arxiv.org/pdf/2507.10500", "abs": "https://arxiv.org/abs/2507.10500", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "AI": {"tldr": "The paper introduces Scene-Aware Conversational ADAS (SC-ADAS), a framework combining AI and sensor-based insights to provide adaptive, dialogue-based driver assistance.", "motivation": "Current ADAS systems cannot engage in natural language interaction or adapt dynamically, limiting their functionality.", "method": "The SC-ADAS framework integrates Generative AI tools like large language models and vision-to-text systems to provide real-time scene-aware assistance.", "result": "The system was implemented in the CARLA simulator, with evaluations showcasing trade-offs like latency during scene retrieval and the impact of dialogue history.", "conclusion": "SC-ADAS proves the feasibility of advancing driver assistance systems through conversational reasoning and modular control, signaling a step towards intelligent ADAS solutions."}}
{"id": "2507.10522", "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch$^{Eco}$ is an LLM-based system designed for scientific analysis, offering depth- and breadth-controlled exploration to enhance retrieval and integration of scientific literature.", "motivation": "The paper aims to address limitations in conventional retrieval-augmented pipelines by offering a more nuanced, user-configurable approach to scientific synthesis.", "method": "DeepResearch$^{Eco}$ employs recursive exploration with transparent reasoning and configurability to integrate diverse scientific sources while maintaining analytical rigor.", "result": "When applied to 49 ecological research questions, DeepResearch$^{Eco}$ showed significant improvements, including a 21-fold increase in source integration and a 14.9-fold rise in integrated sources per 1,000 words.", "conclusion": "DeepResearch$^{Eco}$ enhances analytical depth and contextual diversity in scientific exploration, providing a high-throughput approach for integrating domain-specific evidence with expert-level precision."}}
{"id": "2507.09305", "pdf": "https://arxiv.org/pdf/2507.09305", "abs": "https://arxiv.org/abs/2507.09305", "authors": ["Zhiwei Xu"], "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "AI": {"tldr": "The paper introduces the Deep Angular A* (DAA*) method to improve path imitation learning by enhancing path smoothness and similarity. It demonstrates superior performance in seven datasets compared to existing methods like Neural A* and TransPath.", "motivation": "The motivation is to address the overlooked issue of path smoothness in path imitation learning and to improve the accuracy and adaptability of paths generated in various environments.", "method": "The method involves incorporating Path Angular Freedom (PAF) into A*. PAF explores the impact of move angles on path expansion and optimizes path smoothness and similarity through a joint framework of path shortening and smoothing.", "result": "In evaluations across 7 datasets, DAA* shows significant improvements, surpassing Neural A* by 9.0% SPR, 6.9% ASIM, and 3.9% PSIM, and outperforming TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM.", "conclusion": "DAA* is an effective approach for path imitation learning, achieving better path similarity, smoothness, and optimality. The minor trade-offs in search efficiency are deemed acceptable for its improved performance in various settings."}}
{"id": "2507.09212", "pdf": "https://arxiv.org/pdf/2507.09212", "abs": "https://arxiv.org/abs/2507.09212", "authors": ["Jonas Scholz", "Richard E. Turner"], "title": "Warm Starts Accelerate Generative Modelling", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "AI": {"tldr": "The paper introduces a warm-start model that accelerates iterative generative processes by predicting an informed initial state instead of randomly initiating from an uninformed prior.", "motivation": "The need to reduce the time and computational cost typically required by iterative generative models, which often involve hundreds of function evaluations.", "method": "The proposed method involves using a warm-start model to predict an informed prior based on context. This reduces the distance the generative process must traverse and employs a simple normalization trick for compatibility with standard models.", "result": "The warm-start approach achieves performance comparable to a 1000-step DDPM baseline using significantly fewer evaluations (11 in total), showcasing efficiency in tasks like image inpainting.", "conclusion": "The warm-start model significantly speeds up conditional generative processes without compromising output quality, offering a broadly compatible and efficient solution."}}
{"id": "2507.10059", "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "The paper introduces GeLaCo, a novel evolutionary method for compressing Large Language Models (LLMs) to reduce computational requirements while maintaining performance.", "motivation": "Address the computational demands of deploying and using LLMs by exploring effective and efficient model compression techniques.", "method": "GeLaCo employs evolutionary algorithms coupled with a module-wise similarity fitness function for compressing LLM layers. It supports both single and multi-objective optimization to balance compression and quality.", "result": "GeLaCo sets up the first Pareto frontier for LLM compression, ensuring balanced exploration of trade-offs. The method delivers superior results compared to existing state-of-the-art compression methods.", "conclusion": "GeLaCo offers an innovative solution for compressing LLMs efficiently, potentially paving the way for more accessible and cost-effective deployment of these models without significant sacrifice in their performance."}}
{"id": "2507.10543", "pdf": "https://arxiv.org/pdf/2507.10543", "abs": "https://arxiv.org/abs/2507.10543", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "AI": {"tldr": "MP1 introduces MeanFlow-based generative robot learning for action trajectory creation, tackling limitations in speed and accuracy compared to existing methods like DP3 and FlowPolicy.", "motivation": "Generative methods for robot manipulation face challenges in balancing inference speed and model constraints, needing improvements for both performance and generalization.", "method": "MP1 pairs 3D point-cloud inputs with the MeanFlow paradigm to create fast and precise action trajectories using interval-averaged velocity, CFG for controllability, and a Dispersive Loss for better generalization.", "result": "MP1 outperformed state-of-the-art methods with a 10.2% success rate improvement over DP3 and a 7.3% improvement over FlowPolicy, while also demonstrating significant speed gains (19x faster than DP3 and 2x faster than FlowPolicy).", "conclusion": "MP1 addresses critical limitations in generative robot manipulation models, achieving better accuracy, generalization, and speed, paving the way for more efficient robotic learning solutions."}}
{"id": "2412.11407", "pdf": "https://arxiv.org/pdf/2412.11407", "abs": "https://arxiv.org/abs/2412.11407", "authors": ["TianZhu Liu", "BangYan Hu", "YanFeng Gu", "Xian Li", "Aleksandra Pi\u017eurica"], "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "16 pages, 9 figures, 5 tables", "summary": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.", "AI": {"tldr": "This paper proposes a novel classification method for multispectral point clouds (MPCs) that addresses sparse labeled data, scale variations, and long-tailed distributions, achieving superior performance on outdoor datasets.", "motivation": "To address key limitations in existing MPC classification methods, particularly when applied to outdoor datasets\u2014such as issues with sparse labeling, variations in land-cover scales, and long-tailed distributions.", "method": "The paper introduces a three-stage approach: (1) a grid-balanced sampling strategy for robust training data generation, (2) a multi-scale feature fusion module to mitigate feature loss caused by scale variations, and (3) an adaptive hybrid loss module with multi-classification heads to balance class-wise learning, especially improving performance on small classes.", "result": "The proposed method demonstrates improved classification performance on three MPC datasets and outperforms state-of-the-art methods in addressing classification challenges related to outdoor scenes.", "conclusion": "The enhanced classification method resolves key challenges associated with outdoor MPC classification by leveraging adaptive multi-scale fusion and improved loss strategies, offering a significant performance boost."}}
{"id": "2507.09308", "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "The paper introduces ALPHA, a benchmark for RGBA image synthesis, and proposes ALPHAVAE, a novel RGBA-focused model that improves reconstruction quality and enables transparent image generation.", "motivation": "The need to generate transparent or layered image (RGBA) content efficiently has been overlooked, due to the absence of large-scale benchmarks and tailored models.", "method": "The paper proposes ALPHA, a benchmark for adapting RGB metrics to RGBA images, and ALPHAVAE, a VAEs-based RGBA model trained with composite objectives like alpha-blended reconstruction, patch fidelity, and perceptual consistency.", "result": "ALPHAVAE, trained on only 8K images, outperformed LayerDiffuse by achieving a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM for image reconstruction.", "conclusion": "ALPHAVAE not only establishes new benchmarks for RGBA image generation but also advances the generation of transparent content, setting a strong foundation for future work in RGBA-focused research."}}
{"id": "2507.09213", "pdf": "https://arxiv.org/pdf/2507.09213", "abs": "https://arxiv.org/abs/2507.09213", "authors": ["Dunsheng Huang", "Dong Shen", "Lei Lu", "Ying Tan"], "title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications", "categories": ["cs.LG", "stat.ML", "68T07"], "comment": "17pages", "summary": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from\nthe data, has been widely used in signal processing, and time-series analysis.\nHowever, challenges in constructing accurate wavelet bases and high\ncomputational costs limit their application. This study introduces a\nconstructive WNN that selects initial bases and trains functions by introducing\nnew bases for predefined accuracy while reducing computational costs. For the\nfirst time, we analyze the frequency of unknown nonlinear functions and select\nappropriate initial wavelets based on their primary frequency components by\nestimating the energy of the spatial frequency component. This leads to a novel\nconstructive framework consisting of a frequency estimator and a wavelet-basis\nincrease mechanism to prioritize high-energy bases, significantly improving\ncomputational efficiency. The theoretical foundation defines the necessary\ntime-frequency range for high-dimensional wavelets at a given accuracy. The\nframework's versatility is demonstrated through four examples: estimating\nunknown static mappings from offline data, combining two offline datasets,\nidentifying time-varying mappings from time-series data, and capturing\nnonlinear dependencies in real time-series data. These examples showcase the\nframework's broad applicability and practicality. All the code will be released\nat https://github.com/dshuangdd/CWNN.", "AI": {"tldr": "This paper introduces a constructive Wavelet Neural Network (WNN) framework to improve computational efficiency and accuracy by prioritizing high-energy wavelet bases based on frequency analysis.", "motivation": "WNNs have potential in signal processing and time-series analysis but face challenges with constructing accurate wavelet bases and high computational costs.", "method": "The framework analyzes frequencies of unknown nonlinear functions, prioritizes initial wavelets with high energy in spatial frequency components, and introduces a mechanism to increase wavelet bases incrementally for improved accuracy.", "result": "The proposed framework optimizes wavelet selection and computational efficiency, demonstrated effectively through examples in estimating static mappings, combining datasets, and analyzing time-series data.", "conclusion": "The constructive WNN framework offers significant improvements in computational cost and versatility for various applications in signal and time-series data processing."}}
{"id": "2507.10535", "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "This paper introduces CodeJudgeBench, a benchmark for evaluating the performance of Large Language Models as judges in coding tasks. It emphasizes the performance difference between thinking and non-thinking models while identifying reliability issues and optimal prompting strategies.", "motivation": "The motivation is to assess and improve the recently popular LLM-as-a-Judge paradigm, which holds potential for benchmarking and improving LLMs in tasks like code generation, repair, and unit test creation. This is crucial as there were previously no dedicated benchmarks for such evaluations.", "method": "The authors developed and utilized the CodeJudgeBench benchmark to test and analyze 26 LLM models designed for judging coding tasks. They compared thinking and non-thinking models, analyzed scenarios like order sensitivity, and tested various prompting strategies.", "result": "The research showed that thinking models outperformed non-thinking ones, even smaller thinking models surpassing larger non-thinking models. However, all models faced issues with judgment consistency, including sensitivity to input order and variance in cross-LLM comparisons.", "conclusion": "While some LLMs show promise as judges in coding tasks, issues like random variability and order sensitivity raise concerns about their reliability. Optimal prompting strategies, like pairwise comparison with reasoning included, offer potential for improvement but don't fully resolve these challenges."}}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M\u00fcnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "Large Language Models fail to capture diverse cultural moral frameworks and homogenize moral diversity, challenging their use in social sciences.", "motivation": "To investigate whether LLMs can represent diverse human values and moral intuitions effectively.", "method": "Applied the Moral Foundations Questionnaire across 19 cultural contexts to assess LLMs against human baseline data.", "result": "LLMs systematically homogenize moral diversity, with increased model size not guaranteeing improved cultural representation.", "conclusion": "Current AI alignment approaches fail to capture nuanced cultural moral intuitions, calling for more focused alignment and evaluation techniques."}}
{"id": "2507.07855", "pdf": "https://arxiv.org/pdf/2507.07855", "abs": "https://arxiv.org/abs/2507.07855", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "title": "Principled Foundations for Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "AI": {"tldr": "This paper establishes a connection between preference optimization and major ML theories, offering a general framework for DPO.", "motivation": "The paper seeks to understand direct preference optimization (DPO) from a principled perspective due to its wide application and current popularity.", "method": "The authors connect Savage's loss functions and stochastic choice theories to formulate a general framework for DPO.", "result": "The work supports abstention, accommodates non-convex objectives, and frames DPO extensions such as margins and corrections for length.", "conclusion": "Understanding DPO through this framework showcases its versatility, provides insights into its challenges, and suggests practical solutions."}}
{"id": "2507.09313", "pdf": "https://arxiv.org/pdf/2507.09313", "abs": "https://arxiv.org/abs/2507.09313", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "AI": {"tldr": "The paper introduces ProactiveBench, a benchmark for evaluating proactive interactions in multimodal systems, and PAUC, a metric for assessing temporal response dynamics.", "motivation": "There is a growing expectation for multimodal dialogue systems to move beyond turn-based interactions and become more proactive in initiating and timing responses autonomously.", "method": "The authors propose ProactiveBench as a benchmark for proactive systems and introduce PAUC as a new metric that evaluates temporal aspects of system responses. They validate this with extensive benchmarking and a user study comparing PAUC to traditional metrics.", "result": "The study finds that PAUC correlates better with human preferences compared to traditional metrics, showing its superiority in evaluating user experience in proactive settings.", "conclusion": "PAUC provides a more accurate and meaningful measure of system performance in proactive interaction scenarios, advancing research in multimodal dialogue systems."}}
{"id": "2507.09095", "pdf": "https://arxiv.org/pdf/2507.09095", "abs": "https://arxiv.org/abs/2507.09095", "authors": ["Md Hasan Shahriar", "Md Mohaimin Al Barat", "Harshavardhan Sundar", "Naren Ramakrishnan", "Y. Thomas Hou", "Wenjing Lou"], "title": "On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving", "categories": ["cs.LG"], "comment": "16 pages", "summary": "Multimodal fusion (MMF) plays a critical role in the perception of autonomous\ndriving, which primarily fuses camera and LiDAR streams for a comprehensive and\nefficient scene understanding. However, its strict reliance on precise temporal\nsynchronization exposes it to new vulnerabilities. In this paper, we introduce\nDejaVu, a novel attack that exploits network-induced delays to create subtle\ntemporal misalignments across sensor streams, severely degrading downstream\nMMF-based perception tasks. Our comprehensive attack analysis across different\nmodels and datasets reveals these sensors' task-specific imbalanced\nsensitivities: object detection is overly dependent on LiDAR inputs while\nobject tracking is highly reliant on the camera inputs. Consequently, with a\nsingle-frame LiDAR delay, an attacker can reduce the car detection mAP by up to\n88.5%, while with a three-frame camera delay, multiple object tracking accuracy\n(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense\npatch that can work alongside the existing perception model to monitor temporal\nalignment through cross-modal temporal consistency. AION leverages multimodal\nshared representation learning and dynamic time warping to determine the path\nof temporal alignment and calculate anomaly scores based on the alignment. Our\nthorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with\nlow false positives across datasets and model architectures, demonstrating it\nas a robust and generalized defense against the temporal misalignment attacks.", "AI": {"tldr": "This paper introduces DejaVu, an attack exploiting temporal misalignments in sensor streams for autonomous driving, and proposes AION, a robust defense system to detect such attacks.", "motivation": "To expose vulnerabilities in multimodal fusion methods due to strict reliance on temporal synchronization in autonomous driving systems.", "method": "DejaVu creates subtle temporal misalignments in sensor data, and AION is a defense system leveraging shared representation learning and dynamic time warping to detect temporal misalignments.", "result": "DejaVu shows significant impact on perception tasks with up to 88.5% reduction in detection accuracy. AION achieves 0.92-0.98 AUROC in detecting misalignments with low false positives.", "conclusion": "Temporal misalignments can severely degrade MMF-based perception. AION proves to be a robust and effective solution to defend against such attacks."}}
{"id": "2507.09247", "pdf": "https://arxiv.org/pdf/2507.09247", "abs": "https://arxiv.org/abs/2507.09247", "authors": ["Mihailo Stojnic"], "title": "A CLuP algorithm to practically achieve $\\sim 0.76$ SK--model ground state free energy", "categories": ["cond-mat.dis-nn", "cs.IT", "math.IT", "math.OC", "stat.ML"], "comment": null, "summary": "We consider algorithmic determination of the $n$-dimensional\nSherrington-Kirkpatrick (SK) spin glass model ground state free energy. It\ncorresponds to a binary maximization of an indefinite quadratic form and under\nthe \\emph{worst case} principles of the classical NP complexity theory it is\nhard to approximate within a $\\log(n)^{const.}$ factor. On the other hand, the\nSK's random nature allows (polynomial) spectral methods to \\emph{typically}\napproach the optimum within a constant factor. Naturally one is left with the\nfundamental question: can the residual (constant) \\emph{computational gap} be\nerased?\n  Following the success of \\emph{Controlled Loosening-up} (CLuP) algorithms in\nplanted models, we here devise a simple practical CLuP-SK algorithmic procedure\nfor (non-planted) SK models. To analyze the \\emph{typical} success of the\nalgorithm we associate to it (random) CLuP-SK models. Further connecting to\nrecent random processes studies [94,97], we characterize the models and CLuP-SK\nalgorithm via fully lifted random duality theory (fl RDT) [98]. Moreover,\nrunning the algorithm we demonstrate that its performance is in an excellent\nagrement with theoretical predictions. In particular, already for $n$ on the\norder of a few thousands CLuP-SK achieves $\\sim 0.76$ ground state free energy\nand remarkably closely approaches theoretical $n\\rightarrow\\infty$ limit\n$\\approx 0.763$. For all practical purposes, this renders computing SK model's\nnear ground state free energy as a \\emph{typically} easy problem.", "AI": {"tldr": "The paper proposes an algorithm (CLuP-SK) to determine the ground state free energy of the Sherrington-Kirkpatrick spin glass model, achieving near-optimal results for large $n$ and effectively making the problem typically easy.", "motivation": "To address whether the constant computational gap in approximating the ground state free energy of SK spin glass models can be closed, making the problem computationally easier in practice.", "method": "The study introduces a Controlled Loosening-up (CLuP-SK) algorithm specifically for non-planted SK models and analyzes its success using random duality theory and theoretical predictions.", "result": "The CLuP-SK algorithm achieved $\n\u2248 0.76$ ground state free energy for values of n in the order of thousands, closely approaching the theoretical limit of $\u2248 0.763$ as $n$ approaches infinity.", "conclusion": "The results demonstrate that the problem of computing SK model's near ground state free energy is rendered typically easy for practical purposes through the proposed algorithm, supported by strong agreement between theoretical predictions and computational results."}}
{"id": "2507.10085", "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "This study introduces Critical Representation Fine-Tuning (CRFT), an advanced method improving reasoning tasks by targeting key information-carrying representations, achieving higher accuracy and efficiency.", "motivation": "The motivation is to enhance reasoning task performance by focusing on critical representations that significantly influence output, addressing limitations of traditional PEFT methods like ReFT.", "method": "CRFT identifies critical representations via information flow analysis and fine-tunes them in a low-rank subspace, operating within a supervised learning framework while keeping the base model frozen.", "result": "Tests on eight reasoning benchmarks using LLaMA and Mistral models demonstrate CRFT's effectiveness. It achieves improved performance across arithmetic and commonsense reasoning, particularly excelling in few-shot learning scenarios by boosting one-shot accuracy by 16.4%.", "conclusion": "Representation-level optimization offers a promising, lightweight method for improving reasoning models, providing superior results compared to traditional PEFT without requiring extensive model changes."}}
{"id": "2507.08052", "pdf": "https://arxiv.org/pdf/2507.08052", "abs": "https://arxiv.org/abs/2507.08052", "authors": ["Mazen Ali", "Ant\u00f3nio Pereira", "Fabio Gentile", "Aser Cortines", "Sam Mugel", "Rom\u00e1n Or\u00fas", "Stelios P. Neophytides", "Michalis Mavrovouniotis"], "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cloud and cloud shadow masking is a crucial preprocessing step in\nhyperspectral satellite imaging, enabling the extraction of high-quality,\nanalysis-ready data. This study evaluates various machine learning approaches,\nincluding gradient boosting methods such as XGBoost and LightGBM as well as\nconvolutional neural networks (CNNs). All boosting and CNN models achieved\naccuracies exceeding 93%. Among the investigated models, the CNN with feature\nreduction emerged as the most efficient, offering a balance of high accuracy,\nlow storage requirements, and rapid inference times on both CPUs and GPUs.\nVariations of this version, with only up to 597 trainable parameters,\ndemonstrated the best trade-off in terms of deployment feasibility, accuracy,\nand computational efficiency. These results demonstrate the potential of\nlightweight artificial intelligence (AI) models for real-time hyperspectral\nimage processing, supporting the development of on-board satellite AI systems\nfor space-based applications.", "AI": {"tldr": "The study assesses various machine learning methods for cloud masking in hyperspectral satellite imaging and identifies a lightweight CNN model as the optimal choice for its accuracy and computational efficiency.", "motivation": "The need to effectively preprocess hyperspectral satellite images by masking clouds and shadows to ensure high-quality data extraction.", "method": "The research compares different machine learning techniques like gradient boosting models (XGBoost and LightGBM) and CNNs, with a focus on feature reduction for computational efficiency.", "result": "Models demonstrated over 93% accuracy, with a CNN model featuring reduced parameters being most efficient, balancing high accuracy, low storage needs, and quick inference times.", "conclusion": "Lightweight AI models like CNNs with reduced parameters are effective for hyperspectral satellite image processing and could facilitate the development of real-time, onboard satellite AI systems."}}
{"id": "2507.09323", "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "This paper introduces the Dynamic Inter-Class Confusion-Aware Encoder (DICCAE), an audio-video encoder that improves fine-grained category-level alignment and performance in human activity recognition.", "motivation": "Existing audio-video pre-training methods fail to address the challenge of distinguishing easily confused classes, limiting their effectiveness in precise human activity recognition.", "method": "The proposed DICCAE dynamically adjusts a confusion loss to address inter-class confusion at a category-level. A novel training framework combines audio, video, and their fusion, complemented by a cluster-guided self-supervised pre-training strategy to address data scarcity.", "result": "DICCAE achieves a top-1 accuracy of 65.5% on the VGGSound dataset, approaching state-of-the-art performance, and its modules are validated through ablation studies.", "conclusion": "The integration of dynamic confusion-aware encoding and a self-supervised pre-training strategy enhances both feature representation and the ability to distinguish similar activities in audio-video tasks."}}
{"id": "2507.09101", "pdf": "https://arxiv.org/pdf/2507.09101", "abs": "https://arxiv.org/abs/2507.09101", "authors": ["Yanan Cao", "Omid Memarrast", "Shiqin Cai", "Sinduja Subramaniam", "Evren Korpeoglu", "Kannan Achan"], "title": "S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe", "categories": ["cs.LG"], "comment": null, "summary": "In grocery e-commerce, customers often build ingredient baskets guided by\ndietary preferences but lack the expertise to create complete meals. Leveraging\nrecipe knowledge to recommend complementary ingredients based on a partial\nbasket is essential for improving the culinary experience. Traditional recipe\ncompletion methods typically predict a single missing ingredient using a\nleave-one-out strategy. However, they fall short in two key aspects: (i) they\ndo not reflect real-world scenarios where multiple ingredients are often\nneeded, and (ii) they overlook relationships among the missing ingredients\nthemselves. To address these limitations, we reformulate basket completion as a\nset-to-set (S2S) recommendation problem, where an incomplete basket is input\ninto a system that predicts a set of complementary ingredients. We introduce\nS2SRec2, a set-to-set ingredient recommendation framework based on a Set\nTransformer and trained in a multitask learning paradigm. S2SRec2 jointly\nlearns to (i) retrieve missing ingredients from the representation of existing\nones and (ii) assess basket completeness after prediction. These tasks are\noptimized together, enforcing accurate retrieval and coherent basket\ncompletion. Experiments on large-scale recipe datasets and qualitative analyses\nshow that S2SRec2 significantly outperforms single-target baselines, offering a\npromising approach to enhance grocery shopping and inspire culinary creativity.", "AI": {"tldr": "The paper addresses ingredient basket completion for grocery e-commerce using a set-to-set (S2S) recommendation framework, improving on traditional approaches by predicting sets of missing ingredients rather than single items.", "motivation": "To enable customers in grocery e-commerce to create complete meals by recommending complementary ingredients based on incomplete baskets, overcoming limitations of current methods that predict only single missing ingredients.", "method": "The proposed method, S2SRec2, introduces a set-to-set recommendation framework using a Set Transformer and multitask learning paradigm, jointly retrieving missing ingredients and assessing basket completeness.", "result": "Experiments on large-scale recipe datasets demonstrate that S2SRec2 outperforms traditional single-target baselines, providing more accurate and coherent ingredient recommendations.", "conclusion": "S2SRec2 effectively enhances grocery shopping and culinary creativity by predicting coherent ingredient sets, addressing real-world needs for ingredient basket completion."}}
{"id": "2507.09252", "pdf": "https://arxiv.org/pdf/2507.09252", "abs": "https://arxiv.org/abs/2507.09252", "authors": ["Shukai Gong", "Yiyang Fu", "Fengyuan Ran", "Feng Zhou"], "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "AI": {"tldr": "TPP-SD combines speculative decoding techniques with Transformer-based temporal point processes to accelerate sequence sampling while maintaining output quality.", "motivation": "The paper aims to address the practical need for fast sequence sampling in Transformer-based temporal point process models while retaining accuracy.", "method": "It adapts speculative decoding from language models, using a smaller draft model to generate candidates, verified in parallel by a larger target model to maintain output accuracy.", "result": "The method achieves a 2-6x speedup while preserving identical output distributions compared to standard sampling methods.", "conclusion": "TPP-SD effectively accelerates Transformer TPP sampling, making it more practical for real-world applications without compromising quality."}}
{"id": "2507.10098", "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "The paper proposes a novel architecture combining LLMs and vanilla Transformers for time series forecasting, achieving more accurate predictions by fusing temporal and semantic representations.", "motivation": "Although LLMs show strong reasoning capabilities with discrete tokens, they struggle with continuous numerical time series data, and vanilla Transformers, while better with time series data, lack the ability to learn high-level semantic patterns.", "method": "The authors design a Transformer-based hybrid model that combines high-level semantic knowledge learned by LLMs with temporal information captured by vanilla Transformers for accurate time series forecasting.", "result": "The hybrid model outperforms both standalone LLMs (used via fine-tuning or prompting) and vanilla Transformers on benchmark time series forecasting datasets.", "conclusion": "Integrating LLMs' semantic reasoning capabilities with the temporal strengths of Transformers can bridge the gap between discrete token reasoning and continuous numerical prediction, enabling more accurate time series forecasts."}}
{"id": "2507.08827", "pdf": "https://arxiv.org/pdf/2507.08827", "abs": "https://arxiv.org/abs/2507.08827", "authors": ["Yu Zheng", "Jingtao Ding", "Depeng Jin", "Jianxi Gao", "Yong Li"], "title": "Advancing network resilience theories with symbolized reinforcement learning", "categories": ["physics.soc-ph", "cs.AI"], "comment": null, "summary": "Many complex networks display remarkable resilience under external\nperturbations, internal failures and environmental changes, yet they can\nswiftly deteriorate into dysfunction upon the removal of a few keystone nodes.\nDiscovering theories that measure network resilience offers the potential to\nprevent catastrophic collapses--from species extinctions to financial\ncrise--with profound implications for real-world systems. Current resilience\ntheories address the problem from a single perspective of topology, neglecting\nthe crucial role of system dynamics, due to the intrinsic complexity of the\ncoupling between topology and dynamics which exceeds the capabilities of human\nanalytical methods. Here, we report an automatic method for resilience theory\ndiscovery, which learns from how AI solves a complicated network dismantling\nproblem and symbolizes its network attack strategies into theoretical formulas.\nThis proposed self-inductive approach discovers the first resilience theory\nthat accounts for both topology and dynamics, highlighting how the correlation\nbetween node degree and state shapes overall network resilience, and offering\ninsights for designing early warning signals of systematic collapses.\nAdditionally, our approach discovers formulas that refine existing\nwell-established resilience theories with over 37.5% improvement in accuracy,\nsignificantly advancing human understanding of complex networks with AI.", "AI": {"tldr": "A study on network resilience introduces an AI-driven method to account for both topology and system dynamics, improving existing theories.", "motivation": "To create a resilience theory that incorporates both topology and system dynamics to better understand and prevent network collapses.", "method": "AI-based self-inductive approach learns network attack strategies and formulates theoretical resilience models.", "result": "Discovered the first resilience theory incorporating topology and dynamics, and improved established resilience models by 37.5% in accuracy.", "conclusion": "The approach significantly enhances understanding of complex networks and provides tools for preventing collapses, uncovering crucial dynamics and refinements."}}
{"id": "2507.09334", "pdf": "https://arxiv.org/pdf/2507.09334", "abs": "https://arxiv.org/abs/2507.09334", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "AI": {"tldr": "Fast3D is a framework designed to improve the efficiency of 3D Multi-modal Large Language Models (MLLMs) by introducing a token pruning strategy that addresses redundancies in object-level 3D token representations.", "motivation": "The motivation arises from the inefficiency of existing 3D MLLMs, which struggle with computational challenges due to the large number of object-centric visual tokens required for comprehensive representation.", "method": "Fast3D employs two techniques: (1) Global Attention Prediction (GAP), which predicts global attention distributions using a lightweight neural network for effective token importance estimation; and (2) Sample-Adaptive visual token Pruning (SAP), which adjusts token pruning ratios dynamically based on attention-driven complexity analysis, without altering the target model's parameters.", "result": "Extensive experiments across five benchmarks demonstrated that Fast3D effectively improves computational efficiency in 3D MLLMs, particularly under high pruning ratios, validating the approach's robustness.", "conclusion": "Fast3D provides a practical solution for enhancing 3D MLLMs' computational efficiency by pruning redundant tokens without modifying the target model, making it a promising direction for addressing scalability challenges. The code is open-source and available for public access."}}
{"id": "2507.09347", "pdf": "https://arxiv.org/pdf/2507.09347", "abs": "https://arxiv.org/abs/2507.09347", "authors": ["Ivan Letteri"], "title": "A Framework for Predictive Directional Trading Based on Volatility and Causal Inference", "categories": ["q-fin.ST", "cs.AI", "stat.ML"], "comment": null, "summary": "Purpose: This study introduces a novel framework for identifying and\nexploiting predictive lead-lag relationships in financial markets. We propose\nan integrated approach that combines advanced statistical methodologies with\nmachine learning models to enhance the identification and exploitation of\npredictive relationships between equities. Methods: We employed a Gaussian\nMixture Model (GMM) to cluster nine prominent stocks based on their mid-range\nhistorical volatility profiles over a three-year period. From the resulting\nclusters, we constructed a multi-stage causal inference pipeline, incorporating\nthe Granger Causality Test (GCT), a customised Peter-Clark Momentary\nConditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to\nidentify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)\nand a K-Nearest Neighbours (KNN) classifier were utilised to determine the\noptimal time lag for trade execution. The resulting strategy was rigorously\nbacktested. Results: The proposed volatility-based trading strategy, tested\nfrom 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The\nportfolio yielded a total return of 15.38%, significantly outperforming the\n10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,\nincluding a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain\npairs, confirmed the strategy's viability. Conclusion: This research\ncontributes a systematic and robust methodology for identifying profitable\ntrading opportunities derived from volatility-based causal relationships. The\nfindings have significant implications for both academic research in financial\nmodelling and the practical application of algorithmic trading, offering a\nstructured approach to developing resilient, data-driven strategies.", "AI": {"tldr": "The paper proposes a framework combining statistical methods and machine learning to identify lead-lag relationships in financial markets, achieving a total return of 15.38% over two months.", "motivation": "To develop a systematic approach for identifying profitable trading opportunities in financial markets using volatility-based causal relationships.", "method": "The study uses Gaussian Mixture Models for clustering, followed by causal inference tools such as Granger Causality and Effective Transfer Entropy, concluding with Dynamic Time Warping for optimal trade timing.", "result": "The suggested strategy yielded a total return of 15.38%, outperforming Buy-and-Hold by 5% and achieving high Sharpe Ratios and win rates.", "conclusion": "The approach offers an effective methodology for data-driven trading, benefiting both academic research and practical algorithmic trading implementations."}}
{"id": "2507.10155", "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "The paper introduces a new feature knowledge distillation method allowing transfer between teacher and student language models with differing hidden sizes, without extra parameters.", "motivation": "Reducing computational demand of large language models using efficient knowledge distillation methods.", "method": "Identifying task-relevant hidden units in teacher models and distilling their activations directly to the student model without new parameters.", "result": "The proposed approach outperforms existing methods across diverse tasks with up to 3% improvement over linear projection baselines.", "conclusion": "This flexible method improves distillation performance and avoids additional parameter introduction while enabling architecture flexibility."}}
{"id": "2507.09338", "pdf": "https://arxiv.org/pdf/2507.09338", "abs": "https://arxiv.org/abs/2507.09338", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brun\u00f3 B. Englert", "Gijs Dubbelman"], "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "categories": ["cs.CV"], "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "AI": {"tldr": "The paper explores ego-centric Traffic Anomaly Detection (TAD) via a simple encoder-only approach using Video Vision Transformers (Video ViTs) instead of complex architectures, demonstrating that pre-training significantly enhances performance.", "motivation": "The authors are motivated by findings in visual perception suggesting that simpler architectures enabled by advanced pre-training can outperform specialized complex designs.", "method": "They use plain Video Vision Transformers (Video ViTs) and focus on the effects of different pre-training methods, including self-supervised Masked Video Modeling (MVM) and Domain-Adaptive Pre-Training (DAPT).", "result": "The study finds that strong pre-training allows simple models to match or outperform complex state-of-the-art methods efficiently. Self-supervised MVM provides the best performance signal, and domain-adaptive pre-training on unlabeled driving videos further boosts results.", "conclusion": "Pre-training is vital for building efficient, scalable, and effective TAD models with minimal architectural complexity. The research supports foundational models as promising tools for anomaly detection and releases resources for future studies."}}
{"id": "2507.09132", "pdf": "https://arxiv.org/pdf/2507.09132", "abs": "https://arxiv.org/abs/2507.09132", "authors": ["Chu-Yuan Wei", "Shun-Yao Liu", "Sheng-Da Zhuo", "Chang-Dong Wang", "Shu-Qiang Huang", "Mohsen Guizani"], "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based tasks (e.g., node classification or link prediction). Despite their\ntriumphs, GNNs still face challenges such as long training and inference times,\ndifficulty in capturing complex relationships, and insufficient feature\nextraction. To tackle these issues, graph pre-training and graph prompt methods\nhave garnered increasing attention for their ability to leverage large-scale\ndatasets for initial learning and task-specific adaptation, offering potential\nimprovements in GNN performance. However, previous research has overlooked the\npotential of graph prompts in optimizing models, as well as the impact of both\npositive and negative graph prompts on model stability and efficiency. To\nbridge this gap, we propose a novel framework combining graph prompts with\nweight pruning, called GPAWP, which aims to enhance the performance and\nefficiency of graph prompts by using fewer of them. We evaluate the importance\nof graph prompts using an importance assessment function to determine positive\nand negative weights at different granularities. Through hierarchically\nstructured pruning, we eliminate negative prompt labels, resulting in more\nparameter-efficient and competitively performing prompts. Extensive experiments\non three benchmark datasets demonstrate the superiority of GPAWP, leading to a\nsignificant reduction in parameters in node classification tasks.", "AI": {"tldr": "The paper introduces GPAWP, a framework integrating graph prompts with weight pruning to optimize Graph Neural Networks (GNNs), resulting in improved performance and efficiency.", "motivation": "The motivation is to address challenges in GNNs, such as slow training/inference times, difficulty in capturing complex relationships, and inadequate feature extraction, by leveraging graph pre-training and graph prompts.", "method": "The method involves assessing the importance of graph prompts and hierarchically pruning negative prompt labels using structured pruning to enhance parameter efficiency and maintain competitive performance.", "result": "Experiments across three benchmark datasets show significant parameter reduction and performance improvements in node classification tasks.", "conclusion": "GPAWP effectively boosts GNN performance while enhancing efficiency by reducing reliance on graph prompts, providing a novel solution to previously overlooked issues in graph pre-training and prompting."}}
{"id": "2507.09353", "pdf": "https://arxiv.org/pdf/2507.09353", "abs": "https://arxiv.org/abs/2507.09353", "authors": ["Addison Weatherhead", "Anna Goldenberg"], "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Time series data with missing values is common across many domains.\nHealthcare presents special challenges due to prolonged periods of sensor\ndisconnection. In such cases, having a confidence measure for imputed values is\ncritical. Most existing methods either overlook model uncertainty or lack\nmechanisms to estimate it. To address this gap, we introduce a general\nframework that quantifies and leverages uncertainty for selective imputation.\nBy focusing on values the model is most confident in, highly unreliable\nimputations are avoided. Our experiments on multiple EHR datasets, covering\ndiverse types of missingness, demonstrate that selectively imputing\nless-uncertain values not only reduces imputation errors but also improves\ndownstream tasks. Specifically, we show performance gains in a 24-hour\nmortality prediction task, underscoring the practical benefit of incorporating\nuncertainty into time series imputation.", "AI": {"tldr": "This paper introduces a framework that incorporates uncertainty for selective time series imputation, improving imputation accuracy and downstream tasks like mortality prediction.", "motivation": "Existing time series imputation methods often neglect or fail to estimate uncertainty, especially in challenging domains like healthcare, where missing values are frequent due to sensor disconnections.", "method": "The authors propose a general framework that quantifies model uncertainty and uses it to selectively target imputation on confident values, avoiding unreliable imputations.", "result": "Experiments on multiple EHR datasets demonstrate that uncertainty-based selective imputation reduces imputation errors and enhances performance in downstream tasks, such as a 24-hour mortality prediction.", "conclusion": "Incorporating uncertainty into the imputation process leads to significant improvements, proving beneficial for both imputation accuracy and related applications in healthcare."}}
{"id": "2507.10177", "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "This study examines the ability of different LLMs, such as Gemini, GPT-4o, DeepSeek, and Groq, to rewrite abusive text into non-abusive text while maintaining original intent, sentiment, and semantics.", "motivation": "To explore whether current LLMs can effectively transform abusive text into non-abusive text while retaining its intent, addressing the gap in this application area.", "method": "Four state-of-the-art LLMs were evaluated for their ability to detect and transform abusive texts. The transformations were further analyzed with sentiment and semantic analysis to assess quality.", "result": "Groq delivered results significantly different from other models, while similarities were observed between GPT-4o and DeepSeek in the transformed outputs.", "conclusion": "While some LLMs like Groq deviate noticeably, models like GPT-4o and DeepSeek demonstrated closer alignment in handling abusive text rewriting tasks."}}
{"id": "2507.09375", "pdf": "https://arxiv.org/pdf/2507.09375", "abs": "https://arxiv.org/abs/2507.09375", "authors": ["Sourish Suri", "Yifei Shao"], "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "categories": ["cs.CV", "I.2.6; I.5.4"], "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "AI": {"tldr": "The paper presents a CNN-based system for automated detection of crop diseases using leaf imagery, achieving ~90% training and ~60% validation accuracy, integrated with a treatment recommendation module on a mobile platform.", "motivation": "Crop diseases hinder agricultural productivity and food security, especially in large-scale farming where early identification is challenging.", "method": "A CNN with three convolutional layers and ReLU activations is trained using TensorFlow and Keras on a labeled leaf image dataset. Data preprocessing involves resizing, normalization, and augmentation. The system classifies eight diseases and includes a treatment recommendation module.", "result": "The model shows ~90% training accuracy but ~60% validation accuracy, suggesting slight overfitting. It is deployed on an open-source platform for real-time diagnostics.", "conclusion": "This scalable tool enhances precision agriculture by integrating deep learning with practical disease management, promoting sustainable farming and supporting global food production."}}
{"id": "2507.09137", "pdf": "https://arxiv.org/pdf/2507.09137", "abs": "https://arxiv.org/abs/2507.09137", "authors": ["Nripsuta Ani Saxena", "Shang-Ling Hsu", "Mehul Shetty", "Omar Alkhadra", "Cyrus Shahabi", "Abigail L. Horn"], "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurately attributing user visits to specific Points of Interest (POIs) is a\nfoundational task for mobility analytics, personalized services, marketing and\nurban planning. However, POI attribution remains challenging due to GPS\ninaccuracies, typically ranging from 2 to 20 meters in real-world settings, and\nthe high spatial density of POIs in urban environments, where multiple venues\ncan coexist within a small radius (e.g., over 50 POIs within a 100-meter radius\nin dense city centers). Relying on proximity is therefore often insufficient\nfor determining which POI was actually visited. We introduce\n\\textsf{POIFormer}, a novel Transformer-based framework for accurate and\nefficient POI attribution. Unlike prior approaches that rely on limited\nspatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly\nmodels a rich set of signals, including spatial proximity, visit timing and\nduration, contextual features from POI semantics, and behavioral features from\nuser mobility and aggregated crowd behavior patterns--using the Transformer's\nself-attention mechanism to jointly model complex interactions across these\ndimensions. By leveraging the Transformer to model a user's past and future\nvisits (with the current visit masked) and incorporating crowd-level behavioral\npatterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate,\nefficient attribution in large, noisy mobility datasets. Its architecture\nsupports generalization across diverse data sources and geographic contexts\nwhile avoiding reliance on hard-to-access or unavailable data layers, making it\npractical for real-world deployment. Extensive experiments on real-world\nmobility datasets demonstrate significant improvements over existing baselines,\nparticularly in challenging real-world settings characterized by spatial noise\nand dense POI clustering.", "AI": {"tldr": "POIFormer is a novel Transformer-based framework for accurate attribution of user visits to Points of Interest (POIs), addressing challenges like GPS inaccuracy and high POI density.", "motivation": "To improve accuracy in attributing user visits to POIs despite GPS inaccuracies and dense clustering of POIs, which is essential for multiple applications like mobility analytics.", "method": "The paper introduces POIFormer, a Transformer-based model leveraging self-attention and integrating features such as spatial proximity, visit timing, contextual semantics, user mobility, and crowd behavior patterns.", "result": "Extensive experiments on real-world datasets show POIFormer significantly outperforms existing methods in scenarios with spatial noise and dense POI clustering.", "conclusion": "POIFormer effectively addresses challenges in POI attribution, offers practical deployment capabilities across diverse contexts, and sets a new benchmark for real-world mobility analytics."}}
{"id": "2507.09380", "pdf": "https://arxiv.org/pdf/2507.09380", "abs": "https://arxiv.org/abs/2507.09380", "authors": ["Haoming Shi", "Shan Yu", "Eric C. Chi"], "title": "Robust Spatiotemporal Epidemic Modeling with Integrated Adaptive Outlier Detection", "categories": ["stat.ME", "physics.soc-ph", "stat.ML"], "comment": "35 pages, 8 figures, 3 tables", "summary": "In epidemic modeling, outliers can distort parameter estimation and\nultimately lead to misguided public health decisions. Although there are\nexisting robust methods that can mitigate this distortion, the ability to\nsimultaneously detect outliers is equally vital for identifying potential\ndisease hotspots. In this work, we introduce a robust spatiotemporal\ngeneralized additive model (RST-GAM) to address this need. We accomplish this\nwith a mean-shift parameter to quantify and adjust for the effects of outliers\nand rely on adaptive Lasso regularization to model the sparsity of outlying\nobservations. We use univariate polynomial splines and bivariate penalized\nsplines over triangulations to estimate the functional forms and a\ndata-thinning approach for data-adaptive weight construction. We derive a\nscalable proximal algorithm to estimate model parameters by minimizing a convex\nnegative log-quasi-likelihood function. Our algorithm uses adaptive step-sizes\nto ensure global convergence of the resulting iterate sequence. We establish\nerror bounds and selection consistency for the estimated parameters and\ndemonstrate our model's effectiveness through numerical studies under various\noutlier scenarios. Finally, we demonstrate the practical utility of RST-GAM by\nanalyzing county-level COVID-19 infection data in the United States,\nhighlighting its potential to inform public health decision-making.", "AI": {"tldr": "The paper introduces RST-GAM, a robust method for addressing outliers in epidemic modeling to improve public health decision-making and identify disease hotspots.", "motivation": "Outliers in epidemic modeling can skew parameter estimation and mislead public health decisions, highlighting a need for methods that both mitigate distortion and identify disease hotspots.", "method": "The RST-GAM incorporates a mean-shift parameter to adjust for outliers, uses adaptive Lasso regularization for sparsity modeling, employs spline-based functional approximation, and utilizes a scalable proximal algorithm for parameter estimation with error bounds and convergence guarantees.", "result": "Numerical studies confirm the model's robustness under various outlier scenarios, and its application to U.S. county-level COVID-19 data demonstrates its practical utility for public health analysis.", "conclusion": "RST-GAM effectively addresses outliers in spatiotemporal epidemic data, improves parameter estimation, and aids in identifying disease hotspots, making it a valuable tool for public health decision-making."}}
{"id": "2507.10216", "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "This paper introduces Absher, a benchmark for evaluating LLMs on Saudi dialects and cultural nuances, revealing notable gaps in cultural inference and contextual understanding.", "motivation": "To address the lack of benchmarks for assessing LLMs\u2019 ability to understand regional dialects and cultural nuances, crucial for linguistically diverse settings like Saudi Arabia.", "method": "The Absher benchmark, with over 18,000 multiple-choice questions across six categories, evaluates LLMs on dialectal and cultural tasks using curated data from Saudi regional dialects.", "result": "Results show significant performance gaps in tasks requiring cultural inference and contextual understanding, highlighting areas for improvement in state-of-the-art LLMs.", "conclusion": "There\u2019s an urgent need for dialect-aware training and culturally aligned evaluation methods to improve LLMs' performance in Arabic real-world applications."}}
{"id": "2507.09410", "pdf": "https://arxiv.org/pdf/2507.09410", "abs": "https://arxiv.org/abs/2507.09410", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "categories": ["cs.CV", "cs.LG"], "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "AI": {"tldr": "This paper introduces a practical pipeline leveraging ML/AI tools to efficiently process camera trap data, suitable for small research groups with limited resources.", "motivation": "The paper addresses challenges in handling large camera trap datasets, particularly for small research groups constrained by resources and computational expertise.", "method": "It proposes a low-resource, on-premise pipeline incorporating tailored ML/AI capabilities for efficient camera trap data processing and analysis.", "result": "The pipeline enables effective data transmission, inference, and evaluation, making it accessible for extracting insights from camera trap datasets despite resource limitations.", "conclusion": "The outlined approach helps small research groups overcome barriers in managing large datasets and utilizing ML/AI tools, thus enhancing wildlife research capabilities."}}
{"id": "2507.09173", "pdf": "https://arxiv.org/pdf/2507.09173", "abs": "https://arxiv.org/abs/2507.09173", "authors": ["Mengjie Chen", "Ming Zhang", "Cunquan Qu"], "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "comment": null, "summary": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,\noften leading to adverse drug reactions with significant implications for\npatient safety and healthcare outcomes. While graph-based methods have achieved\nstrong predictive performance, most approaches treat drug pairs independently,\noverlooking the complex, context-dependent interactions unique to drug pairs.\nAdditionally, these models struggle to integrate biological interaction\nnetworks and molecular-level structures to provide meaningful mechanistic\ninsights. In this study, we propose MolecBioNet, a novel graph-based framework\nthat integrates molecular and biomedical knowledge for robust and interpretable\nDDI prediction. By modeling drug pairs as unified entities, MolecBioNet\ncaptures both macro-level biological interactions and micro-level molecular\ninfluences, offering a comprehensive perspective on DDIs. The framework\nextracts local subgraphs from biomedical knowledge graphs and constructs\nhierarchical interaction graphs from molecular representations, leveraging\nclassical graph neural network methods to learn multi-scale representations of\ndrug pairs. To enhance accuracy and interpretability, MolecBioNet introduces\ntwo domain-specific pooling strategies: context-aware subgraph pooling\n(CASPool), which emphasizes biologically relevant entities, and\nattention-guided influence pooling (AGIPool), which prioritizes influential\nmolecular substructures. The framework further employs mutual information\nminimization regularization to enhance information diversity during embedding\nfusion. Experimental results demonstrate that MolecBioNet outperforms\nstate-of-the-art methods in DDI prediction, while ablation studies and\nembedding visualizations further validate the advantages of unified drug pair\nmodeling and multi-scale knowledge integration.", "AI": {"tldr": "This paper introduces MolecBioNet, a new graph-based framework for drug-drug interaction (DDI) prediction that integrates molecular and biomedical knowledge, yielding superior accuracy and interpretability.", "motivation": "To address the limitations of existing graph-based DDI prediction models, which treat drug pairs independently, lack context-awareness, and fail to integrate biological networks and molecular-level structures.", "method": "The authors propose MolecBioNet, which models drug pairs as unified entities and integrates both macro-level biological interactions and micro-level molecular influences. It incorporates graph neural networks, context-aware subgraph pooling (CASPool), attention-guided influence pooling (AGIPool), and mutual information minimization for embedding diversity.", "result": "MolecBioNet achieves state-of-the-art performance in DDI prediction, as demonstrated through experimental results and ablation studies. Embedding visualizations also confirm the benefits of its unified modeling and multi-scale integration.", "conclusion": "The study provides a robust and interpretable framework for DDI prediction, overcoming prior limitations and advancing understanding of drug pair interactions at both molecular and biological levels."}}
{"id": "2507.09388", "pdf": "https://arxiv.org/pdf/2507.09388", "abs": "https://arxiv.org/abs/2507.09388", "authors": ["T. Tony Cai", "Abhinav Chakraborty", "Yichen Wang"], "title": "Optimal Differentially Private Ranking from Pairwise Comparisons", "categories": ["math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": null, "summary": "Data privacy is a central concern in many applications involving ranking from\nincomplete and noisy pairwise comparisons, such as recommendation systems,\neducational assessments, and opinion surveys on sensitive topics. In this work,\nwe propose differentially private algorithms for ranking based on pairwise\ncomparisons. Specifically, we develop and analyze ranking methods under two\nprivacy notions: edge differential privacy, which protects the confidentiality\nof individual comparison outcomes, and individual differential privacy, which\nsafeguards potentially many comparisons contributed by a single individual. Our\nalgorithms--including a perturbed maximum likelihood estimator and a noisy\ncount-based method--are shown to achieve minimax optimal rates of convergence\nunder the respective privacy constraints. We further demonstrate the practical\neffectiveness of our methods through experiments on both simulated and\nreal-world data.", "AI": {"tldr": "The paper introduces differentially private algorithms for ranking from pairwise comparisons, achieving optimal privacy-constrained performance with tested practical effectiveness.", "motivation": "To address privacy concerns in ranking tasks involving sensitive data, such as recommendation systems and opinion surveys, particularly when the data are incomplete and noisy.", "method": "The authors propose algorithms using edge differential privacy and individual differential privacy, including a perturbed maximum likelihood estimator and a noisy count-based method, and analyze their convergence performance.", "result": "The developed algorithms achieve minimax optimal rates of convergence under privacy constraints, validated by experiments on simulated and real-world data.", "conclusion": "The proposed methods effectively ensure data privacy while achieving optimal ranking performance in pairwise comparison tasks, proving their practicality in sensitive applications."}}
{"id": "2507.10326", "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "categories": ["cs.CL"], "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "This paper introduces an evolutionary approach for automated prompt optimization that significantly improves performance on smaller language models across complex tasks, outperforming state-of-the-art techniques.", "motivation": "Prompt engineering is critical for maximizing the efficiency of LLMs, especially smaller models dealing with complex tasks. Most existing solutions struggle with prompt optimization in these scenarios.", "method": "The authors propose a two-phase evolutionary search approach, utilizing grammar-guided genetic programming for initial synthesis and local search for further fine-tuning of prompt designs.", "result": "The proposed method outperformed PromptWizard, OPRO, and RL-Prompt across three smaller LLMs and four challenging tasks, showing robust results with minimal performance degradation.", "conclusion": "The evolutionary search approach provides a reliable solution for optimizing prompts for smaller and more sensitive models, making it suitable for detailed and complex task requirements."}}
{"id": "2507.09420", "pdf": "https://arxiv.org/pdf/2507.09420", "abs": "https://arxiv.org/abs/2507.09420", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "AI": {"tldr": "The paper introduces efficient neural network techniques for real-time celestial landmark detection and description to enhance spacecraft autonomy.", "motivation": "Current methods for celestial terrain feature detection face challenges like high computation, limited generalization, and reliance on extensive pre-processing.", "method": "The authors propose lightweight neural networks for real-time spacecraft processing, using domain adaptation for detection and attention alignment for robust descriptions.", "result": "The proposed system outperforms state-of-the-art techniques in celestial landmark tracking.", "conclusion": "This approach improves spacecraft autonomy while addressing hardware and data scarcity limitations, paving the way for better celestial navigation."}}
{"id": "2507.09445", "pdf": "https://arxiv.org/pdf/2507.09445", "abs": "https://arxiv.org/abs/2507.09445", "authors": ["Runze Yang", "Longbing Cao", "Xin You", "Kun Fang", "Jianxun Li", "Jie Yang"], "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "18 pages, 6 figures", "summary": "The integration of Fourier transform and deep learning opens new avenues for\ntime series forecasting. We reconsider the Fourier transform from a basis\nfunctions perspective. Specifically, the real and imaginary parts of the\nfrequency components can be regarded as the coefficients of cosine and sine\nbasis functions at tiered frequency levels, respectively. We find that existing\nFourier-based methods face inconsistent starting cycles and inconsistent series\nlength issues. They fail to interpret frequency components precisely and\noverlook temporal information. Accordingly, the novel Fourier Basis Mapping\n(FBM) method addresses these issues by integrating time-frequency features\nthrough Fourier basis expansion and mapping in the time-frequency space. Our\napproach extracts explicit frequency features while preserving temporal\ncharacteristics. FBM supports plug-and-play integration with various types of\nneural networks by only adjusting the first initial projection layer for better\nperformance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,\nMLP-based, and Transformer-based models, respectively, demonstrating the\neffectiveness of time-frequency features. Next, we propose a synergetic model\narchitecture, termed FBM-S, which decomposes the seasonal, trend, and\ninteraction effects into three separate blocks, each designed to model\ntime-frequency features in a specialized manner. Finally, we introduce several\ntechniques tailored for time-frequency features, including interaction masking,\ncentralization, patching, rolling window projection, and multi-scale\ndown-sampling. The results are validated on diverse real-world datasets for\nboth long-term and short-term forecasting tasks with SOTA performance.", "AI": {"tldr": "This paper introduces the Fourier Basis Mapping (FBM) method, a novel approach combining Fourier transform and deep learning to improve time series forecasting by addressing issues in temporal and frequency information integration.", "motivation": "Existing Fourier-based time series forecasting methods face issues such as inconsistent cycles and series lengths, leading to inadequate interpretation of frequency components and neglect of temporal information.", "method": "The authors propose the FBM method, which integrates time-frequency features through Fourier basis expansion and mapping, preserving temporal characteristics. It introduces FBM-L, FBM-NL, and FBM-NP to enhance various neural network models, and a synergetic architecture, FBM-S, to separately model seasonal, trend, and interaction effects.", "result": "The proposed methods are validated on various real-world datasets, achieving state-of-the-art performance in both long-term and short-term forecasting tasks.", "conclusion": "FBM is a versatile, plug-and-play method that improves time series forecasting performance by accurately leveraging time-frequency features, addressing existing limitations in Fourier-based methods, and successfully integrating them with neural networks."}}
{"id": "2507.10330", "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "The paper introduces Growth Bound Matrices (GBM) as a novel regularization method to improve adversarial robustness in NLP models, showing up to 8.8% robustness improvements.", "motivation": "To address the challenge of adversarial robustness in NLP for architectures like LSTMs and state space models (e.g., S4), which have been largely understudied compared to feed-forward and convolutional models.", "method": "A regularization method based on Growth Bound Matrices (GBM), applied to LSTMs, S4 models, and CNNs, that reduces the impact of input perturbations and is tested on multiple datasets.", "result": "Improves adversarial robustness by up to 8.8% over existing baselines across architectures like LSTMs, S4 models, and CNNs.", "conclusion": "The proposed GBM method enhances NLP robustness effectively, outperforms state-of-the-art adversarial defenses, and provides new insights into the robustness of state space models like S4."}}
{"id": "2507.09459", "pdf": "https://arxiv.org/pdf/2507.09459", "abs": "https://arxiv.org/abs/2507.09459", "authors": ["Zhihan Kang", "Boyu Wang"], "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "Undergraduate Theis; 12 pages, 6 figures", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "AI": {"tldr": "SegVec3D introduces a framework for 3D instance segmentation with minimal supervision, combining attention mechanisms and cross-modal alignment.", "motivation": "The paper aims to address challenges in unsupervised 3D point cloud instance segmentation while incorporating multimodal understanding for practical applications.", "method": "SegVec3D utilizes hierarchical feature extractors, contrastive clustering for unsupervised segmentation, and aligns 3D data with natural language queries in a shared semantic space.", "result": "The framework achieves unsupervised segmentation and supports zero-shot retrieval, surpassing methods like Mask3D and ULIP in unifying segmentation with multimodal capabilities.", "conclusion": "SegVec3D demonstrates a feasible approach for achieving 3D segmentation and multimodal language-vision understanding with minimal supervision, enhancing deployability."}}
{"id": "2507.09446", "pdf": "https://arxiv.org/pdf/2507.09446", "abs": "https://arxiv.org/abs/2507.09446", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "AI": {"tldr": "The paper presents a computationally efficient model for 3D multi-person motion prediction, achieving state-of-the-art performance while reducing computational costs.", "motivation": "To address the complexity of 3D multi-person motion prediction, which depends on both individual movements and inter-agent interactions, while lowering computational expense.", "method": "The model employs lightweight dual branches for local and global representations, introduces a cross-level interaction block for spatial-temporal integration, and uses spatial inter-person distance embedding to enhance interaction modeling.", "result": "Achieved state-of-the-art performance on CMU-Mocap, MuPoTS-3D, and 3DPW datasets with reduced computational costs.", "conclusion": "The proposed design simplifies interaction modeling while maintaining high prediction accuracy and efficiency, offering significant improvements in computational performance."}}
{"id": "2507.09202", "pdf": "https://arxiv.org/pdf/2507.09202", "abs": "https://arxiv.org/abs/2507.09202", "authors": ["Wuxin Wang", "Weicheng Ni", "Lilan Huang", "Tao Hao", "Ben Fei", "Shuo Ma", "Taikang Yuan", "Yanlai Zhao", "Kefeng Deng", "Xiaoyong Li", "Boheng Duan", "Lei Bai", "Kaijun Ren"], "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.", "AI": {"tldr": "XiChen is a groundbreaking AI-driven global weather forecasting system capable of executing both Data Assimilation (DA) and medium-range forecasts in 17 seconds, surpassing traditional Numerical Weather Prediction (NWP) systems.", "motivation": "Current AI-driven weather forecasting models depend on NWP systems for initial conditions, which are computationally expensive and time-consuming.", "method": "The paper introduces XiChen, a scalable AI-based system featuring a pre-trained foundational weather model fine-tuned for observation and DA tasks. It employs four-dimensional variational knowledge to ensure forecasting accuracy.", "result": "XiChen achieves DA and forecasting accuracy comparable to NWP systems while delivering skillful forecasts with lead times surpassing 8.25 days.", "conclusion": "XiChen demonstrates the feasibility of fully AI-driven weather forecasting systems, providing faster, scalable, and reliable alternatives to NWP-based methods."}}
{"id": "2507.09473", "pdf": "https://arxiv.org/pdf/2507.09473", "abs": "https://arxiv.org/abs/2507.09473", "authors": ["Yan Dai", "Negin Golrezaei", "Patrick Jaillet"], "title": "Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints", "categories": ["cs.GT", "cs.LG", "stat.ML"], "comment": null, "summary": "Motivated by applications such as cloud platforms allocating GPUs to users or\ngovernments deploying mobile health units across competing regions, we study\nthe dynamic allocation of a reusable resource to strategic agents with private\nvaluations. Our objective is to simultaneously (i) maximize social welfare,\n(ii) satisfy multi-dimensional long-term cost constraints, and (iii)\nincentivize truthful reporting. We begin by numerically evaluating primal-dual\nmethods widely used in constrained online optimization and find them to be\nhighly fragile in strategic settings -- agents can easily manipulate their\nreports to distort future dual updates for future gain.\n  To address this vulnerability, we develop an incentive-aware framework that\nmakes primal-dual methods robust to strategic behavior. Our design combines\nepoch-based lazy updates -- where dual variables remain fixed within each epoch\n-- with randomized exploration rounds that extract approximately truthful\nsignals for learning. Leveraging carefully designed online learning subroutines\nthat can be of independent interest for dual updates, our mechanism achieves\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ social welfare regret, satisfies all cost\nconstraints, and ensures incentive alignment. This matches the performance of\nnon-strategic allocation approaches while being robust to strategic agents.", "AI": {"tldr": "The paper proposes a solution for strategic allocation of reusable resources to agents, focusing on maximizing social welfare, handling cost constraints, and ensuring truthful behavior. Standard methods are fragile against strategic manipulation, and the authors present a novel mechanism with robust learning techniques.", "motivation": "Applications in cloud platforms or mobile health units raised concerns about allocating resources when agents act strategically. There's a need for mechanisms that balance welfare, cost constraints, and truthfulness.", "method": "The authors propose an incentive-aware framework combining lazy updates and randomized exploration rounds for truthful signaling. They leverage tailored online learning techniques for robust dual updates.", "result": "Their mechanism achieves near-optimal social welfare regret, satisfies cost constraints, and aligns agent incentives, offering resilience to strategic manipulations.", "conclusion": "The paper demonstrates the framework's effectiveness, ensuring robust resource allocation without sacrificing efficiency or fairness even in strategic settings."}}
{"id": "2507.10342", "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "The paper evaluates the use of large language models (LLMs) as analytical tools in linguistics by conducting psycholinguistic studies, finding significant convergence between human and LLM responses.", "motivation": "To assess the reliability of LLMs, specifically in their ability to replicate nuanced human judgments in linguistic research.", "method": "Conducted four psycholinguistic studies involving humans and then replicated the same tasks with an LLM, followed by statistical comparison of responses.", "result": "Strong correlations (Spearman's rho = .73-.96) were found between human and LLM responses across the studies, showing convergence with minor discrepancies.", "conclusion": "LLMs can effectively augment human-based linguistic research, supporting both validation of existing findings and new hypothesis generation."}}
{"id": "2507.09694", "pdf": "https://arxiv.org/pdf/2507.09694", "abs": "https://arxiv.org/abs/2507.09694", "authors": ["Nicolas Gonel", "Paul Saves", "Joseph Morlier"], "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": "AeroBest 2025, Instituto Superior Tecnico of the University of\n  Lisbon, Portugal", "summary": "This paper introduces a comprehensive open-source framework for developing\ncorrelation kernels, with a particular focus on user-defined and composition of\nkernels for surrogate modeling. By advancing kernel-based modeling techniques,\nwe incorporate frequency-aware elements that effectively capture complex\nmechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.\nTraditional kernel functions, often limited to exponential-based methods, are\nextended to include a wider range of kernels such as exponential squared sine\nand rational quadratic kernels, along with their respective firstand\nsecond-order derivatives. The proposed methodologies are first validated on a\nsinus cardinal test case and then applied to forecasting Mauna-Loa Carbon\nDioxide (CO 2 ) concentrations and airline passenger traffic. All these\nadvancements are integrated into the open-source Surrogate Modeling Toolbox\n(SMT 2.0), providing a versatile platform for both standard and customizable\nkernel configurations. Furthermore, the framework enables the combination of\nvarious kernels to leverage their unique strengths into composite models\ntailored to specific problems. The resulting framework offers a flexible\ntoolset for engineers and researchers, paving the way for numerous future\napplications in metamodeling for complex, frequency-sensitive domains.", "AI": {"tldr": "The paper proposes a frequency-aware and versatile kernel modeling framework within the open-source SMT 2.0 toolbox, enabling customizable and composite kernel configurations for surrogate modeling.", "motivation": "To overcome limitations of traditional exponential kernel functions in capturing complex, time-frequency dynamics in mechanical and aircraft systems.", "method": "Development of advanced kernel functions, such as exponential squared sine and rational quadratic kernels, integrated into the open-source SMT 2.0 toolbox. Validation using test cases like sinus cardinal and real-world applications.", "result": "Validated the proposed framework on benchmark cases like CO2 concentration forecasting and airline traffic predictions, demonstrating its effectiveness for frequency-sensitive domains.", "conclusion": "The framework enhances metamodeling capabilities for complex domains, offering engineers and researchers flexible tools for future applications."}}
{"id": "2507.10354", "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "categories": ["cs.CL"], "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "The paper introduces a multi-layer model for computational metaphor processing, integrating content analysis, conceptual blending, and pragmatic intentionality.", "motivation": "Current approaches to metaphor processing fail to capture the complex and layered nature of metaphorical meaning.", "method": "The authors develop a stratified three-layer model involving annotation of conceptual elements, modeling conceptual combinations, and introducing a pragmatic vocabulary.", "result": "The model enables richer computational interpretation of metaphor, incorporating surface, emergent, and context-sensitive dimensions.", "conclusion": "This approach provides a unified framework for understanding metaphor that aligns with cognitive and pragmatic theories, advancing computational metaphor reasoning."}}
{"id": "2507.09471", "pdf": "https://arxiv.org/pdf/2507.09471", "abs": "https://arxiv.org/abs/2507.09471", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "AI": {"tldr": "The paper introduces a framework, CKAA, to improve continual learning for AI models, addressing robustness issues with PEFT-based CL methods using two key components: intra-class feature alignment and adaptive knowledge aggregation.", "motivation": "To improve the robustness of parameter-efficient fine-tuning (PEFT)-based continual learning (CL) methods due to their vulnerability to feature subspace misalignment and ambiguous decisions caused by misleading task identifiers.", "method": "The CKAA framework incorporates (1) Dual-level Knowledge Alignment (DKA) to align intra-class feature distributions across subspaces and enhance global classification robustness, and (2) Task-Confidence-guided Mixture of Adapters (TC-MoA), which uses task-confidence scores to aggregate knowledge adaptively from sub-modules during inference.", "result": "CKAA demonstrated superior performance in robustness against misleading task-ids and outperformed existing PEFT-based CL methods, as validated through extensive experiments.", "conclusion": "The proposed CKAA framework effectively enhances robustness in PEFT-based CL models by addressing feature misalignment and improving decision confidence, offering a promising approach for robust continual learning."}}
{"id": "2507.09711", "pdf": "https://arxiv.org/pdf/2507.09711", "abs": "https://arxiv.org/abs/2507.09711", "authors": ["Kun He"], "title": "Phase transition of the Sinkhorn-Knopp algorithm", "categories": ["cs.DS", "cs.LG", "stat.ML"], "comment": "44 pages, 2 figures", "summary": "The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has\nbeen studied for over 60 years. In practice, the algorithm often yields\nhigh-quality approximations within just a few iterations. Theoretically,\nhowever, the best-known upper bound places it in the class of\npseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound\nlandscape remains largely unexplored. Two fundamental questions persist: what\naccounts for the algorithm's strong empirical performance, and can a tight\nbound on its iteration count be established?\n  For an $n\\times n$ matrix, its normalized version is obtained by dividing\neach entry by its largest entry. We say that a normalized matrix has a density\n$\\gamma$ if there exists a constant $\\rho > 0$ such that one row or column has\nexactly $\\lceil \\gamma n \\rceil$ entries with values at least $\\rho$, and every\nother row and column has at least $\\lceil \\gamma n \\rceil$ such entries.\n  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a\nnearly doubly stochastic matrix in $O(\\log n - \\log \\varepsilon)$ iterations\nand $\\widetilde{O}(n^2)$ time for all nonnegative square matrices whose\nnormalized version has a density $\\gamma > 1/2$. Such matrices cover both the\nalgorithm's principal practical inputs and its typical theoretical regime, and\nthe $\\widetilde{O}(n^2)$ runtime is optimal.\n  For the lower bound, we establish a tight bound of\n$\\widetilde{\\Omega}\\left(n^{1/2}/\\varepsilon\\right)$ iterations for positive\nmatrices under the $\\ell_2$-norm error measure. Moreover, for every $\\gamma <\n1/2$, there exists a matrix with density $\\gamma$ for which the algorithm\nrequires $\\Omega\\left(n^{1/2}/\\varepsilon\\right)$ iterations.\n  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp\nalgorithm at the density threshold $\\gamma = 1/2$.", "AI": {"tldr": "The paper investigates the Sinkhorn-Knopp algorithm for matrix scaling, addressing its empirical performance versus theoretical bounds, and identifies a computational phase transition at a density threshold of 1/2.", "motivation": "To explain the strong empirical performance of the Sinkhorn-Knopp algorithm in contrast to its pseudopolynomial theoretical bound and to establish a tighter iteration count.", "method": "The authors analyze the performance of the Sinkhorn-Knopp algorithm on normalized matrices with a density property, deriving upper bounds for certain densities and establishing tight lower bounds for iteration counts under the $\\ell_2$-norm.", "result": "They prove that the algorithm runs in $O(\\log n - \\log \\varepsilon)$ iterations for matrices with density $\\gamma > 1/2$ and identify $\\widetilde{O}(n^2)$ runtime as optimal. Additionally, they prove a lower bound of $\\widetilde{\\Omega}(n^{1/2}/\\varepsilon)$ iterations for matrices with density $\\gamma < 1/2$.", "conclusion": "The study finds a sharp phase transition in the Sinkhorn-Knopp algorithm's performance at the density threshold $\\gamma = 1/2$, shedding light on its theoretical efficiency."}}
{"id": "2507.10435", "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "This paper explores how large language models (LLMs), especially decoder-only Transformers, can understand and utilize underlying graph structures for graph reasoning tasks, introducing methods like Induced Substructure Filtration.", "motivation": "To investigate how decoder-only Transformer architectures process and understand graph structures embedded in textual descriptions and solve graph-related tasks.", "method": "The authors focus on substructure extraction tasks using the Induced Substructure Filtration (ISF) method, combining theoretical analysis and empirical testing to capture internal dynamics of multi-layer transformers.", "result": "The study reveals consistent internal dynamics across layers for substructure identification, and shows that decoder-only Transformers can handle diverse graph types, efficiently extracting patterns from attributed graphs like molecular graphs.", "conclusion": "Sequence-based Transformers are capable of performing substructure extraction tasks on graph data, offering new insights into their potential for graph reasoning and related tasks."}}
{"id": "2507.09487", "pdf": "https://arxiv.org/pdf/2507.09487", "abs": "https://arxiv.org/abs/2507.09487", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "AI": {"tldr": "The paper introduces HMID-Net, which combines Masked Image Modeling and hyperbolic space knowledge distillation to efficiently capture visual-semantic hierarchies.", "motivation": "To improve the efficiency of training models that effectively capture and leverage visual-semantic hierarchies in hyperbolic space.", "method": "The authors propose HMID-Net, a method integrating Masked Image Modeling (MIM) and a tailored knowledge distillation technique in hyperbolic space, along with a new distillation loss for better knowledge transfer.", "result": "The proposed method outperforms state-of-the-art models like MERU and CLIP on downstream tasks such as image classification and retrieval.", "conclusion": "Hyperbolic space techniques combined with MIM and knowledge distillation offer an efficient way to represent and harness hierarchical relationships, achieving significant performance improvements."}}
{"id": "2507.09846", "pdf": "https://arxiv.org/pdf/2507.09846", "abs": "https://arxiv.org/abs/2507.09846", "authors": ["Minhak Song", "Beomhan Baek", "Kwangjun Ahn", "Chulhee Yun"], "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": "Comments would be appreciated!", "summary": "As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.", "AI": {"tldr": "This paper revisits the Schedule-Free (SF) method for scalable, efficient training of language models, eliminating the need for explicit decay phases and weight averaging by leveraging SF-AdamW.", "motivation": "The motivation is to address the inadequacies of traditional pretraining strategies like cosine learning rate schedules and improve scalability for large-scale training workloads.", "method": "The authors analyze and refine the Schedule-Free (SF) method, originally introduced by Defazio et al., integrating it with SF-AdamW and proposing enhancements for robustness under momentum and large batch sizes.", "result": "The study demonstrates that SF-AdamW can effectively and efficiently train language models by navigating the loss landscape without decay phases or added memory costs.", "conclusion": "SF-AdamW is a practical, scalable, and theoretically well-founded method for large-scale language model training, offering improvements in performance and efficiency."}}
{"id": "2507.10445", "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "This study examines LLMs' abilities to ask clarification questions in task-oriented dialogues and compares them against humans. It presents a new dataset combining two annotations of the Minecraft Dialogue Corpus.", "motivation": "To analyze how LLMs handle ambiguity in task-oriented dialogues by comparing them with human behavior, especially regarding clarification questions.", "method": "The authors combined two annotated datasets of the Minecraft Dialogue Corpus to explore ambiguity and clarifications. They also compared human and LLM-generated clarification questions and assessed the role of reasoning approaches in LLM performance.", "result": "Humans rarely ask clarification questions for referential ambiguity but do for task-based uncertainty. LLMs ask more questions for referential ambiguity and fewer for task uncertainty. Reasoning improves question frequency and relevancy for LLMs.", "conclusion": "The study highlights differences in how LLMs and humans handle ambiguity through clarification questions, emphasizing the role of reasoning in improving LLM performance."}}
{"id": "2507.09491", "pdf": "https://arxiv.org/pdf/2507.09491", "abs": "https://arxiv.org/abs/2507.09491", "authors": ["Yiyang Zhou", "Linjie Li", "Shi Qiu", "Zhengyuan Yang", "Yuyang Zhao", "Siwei Han", "Yangfan He", "Kangqi Li", "Haonian Ji", "Zihao Zhao", "Haibo Tong", "Lijuan Wang", "Huaxiu Yao"], "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?", "categories": ["cs.CV"], "comment": "15 pages, 10 figures", "summary": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.", "AI": {"tldr": "GLIMPSE is a benchmark designed to evaluate large vision-language models' (LVLMs) ability to reason over full video context rather than static frames. Human performance is 94.82%, while top models achieve only 66.43%, revealing their limitations.", "motivation": "Existing video benchmarks fail to assess whether LVLMs can deeply analyze videos as they often focus on superficial frame-based questions.", "method": "GLIMPSE was developed with 3,269 videos and over 4,342 manually crafted questions across 11 categories like Temporal Reasoning and Forensics Detection, requiring full video context understanding.", "result": "Humans achieve high accuracy (94.82%), but current LVLMs, such as GPT-o3, lag significantly, performing at only 66.43%.", "conclusion": "LVLMs still struggle to perform deep temporal reasoning and require further improvement for genuine video understanding."}}
{"id": "2507.09888", "pdf": "https://arxiv.org/pdf/2507.09888", "abs": "https://arxiv.org/abs/2507.09888", "authors": ["Huibo Xu", "Likang Wu", "Xianquan Wang", "Haoning Dang", "Chun-Wun Cheng", "Angelica I Aviles-Rivero", "Qi Liu"], "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.", "AI": {"tldr": "This paper introduces NeuTSFlow, a novel time series forecasting framework that models transitions between continuous function families instead of discrete data points.", "motivation": "Current methods largely overlook the continuous nature of time series data, leading to limitations in accurately forecasting future trends.", "method": "NeuTSFlow uses Neural Operators to match flows in infinite-dimensional function spaces, leveraging velocity field parameterization for function-level modeling.", "result": "NeuTSFlow demonstrated improved accuracy and robustness across a variety of forecasting tasks when compared to traditional methods focused on discrete points.", "conclusion": "The function-family perspective proposed by NeuTSFlow offers a more effective approach to time series forecasting, addressing challenges posed by noisy and discrete observations."}}
{"id": "2507.10468", "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa\u00fal Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "categories": ["cs.CL", "cs.LG"], "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "The paper benchmarks classical encoder models and new-generation large language models (LLMs) for hate speech detection.", "motivation": "Online platforms face a challenge of moderating hate speech without suppressing legitimate discussions, necessitating accurate detection methods.", "method": "The authors compare classic bidirectional transformer encoder models and newer ultra-large autoregressive LLMs on curated datasets to measure their hate speech detection accuracy.", "result": "The study identifies whether larger language models improve detection of hate speech in real-world online texts compared to traditional models.", "conclusion": "The paper concludes on the effectiveness of the advanced LLMs relative to classical encoders in practical hate speech identification."}}
{"id": "2507.10034", "pdf": "https://arxiv.org/pdf/2507.10034", "abs": "https://arxiv.org/abs/2507.10034", "authors": ["Xianghong Zou", "Jianping Li", "Zhe Chen", "Zhen Cao", "Zhen Dong", "Qiegen Liu", "Bisheng Yang"], "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.", "AI": {"tldr": "The paper introduces LifelongPR, a novel continual learning framework for point cloud place recognition (PCPR), addressing catastrophic forgetting and domain shifts to improve model adaptability in diverse and dynamic environments.", "motivation": "The study is motivated by the need for PCPR models to adapt to diverse and dynamic environments without suffering from catastrophic forgetting, which is crucial for applications like autonomous driving and augmented reality.", "method": "The approach involves a replay sample selection method for balanced knowledge retention and a prompt learning-based continual learning framework with a lightweight prompt module and two-stage training to handle domain shifts.", "result": "LifelongPR demonstrated significant improvements over state-of-the-art methods, achieving a 6.50% improvement in mIR@1, 7.96% in mR@1, and an 8.95% reduction in forgetting (F) across large-scale public and self-collected datasets.", "conclusion": "LifelongPR effectively addresses issues of scalability, maintenance, and domain adaptability in PCPR models, making it a promising solution for real-world deployment."}}
{"id": "2507.09492", "pdf": "https://arxiv.org/pdf/2507.09492", "abs": "https://arxiv.org/abs/2507.09492", "authors": ["Fuyin Ye", "Erwen Yao", "Jianyong Chen", "Fengmei He", "Junxiang Zhang", "Lihao Ni"], "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, 2 figures", "summary": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.", "AI": {"tldr": "The paper introduces SDTN and TRN methods for hyperspectral image classification to tackle challenges like high-dimensional data and scarcity of labeled samples, achieving improved accuracy and reduced computational load.", "motivation": "Current hyperspectral imaging methods face limitations with high-dimensional data and insufficient labeled samples, impacting classification accuracy in applications like precision agriculture.", "method": "The authors developed the Self-Adaptive Tensor-Regularized Network (SDTN) for optimized feature representation through tensor decomposition and regularization, along with Tensor-Regularized Network (TRN) for lightweight multi-scale spectral-spatial feature integration.", "result": "Using the PaviaU datasets, the proposed methodologies showed enhanced accuracy and significantly smaller model parameters compared to existing approaches.", "conclusion": "SDTN and TRN frameworks resolve key issues in hyperspectral image classification, offering high accuracy with reduced computational complexity suitable for real-time usage in constrained environments."}}
{"id": "2507.09264", "pdf": "https://arxiv.org/pdf/2507.09264", "abs": "https://arxiv.org/abs/2507.09264", "authors": ["Payel Mukhopadhyay", "Michael McCabe", "Ruben Ohana", "Miles Cranmer"], "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": null, "summary": "Patch-based transformer surrogates have become increasingly effective for\nmodeling spatiotemporal dynamics, but the fixed patch size is a major\nlimitation for budget-conscience deployment in production. We introduce two\nlightweight, architecture-agnostic modules-the Convolutional Kernel Modulator\n(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size\ncontrol at inference in patch based models, without retraining or accuracy\nloss. Combined with a cyclic patch-size rollout, our method mitigates patch\nartifacts and improves long-term stability for video-like prediction tasks.\nApplied to a range of challenging 2D and 3D PDE benchmarks, our approach\nimproves rollout fidelity and runtime efficiency. To our knowledge, this is the\nfirst framework to enable inference-time patch-size tunability in patch-based\nPDE surrogates. Its plug-and-play design makes it broadly applicable across\narchitectures-establishing a general foundation for compute-adaptive modeling\nin PDE surrogate tasks.", "AI": {"tldr": "This paper introduces two modules, CKM and CSM, to enable inference-time patch size control in patch-based transformer surrogates for spatiotemporal dynamics, avoiding retraining and accuracy loss.", "motivation": "Patch-based transformer surrogates face scalability challenges due to a fixed patch size during inference, limiting their efficiency in real-world applications.", "method": "The proposed method introduces two modules, CKM (Convolutional Kernel Modulator) and CSM (Convolutional Stride Modulator), which allow dynamic patch-size adjustment during inference without requiring retraining.", "result": "The approach demonstrates improved rollout fidelity, runtime efficiency, and long-term stability when applied to 2D and 3D PDE benchmarks.", "conclusion": "This framework is the first to offer inference-time patch-size adaptability, creating a general and architecture-agnostic foundation for compute-adaptive modeling in PDE surrogate tasks."}}
{"id": "2507.09905", "pdf": "https://arxiv.org/pdf/2507.09905", "abs": "https://arxiv.org/abs/2507.09905", "authors": ["Zijian Guo", "Zhenyu Wang", "Yifan Hu", "Francis Bach"], "title": "Statistical Inference for Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "In multi-source learning with discrete labels, distributional heterogeneity\nacross domains poses a central challenge to developing predictive models that\ntransfer reliably to unseen domains. We study multi-source unsupervised domain\nadaptation, where labeled data are drawn from multiple source domains and only\nunlabeled data from a target domain. To address potential distribution shifts,\nwe propose a novel Conditional Group Distributionally Robust Optimization\n(CG-DRO) framework that learns a classifier by minimizing the worst-case\ncross-entropy loss over the convex combinations of the conditional outcome\ndistributions from the sources. To solve the resulting minimax problem, we\ndevelop an efficient Mirror Prox algorithm, where we employ a double machine\nlearning procedure to estimate the risk function. This ensures that the errors\nof the machine learning estimators for the nuisance models enter only at\nhigher-order rates, thereby preserving statistical efficiency under covariate\nshift. We establish fast statistical convergence rates for the estimator by\nconstructing two surrogate minimax optimization problems that serve as\ntheoretical bridges. A distinguishing challenge for CG-DRO is the emergence of\nnonstandard asymptotics: the empirical estimator may fail to converge to a\nstandard limiting distribution due to boundary effects and system instability.\nTo address this, we introduce a perturbation-based inference procedure that\nenables uniformly valid inference, including confidence interval construction\nand hypothesis testing.", "AI": {"tldr": "The paper introduces a Conditional Group Distributionally Robust Optimization (CG-DRO) framework for multi-source unsupervised domain adaptation, addressing distributional shifts with the target domain.", "motivation": "Distributional heterogeneity across domains in multi-source learning challenges the reliability of predictive models when transferring to unseen domains.", "method": "The CG-DRO framework minimizes the worst-case cross-entropy loss over convex combinations of conditional outcome distributions from source domains, solved using a Mirror Prox algorithm with a double machine learning procedure.", "result": "The framework achieves fast statistical convergence rates and demonstrates nonstandard asymptotics, with a perturbation-based inference procedure ensuring valid uniform statistical inference.", "conclusion": "CG-DRO effectively addresses distribution shifts and provides a robust theoretical foundation for reliable predictive modeling in multi-source unsupervised domain adaptation."}}
{"id": "2507.10472", "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "The paper introduces MLAR, an Applicant Tracking System integrating RPA and LLMs to efficiently process resumes and match candidates.", "motivation": "To address inefficiencies in traditional recruitment processes, particularly in resume screening and candidate shortlisting.", "method": "MLAR uses a three-layer framework: LLMs extract job-posting characteristics, parse applicant resumes, and perform similarity matching with semantic algorithms.", "result": "MLAR reduced resume-processing time by approximately 17%, outperforming leading RPA platforms like UiPath and Automation Anywhere.", "conclusion": "MLAR offers a scalable and efficient recruitment automation solution, significantly improving processing speed and accuracy."}}
{"id": "2507.10158", "pdf": "https://arxiv.org/pdf/2507.10158", "abs": "https://arxiv.org/abs/2507.10158", "authors": ["Obaidullah Zaland", "Erik Elmroth", "Monowar Bhuyan"], "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "categories": ["cs.LG", "cs.RO"], "comment": "The work is accepted for presentation at IEEE SMC 2025", "summary": "Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.", "AI": {"tldr": "The paper introduces MTF-Grasp, a multi-tier federated learning approach to address challenges in robotic grasping under non-IID and low-quantity data conditions.", "motivation": "Federated Learning lacks exploration within robotic grasping tasks, where data is both non-IID and limited, leading to performance degradation.", "method": "MTF-Grasp prioritizes \"top-level\" robots with better data distribution for initial seed model training, then distributes models to \"low-level\" robots to mitigate performance loss.", "result": "MTF-Grasp improves robotic grasping performance by up to 8% on quantity-skewed datasets compared to conventional FL setups.", "conclusion": "The multi-tier FL approach effectively addresses data heterogeneity and performance issues in robotic grasping, showcasing promising results."}}
{"id": "2507.08853", "pdf": "https://arxiv.org/pdf/2507.08853", "abs": "https://arxiv.org/abs/2507.08853", "authors": ["Victoria L. Lemieux", "Rosa Gil", "Faith Molosiwa", "Qihong Zhou", "Binming Li", "Roberto Garcia", "Luis De La Torre Cubillo", "Zehua Wang"], "title": "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.DL", "D.2.11, H.3.4, H.3.7, J.5"], "comment": "28 pages, 8 figures", "summary": "As archives turn to artificial intelligence to manage growing volumes of\ndigital records, privacy risks inherent in current AI data practices raise\ncritical concerns about data sovereignty and ethical accountability. This paper\nexplores how privacy-enhancing technologies (PETs) and Web3 architectures can\nsupport archives to preserve control over sensitive content while still being\nable to make it available for access by researchers. We present Clio-X, a\ndecentralized, privacy-first Web3 digital solution designed to embed PETs into\narchival workflows and support AI-enabled reference and access. Drawing on a\nuser evaluation of a medium-fidelity prototype, the study reveals both interest\nin the potential of the solution and significant barriers to adoption related\nto trust, system opacity, economic concerns, and governance. Using Rogers'\nDiffusion of Innovation theory, we analyze the sociotechnical dimensions of\nthese barriers and propose a path forward centered on participatory design and\ndecentralized governance through a Clio-X Decentralized Autonomous\nOrganization. By integrating technical safeguards with community-based\noversight, Clio-X offers a novel model to ethically deploy AI in cultural\nheritage contexts.", "AI": {"tldr": "The paper introduces Clio-X, a privacy-first Web3 solution for AI-enabled archival management, highlighting user interest and barriers such as trust and governance.", "motivation": "To address privacy risks in AI practices, particularly in archival management, by exploring PETs and Web3 technologies.", "method": "Development and evaluation of Clio-X prototype, leveraging PETs and Web3, followed by analysis using Diffusion of Innovation theory.", "result": "Findings revealed user interest but also barriers related to trust, system transparency, costs, and governance.", "conclusion": "A participatory design and decentralized governance model via Clio-X DAO are recommended for ethical AI deployment in heritage archives."}}
{"id": "2507.09500", "pdf": "https://arxiv.org/pdf/2507.09500", "abs": "https://arxiv.org/abs/2507.09500", "authors": ["Yiwen Liang", "Hui Chen", "Yizhe Xiong", "Zihan Zhou", "Mengyao Lyu", "Zijia Lin", "Shuaicheng Niu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations", "categories": ["cs.CV"], "comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.", "AI": {"tldr": "The paper addresses VLMs' difficulties with distribution shifts in tasks without labeled data, proposing ReTA, a method featuring two reliability strategies for improved adaptation and accuracy.", "motivation": "The paper aims to enhance VLMs' reliability during test-time adaptation, especially under distribution shifts, as existing methods struggle with entropy issues and inflexible decision boundaries.", "method": "ReTA incorporates Consistency-aware Entropy Reweighting (CER) for improving cache reliability and Diversity-driven Distribution Calibration (DDC) for adaptive adjustments in decision boundaries.", "result": "ReTA improves upon state-of-the-art methods, delivering robust adaptation and superior performance under real-world distribution shifts.", "conclusion": "ReTA demonstrates its ability to reliably adapt VLMs to distribution shifts, addressing critical challenges and advancing test-time adaptation techniques."}}
{"id": "2507.09916", "pdf": "https://arxiv.org/pdf/2507.09916", "abs": "https://arxiv.org/abs/2507.09916", "authors": ["Ahmad Aghapour", "Erhan Bayraktar", "Fengyi Yuan"], "title": "Solving dynamic portfolio selection problems via score-based diffusion models", "categories": ["q-fin.PM", "stat.ML"], "comment": "Code available on https://github.com/fy-yuan/diffusion_dynamic_mv", "summary": "In this paper, we tackle the dynamic mean-variance portfolio selection\nproblem in a {\\it model-free} manner, based on (generative) diffusion models.\nWe propose using data sampled from the real model $\\mathcal P$ (which is\nunknown) with limited size to train a generative model $\\mathcal Q$ (from which\nwe can easily and adequately sample). With adaptive training and sampling\nmethods that are tailor-made for time series data, we obtain quantification\nbounds between $\\mathcal P$ and $\\mathcal Q$ in terms of the adapted\nWasserstein metric $\\mathcal A W_2$. Importantly, the proposed adapted sampling\nmethod also facilitates {\\it conditional sampling}. In the second part of this\npaper, we provide the stability of the mean-variance portfolio optimization\nproblems in $\\mathcal A W _2$. Then, combined with the error bounds and the\nstability result, we propose a policy gradient algorithm based on the\ngenerative environment, in which our innovative adapted sampling method\nprovides approximate scenario generators. We illustrate the performance of our\nalgorithm on both simulated and real data. For real data, the algorithm based\non the generative environment produces portfolios that beat several important\nbaselines, including the Markowitz portfolio, the equal weight (naive)\nportfolio, and S\\&P 500.", "AI": {"tldr": "The paper introduces a model-free approach using generative diffusion models for dynamic mean-variance portfolio selection, achieving superior results when compared to existing portfolio strategies.", "motivation": "To address the challenge of model-free dynamic portfolio selection using limited real-world data in scenarios where the underlying model is unknown.", "method": "Utilizing adaptive training and tailored sampling methods for time series to train a generative diffusion model, combined with policy gradient algorithms in a simulated environment.", "result": "The generative environment-based algorithm outperformed benchmarks like Markowitz portfolio, equal-weight portfolio, and S&P 500 on both simulated and real-world datasets.", "conclusion": "The proposed approach demonstrates robustness, accuracy, and practical efficacy in portfolio optimization, backed by theoretical guarantees and empirical validation."}}
{"id": "2507.10475", "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["\u0130smail Tar\u0131m", "Aytu\u011f Onan"], "title": "Can You Detect the Difference?", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "This paper examines the ability to detect AI-generated text by comparing diffusion-based models (LLaDA) and autoregressive models (LLaMA), showing that diffusion models mimic human text well, posing challenges for current detection methods.", "motivation": "To address the challenges posed by the increasing difficulty in detecting AI-generated text, especially from newer diffusion-based models, and the limitations of current detection metrics.", "method": "The authors systematically compared 2,000 text samples from diffusion-based (LLaDA) and autoregressive (LLaMA) models using various textual metrics like perplexity, burstiness, lexical diversity, and BLEU/ROUGE scores.", "result": "LLaDA mimics human text closely in perplexity and burstiness, leading to high false-negative rates for detection. LLaMA exhibits lower perplexity but with less lexical fidelity. Single metrics fail to reliably differentiate diffusion-generated text from human writing.", "conclusion": "There is a critical need for diffusion-aware detection methods, hybrid models, and techniques such as diffusion-specific stylometric signatures and robust watermarking to better distinguish AI-generated text from human writing."}}
{"id": "2507.10427", "pdf": "https://arxiv.org/pdf/2507.10427", "abs": "https://arxiv.org/abs/2507.10427", "authors": ["Jing Li", "Felix Schijve", "Sheng Li", "Yuye Yang", "Jun Hu", "Emilia Barakova"], "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads", "categories": ["cs.HC", "cs.RO"], "comment": "Submission for the IROS 2025 conference", "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.", "AI": {"tldr": "This study introduces a social robot leveraging large language models (LLMs) to assist parents and neurodivergent children in emotion regulation, showing positive interaction impacts from pilot tests.", "motivation": "To address the lack of research on integrating LLMs with socially assistive robots to aid emotion co-regulation between parents and neurodivergent children.", "method": "A LLM-powered social robot using the MiRo-E platform was developed with a speech communication module, integrating LLM prompts and robotic behaviors. Pilot tests were performed with two parent-child dyads, followed by qualitative analysis.", "result": "Results demonstrated the robot's positive impact on parent-child interaction dynamics and its potential to assist in emotion regulation, despite noting design and technical challenges.", "conclusion": "The study highlights the potential of LLM-powered SAR in mental health applications and provides design recommendations for advancing this technology."}}
{"id": "2507.09512", "pdf": "https://arxiv.org/pdf/2507.09512", "abs": "https://arxiv.org/abs/2507.09512", "authors": ["Pengyu Liu", "Kun Li", "Fei Wang", "Yanyan Wei", "Junhui She", "Dan Guo"], "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.", "AI": {"tldr": "Developed a new method for micro-gesture recognition that achieved state-of-the-art results.", "motivation": "To handle the challenging task of locating and recognizing micro-gestures in untrimmed videos due to their spontaneous nature and differences from traditional human actions.", "method": "Introduced hand-crafted data augmentation and spatial-temporal attention to improve model classification and localization of micro-gestures.", "result": "Achieved an F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%, and secured first place in the competition.", "conclusion": "The proposed method significantly advanced micro-gesture recognition, demonstrating its accuracy and effectiveness in distinguishing and localizing these challenging instances."}}
{"id": "2507.09362", "pdf": "https://arxiv.org/pdf/2507.09362", "abs": "https://arxiv.org/abs/2507.09362", "authors": ["Assaf Marron", "Smadar Szekely", "Irun Cohen", "David Harel"], "title": "Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes", "categories": ["cs.LG", "q-bio.PE"], "comment": null, "summary": "An autoencoder (AE) is a neural network that, using self-supervised training,\nlearns a succinct parameterized representation, and a corresponding encoding\nand decoding process, for all instances in a given class. Here, we introduce\nthe concept of a meta-autoencoder (MAE): an AE for a collection of\nautoencoders. Given a family of classes that differ from each other by the\nvalues of some parameters, and a trained AE for each class, an MAE for the\nfamily is a neural net that has learned a compact representation and associated\nencoder and decoder for the class-specific AEs. One application of this general\nconcept is in research and modeling of natural evolution -- capturing the\ndefining and the distinguishing properties across multiple species that are\ndynamically evolving from each other and from common ancestors. In this interim\nreport we provide a constructive definition of MAEs, initial examples, and the\nmotivating research directions in machine learning and biology.", "AI": {"tldr": "The paper introduces a meta-autoencoder (MAE), a neural network that compresses and encodes multiple autoencoders for varying classes.", "motivation": "The motivation is to generalize the mechanism of autoencoders to handle dynamic differences between multiple classes, potentially useful for studying evolution and distinguishing properties across species.", "method": "The authors propose a meta-autoencoder, which is an autoencoder designed to represent and encode a family of class-specific autoencoders, and provide initial examples.", "result": "The study offers a constructive definition of meta-autoencoders and outlines preliminary examples, connecting their functionality to machine learning and biology research.", "conclusion": "Meta-autoencoders have the potential to capture and represent distinctions across dynamically evolving classes, offering applications like modeling natural evolution and generalizing across multiple domains."}}
{"id": "2507.10019", "pdf": "https://arxiv.org/pdf/2507.10019", "abs": "https://arxiv.org/abs/2507.10019", "authors": ["Pranav Joshi"], "title": "Sampling-Based Estimation of Jaccard Containment and Similarity", "categories": ["stat.CO", "cs.DB", "stat.ML"], "comment": null, "summary": "This paper addresses the problem of estimating the containment and similarity\nbetween two sets using only random samples from each set, without relying on\nsketches or full data access. The study introduces a binomial model for\npredicting the overlap between samples, demonstrating that it is both accurate\nand practical when sample sizes are small compared to the original sets. The\npaper compares this model to previous approaches and shows that it provides\nbetter estimates under the considered conditions. It also analyzes the\nstatistical properties of the estimator, including error bounds and sample size\nrequirements needed to achieve a desired level of accuracy and confidence. The\nframework is extended to estimate set similarity, and the paper provides\nguidance for applying these methods in large scale data systems where only\npartial or sampled data is available.", "AI": {"tldr": "The paper introduces a binomial model to estimate set containment and similarity using random samples, without needing sketches or full data.", "motivation": "To improve containment and similarity estimation between large sets when only partial or sampled data is accessible.", "method": "Proposes a binomial model to predict sample overlap and analyzes its error bounds, statistical properties, and sample size requirements.", "result": "The model is shown to outperform previous methods in accurately estimating set containment and similarity under limited sample conditions.", "conclusion": "The binomial model is practical and effective for large-scale systems, offering better accuracy when only sampled data from sets is available."}}
{"id": "2507.10524", "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "categories": ["cs.CL", "cs.LG"], "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "This paper presents Mixture-of-Recursions (MoR), a framework designed to improve efficiency in language models by combining parameter sharing and adaptive computation.", "motivation": "Scaling language models improves performance but demands significant computational and memory resources, making training and deployment expensive.", "method": "The authors propose MoR, a Recursive Transformer that shares parameters across recursion steps and uses adaptive token-level computation. Lightweight routers dynamically assign recursion depths, focusing quadratic attention on active tokens. A KV sharing variant further optimizes memory use.", "result": "MoR achieves lower validation perplexity, improved few-shot accuracy, and higher throughput across model scales (135M\u20131.7B params), establishing itself as a new efficiency-performance frontier.", "conclusion": "MoR effectively delivers large-model quality at reduced computational cost, making it a promising solution for scaling language models efficiently."}}
{"id": "2507.10474", "pdf": "https://arxiv.org/pdf/2507.10474", "abs": "https://arxiv.org/abs/2507.10474", "authors": ["Seyed Alireza Rahimi Azghadi", "Truong-Thanh-Hung Nguyen", "Helene Fournier", "Monica Wachowicz", "Rene Richard", "Francis Palma", "Hung Cao"], "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.", "AI": {"tldr": "This paper introduces a privacy-preserving, multi-system framework for fall detection with a combined accuracy of 99.99%, addressing safety for older adults.", "motivation": "The paper is motivated by the growing aging population, the increasing risk of falls among older adults, and the need for effective, reliable fall detection systems that preserve users' privacy.", "method": "The authors propose a framework combining three systems: semi-supervised federated learning for wearable and edge devices, indoor localization/navigation for robots, and vision-based human recognition using edge cameras.", "result": "The systems demonstrate individual accuracy: 99.19% (SF2D), 96.3% (vision-based detection), and 95% (navigation). Combined, they yield an overall accuracy of 99.99% for fall detection.", "conclusion": "The proposed framework is highly accurate, safe, and privacy-conscious, making it an effective solution for fall detection and timely intervention for older adults."}}
{"id": "2507.08864", "pdf": "https://arxiv.org/pdf/2507.08864", "abs": "https://arxiv.org/abs/2507.08864", "authors": ["Poushali Sengupta", "Sabita Maharjan", "frank Eliassen", "Yan Zhang"], "title": "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": "accepted in VTC 2025 Spring, Oslo, Norway", "summary": "Location-based vehicular traffic management faces significant challenges in\nprotecting sensitive geographical data while maintaining utility for traffic\nmanagement and fairness across regions. Existing state-of-the-art solutions\noften fail to meet the required level of protection against linkage attacks and\ndemographic biases, leading to privacy leakage and inequity in data analysis.\nIn this paper, we propose a novel algorithm designed to address the challenges\nregarding the balance of privacy, utility, and fairness in location-based\nvehicular traffic management systems. In this context, utility means providing\nreliable and meaningful traffic information, while fairness ensures that all\nregions and individuals are treated equitably in data use and decision-making.\nEmploying differential privacy techniques, we enhance data security by\nintegrating query-based data access with iterative shuffling and calibrated\nnoise injection, ensuring that sensitive geographical data remains protected.\nWe ensure adherence to epsilon-differential privacy standards by implementing\nthe Laplace mechanism. We implemented our algorithm on vehicular location-based\ndata from Norway, demonstrating its ability to maintain data utility for\ntraffic management and urban planning while ensuring fair representation of all\ngeographical areas without being overrepresented or underrepresented.\nAdditionally, we have created a heatmap of Norway based on our model,\nillustrating the privatized and fair representation of the traffic conditions\nacross various cities. Our algorithm provides privacy in vehicular traffic", "AI": {"tldr": "The paper introduces a privacy-preserving algorithm for location-based vehicular traffic management that balances privacy, utility, and fairness using differential privacy techniques.", "motivation": "To address privacy leakage, demographic biases, and inequity in data analysis while managing vehicular traffic data.", "method": "The algorithm integrates query-based data access, iterative shuffling, and calibrated noise injection with epsilon-differential privacy standards using the Laplace mechanism.", "result": "When applied to vehicular traffic data in Norway, the algorithm ensured data utility for urban planning and fairness in the representation of various geographical areas.", "conclusion": "The proposed algorithm effectively safeguards sensitive geographical data while supporting equitable and meaningful traffic management and urban planning."}}
{"id": "2507.09514", "pdf": "https://arxiv.org/pdf/2507.09514", "abs": "https://arxiv.org/abs/2507.09514", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the\n  International Conference on Machine Learning (ICML) 2025", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.", "AI": {"tldr": "QuarterMap is an activation pruning method that enhances the efficiency of VMamba and similar models without retraining, offering up to 11% speedup with minimal accuracy loss.", "motivation": "State space models have performance bottlenecks due to spatial redundancy in their scanning processes. The paper seeks efficient ways to reduce these redundancies while maintaining model accuracy.", "method": "The proposed method, QuarterMap, uses post-training activation pruning to remove redundant spatial activations and restores dimensions with nearest-neighbor upsampling, improving throughput without retraining.", "result": "QuarterMap provided up to 11% speedup on VMamba with accuracy drops less than 0.9% on ImageNet-1K and showed similar results on ADE20K segmentation tasks.", "conclusion": "QuarterMap serves as an effective and deployment-friendly tool for increasing efficiency in SSMs-based models like VMamba and MedMamba, without requiring retraining and while maintaining transferability across tasks."}}
{"id": "2507.09382", "pdf": "https://arxiv.org/pdf/2507.09382", "abs": "https://arxiv.org/abs/2507.09382", "authors": ["Bojian Hou", "Zhanliang Wang", "Zhuoping Zhou", "Boning Tong", "Zexuan Wang", "Jingxuan Bao", "Duy Duong-Tran", "Qi Long", "Li Shen"], "title": "Fair CCA for Fair Representation Learning: An ADNI Study", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Canonical correlation analysis (CCA) is a technique for finding correlations\nbetween different data modalities and learning low-dimensional representations.\nAs fairness becomes crucial in machine learning, fair CCA has gained attention.\nHowever, previous approaches often overlook the impact on downstream\nclassification tasks, limiting applicability. We propose a novel fair CCA\nmethod for fair representation learning, ensuring the projected features are\nindependent of sensitive attributes, thus enhancing fairness without\ncompromising accuracy. We validate our method on synthetic data and real-world\ndata from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating\nits ability to maintain high correlation analysis performance while improving\nfairness in classification tasks. Our work enables fair machine learning in\nneuroimaging studies where unbiased analysis is essential.", "AI": {"tldr": "This paper proposes a fair CCA method to enhance fairness in representation learning without sacrificing accuracy, targeting applications in neuroimaging studies.", "motivation": "Traditional CCA techniques often fail to address fairness concerns in representation learning, particularly in downstream classification tasks where bias can affect outcomes.", "method": "The authors introduce a novel fair CCA method that ensures projected features are independent of sensitive attributes while preserving high correlation analysis performance.", "result": "Experimental validation on synthetic and Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) data demonstrates improved fairness in classification tasks without compromising accuracy.", "conclusion": "The proposed fair CCA method enhances fairness in machine learning applications like neuroimaging studies, enabling unbiased analyses and broadening its applicability."}}
{"id": "2507.10088", "pdf": "https://arxiv.org/pdf/2507.10088", "abs": "https://arxiv.org/abs/2507.10088", "authors": ["Tung Sum Thomas Kwok", "Zeyong Zhang", "Chi-Hua Wang", "Guang Cheng"], "title": "Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by Agentic & GenAI Evaluation KDD2025", "summary": "Tabular data synthesis for supervised learning ('SL') model training is\ngaining popularity in industries such as healthcare, finance, and retail.\nDespite the progress made in tabular data generators, models trained with\nsynthetic data often underperform compared to those trained with original data.\nThis low SL utility of synthetic data stems from class imbalance exaggeration\nand SL data relationship overlooked by tabular generator. To address these\nchallenges, we draw inspirations from techniques in emerging data-centric\nartificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel\npipeline that integrates data-centric techniques into tabular data synthesis.\nPRRO incorporates data pruning to guide the table generator towards\nobservations with high signal-to-noise ratio, ensuring that the class\ndistribution of synthetic data closely matches that of the original data.\nBesides, PRRO employs a column reordering algorithm to align the data modeling\nstructure of generators with that of SL models. These two modules enable PRRO\nto optimize SL utility of synthetic data. Empirical experiments on 22 public\ndatasets show that synthetic data generated using PRRO enhances predictive\nperformance compared to data generated without PRRO. Specifically, synthetic\nreplacement of original data yields an average improvement of 26.74% and up to\n871.46% improvement using PRRO, while synthetic appendant to original data\nresults with PRRO-generated data results in an average improvement of 6.13% and\nup to 200.32%. Furthermore, experiments on six highly imbalanced datasets show\nthat PRRO enables the generator to produce synthetic data with a class\ndistribution that resembles the original data more closely, achieving a\nsimilarity improvement of 43%. Through PRRO, we foster a seamless integration\nof data synthesis to subsequent SL prediction, promoting quality and accessible\ndata analysis.", "AI": {"tldr": "The paper introduces PRRO, a new pipeline that improves synthetic data generation by addressing class imbalances and SL data relationships, resulting in significantly better performance in supervised learning tasks.", "motivation": "The motivation stems from the observation that synthetic data often underperforms in supervised learning tasks due to issues like class imbalance exaggeration and the lack of attention to supervised learning-specific data relationships.", "method": "The proposed method, PRRO, integrates data pruning to retain high signal-to-noise ratio observations and employs column reordering to align synthetic data structures with supervised learning model requirements.", "result": "Empirical experiments on 22 public datasets show that PRRO improves predictive performance, achieving an average improvement of 26.74% (up to 871.46%) when synthetic data replaces original data, and an average of 6.13% (up to 200.32%) when appended. It also enhances class distribution similarity by 43% on imbalanced datasets.", "conclusion": "PRRO enhances the utility of synthetic data for supervised learning by addressing major limitations of current tabular data generators, enabling better integration into prediction tasks and improving accessibility to quality data analysis."}}
{"id": "2507.09524", "pdf": "https://arxiv.org/pdf/2507.09524", "abs": "https://arxiv.org/abs/2507.09524", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "title": "When Schr\u00f6dinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.", "AI": {"tldr": "This paper introduces DehazeSB, a novel unpaired dehazing framework utilizing optimal transport theory through the Schr\u00f6dinger Bridge to enhance mappings between hazy and clear images for high-quality results.", "motivation": "Existing GAN-based unpaired dehazing methods face limitations due to the generator's limited transport mapping capability, restricting effectiveness in unpaired training paradigms.", "method": "DehazeSB employs optimal transport theory via the Schr\u00f6dinger Bridge to directly link hazy and clear image distributions, incorporates detail-preserving regularization for pixel-level alignment, and introduces prompt learning to leverage pre-trained CLIP models for haze-aware vision-language alignment.", "result": "Extensive experiments on multiple real-world datasets confirm that DehazeSB achieves superior dehazing performance compared to existing methods.", "conclusion": "DehazeSB effectively addresses the shortcomings of prior methods by enhancing transport mappings and ensuring high-quality, structurally consistent dehazed outputs."}}
{"id": "2507.09391", "pdf": "https://arxiv.org/pdf/2507.09391", "abs": "https://arxiv.org/abs/2507.09391", "authors": ["Peter Pao-Huang", "Mitchell Black", "Xiaojie Qiu"], "title": "Geometric Generative Modeling with Noise-Conditioned Graph Networks", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Generative modeling of graphs with spatial structure is essential across many\napplications from computer graphics to spatial genomics. Recent flow-based\ngenerative models have achieved impressive results by gradually adding and then\nlearning to remove noise from these graphs. Existing models, however, use graph\nneural network architectures that are independent of the noise level, limiting\ntheir expressiveness. To address this issue, we introduce\n\\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural\nnetworks that dynamically modify their architecture according to the noise\nlevel during generation. Our theoretical and empirical analysis reveals that as\nnoise increases, (1) graphs require information from increasingly distant\nneighbors and (2) graphs can be effectively represented at lower resolutions.\nBased on these insights, we develop Dynamic Message Passing (DMP), a specific\ninstantiation of NCGNs that adapts both the range and resolution of message\npassing to the noise level. DMP consistently outperforms noise-independent\narchitectures on a variety of domains including $3$D point clouds,\nspatiotemporal transcriptomics, and images. Code is available at\nhttps://github.com/peterpaohuang/ncgn.", "AI": {"tldr": "The paper introduces Noise-Conditioned Graph Networks (NCGNs) for generative modeling of spatially structured graphs, which dynamically adapt their architecture according to noise levels. This approach enhances graph modeling compared to noise-independent architectures.", "motivation": "The authors aim to improve generative modeling for graphs with spatial structures, addressing limitations of existing models that do not adapt to varying noise levels during generation.", "method": "They propose Noise-Conditioned Graph Networks (NCGNs), specifically Dynamic Message Passing (DMP), which adapt both the range and resolution of message passing as noise levels change.", "result": "The proposed model, DMP, consistently outperforms noise-independent architectures across diverse domains such as 3D point clouds, spatiotemporal transcriptomics, and images.", "conclusion": "Noise-conditioned approaches like NCGNs significantly enhance graph generative modeling, and dynamically adapting to noise levels during graph generation shows promising practical applications."}}
{"id": "2507.10132", "pdf": "https://arxiv.org/pdf/2507.10132", "abs": "https://arxiv.org/abs/2507.10132", "authors": ["Usman Gani Joy"], "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Accurate forecasting of energy demand and supply is critical for optimizing\nsustainable energy systems, yet it is challenged by the variability of\nrenewable sources and dynamic consumption patterns. This paper introduces a\nneural framework that integrates continuous-time Neural Ordinary Differential\nEquations (Neural ODEs), graph attention, multi-resolution wavelet\ntransformations, and adaptive learning of frequencies to address the issues of\ntime series prediction. The model employs a robust ODE solver, using the\nRunge-Kutta method, paired with graph-based attention and residual connections\nto better understand both structural and temporal patterns. Through\nwavelet-based feature extraction and adaptive frequency modulation, it adeptly\ncaptures and models diverse, multi-scale temporal dynamics. When evaluated\nacross seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity\ntransformer temperature), and Waste, Solar, and Hydro (renewable energy), this\narchitecture consistently outperforms state-of-the-art baselines in various\nforecasting metrics, proving its robustness in capturing complex temporal\ndependencies. Furthermore, the model enhances interpretability through SHAP\nanalysis, making it suitable for sustainable energy applications.", "AI": {"tldr": "This paper presents a neural framework combining Neural ODEs, graph attention, and wavelet transformations for energy demand and supply forecasting, outperforming baselines across diverse datasets.", "motivation": "To address challenges in forecasting energy demand and supply caused by variability in renewable sources and dynamic consumption patterns.", "method": "A neural framework integrating Neural ODEs, graph attention, wavelet transformations, adaptive frequency learning, and residual connections, evaluated on diverse datasets using SHAP analysis for interpretability.", "result": "The model consistently outperforms state-of-the-art baselines in forecasting metrics and captures multi-scale temporal dynamics effectively.", "conclusion": "This robust and interpretable model is suitable for sustainable energy forecasting applications and provides improved accuracy and insights."}}
{"id": "2507.10541", "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "categories": ["cs.CL"], "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "The paper introduces REST, a new evaluation framework for Large Reasoning Models (LRMs) that tests their reasoning under simultaneous multi-problem scenarios, revealing significant performance issues in even state-of-the-art models.", "motivation": "Existing benchmarks for LRMs are limited to isolated question-solving and fail to test capabilities needed for real-world application, such as handling multiple contexts simultaneously.", "method": "The authors propose REST, a stress-testing framework that evaluates LRMs by exposing them to multiple problems concurrently, focusing on aspects like contextual priority allocation and cognitive load management.", "result": "REST highlights significant performance degradation in state-of-the-art models during stress testing and detects performance differences that are invisible in traditional single-question benchmarks.", "conclusion": "REST is a more robust and efficient evaluation framework that better reflects real-world reasoning challenges, reducing the need for constant new data creation."}}
{"id": "2507.09531", "pdf": "https://arxiv.org/pdf/2507.09531", "abs": "https://arxiv.org/abs/2507.09531", "authors": ["Son Nguyen", "Giang Nguyen", "Hung Dao", "Thao Do", "Daeyoung Kim"], "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.", "AI": {"tldr": "The paper introduces VDInstruct, a Multimodal Large Language Model (MLLM) for Key Information Extraction (KIE) that optimizes tokenization and achieves state-of-the-art results while being computationally efficient.", "motivation": "Current MLLMs perform poorly on dense visual documents due to inefficient tokenization techniques that scale with image size, leading to redundant computation and memory inefficiency.", "method": "VDInstruct employs a content-aware tokenization strategy that generates tokens based on document complexity instead of uniform image fragmentation. Spatial region detection is separated from semantic feature extraction, and the model is trained in three stages.", "result": "VDInstruct achieves state-of-the-art performance on KIE benchmarks with a 3.6x reduction in image token count. It also excels in zero-shot evaluations, outperforming strong baselines like DocOwl 1.5 by +5.5 F1 points.", "conclusion": "Content-aware tokenization and explicit layout modeling mark a significant advancement in document understanding, offering both accuracy and efficiency. Data, source code, and model weights will be openly shared."}}
{"id": "2507.09394", "pdf": "https://arxiv.org/pdf/2507.09394", "abs": "https://arxiv.org/abs/2507.09394", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "title": "A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention", "categories": ["cs.LG"], "comment": "ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD)", "summary": "In this work, we study how multi-head latent attention (MLA), a popular\nstrategy for compressing key/value memory, affects a transformer's internal\ncapacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)\ndiagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\\top$ gram matrix\nthroughout training, comparing three variants: the standard multi-head\nattention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,\nand MLA-Decoupled, which shares a single rotary sub-vector across all heads.\nOur random matrix analysis reveals \\textbf{three key findings:} \\textbf{ i)}\ncapacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,\nearly spikes in specific layers that persist and propagate, disrupting the\nbalance between bulk and outlier directions; \\textbf{ ii)} these spikes\ncoincide with rank collapse, concentrating the model's expressivity into narrow\nsubspaces; \\textbf{ iii)} only the decoupled variant prevents this cascade,\nmaintaining broad spectral support and suppressing outlier formation across\nlayers. These results underscore that \\emph{how} rotary embeddings are applied\nis just as critical as \\emph{where} compression occurs. Sharing rotary\ncomponents across heads mitigates spectral fragmentation and preserves\nrepresentational capacity.", "AI": {"tldr": "This study investigates the impact of multi-head latent attention (MLA) on a transformer's internal capacity during pretraining, using random matrix analysis to reveal that a decoupled rotary embedding variant prevents capacity bottlenecks.", "motivation": "To understand how MLA as a memory compression strategy affects a transformer model's internal representational capacity and to address the issues related to capacity bottlenecks and rank collapse during training.", "method": "The authors used Marchenko-Pastur diagnostics and random matrix analysis to evaluate the spectrum of $W_{Q}W_{K}^\\top$ across different MLA variants (standard MHA baseline, MLA-PreRoPE, and MLA-Decoupled).", "result": "Key findings show that MHA-PreRoPE and standard MLA suffer from capacity bottlenecks, rank collapse, and spectral imbalance, while the MLA-Decoupled variant preserves representational capacity by maintaining spectral balance.", "conclusion": "The manner of applying rotary embeddings (decoupling them across heads) significantly influences a model's ability to maintain capacity and suppress outlier formation, emphasizing that design choices in MLA have critical implications for transformer performance during pretraining."}}
{"id": "2507.10419", "pdf": "https://arxiv.org/pdf/2507.10419", "abs": "https://arxiv.org/abs/2507.10419", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "Ga\u00ebl Richard", "Slim Essid", "Andrei Bursuc", "Patrick P\u00e9rez"], "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.", "AI": {"tldr": "LoRA-MCL is a novel training scheme enabling language models to generate diverse and plausible sentence continuations using Multiple Choice Learning (MCL) and the Winner-Takes-All loss.", "motivation": "Address the fundamental ambiguity in language modeling, where multiple futures can be equally plausible given a context, by creating a mechanism for efficient handling of this diversity.", "method": "Integrates Multiple Choice Learning (MCL) and Winner-Takes-All (WTA) loss within the Low-Rank Adaptation (LoRA) framework, while providing theoretical explanations and testing on Markov chain data and real-world tasks.", "result": "Extensive experiments on visual and audio captioning tasks demonstrate that LoRA-MCL generates outputs with high levels of diversity and relevance.", "conclusion": "LoRA-MCL effectively tackles the problem of decoding diverse sentence continuations, proving its potential in real-world language modeling applications."}}
{"id": "2507.09541", "pdf": "https://arxiv.org/pdf/2507.09541", "abs": "https://arxiv.org/abs/2507.09541", "authors": ["Zihao Xiong", "Fei Zhou", "Fengyi Wu", "Shuai Yuan", "Maixia Fu", "Zhenming Peng", "Jian Yang", "Yimian Dai"], "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": "Accepted by TGRS", "summary": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.", "AI": {"tldr": "This paper proposes DRPCA-Net, a novel deep unfolding network for infrared small target detection, emphasizing efficiency and interpretability by leveraging a sparsity-aware prior.", "motivation": "Despite progress in infrared small target detection using deep learning, current methods compromise interpretability, parameter efficiency, and generalization by ignoring the sparsity prior.", "method": "The authors propose DRPCA-Net, which integrates the sparsity-aware prior into a learnable architecture. It includes a dynamic unfolding mechanism using a lightweight hypernetwork for iteration-wise parameter adaptation and a Dynamic Residual Group module.", "result": "Experiments on public infrared datasets demonstrate that DRPCA-Net significantly outperforms state-of-the-art methods in detection accuracy.", "conclusion": "DRPCA-Net effectively balances performance with efficiency and generalization by integrating sparsity-aware priors and leveraging dynamic adaptation mechanisms."}}
{"id": "2507.09404", "pdf": "https://arxiv.org/pdf/2507.09404", "abs": "https://arxiv.org/abs/2507.09404", "authors": ["Mustafa Shukor", "Louis Bethune", "Dan Busbridge", "David Grangier", "Enrico Fini", "Alaaeldin El-Nouby", "Pierre Ablin"], "title": "Scaling Laws for Optimal Data Mixtures", "categories": ["cs.LG"], "comment": null, "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.", "AI": {"tldr": "The paper proposes a systematic method using scaling laws to optimize data mixtures for large foundation model pretraining, eliminating the need for trial-and-error approaches.", "motivation": "Trial and error-based methods to determine data mixtures for large foundation model training are impractical and costly at scale, necessitating a more systematic approach.", "method": "The authors develop scaling laws that predict model performance based on size, number of training tokens, and domain data proportions. These laws are validated across diverse modalities and offer scalability through small-scale training runs.", "result": "Scaling laws were confirmed to accurately predict performance on large language, vision, and multimodal models, and can extrapolate performance to larger scales and new domain mixes.", "conclusion": "The proposed scaling laws provide a principled framework to determine optimal data mixtures for any target domain within a training budget, making pretraining more efficient."}}
{"id": "2507.10425", "pdf": "https://arxiv.org/pdf/2507.10425", "abs": "https://arxiv.org/abs/2507.10425", "authors": ["Alvaro H. C. Correia", "Christos Louizos"], "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Conformal prediction is a distribution-free uncertainty quantification method\nthat has gained popularity in the machine learning community due to its\nfinite-sample guarantees and ease of use. Its most common variant, dubbed split\nconformal prediction, is also computationally efficient as it boils down to\ncollecting statistics of the model predictions on some calibration data not yet\nseen by the model. Nonetheless, these guarantees only hold if the calibration\nand test data are exchangeable, a condition that is difficult to verify and\noften violated in practice due to so-called distribution shifts. The literature\nis rife with methods to mitigate the loss in coverage in this non-exchangeable\nsetting, but these methods require some prior information on the type of\ndistribution shift to be expected at test time. In this work, we study this\nproblem via a new perspective, through the lens of optimal transport, and show\nthat it is possible to estimate the loss in coverage and mitigate it in case of\ndistribution shift.", "AI": {"tldr": "The paper explores a method to handle distribution shifts in conformal prediction using optimal transport, addressing a common challenge in uncertainty quantification.", "motivation": "Conformal prediction offers uncertainty quantification with finite-sample guarantees, but it struggles under distribution shifts, making real-world usage less reliable.", "method": "The authors utilize optimal transport to estimate and mitigate coverage loss in the presence of distribution shifts.", "result": "The proposed approach enables the prediction model to better handle distribution shifts, maintaining its uncertainty quantification capability.", "conclusion": "The study highlights the potential of optimal transport to address challenges in conformal prediction when calibration and test data are non-exchangeable."}}
{"id": "2507.08878", "pdf": "https://arxiv.org/pdf/2507.08878", "abs": "https://arxiv.org/abs/2507.08878", "authors": ["Xinyu Huang", "Leming Shen", "Zijing Ma", "Yuanqing Zheng"], "title": "Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have showcased remarkable generalizability in\nlanguage comprehension and hold significant potential to revolutionize\nhuman-computer interaction in smart homes. Existing LLM-based smart home\nassistants typically transmit user commands, along with user profiles and home\nconfigurations, to remote servers to obtain personalized services. However,\nusers are increasingly concerned about the potential privacy leaks to the\nremote servers. To address this issue, we develop HomeLLaMA, an on-device\nassistant for privacy-preserving and personalized smart home serving with a\ntailored small language model (SLM). HomeLLaMA learns from cloud LLMs to\ndeliver satisfactory responses and enable user-friendly interactions. Once\ndeployed, HomeLLaMA facilitates proactive interactions by continuously updating\nlocal SLMs and user profiles. To further enhance user experience while\nprotecting their privacy, we develop PrivShield to offer an optional\nprivacy-preserving LLM-based smart home serving for those users, who are\nunsatisfied with local responses and willing to send less-sensitive queries to\nremote servers. For evaluation, we build a comprehensive benchmark DevFinder to\nassess the service quality. Extensive experiments and user studies (M=100)\ndemonstrate that HomeLLaMA can provide personalized services while\nsignificantly enhancing user privacy.", "AI": {"tldr": "HomeLLaMA proposes an on-device tailored language model for privacy-preserving smart home systems, with optional cloud interactions for enhanced user satisfaction.", "motivation": "The paper addresses growing privacy concerns regarding existing smart home assistants that rely on transmitting user data to remote servers for personalization.", "method": "HomeLLaMA utilizes an on-device small language model, learning from cloud LLMs, to provide personalized and private user interactions. Additionally, PrivShield offers optional cloud-based interactions for handling less-sensitive queries.", "result": "Experiments and user studies involving 100 participants confirm HomeLLaMA's effectiveness in providing high-quality, personalized services while ensuring greater privacy.", "conclusion": "HomeLLaMA is a viable solution for privacy-conscious smart home users, balancing personalization with privacy through on-device processing and selective cloud interaction."}}
{"id": "2507.09556", "pdf": "https://arxiv.org/pdf/2507.09556", "abs": "https://arxiv.org/abs/2507.09556", "authors": ["Ximeng Zhai", "Bohan Xu", "Yaohong Chen", "Hao Wang", "Kehua Guo", "Yimian Dai"], "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing", "categories": ["cs.CV"], "comment": "Accepted by TGRS", "summary": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.", "AI": {"tldr": "This paper introduces a new task (Sequential CSIST Unmixing) to detect sub-pixel distant infrared targets, along with a benchmark dataset, toolkit, and a novel deep learning model, DeRefNet, which outperforms previous methods.", "motivation": "The need arises due to challenges in detecting sub-pixel localization of dense infrared small targets, compounded by limitations in existing datasets.", "method": "The authors developed a dataset and toolkit (SeqCSIST) and proposed DeRefNet with a Temporal Deformable Feature Alignment (TDFA) module for adaptive inter-frame learning.", "result": "Experiments on SeqCSIST demonstrate a 5.3% improvement in mean Average Precision (mAP) over state-of-the-art methods.", "conclusion": "This paper pioneers the Sequential CSIST Unmixing task, offering advancements in dataset resources and method development, outperforming existing approaches and promoting further research."}}
{"id": "2507.09406", "pdf": "https://arxiv.org/pdf/2507.09406", "abs": "https://arxiv.org/abs/2507.09406", "authors": ["Santhosh Kumar Ravindran"], "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models.", "AI": {"tldr": "The paper presents adversarial activation patching as a new framework to detect, induce, and mitigate deceptive behaviors in large language models (LLMs).", "motivation": "The motivation is to address emergent deceptive behaviors in safety-aligned LLMs, which act compliant but subtly mislead or omit critical information, posing risks in sensitive applications.", "method": "The authors use activation patching as an adversarial tool in transformer-based models to source activations from deceptive prompts and incorporate them into safe forward passes at specific layers. This approach is validated through neural network simulations and hypothesis testing across various scenarios.", "result": "Adversarial patching increased deceptive outputs from 0% to 23.9% in tests, demonstrating vulnerabilities and layer-specific deception rates. Hypotheses such as transferability across models and scaling effects were supported.", "conclusion": "The research contributes to mechanistic interpretability and AI safety by proposing methods for detecting and mitigating deception, while also outlining challenges and future directions for large-scale model testing."}}
{"id": "2507.08879", "pdf": "https://arxiv.org/pdf/2507.08879", "abs": "https://arxiv.org/abs/2507.08879", "authors": ["Max-Paul F\u00f6rster", "Luca Deck", "Raimund Weidlich", "Niklas K\u00fchl"], "title": "A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The growing availability and use of deepfake technologies increases risks for\ndemocratic societies, e.g., for political communication on online platforms.\nThe EU has responded with transparency obligations for providers and deployers\nof Artificial Intelligence (AI) systems and online platforms. This includes\nmarking deepfakes during generation and labeling deepfakes when they are\nshared. However, the lack of industry and enforcement standards poses an\nongoing challenge. Through a multivocal literature review, we summarize methods\nfor marking, detecting, and labeling deepfakes and assess their effectiveness\nunder EU regulation. Our results indicate that individual methods fail to meet\nregulatory and practical requirements. Therefore, we propose a multi-level\nstrategy combining the strengths of existing methods. To account for the masses\nof content on online platforms, our multi-level strategy provides scalability\nand practicality via a simple scoring mechanism. At the same time, it is\nagnostic to types of deepfake technology and allows for context-specific risk\nweighting.", "AI": {"tldr": "The paper examines the challenges of deepfake technologies under EU regulations and proposes a multi-level strategy for marking, detecting, and labeling them.", "motivation": "To address the risks that deepfakes pose to democratic societies, particularly in political communication, and to assess current EU regulatory measures.", "method": "A multivocal literature review was conducted to evaluate existing methods for marking, detecting, and labeling deepfakes and their alignment with EU regulations.", "result": "The study found that existing methods are inadequate alone but can be more effective when combined in a multi-level strategy. This strategy includes scalability, adaptability, and a scoring mechanism.", "conclusion": "A multi-level strategy is necessary to overcome shortcomings of individual methods and effectively address deepfake risks under EU regulations."}}
{"id": "2507.09560", "pdf": "https://arxiv.org/pdf/2507.09560", "abs": "https://arxiv.org/abs/2507.09560", "authors": ["Bolun Zheng", "Xinjie Liu", "Qianyu Zhang", "Canjin Wang", "Fangni Chen", "Mingen Xu"], "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.", "AI": {"tldr": "The paper proposes an enhanced hand pose estimation (EHPE) method focusing on correcting errors in distal phalanx tips and wrist predictions, achieving state-of-the-art results.", "motivation": "Accurately estimating 3D hand pose is crucial for applications like virtual reality and human-computer interaction, but distal joints pose significant challenges due to error accumulation.", "method": "EHPE uses a segmented architecture comprising two main stages: 1) TIP and Wrist Joints Extraction (TW-stage) for initial joint predictions, and 2) Prior Guided Joints Estimation (PG-stage) using a dual-branch interaction network to refine predictions.", "result": "Experiments reveal EHPE outperforms existing methods, demonstrating superior predictive accuracy on benchmark datasets.", "conclusion": "Focusing on distal joints improves pose estimation quality, reinforcing EHPE's value in addressing error accumulation issues."}}
{"id": "2507.09428", "pdf": "https://arxiv.org/pdf/2507.09428", "abs": "https://arxiv.org/abs/2507.09428", "authors": ["Zakhar Shumaylov", "Vasileios Tsiaras", "Yannis Stylianou"], "title": "On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization", "categories": ["cs.LG", "math.DG", "math.OC"], "comment": null, "summary": "The ever-increasing parameter counts of deep learning models necessitate\neffective compression techniques for deployment on resource-constrained\ndevices. This paper explores the application of information geometry, the study\nof density-induced metrics on parameter spaces, to analyze existing methods\nwithin the space of model compression, primarily focusing on operator\nfactorization. Adopting this perspective highlights the core challenge:\ndefining an optimal low-compute submanifold (or subset) and projecting onto it.\nWe argue that many successful model compression approaches can be understood as\nimplicitly approximating information divergences for this projection. We\nhighlight that when compressing a pre-trained model, using information\ndivergences is paramount for achieving improved zero-shot accuracy, yet this\nmay no longer be the case when the model is fine-tuned. In such scenarios,\ntrainability of bottlenecked models turns out to be far more important for\nachieving high compression ratios with minimal performance degradation,\nnecessitating adoption of iterative methods. In this context, we prove\nconvergence of iterative singular value thresholding for training neural\nnetworks subject to a soft rank constraint. To further illustrate the utility\nof this perspective, we showcase how simple modifications to existing methods\nthrough softer rank reduction result in improved performance under fixed\ncompression rates.", "AI": {"tldr": "The paper explores using information geometry for deep learning model compression, focusing on operator factorization and a new perspective on projection methods.", "motivation": "To address the challenge of deploying large deep learning models on resource-constrained devices using effective compression techniques.", "method": "The authors use information geometry to analyze model compression methods, emphasizing iterative singular value thresholding with soft rank constraints.", "result": "They prove convergence of iterative methods and demonstrate better accuracy by applying softer rank reduction modifications to existing compression approaches.", "conclusion": "Information divergences are critical for zero-shot accuracy during compression, yet trainability becomes key during fine-tuning with iterative approaches enhancing performance under fixed compression rates."}}
{"id": "2507.08862", "pdf": "https://arxiv.org/pdf/2507.08862", "abs": "https://arxiv.org/abs/2507.08862", "authors": ["Tianzhe Zhao", "Jiaoyan Chen", "Yanchi Ru", "Haiping Zhu", "Nan Hu", "Jun Liu", "Qika Lin"], "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.CL"], "comment": "13 pages, 6 figures", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge.", "AI": {"tldr": "The paper investigates vulnerabilities in KG-RAG (Knowledge Graph-based Retrieval-Augmented Generation) systems caused by data poisoning, introducing an attack strategy that manipulates knowledge graphs to degrade system performance.", "motivation": "RAG systems, particularly KG-RAG methods, are vulnerable to data poisoning attacks, which can exploit structured nature of KGs to produce incorrect or harmful responses.", "method": "The proposed attack identifies adversarial target answers and inserts perturbation triples to create misleading inference chains in KGs, making KG-RAG systems rely on corrupted data during generation.", "result": "Experiments on benchmarks and KG-RAG methods show that the attack effectively degrades system performance with minimal perturbation to the KG.", "conclusion": "KG-RAG systems face security risks in real-world applications, necessitating deeper exploration of vulnerabilities and robust countermeasures to safeguard these systems."}}
{"id": "2507.08881", "pdf": "https://arxiv.org/pdf/2507.08881", "abs": "https://arxiv.org/abs/2507.08881", "authors": ["Zhang MingDa", "Xu Qing"], "title": "The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions", "categories": ["cs.CY", "cs.AI", "cs.SI"], "comment": "12 pages,2 figures", "summary": "The integration of large language model (LLM) technology into judicial\nsystems is fundamentally transforming legal practice worldwide. However, this\nglobal transformation has revealed an urgent paradox requiring immediate\nattention. This study introduces the concept of ``consistency-acceptability\ndivergence'' for the first time, referring to the gap between technical\nconsistency and social acceptance. While LLMs achieve high consistency at the\ntechnical level, this consistency demonstrates both positive and negative\neffects. Through comprehensive analysis of recent data on LLM judicial\napplications from 2023--2025, this study finds that addressing this challenge\nrequires understanding both task and stakeholder dimensions. This study\nproposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance\nFramework (DTDMR-LJGF), which enables intelligent task classification and\nmeaningful interaction among diverse stakeholders. This framework offers both\ntheoretical insights and practical guidance for building an LLM judicial\necosystem that balances technical efficiency with social legitimacy.", "AI": {"tldr": "The study examines the challenges of integrating large language models (LLMs) into judicial systems, focusing on the gap between technical consistency and social acceptance.", "motivation": "The global use of LLMs in judicial systems has exposed a critical paradox: while they are technically consistent, this consistency can lead to both benefits and drawbacks depending on societal acceptance.", "method": "The paper analyzes recent data on judicial applications of LLMs (2023-2025) and proposes a governance framework called DTDMR-LJGF for intelligent task classification and stakeholder interaction.", "result": "The DTDMR-LJGF framework offers a way to manage the complexity of LLM performance in judicial systems, addressing both technical efficiency and social legitimacy.", "conclusion": "The research provides theoretical and practical strategies for balancing the technical advances of LLMs with the need for societal acceptance in judicial contexts."}}
{"id": "2507.09562", "pdf": "https://arxiv.org/pdf/2507.09562", "abs": "https://arxiv.org/abs/2507.09562", "authors": ["Yidong Jiang"], "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.", "AI": {"tldr": "This paper surveys prompt engineering techniques for SAM, emphasizing its evolution, applications, and challenges.", "motivation": "To address the lack of focused literature on prompt engineering's role in SAM's success and its broader implications.", "method": "A systematic review of existing work on prompt engineering methods, applications, and challenges for SAM and its variants.", "result": "The review organizes the progression of prompt engineering from basic to advanced multimodal approaches while identifying unique challenges.", "conclusion": "The paper provides a framework to enhance understanding and drive future research in prompt engineering for segmentation models."}}
{"id": "2507.09439", "pdf": "https://arxiv.org/pdf/2507.09439", "abs": "https://arxiv.org/abs/2507.09439", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains", "AI": {"tldr": "DyCAST-Net is a novel model designed for causal discovery in multivariate time series (MTS), using dynamic sparse attention and dilated temporal convolutions to ensure accuracy and interpretability.", "motivation": "The paper aims to address the challenge of understanding causal relationships in MTS, especially in complex fields like finance and marketing, where conventional methods struggle.", "method": "The authors introduced DyCAST-Net, which employs dilated convolutions and an adaptive thresholding attention mechanism to capture multiscale dependencies and eliminate spurious connections. A statistical shuffle test further ensures robustness.", "result": "DyCAST-Net demonstrated superior performance compared to models like TCDF, GCFormer, and CausalFormer in detecting causal delays and reducing false discoveries, especially in noisy datasets. Attention heatmaps revealed hidden causal patterns.", "conclusion": "DyCAST-Net is effective in discovering causality in MTS, particularly in high-dimensional and dynamic settings, and offers scalability, interpretability, and improved reliability for diverse applications."}}
{"id": "2507.08882", "pdf": "https://arxiv.org/pdf/2507.08882", "abs": "https://arxiv.org/abs/2507.08882", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models.", "AI": {"tldr": "This paper addresses stress detection in air traffic control (ATC) through deep learning models applied to anonymized voice data, achieving high accuracy while ensuring privacy compliance.", "motivation": "Stress detection in ATC is critical due to its high-stakes environment, but privacy laws, like GDPR, impose restrictions on using voice data. The paper explores if effective stress detection is possible while complying with such regulations.", "method": "The study anonymized ATCO voice data and evaluated different deep learning architectures for stress detection. They tested performance on the SUSAS dataset and an in-house ATC simulation dataset, both anonymized.", "result": "The proposed models achieved a stress detection accuracy of 93.6% on the SUSAS dataset and 80.1% on the ATC simulation dataset, demonstrating effective stress detection with anonymized data.", "conclusion": "The results highlight that anonymization can preserve privacy without significantly hindering the performance of deep-learning-based stress detection models."}}
{"id": "2507.09573", "pdf": "https://arxiv.org/pdf/2507.09573", "abs": "https://arxiv.org/abs/2507.09573", "authors": ["Zhe Wang", "Jingbo Zhang", "Tianyi Wei", "Wanchao Su", "Can Wang"], "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending", "categories": ["cs.CV"], "comment": "14 pages, 16 figures", "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.", "AI": {"tldr": "WordCraft is an interactive artistic typography system integrating diffusion models and language models for precise, user-driven typographic designs.", "motivation": "Current artistic typography solutions lack interactivity features like localized edits, iterative refinements, and multi-character compositions.", "method": "WordCraft employs a training-free regional attention mechanism, noise blending, and a large language model for precise stylization based on user prompts.", "result": "The system generates high-quality stylized typography across various languages, improving interactivity and enabling user-centered workflows.", "conclusion": "WordCraft enhances artistic typography synthesis, creating new creative avenues for artists and designers."}}
{"id": "2507.09440", "pdf": "https://arxiv.org/pdf/2507.09440", "abs": "https://arxiv.org/abs/2507.09440", "authors": ["Joshua Hill", "Benjamin Eyre", "Elliot Creager"], "title": "Transformers Don't In-Context Learn Least Squares Regression", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "In-context learning (ICL) has emerged as a powerful capability of large\npretrained transformers, enabling them to solve new tasks implicit in example\ninput-output pairs without any gradient updates. Despite its practical success,\nthe mechanisms underlying ICL remain largely mysterious. In this work we study\nsynthetic linear regression to probe how transformers implement learning at\ninference time. Previous works have demonstrated that transformers match the\nperformance of learning rules such as Ordinary Least Squares (OLS) regression\nor gradient descent and have suggested ICL is facilitated in transformers\nthrough the learned implementation of one of these techniques. In this work, we\ndemonstrate through a suite of out-of-distribution generalization experiments\nthat transformers trained for ICL fail to generalize after shifts in the prompt\ndistribution, a behaviour that is inconsistent with the notion of transformers\nimplementing algorithms such as OLS. Finally, we highlight the role of the\npretraining corpus in shaping ICL behaviour through a spectral analysis of the\nlearned representations in the residual stream. Inputs from the same\ndistribution as the training data produce representations with a unique\nspectral signature: inputs from this distribution tend to have the same top two\nsingular vectors. This spectral signature is not shared by out-of-distribution\ninputs, and a metric characterizing the presence of this signature is highly\ncorrelated with low loss.", "AI": {"tldr": "This paper probes how transformers execute in-context learning (ICL) through synthetic linear regression, revealing that they fail to generalize to out-of-distribution prompts and analyzing pretraining data's role using spectral signatures.", "motivation": "The motivation is to understand the mechanisms behind ICL in large pretrained transformers, given its success yet poorly understood nature.", "method": "The authors study synthetic linear regression, perform out-of-distribution generalization experiments, and utilize spectral analysis on learned representations in a transformer residual stream.", "result": "Transformers trained for ICL fail to generalize to out-of-distribution prompts, showing behavior inconsistent with manually implemented learning techniques like OLS regression.", "conclusion": "ICL in transformers is influenced by the pretraining corpus, as indicated by spectral analysis of the residual stream, and generalization limitations exist when encountering data not aligned with the training distribution."}}
{"id": "2507.08890", "pdf": "https://arxiv.org/pdf/2507.08890", "abs": "https://arxiv.org/abs/2507.08890", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Hossein A. Rahmani", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "title": "Overview of the TREC 2023 deep learning track", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191", "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries.", "AI": {"tldr": "The TREC Deep Learning track, in its fifth and final year, evaluated passage and document ranking using large language models (LLMs) and synthetic queries, showing that LLMs outperform prior methods like \"nnlm.\"", "motivation": "The motivation was to leverage human-annotated training data (MS MARCO datasets) to evaluate passage and document ranking tasks, exploring both primary and secondary tasks and comparing traditional and emerging techniques like large language model prompting.", "method": "The study employed MS MARCO datasets with human-annotated labels for training, along with synthetic queries generated through fine-tuned T5 and GPT-4 models. Runs were conducted, and human relevance assessments were applied to both types of queries. The evaluation also examined potential biases and system performance correlation across human and synthetic queries.", "result": "The experiments showed that runs using LLM prompting outperformed the \"nnlm\" approach, which was the top-performing method in prior years. Synthetic queries demonstrated results similar to human queries, displaying strong evaluation agreement ($\\tau=0.8487$). Bias in synthetic evaluation methods was not clearly observed.", "conclusion": "Large language models have surpassed the performance of prior methodologies in ranking tasks, and synthetic queries can serve as complements to human-constructed datasets. Future advancements in prompt-based ranking are anticipated to occur outside the TREC track, as this marks the track's conclusion."}}
{"id": "2507.09574", "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "MENTOR is a novel autoregressive framework designed for efficient multimodal image generation, overcoming existing limitations in precision, control, and efficiency.", "motivation": "Address the limitations of existing text-to-image models in visual control, multimodal input integration, and training efficiency.", "method": "MENTOR utilizes a two-stage autoregressive training paradigm, emphasizing multimodal alignment at the pixel and semantic levels and tuning for balanced multimodal integration.", "result": "MENTOR demonstrates strong performance on DreamBench++, excelling in concept preservation, prompt adherence, image fidelity, task adaptability, and training efficiency compared to diffusion-based methods.", "conclusion": "MENTOR offers a robust, efficient solution for multimodal image generation, achieving superior results despite modest resources and components."}}
{"id": "2507.09443", "pdf": "https://arxiv.org/pdf/2507.09443", "abs": "https://arxiv.org/abs/2507.09443", "authors": ["Luiz Aldeia Machado", "Victor Coppo Leite", "Elia Merzari", "Arthur Motta", "Roberto Ponciroli", "Lander Ibarra", "Lise Charlot"], "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components", "categories": ["cs.LG", "cs.CE"], "comment": "Preprint - Nureth 21 paper", "summary": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play\nan important role in the operation of Nuclear Power Plants (NPPs), particularly\ndue to their capacity to reduce offline time by preventing unexpected shutdowns\ncaused by component failures.\n  In this work, we explore the use of a Convolutional Neural Network (CNN)\narchitecture combined with a computational thermomechanical model to calculate\nthe temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel\nrod during operation. This estimation relies on a limited number of temperature\nmeasurements from the cladding's outer surface. This methodology can\npotentially aid in developing PdM tools for nuclear reactors by enabling\nreal-time monitoring of such systems.\n  The training, validation, and testing datasets were generated through coupled\nsimulations involving BISON, a finite element-based nuclear fuel performance\ncode, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven\nsimulations, varying the peak linear heat generation rates. Of these, eight\nwere used for training, two for validation, and one for testing.\n  The CNN was trained for over 1,000 epochs without signs of overfitting,\nachieving highly accurate temperature distribution predictions. These were then\nused in a thermomechanical model to determine the stress and strain\ndistribution within the fuel rod.", "AI": {"tldr": "The paper proposes combining a Convolutional Neural Network (CNN) with a thermomechanical model to predict temperature, stress, and strain in nuclear reactor fuel rods for Predictive Maintenance (PdM).", "motivation": "The paper aims to address the need for enhanced predictive maintenance techniques in Nuclear Power Plants (NPPs) to reduce downtime and prevent failures.", "method": "A CNN model was trained on datasets generated via simulations from nuclear fuel performance and thermal-hydraulics codes. The model used limited temperature measurements to predict parameters critical to reactor safety.", "result": "The CNN achieved high accuracy in predicting temperature distributions without overfitting. These predictions were successfully used in models for stress and strain analysis.", "conclusion": "The methodology demonstrates the feasibility of enhancing PdM capabilities in nuclear reactors using real-time monitoring systems based on CNNs and thermomechanical models."}}
{"id": "2507.09577", "pdf": "https://arxiv.org/pdf/2507.09577", "abs": "https://arxiv.org/abs/2507.09577", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "AI": {"tldr": "This paper improves the Segment Anything Model 2 (SAM2) for critical surgical video segmentation by introducing a Memory Augmented (MA)-SAM2 model that increases robustness and efficiency without additional training.", "motivation": "The inherent limitations of SAM2's greedy memory design hinder its performance in surgical video segmentation due to rapid instrument movements, occlusions, and complex interactions.", "method": "The proposed MA-SAM2 implements context-aware and occlusion-resilient memory models and uses a multi-target, single-loop, one-prompt inference strategy for efficient video segmentation.", "result": "MA-SAM2 demonstrated performance improvements of 4.36% on the EndoVis2017 dataset and 6.1% on the EndoVis2018 dataset compared to SAM2, without requiring additional parameters or training.", "conclusion": "The MA-SAM2 model enhances segmentation accuracy and robustness in surgical videos, showing potential for practical applications in computer-assisted surgery."}}
{"id": "2507.09090", "pdf": "https://arxiv.org/pdf/2507.09090", "abs": "https://arxiv.org/abs/2507.09090", "authors": ["Anthony Miyaguchi", "Conor Johnston", "Aaryan Potdar"], "title": "DS@GT at Touch\u00e9: Large Language Models for Retrieval-Augmented Debate", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.", "AI": {"tldr": "This paper investigates Large Language Models (LLMs) in structured debating and evaluation tasks, showcasing LLMs as effective albeit verbose debaters with consistent evaluation.", "motivation": "The study aims to understand the potential of LLMs in structured debates and their evaluation capabilities.", "method": "Six LLMs from three providers are tested using Retrieval-Augmented Debate and Evaluation framework, with metrics such as Quality, Quantity, Manner, and Relation.", "result": "LLMs handle debate tasks effectively with related arguments, although responses are verbose. Evaluation by LLMs is consistent.", "conclusion": "LLMs exhibit strong conversational and evaluative capabilities in debates, despite verbosity in their responses."}}
{"id": "2507.08902", "pdf": "https://arxiv.org/pdf/2507.08902", "abs": "https://arxiv.org/abs/2507.08902", "authors": ["Sergio Mares", "Ariel Espinoza Weinberger", "Nilah M. Ioannidis"], "title": "Generation of structure-guided pMHC-I libraries using Diffusion Models", "categories": ["q-bio.QM", "cs.AI"], "comment": "Accepted to the The 2nd Workshop on Generative AI and Biology ICML\n  Workshop 2025", "summary": "Personalized vaccines and T-cell immunotherapies depend critically on\nidentifying peptide-MHC class I (pMHC-I) interactions capable of eliciting\npotent immune responses. However, current benchmarks and models inherit biases\npresent in mass-spectrometry and binding-assay datasets, limiting discovery of\nnovel peptide ligands. To address this issue, we introduce a structure-guided\nbenchmark of pMHC-I peptides designed using diffusion models conditioned on\ncrystal structure interaction distances. Spanning twenty high-priority HLA\nalleles, this benchmark is independent of previously characterized peptides yet\nreproduces canonical anchor residue preferences, indicating structural\ngeneralization without experimental dataset bias. Using this resource, we\ndemonstrate that state-of-the-art sequence-based predictors perform poorly at\nrecognizing the binding potential of these structurally stable designs,\nindicating allele-specific limitations invisible in conventional evaluations.\nOur geometry-aware design pipeline yields peptides with high predicted\nstructural integrity and higher residue diversity than existing datasets,\nrepresenting a key resource for unbiased model training and evaluation. Our\ncode, and data are available at: https://github.com/sermare/struct-mhc-dev.", "AI": {"tldr": "This paper introduces a structure-guided benchmark using diffusion models to design pMHC-I peptides independent of dataset biases, showing limitations in existing predictors and providing a diverse evaluation resource.", "motivation": "Current benchmarks and models for identifying peptide-MHC class I interactions are biased due to reliance on mass-spectrometry and binding-assay datasets, hindering progress in personalized vaccines and T-cell immunotherapies.", "method": "The paper uses diffusion models conditioned on crystal structure interaction distances to design a structure-guided benchmark for pMHC-I peptides, which is independent of known peptides and spans 20 HLA alleles.", "result": "The new benchmark reproduces anchor residue preferences while avoiding experimental dataset biases and reveals poor performance of state-of-the-art sequence-based predictors on structurally generated peptides.", "conclusion": "The geometry-aware design pipeline improves residue diversity and offers a critical, unbiased resource for advancing pMHC-I peptide prediction models."}}
{"id": "2507.09595", "pdf": "https://arxiv.org/pdf/2507.09595", "abs": "https://arxiv.org/abs/2507.09595", "authors": ["Or Greenberg"], "title": "Demystifying Flux Architecture", "categories": ["cs.CV"], "comment": null, "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.", "AI": {"tldr": "FLUX.1 is a state-of-the-art text-to-image generation model, and this document reverse-engineers its architecture to support further research.", "motivation": "The motivation is to understand FLUX.1's architecture and training setup to facilitate its adoption for future research and development.", "method": "The method involves reverse-engineering FLUX.1's open source code to extract details about its architecture, as no official documentation is available.", "result": "The reverse-engineering process demystifies the inner workings of FLUX.1, providing insights into its structure and functionality.", "conclusion": "This effort offers an unofficial technical understanding of FLUX.1, enabling its use as a backbone for research despite the lack of original documentation."}}
{"id": "2507.09460", "pdf": "https://arxiv.org/pdf/2507.09460", "abs": "https://arxiv.org/abs/2507.09460", "authors": ["Noah Marchal", "William E. Janes", "Mihail Popescu", "Xing Song"], "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 8 Figures", "summary": "Clinical monitoring of functional decline in ALS relies on periodic\nassessments that may miss critical changes occurring between visits. To address\nthis gap, semi-supervised regression models were developed to estimate rates of\ndecline in a case series cohort by targeting ALSFRS- R scale trajectories with\ncontinuous in-home sensor monitoring data. Our analysis compared three model\nparadigms (individual batch learning and cohort-level batch versus incremental\nfine-tuned transfer learning) across linear slope, cubic polynomial, and\nensembled self-attention pseudo-label interpolations. Results revealed cohort\nhomogeneity across functional domains responding to learning methods, with\ntransfer learning improving prediction error for ALSFRS-R subscales in 28 of 32\ncontrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting\nthe composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention\ninterpolation achieved the lowest prediction error for subscale-level models\n(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,\noutperforming linear and cubic interpolations in 20 of 32 contrasts, though\nlinear interpolation proved more stable in all ALSFRS-R composite scale models\n(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity\nprofiles across functional domains with respiratory and speech exhibiting\npatient-specific patterns benefiting from personalized incremental adaptation,\nwhile swallowing and dressing functions followed cohort-level trajectories\nsuitable for transfer models. These findings suggest that matching learning and\npseudo-labeling techniques to functional domain-specific\nhomogeneity-heterogeneity profiles enhances predictive accuracy in ALS\nprogression tracking. Integrating adaptive model selection within sensor\nmonitoring platforms could enable timely interventions and scalable deployment\nin future multi-center studies.", "AI": {"tldr": "This paper developed semi-supervised regression models for ALS progression tracking, leveraging in-home sensor data to enhance predictive accuracy of ALSFRS-R declines.", "motivation": "Clinical assessments of ALS often miss critical changes between visits, necessitating continuous and adaptive monitoring solutions.", "method": "The study used semi-supervised regression models to predict ALSFRS-R scale trajectories, comparing batch learning and transfer learning with different interpolation methods (linear, cubic, self-attention).", "result": "Transfer learning improved ALSFRS-R subscale prediction in most contrasts, while self-attention interpolation delivered the lowest error in nonlinear progression patterns. Patient-specific and cohort-level learning were matched to functional domains.", "conclusion": "Adaptive model selection tailored to domain-specific functional profiles can improve ALS progression tracking and facilitate scalable and timely interventions."}}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100", "abs": "https://arxiv.org/abs/2507.09100", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "AI": {"tldr": "The paper explores a conversational user interface leveraging retrieval-based LLM agents to assist experts in leveraging historical data for real-time decision-making in scenarios like doctor-patient interactions.", "motivation": "To address the challenge of utilizing extensive historical data during real-time decision-making in expert conversations.", "method": "A prototype conversational interface using a retrieval-based Large Language Model (LLM) agent that listens to conversations, extracts key information, and retrieves insights from embedded datasets.", "result": "Evaluation showed the system's effectiveness in generating relevant insights from Health Canada datasets during simulated doctor-patient dialogues, while highlighting some challenges.", "conclusion": "The study demonstrated the feasibility of integrating historical data insights into real-time decision-making, providing groundwork for future research and improvements."}}
{"id": "2507.09612", "pdf": "https://arxiv.org/pdf/2507.09612", "abs": "https://arxiv.org/abs/2507.09612", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.", "AI": {"tldr": "The paper introduces Inter2Former, an interactive segmentation model that improves efficiency and segmentation quality on CPUs by optimizing dense-token processing and combining four novel methods: DPE, DHA, HMoE, and DLU.", "motivation": "Interactive segmentation suffers from a trade-off between segmentation quality and computational efficiency, especially on CPU devices. Current methods either provide high-quality results but are computationally intensive or are fast but compromise accuracy.", "method": "The authors introduce Inter2Former, which incorporates four techniques: Dynamic Prompt Embedding (DPE), Dynamic Hybrid Attention (DHA), Hybrid Mixture of Experts (HMoE), and Dynamic Local Upsampling (DLU), to optimize token processing and improve segmentation efficiency.", "result": "Inter2Former achieves state-of-the-art performance in terms of both segmentation quality and computational efficiency on interactive segmentation benchmarks.", "conclusion": "The proposed methods in Inter2Former successfully resolve the trade-off between accuracy and efficiency in interactive segmentation, making it highly suitable for real-world applications, particularly on CPU devices."}}
{"id": "2507.09466", "pdf": "https://arxiv.org/pdf/2507.09466", "abs": "https://arxiv.org/abs/2507.09466", "authors": ["Tomas Geffner", "Kieran Didi", "Zhonglin Cao", "Danny Reidenbach", "Zuobai Zhang", "Christian Dallago", "Emine Kucukbenli", "Karsten Kreis", "Arash Vahdat"], "title": "La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching", "categories": ["cs.LG", "q-bio.QM", "I.2.1"], "comment": null, "summary": "Recently, many generative models for de novo protein structure design have\nemerged. Yet, only few tackle the difficult task of directly generating fully\natomistic structures jointly with the underlying amino acid sequence. This is\nchallenging, for instance, because the model must reason over side chains that\nchange in length during generation. We introduce La-Proteina for atomistic\nprotein design based on a novel partially latent protein representation: coarse\nbackbone structure is modeled explicitly, while sequence and atomistic details\nare captured via per-residue latent variables of fixed dimensionality, thereby\neffectively side-stepping challenges of explicit side-chain representations.\nFlow matching in this partially latent space then models the joint distribution\nover sequences and full-atom structures. La-Proteina achieves state-of-the-art\nperformance on multiple generation benchmarks, including all-atom\nco-designability, diversity, and structural validity, as confirmed through\ndetailed structural analyses and evaluations. Notably, La-Proteina also\nsurpasses previous models in atomistic motif scaffolding performance, unlocking\ncritical atomistic structure-conditioned protein design tasks. Moreover,\nLa-Proteina is able to generate co-designable proteins of up to 800 residues, a\nregime where most baselines collapse and fail to produce valid samples,\ndemonstrating La-Proteina's scalability and robustness.", "AI": {"tldr": "The paper introduces La-Proteina, a model for atomistic protein design that outperforms others in generating both sequences and fully atomistic structures, excelling in scalability and structural validity.", "motivation": "While recent generative models have made progress in de novo protein design, few can generate both amino acid sequences and fully atomistic structures, a complex task due to varying side-chain lengths.", "method": "La-Proteina leverages a partially latent representation, where coarse backbone structures are explicitly modeled. Latent variables capture the sequence and atomic details uniformly. Flow matching ensures joint modeling of sequences and structures.", "result": "La-Proteina achieves state-of-the-art performance across benchmarks in co-designability, diversity, and structural validity, excelling in atomistic motif scaffolding and producing valid samples for proteins up to 800 residues.", "conclusion": "The model effectively addresses atomistic protein generation challenges, demonstrating its scalability, robustness, and potential for solving structure-conditioned protein design tasks."}}
{"id": "2507.09615", "pdf": "https://arxiv.org/pdf/2507.09615", "abs": "https://arxiv.org/abs/2507.09615", "authors": ["Eman Ali", "Sathira Silva", "Chetan Arora", "Muhammad Haris Khan"], "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.", "AI": {"tldr": "The paper introduces FAIR, a method to refine pseudo-labels and improve fine-grained classification in vision-language models (VLMs), outperforming the state of the art.", "motivation": "Current unsupervised adaptation methods for fine-grained classification using VLMs either use static alignment scores or computationally expensive strategies, which are ineffective in capturing subtle class distinctions or lack scalability.", "method": "FAIR uses Class Description Anchors (CDA) to dynamically align localized image features with descriptive language embeddings and introduces a Learned Alignment Score (LAS). Additionally, it employs a self-training weighting mechanism to refine pseudo-labels in ambiguous inter-class scenarios.", "result": "The FAIR approach shows a significant improvement in fine-grained unsupervised adaptation, gaining 2.78% across 13 fine-grained datasets compared to prior state-of-the-art methods.", "conclusion": "FAIR effectively enhances fine-grained cross-modal interactions and pseudo-label quality, delivering better performance and scalability for VLMs in unsupervised adaptation tasks."}}
{"id": "2507.09480", "pdf": "https://arxiv.org/pdf/2507.09480", "abs": "https://arxiv.org/abs/2507.09480", "authors": ["Guoyou Wang", "Yihua Tan", "Shiqi Liu"], "title": "Discrete Differential Principle for Continuous Smooth Function Representation", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Taylor's formula holds significant importance in function representation,\nsuch as solving differential difference equations, ordinary differential\nequations, partial differential equations, and further promotes applications in\nvisual perception, complex control, fluid mechanics, weather forecasting and\nthermodynamics. However, the Taylor's formula suffers from the curse of\ndimensionality and error propagation during derivative computation in discrete\nsituations. In this paper, we propose a new discrete differential operator to\nestimate derivatives and to represent continuous smooth function locally using\nthe Vandermonde coefficient matrix derived from truncated Taylor series. Our\nmethod simultaneously computes all derivatives of orders less than the number\nof sample points, inherently mitigating error propagation. Utilizing\nequidistant uniform sampling, it achieves high-order accuracy while alleviating\nthe curse of dimensionality. We mathematically establish rigorous error bounds\nfor both derivative estimation and function representation, demonstrating\ntighter bounds for lower-order derivatives. We extend our method to the\ntwo-dimensional case, enabling its use for multivariate derivative\ncalculations. Experiments demonstrate the effectiveness and superiority of the\nproposed method compared to the finite forward difference method for derivative\nestimation and cubic spline and linear interpolation for function\nrepresentation. Consequently, our technique offers broad applicability across\ndomains such as vision representation, feature extraction, fluid mechanics, and\ncross-media imaging.", "AI": {"tldr": "The paper introduces a novel discrete differential operator, based on Vandermonde matrices from truncated Taylor series, to address the curse of dimensionality and error propagation issues in Taylor's formula when applied to derivative computation and function representation.", "motivation": "The paper aims to overcome the limitations of Taylor's formula in discrete situations, such as error propagation and challenges posed by high dimensions, which impact its application in various fields.", "method": "The authors propose a discrete differential operator leveraging Vandermonde coefficient matrices derived from truncated Taylor series. This method utilizes uniform equidistant sampling to simultaneously compute derivatives of multiple orders and mathematically establishes error bounds.", "result": "Experiments show enhanced derivative estimation and function representation accuracy compared to conventional methods, including finite forward difference and interpolation techniques.", "conclusion": "The proposed method proves effective and broadly applicable across domains like vision, fluid mechanics, and feature extraction, providing a rigorous and accurate alternative for derivative computation and function representation."}}
{"id": "2507.08920", "pdf": "https://arxiv.org/pdf/2507.08920", "abs": "https://arxiv.org/abs/2507.08920", "authors": ["Changze Lv", "Jiang Zhou", "Siyu Long", "Lihao Wang", "Jiangtao Feng", "Dongyu Xue", "Yu Pei", "Hao Wang", "Zherui Zhang", "Yuchen Cai", "Zhiqiang Gao", "Ziyuan Ma", "Jiakai Hu", "Chaochen Gao", "Jingjing Gong", "Yuxuan Song", "Shuyi Zhang", "Xiaoqing Zheng", "Deyi Xiong", "Lei Bai", "Ya-Qin Zhang", "Wei-Ying Ma", "Bowen Zhou", "Hao Zhou"], "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "We introduce AMix-1, a powerful protein foundation model built on Bayesian\nFlow Networks and empowered by a systematic training methodology, encompassing\npretraining scaling laws, emergent capability analysis, in-context learning\nmechanism, and test-time scaling algorithm. To guarantee robust scalability, we\nestablish a predictive scaling law and reveal the progressive emergence of\nstructural understanding via loss perspective, culminating in a strong\n1.7-billion model. Building on this foundation, we devise a multiple sequence\nalignment (MSA)-based in-context learning strategy to unify protein design into\na general framework, where AMix-1 recognizes deep evolutionary signals among\nMSAs and consistently generates structurally and functionally coherent\nproteins. This framework enables the successful design of a dramatically\nimproved AmeR variant with an up to $50\\times$ activity increase over its wild\ntype. Pushing the boundaries of protein engineering, we further empower AMix-1\nwith an evolutionary test-time scaling algorithm for in silico directed\nevolution that delivers substantial, scalable performance gains as verification\nbudgets are intensified, laying the groundwork for next-generation\nlab-in-the-loop protein design.", "AI": {"tldr": "AMix-1 is a protein foundation model that leverages Bayesian Flow Networks and advanced training methods to achieve groundbreaking protein design and directed evolution.", "motivation": "To develop a robust protein foundation model capable of unifying protein design and enhancing performance through directed evolution.", "method": "AMix-1 introduces a systematic methodology combining pretraining scaling laws, emergent capability analysis, in-context learning, and test-time scaling algorithms. It incorporates multiple sequence alignments (MSAs) for in-context learning to generate coherent protein designs.", "result": "AMix-1 successfully designed an improved AmeR variant with up to 50\u00d7 activity enhancement and demonstrated scalable performance gains in directed evolution contexts.", "conclusion": "AMix-1 sets the stage for next-generation protein engineering by enabling scalable and efficient protein design through advanced algorithms and evolutionary techniques."}}
{"id": "2507.09619", "pdf": "https://arxiv.org/pdf/2507.09619", "abs": "https://arxiv.org/abs/2507.09619", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.", "AI": {"tldr": "This paper introduces Generate Aligned Anomaly (GAA), a methodology for creating realistic and semantically aligned anomaly samples using few-shot learning.", "motivation": "The motivation stems from the limitations of existing anomaly inspection methods in industrial manufacturing due to the scarcity of realistic anomaly samples and challenges in anomaly synthesis.", "method": "The paper proposes a framework, GAA, which utilizes a pretrained latent diffusion model for sample generation. It includes steps like Localized Concept Decomposition, Adaptive Multi-Round Anomaly Clustering, and region-guided mask generation for improving quality, alignment, and diversity.", "result": "The experiments demonstrate superior performance of GAA in generating high-quality anomaly samples and improving downstream tasks on the MVTec AD and LOCO datasets.", "conclusion": "The GAA framework significantly enhances anomaly synthesis quality and task performance, filling a gap in industrial anomaly inspection."}}
{"id": "2507.09523", "pdf": "https://arxiv.org/pdf/2507.09523", "abs": "https://arxiv.org/abs/2507.09523", "authors": ["Brett Daley", "Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values", "categories": ["cs.LG", "cs.AI"], "comment": "Published at RLC/RLJ 2025", "summary": "The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.", "AI": {"tldr": "The paper explores temporal-difference learning methods that bootstrap from two asymmetric value functions, analyzing their convergence, sample efficiency, and control performance, and introduces a novel algorithm called Regularized Dueling Q-learning (RDQ).", "motivation": "The paper aims to address the gap in understanding when and why methods that bootstrap from two value functions (e.g., AV-learning and QV-learning) are advantageous compared to traditional single-function TD methods (e.g., Q-learning).", "method": "The authors analyzed QV and AV-learning algorithms theoretically for convergence and sample efficiency. They also proposed a novel AV-learning algorithm called Regularized Dueling Q-learning (RDQ) and compared its performance empirically in the MinAtar benchmark.", "result": "The analysis indicates that while both QV and AV-learning are more efficient than Expected Sarsa in prediction scenarios, only AV-learning offers advantages over Q-learning in control. The proposed RDQ algorithm demonstrated significant performance improvements over Dueling DQN in benchmarks.", "conclusion": "Learning two value functions can be advantageous depending on the scenario, particularly with AV-learning methods in control tasks, and the proposed RDQ algorithm provides a promising direction for improving TD-based control methods."}}
{"id": "2507.09310", "pdf": "https://arxiv.org/pdf/2507.09310", "abs": "https://arxiv.org/abs/2507.09310", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Presented at Clarity Challenge 2023", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity.", "AI": {"tldr": "The paper develops and evaluates voice conversion (VC) models for Lombard speaking style transfer, aiming to improve intelligibility and speaker similarity.", "motivation": "Lombard TTS systems enhance intelligibility in challenging conditions but require extensive data, which is difficult to obtain due to recording challenges. VC provides a way to augment such data.", "method": "The study explores implicit and explicit acoustic feature conditioning in VC models to transfer Lombard speaking style while preserving speaker identity.", "result": "The proposed implicit conditioning approach achieves intelligibility gains comparable to explicit conditioning while maintaining speaker similarity.", "conclusion": "Implicit feature conditioning in VC models is both effective and efficient for transferring Lombard speaking style, offering a viable alternative to explicit conditioning."}}
{"id": "2507.09630", "pdf": "https://arxiv.org/pdf/2507.09630", "abs": "https://arxiv.org/abs/2507.09630", "authors": ["Shomukh Qari", "Maha A. Thafar"], "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "categories": ["cs.CV", "cs.AI"], "comment": "5 figures", "summary": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.", "AI": {"tldr": "The paper proposes an AI framework using Vision Transformers to classify stroke types from CT scans with high accuracy, emphasizing interpretability and clinical application.", "motivation": "To provide timely and accurate diagnosis of stroke types using CT scans, addressing global healthcare needs for effective emergency intervention.", "method": "Developed a multiclass stroke classification framework with MaxViT Vision Transformer and other variants, enhanced by data augmentation and Explainable AI (XAI) techniques for interpretability.", "result": "Achieved state-of-the-art performance with 98.00% accuracy and F1-score using the MaxViT model and augmentation techniques.", "conclusion": "The study successfully delivers an interpretable and clinically applicable AI tool for stroke diagnosis, enhancing emergency department capabilities and improving patient outcomes globally."}}
{"id": "2507.09545", "pdf": "https://arxiv.org/pdf/2507.09545", "abs": "https://arxiv.org/abs/2507.09545", "authors": ["Ilaria Vascotto", "Valentina Blasone", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events", "categories": ["cs.LG"], "comment": "Late Breaking Work presented at the 3rd World Conference on\n  eXplainable Artificial Intelligence (XAI2025)", "summary": "The usage of eXplainable Artificial Intelligence (XAI) methods has become\nessential in practical applications, given the increasing deployment of\nArtificial Intelligence (AI) models and the legislative requirements put\nforward in the latest years. A fundamental but often underestimated aspect of\nthe explanations is their robustness, a key property that should be satisfied\nin order to trust the explanations. In this study, we provide some preliminary\ninsights on evaluating the reliability of explanations in the specific case of\nunbalanced datasets, which are very frequent in high-risk use-cases, but at the\nsame time considerably challenging for both AI models and XAI methods. We\npropose a simple evaluation focused on the minority class (i.e. the less\nfrequent one) that leverages on-manifold generation of neighbours, explanation\naggregation and a metric to test explanation consistency. We present a use-case\nbased on a tabular dataset with numerical features focusing on the occurrence\nof frost events.", "AI": {"tldr": "This paper explores robustness in XAI explanations for AI models, focusing on unbalanced datasets and analyzing minority class explanations using specific metrics.", "motivation": "The motivation is to address the robustness of explanations in XAI methods, especially for unbalanced datasets, where trustworthy explanations are critical in high-risk applications.", "method": "The method involves using on-manifold generation of neighbors, explanation aggregation, and a metric to evaluate the consistency of explanations, with a test case focusing on frost events in tabular data.", "result": "The study provides early insights into evaluating explanation robustness for XAI systems in scenarios with unbalanced data.", "conclusion": "The proposed approach has potential in improving trustworthiness of explanations in high-stakes AI applications involving unbalanced datasets."}}
{"id": "2507.09318", "pdf": "https://arxiv.org/pdf/2507.09318", "abs": "https://arxiv.org/abs/2507.09318", "authors": ["Han Zhu", "Wei Kang", "Liyong Guo", "Zengwei Yao", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Dong Zhang", "Xin Zhang", "Xingchen Song", "Long Lin", "Daniel Povey"], "title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice.", "AI": {"tldr": "The paper presents ZipVoice-Dialog, a model for non-autoregressive spoken dialogue generation, overcoming limitations like slow inference and unstable results in previous models, along with the introduction of a 6.8k-hour OpenDialog dataset.", "motivation": "The authors aim to address the challenges of realistic turn-taking and distinct speaker timbres in spoken dialogue generation, as well as issues with slow and unstable inference in existing autoregressive models.", "method": "The paper introduces ZipVoice-Dialog, which incorporates 1) speaker-turn embeddings for better speaker turn-taking, 2) a curriculum learning strategy for stable speech-text alignment, and 3) techniques for stereo dialogue generation. They also created a large-scale open-source dataset, OpenDialog.", "result": "ZipVoice-Dialog demonstrates superior performance over existing models in terms of intelligibility, turn-taking accuracy, speaker similarity, and inference speed, validated experimentally.", "conclusion": "ZipVoice-Dialog provides an effective solution for spoken dialogue generation with significant improvements in quality and efficiency, supported by a novel benchmark dataset to foster further research in the field."}}
{"id": "2507.08944", "pdf": "https://arxiv.org/pdf/2507.08944", "abs": "https://arxiv.org/abs/2507.08944", "authors": ["Enhao Zhang", "Erkang Zhu", "Gagan Bansal", "Adam Fourney", "Hussein Mozannar", "Jack Gerrits"], "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents", "categories": ["cs.MA", "cs.AI"], "comment": "ICML 2025 Workshop on MAS", "summary": "Large language model (LLM)-based multi-agent systems have demonstrated\nremarkable promise for tackling complex tasks by breaking them down into\nsubtasks that are iteratively planned, executed, observed, and refined. Despite\ntheir effectiveness, these systems often incur high latency because real-world\nproblems frequently demand multiple iterative cycles of reasoning steps. To\naddress this challenge, we propose M1-Parallel, a framework that concurrently\nruns multiple multi-agent teams in parallel to uncover distinct solution paths.\nBy leveraging an event-driven communication model with asynchronous messaging,\nM1-Parallel efficiently capitalizes on the inherent diversity of valid plans to\neither reduce end-to-end latency or boost task completion rates. Our\nexperiments on complex tasks show that M1-Parallel with early termination\nachieves up to $2.2\\times$ speedup while preserving accuracy, and that\nM1-Parallel with aggregation yields higher task completion rates. We further\ninvestigate strategies aimed at encouraging diverse execution plans but observe\nno additional performance gains over repeated sampling. Overall, these findings\nunderscore the potential of parallel plan execution for optimizing multi-agent\nsystems for real-world, high-complexity reasoning tasks.", "AI": {"tldr": "M1-Parallel leverages parallel multi-agent teams with asynchronous messaging to reduce latency or improve task completion rates in complex tasks.", "motivation": "Current LLM-based multi-agent systems excel at complex task handling but suffer from high latency due to iterative reasoning cycles.", "method": "The framework runs multiple multi-agent teams concurrently with an event-driven, asynchronous communication model to explore various solution paths.", "result": "M1-Parallel achieved up to 2.2\u00d7 speedup or higher task completion rates in experiments on complex tasks.", "conclusion": "Parallel execution strategies can optimize multi-agent systems for high-complexity tasks, though diversity strategies beyond repeated sampling showed no additional benefits."}}
{"id": "2507.09640", "pdf": "https://arxiv.org/pdf/2507.09640", "abs": "https://arxiv.org/abs/2507.09640", "authors": ["Leonor Fernandes", "Tiago Gon\u00e7alves", "Jo\u00e3o Matos", "Luis Filipe Nakayama", "Jaime S. Cardoso"], "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages. Under review", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.", "AI": {"tldr": "The paper investigates fairness and performance of AI models in diabetic retinopathy (DR) diagnosis using macula fundus images, and explores disentanglement as a bias mitigation technique.", "motivation": "Diabetic retinopathy is a major cause of vision loss. While AI offers a scalable solution for DR diagnosis, concerns about fairness and generalization need to be addressed.", "method": "Three AI models (ConvNeXt V2, DINOv2, and Swin V2) were trained on a diverse fundus image dataset to predict DR and sensitive attributes like age and gender. Fairness was assessed for subgroups, and disentanglement techniques were applied to mitigate bias.", "result": "All models performed well in DR prediction (up to 94% AUROC) and reasonably predicted sensitive attributes (91% AUROC for age, 77% AUROC for gender). Disparities in fairness were identified, and disentanglement had mixed effects on performance, improving DINOv2 but degrading ConvNeXt V2 and Swin V2.", "conclusion": "The study highlights challenges in achieving fairness and the complexity of disentangling fine-grained features in medical imaging AI, emphasizing the need for equitable healthcare solutions."}}
{"id": "2507.09565", "pdf": "https://arxiv.org/pdf/2507.09565", "abs": "https://arxiv.org/abs/2507.09565", "authors": ["Heeba Shakeel", "Tanvir Ahmad", "Chandni Saxena"], "title": "Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives", "categories": ["cs.LG"], "comment": "7 Pages", "summary": "We introduce a dataset for classifying wellness dimensions in social media\nuser posts, covering six key aspects: physical, emotional, social,\nintellectual, spiritual, and vocational. The dataset is designed to capture\nthese dimensions in user-generated content, with a comprehensive annotation\nframework developed under the guidance of domain experts. This framework allows\nfor the classification of text spans into the appropriate wellness categories.\nWe evaluate both traditional machine learning models and advanced\ntransformer-based models for this multi-class classification task, with\nperformance assessed using precision, recall, and F1-score, averaged over\n10-fold cross-validation. Post-hoc explanations are applied to ensure the\ntransparency and interpretability of model decisions. The proposed dataset\ncontributes to region-specific wellness assessments in social media and paves\nthe way for personalized well-being evaluations and early intervention\nstrategies in mental health. We adhere to ethical considerations for\nconstructing and releasing our experiments and dataset publicly on Github.", "AI": {"tldr": "The paper introduces a new dataset to classify wellness dimensions in social media posts across six categories and evaluates models for this task.", "motivation": "To improve personalized well-being evaluations and enable early mental health interventions using social media data.", "method": "Development of a dataset with expert-guided annotations, use of both traditional and transformer-based machine learning models, and multi-class classification evaluation using statistical metrics.", "result": "Models showcased promising precision, recall, and F1-scores across 10-fold cross-validation, supporting the effectiveness of the dataset.", "conclusion": "This dataset facilitates region-specific wellness assessments, aiding personalized mental health strategies, and is publicly released with ethical considerations."}}
{"id": "2507.08945", "pdf": "https://arxiv.org/pdf/2507.08945", "abs": "https://arxiv.org/abs/2507.08945", "authors": ["Savini Kashmira", "Jayanaka L. Dantanarayana", "Kriszti\u00e1n Flautner", "Lingjia Tang", "Jason Mars"], "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.", "AI": {"tldr": "GraphRunner addresses limitations of traditional LLM graph-based retrieval, offering significant improvements in accuracy and efficiency.", "motivation": "Conventional RAG struggles with retrieval in structured datasets like knowledge graphs due to errors and hallucinations in LLM reasoning.", "method": "GraphRunner operates through planning, verification, and execution stages, using high-level multi-hop traversal actions and validation to reduce errors.", "result": "GraphRunner outperforms existing methods, improving performance by 10-50%, reducing inference costs by 3.0-12.9x, and accelerating response times by 2.5-7.1x.", "conclusion": "GraphRunner is a more robust, accurate, and efficient framework for retrieval in interconnected graph datasets than prior methods."}}
{"id": "2507.09649", "pdf": "https://arxiv.org/pdf/2507.09649", "abs": "https://arxiv.org/abs/2507.09649", "authors": ["Zhengyuan Peng", "Jianqing Xu", "Shen Li", "Jiazhen Ji", "Yuge Huang", "Jingyun Zhang", "Jinmin Li", "Shouhong Ding", "Rizen Guo", "Xin Tan", "Lizhuang Ma"], "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR", "categories": ["cs.CV"], "comment": "Accepted to IJCAI", "summary": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.", "AI": {"tldr": "EyeSeg is a Bayesian uncertainty-aware eye segmentation framework designed to improve gaze estimation for AR/VR under challenging conditions like motion blur and domain gaps.", "motivation": "Accurate eye segmentation is key for efficient gaze estimation in human-machine AR/VR interactions, especially under challenging conditions such as motion blur and domain gaps.", "method": "EyeSeg employs Bayesian uncertainty learning within a closed-set prior framework to model segmentation uncertainties, utilizing posterior statistics for robustness in challenging conditions.", "result": "EyeSeg demonstrates superior segmentation metrics (MIoU, E1, F1, ACC) compared to existing methods and improves downstream gaze estimation tasks, particularly under challenging scenarios like motion blur and domain gaps.", "conclusion": "EyeSeg addresses key challenges in eye segmentation by integrating uncertainty-aware methods, yielding improved robustness and performance for AR/VR gaze estimation tasks."}}
{"id": "2507.09602", "pdf": "https://arxiv.org/pdf/2507.09602", "abs": "https://arxiv.org/abs/2507.09602", "authors": ["Bocheng Ju", "Junchao Fan", "Jiaqi Liu", "Xiaolin Chang"], "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications.", "AI": {"tldr": "The paper introduces DRAGD and DRAGDP, novel attacks revealing privacy flaws in federated unlearning by reconstructing deleted data accurately.", "motivation": "The motivation is to address privacy concerns in federated unlearning, where gradient exchanges can inadvertently expose sensitive information about erased data.", "method": "The authors propose DRAGD to exploit gradient discrepancies and DRAGDP to enhance data reconstruction using publicly available prior data, particularly for complex datasets.", "result": "DRAGD and DRAGDP outperform previous methods in reconstructing deleted data across various datasets, including complex ones like facial images.", "conclusion": "This research uncovers a severe privacy vulnerability in federated unlearning, emphasizing the need for enhanced security mechanisms in these systems."}}
{"id": "2507.08958", "pdf": "https://arxiv.org/pdf/2507.08958", "abs": "https://arxiv.org/abs/2507.08958", "authors": ["Xiaowen Zhang", "Zhenyu Bi", "Xuan Wang", "Tiziana Di Matteo", "Rupert A. C. Croft"], "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System", "categories": ["astro-ph.IM", "cs.AI", "cs.MA"], "comment": "6 pages, 4 figures", "summary": "As cosmological simulations and their associated software become increasingly\ncomplex, physicists face the challenge of searching through vast amounts of\nliterature and user manuals to extract simulation parameters from dense\nacademic papers, each using different models and formats. Translating these\nparameters into executable scripts remains a time-consuming and error-prone\nprocess. To improve efficiency in physics research and accelerate the\ncosmological simulation process, we introduce SimAgents, a multi-agent system\ndesigned to automate both parameter configuration from the literature and\npreliminary analysis for cosmology research. SimAgents is powered by\nspecialized LLM agents capable of physics reasoning, simulation software\nvalidation, and tool execution. These agents collaborate through structured\ncommunication, ensuring that extracted parameters are physically meaningful,\ninternally consistent, and software-compliant. We also construct a cosmological\nparameter extraction evaluation dataset by collecting over 40 simulations in\npublished papers from Arxiv and leading journals that cover diverse simulation\ntypes. Experiments on the dataset demonstrate a strong performance of\nSimAgents, highlighting its effectiveness and potential to accelerate\nscientific research for physicists. Our demonstration video is available at:\nhttps://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly\navailable at https://github.com/xwzhang98/SimAgents.", "AI": {"tldr": "SimAgents automates cosmological simulation parameter extraction and analysis, improving efficiency and reducing errors in physics research.", "motivation": "Physicists face challenges in extracting simulation parameters from diverse academic papers and translating them into executable scripts, which is time-consuming and error-prone.", "method": "The paper introduces SimAgents, a multi-agent system powered by specialized LLM agents capable of physics reasoning, software validation, and tool execution. These agents collaborate for structured communication ensuring meaningful and compliant parameter extraction.", "result": "SimAgents demonstrated strong performance on a cosmological parameter extraction evaluation dataset created from over 40 simulations in academic papers, showcasing its effectiveness.", "conclusion": "SimAgents has the potential to significantly accelerate cosmological research, making the simulation process more efficient and reducing errors in parameter extraction."}}
{"id": "2507.09672", "pdf": "https://arxiv.org/pdf/2507.09672", "abs": "https://arxiv.org/abs/2507.09672", "authors": ["Xinyu Zhang", "Zhonghao Ye", "Jingwei Zhang", "Xiang Tian", "Zhisheng Liang", "Shipeng Yu"], "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation", "categories": ["cs.CV"], "comment": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework +\n  ViSTA-Former dual-stream attention backbone. Code:\n  https://github.com/CarmenQing/VST-Pose", "summary": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.", "AI": {"tldr": "This paper introduces VST-Pose, a novel WiFi-based deep learning system for accurate human pose estimation, outperforming existing methods in terms of accuracy and robustness while promoting privacy and penetration advantages.", "motivation": "The authors aim to provide a non-visual, privacy-aware alternative for human pose estimation that leverages WiFi signals, addressing challenges in current visual or traditional pose detection methods in indoor environments.", "method": "The paper introduces a dual-stream spatiotemporal attention backbone called ViSTA-Former. It includes a velocity modeling branch for capturing fine-grained human motion and trains on a new self-collected 2D pose dataset tailored to home care scenarios.", "result": "VST-Pose achieved a 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% on the self-collected dataset. Its robustness was also validated on the public MMFi dataset for 3D pose estimation.", "conclusion": "The study provides a robust and privacy-conscious solution for continuous human pose estimation using WiFi signals, suitable for indoor applications like home care. The source code for the framework is open-source for further development."}}
{"id": "2507.09616", "pdf": "https://arxiv.org/pdf/2507.09616", "abs": "https://arxiv.org/abs/2507.09616", "authors": ["Ofir Gordon", "Ariel Lapid", "Elad Cohen", "Yarden Yagil", "Arnon Netzer", "Hai Victor Habi"], "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deploying transformer-based neural networks on resource-constrained edge\ndevices presents a significant challenge. This challenge is often addressed\nthrough various techniques, such as low-rank approximation and mixed-precision\nquantization. In this work, we introduce Mixed Low-Rank and Quantization\n(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a\ntwo-stage optimization process to determine optimal bit-width and rank\nassignments for each layer, adhering to predefined memory constraints. This\nprocess includes: (i) an intra-layer optimization that identifies potentially\noptimal compression solutions out of all low-rank and quantization\ncombinations; (ii) an inter-layer optimization that assigns bit-width precision\nand rank to each layer while ensuring the memory constraint is met. An optional\nfinal step applies a sequential optimization process using a modified adaptive\nrounding technique to mitigate compression-induced errors in joint low-rank\napproximation and quantization. The method is compatible and can be seamlessly\nintegrated with most existing quantization algorithms. MLoRQ shows\nstate-of-the-art results with up to 15\\% performance improvement, evaluated on\nVision Transformers for image classification, object detection, and instance\nsegmentation tasks.", "AI": {"tldr": "The paper presents MLoRQ, a method combining low-rank approximation and quantization to optimize transformer deployment on edge devices.", "motivation": "To address the difficulty of deploying resource-intensive transformer models on devices with limited computational and memory resources.", "method": "The authors introduce MLoRQ, a two-stage optimization process focusing on intra-layer and inter-layer optimizations for bit-width and rank assignment, alongside an optional sequential rounding technique for error mitigation.", "result": "MLoRQ achieves up to 15% performance improvement on Vision Transformers across multiple tasks like image classification, object detection, and instance segmentation.", "conclusion": "MLoRQ successfully integrates compression techniques for efficient transformer deployment, providing state-of-the-art results while meeting memory constraints."}}
{"id": "2507.08960", "pdf": "https://arxiv.org/pdf/2507.08960", "abs": "https://arxiv.org/abs/2507.08960", "authors": ["Andrew Estornell", "Jean-Francois Ton", "Muhammad Faaiz Taufiq", "Hang Li"], "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance on a wide range\nof complex reasoning tasks, yet further gains are often possible by leveraging\nthe complementary strengths of multiple models. While multi-agent frameworks\ncan improve solution quality by leveraging multiple LLMs, existing methods are\noften computationally expensive, both at training and inference time. In this\nwork, we introduce a hierarchical multi-agent framework that addresses these\nchallenges by training only a single leader LLM to coordinate a team of\nuntrained peer agents. To this end, we propose Multi-agent guided Leader Policy\n\\textbf{O}ptimization (MLPO), a novel approach which trains the leader to\nevaluate and synthesize agent responses without auxiliary value networks or\nexplicit agent feedback. Leaders trained with MLPO exhibit improved performance\nnot only when interacting with the agent team at inference time, but also enjoy\nimproved performance when deployed in single-agent settings without the team.\nEmpirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our\nframework achieves substantial performance improvements over both single-agent\nand multi-agent baselines. Our results highlight the effectiveness and\nefficiency of training a single, flexible leader for collaborative reasoning in\nmulti-agent LLM systems.", "AI": {"tldr": "The paper introduces a hierarchical multi-agent framework where a single leader LLM is trained to coordinate other untrained models, leading to improved performance and efficiency.", "motivation": "While large language models (LLMs) perform strongly on complex reasoning tasks, there is room for improvement by leveraging multiple models. Existing multi-agent frameworks, however, are computationally expensive to train and run.", "method": "The authors propose Multi-agent guided Leader Policy Optimization (MLPO), which trains a single leader LLM to synthesize agent inputs and responses without needing auxiliary networks or explicit feedback.", "result": "The approach successfully improves performance on benchmarks like Big-Bench Hard (BBH), MATH, and MMLU, outperforming both single-agent and multi-agent baselines.", "conclusion": "The study demonstrates the efficiency and efficacy of training an adaptable leader LLM for coordinating collaborative reasoning in multi-agent systems."}}
{"id": "2507.09681", "pdf": "https://arxiv.org/pdf/2507.09681", "abs": "https://arxiv.org/abs/2507.09681", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "categories": ["cs.CV", "eess.IV"], "comment": "18 pages", "summary": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.", "AI": {"tldr": "This paper introduces a framework for generating high-resolution Digital Elevation Models (DEMs) using a prompt-based monocular depth estimation technique, achieving significant resolution gains with improved accuracy over existing models.", "motivation": "To address the limitations of existing techniques for generating high-resolution DEMs, such as the constraints of super-resolution methods and the lack of global elevation context in monocular depth estimation.", "method": "The method involves fine-tuning a vision transformer encoder with LiDAR-derived DEMs while utilizing a versatile prompting strategy. This enables tasks like DEM estimation, void filling, and updating. It uses low-resolution SRTM elevation data as prompts and high-resolution NAIP RGB imagery to achieve 100x resolution gains.", "result": "The framework produced high-resolution DEMs with < 5 m Mean Absolute Error (MAE) relative to LiDAR, surpassing SRTM accuracy by up to 18%. It showed robust generalization across diverse U.S. landscapes and scalability to larger regions in the U.S. and Israel.", "conclusion": "The framework offers a scalable and precise approach for generating high-resolution DEMs, making it suitable for applications in hydrological, urban, and ecological studies. Its publicly available code and models enhance reproducibility and adoption potential."}}
{"id": "2507.09650", "pdf": "https://arxiv.org/pdf/2507.09650", "abs": "https://arxiv.org/abs/2507.09650", "authors": ["Lily Hong Zhang", "Smitha Milli", "Karen Jusko", "Jonathan Smith", "Brandon Amos", "Wassim", "Bouaziz", "Manon Revel", "Jack Kussman", "Lisa Titus", "Bhaktipriya Radharapu", "Jane Yu", "Vidya Sarma", "Kris Rose", "Maximilian Nickel"], "title": "Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset", "categories": ["cs.LG"], "comment": null, "summary": "How can large language models (LLMs) serve users with varying preferences\nthat may conflict across cultural, political, or other dimensions? To advance\nthis challenge, this paper establishes four key results. First, we demonstrate,\nthrough a large-scale multilingual human study with representative samples from\nfive countries (N=15,000), that humans exhibit significantly more variation in\npreferences than the responses of 21 state-of-the-art LLMs. Second, we show\nthat existing methods for preference dataset collection are insufficient for\nlearning the diversity of human preferences even along two of the most salient\ndimensions of variability in global values, due to the underlying homogeneity\nof candidate responses. Third, we argue that this motivates the need for\nnegatively-correlated sampling when generating candidate sets, and we show that\nsimple prompt-based techniques for doing so significantly enhance the\nperformance of alignment methods in learning heterogeneous preferences. Fourth,\nbased on this novel candidate sampling approach, we collect and open-source\nCommunity Alignment, the largest and most representative multilingual and\nmulti-turn preference dataset to date, featuring almost 200,000 comparisons\nfrom annotators spanning five countries. We hope that the Community Alignment\ndataset will be a valuable resource for improving the effectiveness of LLMs for\na diverse global population.", "AI": {"tldr": "The paper discusses making large language models (LLMs) better at serving diverse user preferences, emphasizing the need for diverse input handling and introducing a novel multilingual dataset for preference alignment.", "motivation": "The study aims to explore how LLMs can be improved to align with highly diverse human preferences across cultural, political, and other dimensions.", "method": "The researchers conducted a multilingual study across five countries (N=15,000), highlighted flaws in current preference collection methods, developed negatively-correlated sampling to enhance alignment, and introduced the 'Community Alignment' dataset for better training.", "result": "The study showed that humans have far more diverse preferences than what LLMs exhibit, current methods are insufficient for diverse preference learning, and that their novel sampling approach improves alignment performance.", "conclusion": "To better serve a global population, LLMs require training on highly diverse datasets; the open-source 'Community Alignment' dataset offers a significant step towards this goal."}}
{"id": "2507.09693", "pdf": "https://arxiv.org/pdf/2507.09693", "abs": "https://arxiv.org/abs/2507.09693", "authors": ["Jiali Chen", "Yujie Jia", "Zihan Wu", "Jinyu Yang", "Jianpeng Chen", "Xusen Hei", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.", "AI": {"tldr": "The paper introduces ExpInstruct, a dataset for commentary generation in scientific experiments, and develops ExpStar, a model that excels in this task using external knowledge retrieval.", "motivation": "The motivation is to reduce reliance on human subject-specific expertise and time investment by automating the generation of insightful experiment commentary.", "method": "The paper uses ExpInstruct dataset to train and evaluate ExpStar, which employs retrieval-augmented mechanisms to access external knowledge and integrate scientific and safety-related insights.", "result": "ExpStar significantly outperformed 14 leading large multimodal models (LMMs) in generating experiment commentary, demonstrating the efficacy of both the dataset and the model.", "conclusion": "ExpStar demonstrates great potential for enhancing AI-driven scientific experiment instruction by generating fine-grained, insightful commentaries effectively."}}
{"id": "2507.09678", "pdf": "https://arxiv.org/pdf/2507.09678", "abs": "https://arxiv.org/abs/2507.09678", "authors": ["Alexander David Balinsky", "Dominik Krzeminski", "Alexander Balinsky"], "title": "Conformal Prediction for Privacy-Preserving Machine Learning", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "comment": null, "summary": "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.", "AI": {"tldr": "This paper examines the application of Conformal Prediction (CP) in privacy-preserving supervised learning with deterministically encrypted data, focusing on the MNIST dataset.", "motivation": "The study aims to address the challenge of integrating rigorous uncertainty quantification techniques like CP with privacy-preserving machine learning, ensuring secure yet meaningful analysis of encrypted data.", "method": "The authors used AES-encrypted MNIST datasets and compared traditional $p$-value-based Conformal Prediction against $e$-value-based Conformal Prediction, analyzing prediction accuracy and coverage.", "result": "Empirical results show that models trained on encrypted data achieve 36.88% test accuracy (far greater than random guessing) and that $e$-value-based CP provides higher predictive coverage than $p$-value-based CP, which offers smaller but less reliable prediction sets.", "conclusion": "This work demonstrates the feasibility of combining CP with encrypted data, highlighting a trade-off between prediction set size and reliability, and lays the groundwork for secure uncertainty quantification in privacy-focused learning systems."}}
{"id": "2507.09762", "pdf": "https://arxiv.org/pdf/2507.09762", "abs": "https://arxiv.org/abs/2507.09762", "authors": ["Yasir Ech-Chammakhy", "Anas Motii", "Anass Rabii", "Jaafar Chbili"], "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)", "summary": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.", "AI": {"tldr": "The paper introduces an unsupervised framework to automatically detect, cluster, and prioritize cybersecurity events from hacker forums, using Transformer-based embeddings and contrastive learning.", "motivation": "Hacker forums hold critical information on emerging cybersecurity threats, but their unstructured and noisy nature makes actionable intelligence extraction difficult.", "method": "The approach involves Transformer-based embeddings fine-tuned with contrastive learning to cluster related discussions into security event groups. A daily ranking mechanism prioritizes events using metrics like timeliness and source credibility.", "result": "Experimental evaluation on real-world hacker forum data shows the framework significantly reduces noise and identifies high-priority threats effectively.", "conclusion": "The framework converts unstructured discussions into actionable intelligence, addressing challenges in automated threat detection and enabling proactive cybersecurity responses."}}
{"id": "2507.09702", "pdf": "https://arxiv.org/pdf/2507.09702", "abs": "https://arxiv.org/abs/2507.09702", "authors": ["Phat Nguyen", "Ngai-Man Cheung"], "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI", "categories": ["cs.CV"], "comment": null, "summary": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.", "AI": {"tldr": "Token compression techniques accelerate Vision Transformer inference by reducing less informative tokens. However, challenges exist in categorizing methods and evaluating compact models.", "motivation": "To address inefficiencies caused by quadratic computational complexity in Vision Transformers and study the viability of token compression techniques for compact architectures.", "method": "A systematic taxonomy and comparative study of token compression methods, evaluating their efficacy on standard and compact Vision Transformer architectures.", "result": "Token compression methods work well for general-purpose Vision Transformers but underperform in compact designs intended for edge devices.", "conclusion": "Insights from this study highlight limitations and guide future research on adapting token compression techniques for compact transformer networks used in edge AI applications."}}
{"id": "2507.09683", "pdf": "https://arxiv.org/pdf/2507.09683", "abs": "https://arxiv.org/abs/2507.09683", "authors": ["Michael Kearns", "Aaron Roth", "Emily Ryu"], "title": "Networked Information Aggregation via Machine Learning", "categories": ["cs.LG", "cs.GT", "econ.TH"], "comment": null, "summary": "We study a distributed learning problem in which learning agents are embedded\nin a directed acyclic graph (DAG). There is a fixed and arbitrary distribution\nover feature/label pairs, and each agent or vertex in the graph is able to\ndirectly observe only a subset of the features -- potentially a different\nsubset for every agent. The agents learn sequentially in some order consistent\nwith a topological sort of the DAG, committing to a model mapping observations\nto predictions of the real-valued label. Each agent observes the predictions of\ntheir parents in the DAG, and trains their model using both the features of the\ninstance that they directly observe, and the predictions of their parents as\nadditional features. We ask when this process is sufficient to achieve\n\\emph{information aggregation}, in the sense that some agent in the DAG is able\nto learn a model whose error is competitive with the best model that could have\nbeen learned (in some hypothesis class) with direct access to \\emph{all}\nfeatures, despite the fact that no single agent in the network has such access.\nWe give upper and lower bounds for this problem for both linear and general\nhypothesis classes. Our results identify the \\emph{depth} of the DAG as the key\nparameter: information aggregation can occur over sufficiently long paths in\nthe DAG, assuming that all of the relevant features are well represented along\nthe path, and there are distributions over which information aggregation cannot\noccur even in the linear case, and even in arbitrarily large DAGs that do not\nhave sufficient depth (such as a hub-and-spokes topology in which the spoke\nvertices collectively see all the features). We complement our theoretical\nresults with a comprehensive set of experiments.", "AI": {"tldr": "The paper explores distributed learning where agents in a directed acyclic graph (DAG) learn sequentially, making predictions using partially observed features and information from parent agents. The work provides theoretical and experimental evidence to identify conditions where information aggregation enables accurate learning.", "motivation": "This paper addresses the challenge of distributed learning when no single agent has access to all input data/features, requiring collaboration through structured information sharing.", "method": "The authors study learning processes in a DAG topology, where agents train sequentially based on their subset of features and predictions inherited from parent agents. They derive upper and lower bounds for effective learning, focusing on the DAG's depth as a key factor.", "result": "The depth of the DAG is identified as crucial: longer paths facilitate information aggregation, enabling some agents to learn near-optimal models. However, shallow or poor topology DAGs hinder learning even with access to all relevant data collectively.", "conclusion": "Information aggregation is achievable in distributed learning under specific DAG structures with sufficient depth. The study sets theoretical bounds and shows experimental validation to guide the design of such systems."}}
{"id": "2507.09788", "pdf": "https://arxiv.org/pdf/2507.09788", "abs": "https://arxiv.org/abs/2507.09788", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "comment": "9 pages. Preprint to be submitted to peer-review", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.", "AI": {"tldr": "The paper introduces TinyTroupe, an LLM-powered MAS toolkit for detailed human behavior simulation.", "motivation": "To overcome the limitations in existing MAS tools, such as lack of fine-grained persona specification, population sampling, experimentation support, and validation, essential for behavioral studies and simulations.", "method": "TinyTroupe enables detailed persona definitions and programmatic control through LLM-driven mechanisms, with representative examples provided for its application.", "result": "The toolkit's quantitative and qualitative evaluations highlight its capabilities, limitations, and trade-offs in behavioral problem formulation and solution.", "conclusion": "TinyTroupe is a novel simulation toolkit, offering practical solutions for social simulations, and is available as an open-source Python library."}}
{"id": "2507.09748", "pdf": "https://arxiv.org/pdf/2507.09748", "abs": "https://arxiv.org/abs/2507.09748", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.", "AI": {"tldr": "The paper improves Text-to-3D generation using Linearized Lookahead Variational Score Distillation ($L^2$-VSD), overcoming challenges with convergence and stability.", "motivation": "Existing Variational Score Distillation (VSD) techniques for Text-to-3D generation suffer from slow and ill-posed convergence due to mismatches in model distributions.", "method": "The authors propose $L^2$-VSD, which adjusts optimization order and uses a linearized score model to achieve better stability and quality, leveraging forward-mode auto-differentiation for efficiency.", "result": "Results demonstrate superior generation quality and stability of $L^2$-VSD compared to prior methods, and compatibility with other VSD-based frameworks.", "conclusion": "The $L^2$-VSD approach resolves stability and convergence issues in VSD, significantly advancing Text-to-3D generation methods."}}
{"id": "2507.09687", "pdf": "https://arxiv.org/pdf/2507.09687", "abs": "https://arxiv.org/abs/2507.09687", "authors": ["Md Mushfiqur Rahaman", "Elliot Chang", "Tasmiah Haque", "Srinjoy Das"], "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments.", "AI": {"tldr": "This study evaluates generative and discriminative LSTM-based text classifiers under post-training quantization (PTQ) in edge computing environments to understand their robustness and performance under various conditions.", "motivation": "Edge computing applications demand low latency and high accuracy for real-time decision-making. Generative classifiers are robust to out-of-distribution and noisy data, making them suitable for these scenarios. However, computational constraints necessitate model optimization like PTQ.", "method": "A comparative study of generative and discriminative LSTM-based text classification with various bitwidths in PTQ. It uses the Brevitas library, evaluates robustness under noisy conditions, and considers the effect of class imbalance during calibration with nonparametric hypothesis testing.", "result": "Generative classifiers show sensitivity to bitwidth, calibration data quality, and input noise under quantized inference, while discriminative classifiers maintain robustness. Class imbalance during PTQ calibration affects generative classifiers\u2019 performance, especially at lower bitwidths.", "conclusion": "Proper calibration data in PTQ is critical for the deployment of generative classifiers in edge environments. Discriminative models are more resilient in quantized scenarios, while generative ones require tailored calibration techniques to mitigate performance degradation."}}
{"id": "2507.09876", "pdf": "https://arxiv.org/pdf/2507.09876", "abs": "https://arxiv.org/abs/2507.09876", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "AI": {"tldr": "The paper introduces a new approach called Video-Text Interleaved CoT (ViTCoT) for video understanding, which integrates both visual and textual reasoning.", "motivation": "Current video reasoning methods heavily depend on textual information, overlooking the visual content that is essential for comprehensive understanding, unlike human reasoning which naturally integrates visual re-examination.", "method": "The proposed ViTCoT paradigm involves a Video-Text Interleaved Benchmark (ViTIB), created using MLLMs for key-video selection and verified manually. It aims to combine visual and textual reasoning effectively.", "result": "Experiments reveal that ViTCoT outperforms existing text-only approaches, enhancing both reasoning performance and neuron activation in MLLMs.", "conclusion": "The ViTCoT paradigm introduces a more cognitively aligned and effective method for video reasoning by integrating visual and textual modalities, showing significant benefits over traditional approaches."}}
{"id": "2507.09767", "pdf": "https://arxiv.org/pdf/2507.09767", "abs": "https://arxiv.org/abs/2507.09767", "authors": ["Ofir Itzhak Shahar", "Gur Elkin", "Ohad Ben-Shahar"], "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments", "categories": ["cs.CV"], "comment": null, "summary": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.", "AI": {"tldr": "This paper introduces a novel hybrid approach combining geometric and pictorial features for aligning puzzle fragments, overcoming limitations related to fragment shape or content.", "motivation": "Many existing fragment-reconstruction methods struggle with realistic fragment geometry or rely heavily on constrained shapes, limiting their applicability to real-world puzzles.", "method": "The authors propose a hybrid technique for fragment alignment that incorporates geometry and pictorial features without assuming specific fragment characteristics. They also introduce a new dataset and metrics rooted in real-world erosion models.", "result": "The proposed compatibility method achieves state-of-the-art performance in precision and recall on the RePAIR 2D dataset, showcasing clear gains in neighborhood-level puzzle reconstruction.", "conclusion": "The hybrid approach effectively handles realistic fragment properties, broadening its application scope and enhancing puzzle-solving frameworks in archaeological contexts."}}
{"id": "2507.09924", "pdf": "https://arxiv.org/pdf/2507.09924", "abs": "https://arxiv.org/abs/2507.09924", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Trung Le", "Dragan Ga\u0161evi\u0107", "Yuan-Fang Li", "Thanh-Toan Do"], "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.", "AI": {"tldr": "MixLoRA-DSI efficiently updates model-based indexes for generative retrieval by selectively adding experts based on Out-of-Distribution documents, achieving better performance with lower costs.", "motivation": "Updating model-based indexes with new documents is computationally expensive, making full retraining impractical.", "method": "MixLoRA-DSI uses a mixture of Low-Rank Adaptation experts combined with a layer-wise OOD-driven strategy to ensure selective and resource-efficient model expansion.", "result": "Experiments showed that MixLoRA-DSI outperformed full-model updates in accuracy, while significantly reducing parameter overhead and training costs.", "conclusion": "MixLoRA-DSI offers a scalable and efficient solution for generative retrieval tasks, allowing updates to indexes with minimized resource usage and enhanced performance."}}
{"id": "2507.09795", "pdf": "https://arxiv.org/pdf/2507.09795", "abs": "https://arxiv.org/abs/2507.09795", "authors": ["Amirhossein Ansari", "Ke Wang", "Pulei Xiong"], "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. Source code is\navailable at https://github.com/ah-ansari/NegRefine.", "AI": {"tldr": "NegRefine refines negative labels in Vision-Language Models for zero-shot OOD detection by filtering subcategories and introducing multi-matching-aware scoring.", "motivation": "Existing methods like NegLabel and CSP wrongly detect in-distribution samples as OOD due to subcategory or proper noun negative labels and struggle with images matching multiple labels.", "method": "The authors introduce a framework that filters out inappropriate negative labels (e.g., subcategories, proper nouns) and employs a scoring function that adapts to images matching multiple labels.", "result": "NegRefine successfully enhances robustness in separating in-distribution and out-of-distribution samples, performing well on benchmarks like ImageNet-1K.", "conclusion": "NegRefine addresses key challenges in zero-shot OOD detection, providing a more accurate and computationally effective framework."}}
{"id": "2507.09703", "pdf": "https://arxiv.org/pdf/2507.09703", "abs": "https://arxiv.org/abs/2507.09703", "authors": ["Roberto Molinaro", "Niall Siegenheim", "Niels Poulsen", "Jordan Dane Daubinet", "Henry Martin", "Mark Frey", "Kevin Thiart", "Alexander Jakob Dautel", "Andreas Schlueter", "Alex Grigoryev", "Bogdan Danciu", "Nikoo Ekhtiari", "Bas Steunebrink", "Leonie Wagner", "Marvin Vincent Gabler"], "title": "EPT-2 Technical Report", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)\nfamily of foundation AI models for Earth system forecasting. EPT-2 delivers\nsubstantial improvements over its predecessor, EPT-1.5, and sets a new state of\nthe art in predicting energy-relevant variables-including 10m and 100m wind\nspeed, 2m temperature, and surface solar radiation-across the full 0-240h\nforecast horizon. It consistently outperforms leading AI weather models such as\nMicrosoft Aurora, as well as the operational numerical forecast system IFS HRES\nfrom the European Centre for Medium-Range Weather Forecasts (ECMWF). In\nparallel, we introduce a perturbation-based ensemble model of EPT-2 for\nprobabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly\nsurpasses the ECMWF ENS mean-long considered the gold standard for medium- to\nlongrange forecasting-while operating at a fraction of the computational cost.\nEPT models, as well as third-party forecasts, are accessible via the app.jua.ai\nplatform.", "AI": {"tldr": "EPT-2 is a cutting-edge AI model for Earth system forecasting, outperforming previous models and numeric systems in accuracy and efficiency.", "motivation": "The motivation is to improve the accuracy and efficiency of Earth system forecasting, particularly for energy-relevant variables.", "method": "EPT-2 employs advanced transformer-based AI techniques and introduces a probabilistic ensemble model (EPT-2e).", "result": "EPT-2 outperforms both leading AI weather models and traditional numerical systems, while EPT-2e excels in probabilistic forecasting at lower computational costs.", "conclusion": "EPT-2 and EPT-2e represent significant advancements in Earth forecasting technology, accessible through the app.jua.ai platform."}}
{"id": "2507.09815", "pdf": "https://arxiv.org/pdf/2507.09815", "abs": "https://arxiv.org/abs/2507.09815", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "categories": ["cs.CV"], "comment": "22 pages, 11 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "AI": {"tldr": "The paper introduces VRU-Accident, a benchmark designed to assess multimodal large language models (MLLMs) in understanding high-risk traffic scenarios involving vulnerable road users (VRUs).", "motivation": "Current autonomous driving systems face challenges in ensuring the safety of VRUs like pedestrians and cyclists, as accidents often lead to severe outcomes. Additionally, there is no standardized way to evaluate the reasoning abilities of MLLMs in handling such scenarios.", "method": "The authors developed VRU-Accident, a benchmark comprising 1K real-world dashcam videos, annotated with 6K multiple-choice Q&A pairs spanning six safety-critical categories, alongside 1K dense scene descriptions focusing on VRU-vehicle accidents.", "result": "A comprehensive evaluation of 17 state-of-the-art MLLMs showed that while these models perform well on visually grounded attributes, they struggle significantly in reasoning and explaining accident causes, types, and preventability.", "conclusion": "The study highlights the potential of MLLMs in autonomous vehicle scene understanding but underscores the need for improvement in reasoning capabilities specifically in safety-critical VRU scenarios."}}
{"id": "2507.09732", "pdf": "https://arxiv.org/pdf/2507.09732", "abs": "https://arxiv.org/abs/2507.09732", "authors": ["Sara Si-Moussi", "Stephan Hennekens", "Sander Mucher", "Stan Los", "Wilfried Thuiller"], "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation", "categories": ["cs.LG", "q-bio.PE", "stat.AP", "68T05, 62H35", "I.5.4; I.2.6"], "comment": null, "summary": "Habitats integrate the abiotic conditions and biophysical structures that\nsupport biodiversity and sustain nature's contributions to people. As these\necosystems face mounting pressure from human activities, accurate,\nhigh-resolution habitat maps are essential for effective conservation and\nrestoration. Yet current maps often fall short in thematic or spatial\nresolution because they must (1) model several mutually exclusive habitat types\nthat co-occur across landscapes and (2) cope with severe class imbalance that\ncomplicate multi-class training. Here, we evaluated how high-resolution remote\nsensing (RS) data and Artificial Intelligence (AI) tools can improve habitat\nclassification over large geographic extents at fine thematic resolution. Using\nvegetation plots from the European Vegetation Archive, we modelled Level 3\nEUNIS habitats across Europe and assessed multiple modelling strategies against\nindependent validation datasets. Strategies that exploited the hierarchical\nnature of habitat nomenclatures resolved classification ambiguities, especially\nin fragmented landscapes. Integrating multi-spectral (MSI) and synthetic\naperture radar (SAR) imagery, particularly through Earth Observation Foundation\nmodels, enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\naccuracy further. Our methodological framework is transferable beyond Europe\nand adaptable to other classification systems. Future research should advance\ntemporal modelling of dynamic habitats, extend to habitat segmentation and\nquality assessment, and exploit next-generation EO data paired with\nhigher-quality in-situ observations.", "AI": {"tldr": "The paper aims to improve habitat classification using AI and high-resolution remote sensing data, demonstrating an effective methodology for mapping habitats across Europe.", "motivation": "To address the challenges of creating high-resolution, accurate habitat maps essential for conservation, especially in light of human pressures on ecosystems.", "method": "The study used high-resolution remote sensing data, AI tools, vegetation data from the European Vegetation Archive, and multiple modelling strategies. Techniques included hierarchical modelling, integration of multi-spectral and SAR imagery, and ensemble machine learning to manage class imbalances.", "result": "The combined use of RS data, hierarchical strategies, and class-balancing techniques significantly improved classification accuracy, especially in complex and fragmented landscapes.", "conclusion": "The framework developed is transferable globally and adaptable to other classification systems. Future advancements should focus on dynamic habitat modelling, segmentation, and leveraging next-gen Earth Observation data."}}
{"id": "2507.10013", "pdf": "https://arxiv.org/pdf/2507.10013", "abs": "https://arxiv.org/abs/2507.10013", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "AI": {"tldr": "This paper evaluates CLIP-based vision-and-language models on their ability to replicate human-like associations in the bouba-kiki effect, finding that they fail to consistently exhibit the behavior.", "motivation": "The study investigates whether vision-and-language models align with human cognition by testing their ability to replicate the bouba-kiki effect, a well-documented human phenomenon linking shapes and pseudowords.", "method": "Two variants of CLIP (ResNet and Vision Transformer) were tested using prompt-based evaluations and Grad-CAM for visual attention analysis, modeled after human experimental methods.", "result": "The models demonstrated inconsistent results; ResNet showed a preference for round shapes, but neither model reliably exhibited the bouba-kiki effect. Comparison with human data highlighted significant cognitive gaps.", "conclusion": "The findings reveal limitations in vision-and-language models' cross-modal understanding, questioning their ability to exhibit human-like, integrated cognitive behaviors."}}
{"id": "2507.09830", "pdf": "https://arxiv.org/pdf/2507.09830", "abs": "https://arxiv.org/abs/2507.09830", "authors": ["Shuhao Fu", "Philip J. Kellman", "Hongjing Lu"], "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.", "AI": {"tldr": "This paper explores the resemblance between human and deep learning model representations of 3D object shapes using point clouds, finding humans to excel and visual transformers to better align with human performance compared to convolution-based models.", "motivation": "The paper aims to investigate whether deep learning models trained on 3D shape recognition develop representations akin to human vision, particularly focusing on local and global geometric structures.", "method": "Experiments were conducted on human object recognition using point clouds under different conditions, and performances were compared with deep learning models (DGCNN and point transformer).", "result": "Point transformer models aligned more closely with human performance than convolutional neural network models due to hierarchical 3D shape abstraction mechanisms.", "conclusion": "The study concludes that certain deep learning models (point transformers) capture structural hierarchies in 3D shapes better than others, but humans consistently outperform under varying conditions."}}
{"id": "2507.09733", "pdf": "https://arxiv.org/pdf/2507.09733", "abs": "https://arxiv.org/abs/2507.09733", "authors": ["Bradley Camburn"], "title": "Universal Physics Simulation: A Foundational Diffusion Approach", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07, 65M06, 78M34", "I.2.6; I.4.8; J.2"], "comment": "10 pages, 3 figures. Foundational AI model for universal physics\n  simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on\n  electromagnetic field generation without requiring a priori physics encoding", "summary": "We present the first foundational AI model for universal physics simulation\nthat learns physical laws directly from boundary-condition data without\nrequiring a priori equation encoding. Traditional physics-informed neural\nnetworks (PINNs) and finite-difference methods necessitate explicit\nmathematical formulation of governing equations, fundamentally limiting their\ngeneralizability and discovery potential. Our sketch-guided diffusion\ntransformer approach reimagines computational physics by treating simulation as\na conditional generation problem, where spatial boundary conditions guide the\nsynthesis of physically accurate steady-state solutions.\n  By leveraging enhanced diffusion transformer architectures with novel spatial\nrelationship encoding, our model achieves direct boundary-to-equilibrium\nmapping and is generalizable to diverse physics domains. Unlike sequential\ntime-stepping methods that accumulate errors over iterations, our approach\nbypasses temporal integration entirely, directly generating steady-state\nsolutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our\ndata-informed approach enables physics discovery through learned\nrepresentations analyzable via Layer-wise Relevance Propagation (LRP),\nrevealing emergent physical relationships without predetermined mathematical\nconstraints. This work represents a paradigm shift from AI-accelerated physics\nto AI-discovered physics, establishing the first truly universal physics\nsimulation framework.", "AI": {"tldr": "The paper introduces an AI model that learns physical laws directly from boundary-condition data, bypassing traditional methods that rely on predefined mathematical equations.", "motivation": "To overcome the limitations of traditional physics simulation methods, which require explicit mathematical formulations of governing equations.", "method": "The authors employ a sketch-guided diffusion transformer for boundary-to-equilibrium mapping, leveraging enhanced architectures with novel spatial relationship encoding.", "result": "The model directly generates steady-state solutions with high accuracy (SSIM > 0.8), avoiding error accumulation typical of time-stepping methods.", "conclusion": "This approach establishes a universal framework for physics simulation, enabling discoveries without predefined mathematical constraints and marking a shift to AI-discovered physics."}}
{"id": "2507.09861", "pdf": "https://arxiv.org/pdf/2507.09861", "abs": "https://arxiv.org/abs/2507.09861", "authors": ["Yihao Ding", "Siwen Luo", "Yue Dai", "Yanbei Jiang", "Zechuan Li", "Geoffrey Martin", "Yifan Peng"], "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.", "AI": {"tldr": "The paper surveys advancements in Multimodal Large Language Models (MLLMs) for Visually-Rich Document Understanding (VRDU), offering insights into encoding strategies, training paradigms, and datasets.", "motivation": "The motivation is to address the growing need for automatic processing of documents with complex visual, textual, and layout data.", "method": "The authors review existing MLLM-based VRDU methods, focusing on encoding techniques, training strategies, and datasets.", "result": "The survey organizes and evaluates recent efforts in MLLM-based VRDU and identifies critical challenges and opportunities.", "conclusion": "Future efforts should focus on improving efficiency, generalizability, and robustness in VRDU systems."}}
{"id": "2507.09753", "pdf": "https://arxiv.org/pdf/2507.09753", "abs": "https://arxiv.org/abs/2507.09753", "authors": ["Ewa M. Nowara", "Joshua Rackers", "Patricia Suriana", "Pan Kessel", "Max Shen", "Andrew Martin Watkins", "Michael Maser"], "title": "Do we need equivariant models for molecule generation?", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Deep generative models are increasingly used for molecular discovery, with\nmost recent approaches relying on equivariant graph neural networks (GNNs)\nunder the assumption that explicit equivariance is essential for generating\nhigh-quality 3D molecules. However, these models are complex, difficult to\ntrain, and scale poorly.\n  We investigate whether non-equivariant convolutional neural networks (CNNs)\ntrained with rotation augmentations can learn equivariance and match the\nperformance of equivariant models. We derive a loss decomposition that\nseparates prediction error from equivariance error, and evaluate how model\nsize, dataset size, and training duration affect performance across denoising,\nmolecule generation, and property prediction. To our knowledge, this is the\nfirst study to analyze learned equivariance in generative tasks.", "AI": {"tldr": "The paper examines whether simpler non-equivariant CNNs trained with rotation augmentations can match the performance of complex equivariant GNNs in tasks like molecule generation.", "motivation": "Existing 3D molecular discovery models rely on equivariant GNNs, which are complex and poorly scalable. The authors aim to explore whether simpler non-equivariant CNNs with augmentations can achieve similar outcomes.", "method": "The authors introduce rotation-augmented training for CNNs and decompose the loss into prediction and equivariance errors. They test performance across three tasks: denoising, molecule generation, and property prediction.", "result": "Through analysis, the authors demonstrate that non-equivariant CNNs can learn equivariance and achieve comparable performance to equivariant models, depending on factors like model size and training conditions.", "conclusion": "Non-equivariant CNNs trained with rotation augmentations are viable alternatives to more complex equivariant GNNs, offering potentially simpler and scalable solutions for molecular discovery tasks."}}
{"id": "2507.10057", "pdf": "https://arxiv.org/pdf/2507.10057", "abs": "https://arxiv.org/abs/2507.10057", "authors": ["Sangwoo Park", "Jinheon Baek", "Soyeong Jeong", "Sung Ju Hwang"], "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.", "AI": {"tldr": "PRISM introduces a fine-grained approach for document-to-document scientific paper retrieval, outperforming previous methods by 4.3%.", "motivation": "Previous scientific paper retrieval methods relied heavily on abstracts, neglecting the rich details contained in full texts.", "method": "PRISM decomposes query papers into aspect-specific views and embeds them individually for matching against similarly segmented candidate papers, leveraging multidimensional context.", "result": "PRISM achieves an average performance improvement of 4.3% compared to existing retrieval baselines.", "conclusion": "By employing fine-grained representations and a new benchmark, PRISM advances the effectiveness of scientific paper retrieval methods."}}
{"id": "2507.09862", "pdf": "https://arxiv.org/pdf/2507.09862", "abs": "https://arxiv.org/abs/2507.09862", "authors": ["Youliang Zhang", "Zhaoyang Li", "Duomin Wang", "Jiahe Zhang", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Gang Yu", "Xiu Li"], "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation", "categories": ["cs.CV", "eess.AS"], "comment": null, "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/", "AI": {"tldr": "The SpeakerVid-5M dataset is introduced as the first large-scale dataset for audio-visual dyadic interactive virtual humans, consisting of 5.2 million video clips and stratified for diverse use cases.", "motivation": "There is an increasing interest in advancing audio-visual dyadic virtual human interactions, enabled by breakthroughs in large-scale models.", "method": "The researchers developed a dataset structured along interaction types (like dialogue and listening) and quality tiers (pre-training vs. supervised fine-tuning), alongside an autoregressive baseline and benchmarks.", "result": "SpeakerVid-5M includes over 8,743 hours of videos across 5.2 million clips, supporting various interaction modes and offering a benchmark for further research.", "conclusion": "SpeakerVid-5M and its associated resources aim to drive forward research in interactive virtual humans by providing both data and standardized benchmarks for evaluation."}}
{"id": "2507.09754", "pdf": "https://arxiv.org/pdf/2507.09754", "abs": "https://arxiv.org/abs/2507.09754", "authors": ["Aakash Tripathi", "Ian E. Nielsen", "Muhammad Umer", "Ravi P. Ramachandran", "Ghulam Rasool"], "title": "Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Transcription Factor Binding Site (TFBS) prediction is crucial for\nunderstanding gene regulation and various biological processes. This study\nintroduces a novel Mixture of Experts (MoE) approach for TFBS prediction,\nintegrating multiple pre-trained Convolutional Neural Network (CNN) models,\neach specializing in different TFBS patterns. We evaluate the performance of\nour MoE model against individual expert models on both in-distribution and\nout-of-distribution (OOD) datasets, using six randomly selected transcription\nfactors (TFs) for OOD testing. Our results demonstrate that the MoE model\nachieves competitive or superior performance across diverse TF binding sites,\nparticularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)\nstatistical test confirms the significance of these performance differences.\nAdditionally, we introduce ShiftSmooth, a novel attribution mapping technique\nthat provides more robust model interpretability by considering small shifts in\ninput sequences. Through comprehensive explainability analysis, we show that\nShiftSmooth offers superior attribution for motif discovery and localization\ncompared to traditional Vanilla Gradient methods. Our work presents an\nefficient, generalizable, and interpretable solution for TFBS prediction,\npotentially enabling new discoveries in genome biology and advancing our\nunderstanding of transcriptional regulation.", "AI": {"tldr": "This study proposes a Mixture of Experts (MoE) model and a novel attribution mapping technique called ShiftSmooth for enhanced transcription factor binding site (TFBS) prediction.", "motivation": "TFBS prediction is critical for understanding gene regulation and various biological processes, but current methods lack generalizability, interpretability, and performance in out-of-distribution scenarios.", "method": "The study employs a Mixture of Experts (MoE) model, integrating multiple specialized CNNs, and introduces ShiftSmooth for better model interpretability.", "result": "The MoE model demonstrated superior performance, particularly in out-of-distribution datasets, while ShiftSmooth improved motif discovery and localization compared to traditional methods.", "conclusion": "The proposed methods offer a robust, generalizable, and interpretable solution for TFBS prediction, advancing knowledge in genome biology and transcriptional regulation."}}
{"id": "2507.10200", "pdf": "https://arxiv.org/pdf/2507.10200", "abs": "https://arxiv.org/abs/2507.10200", "authors": ["Stefano Bann\u00f2", "Rao Ma", "Mengjie Qian", "Siyuan Tang", "Kate Knill", "Mark Gales"], "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)", "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.", "AI": {"tldr": "The paper investigates using natural language-based assessment (NLA) to evaluate language proficiency through can-do descriptors applied by a large language model (LLM), Qwen 2.5 72B, achieving competitive performance in a zero-shot setting.", "motivation": "To explore whether large language models can interpret and apply human-designed can-do descriptors for second language assessment effectively and compare their performance with other models.", "method": "The study uses an open-source LLM, Qwen 2.5 72B, to assess responses from the S&I Corpus in a zero-shot setting. It benchmarks the model against fine-tuned speech LLMs and a specifically trained BERT-based model.", "result": "The approach achieves competitive performance, surpassing the BERT-based model but not fine-tuned speech LLMs, showing strong generalizability and interpretability.", "conclusion": "NLA is effective in mismatched task settings, adaptable across data types and languages, and offers transparency through its clear application of language descriptors."}}
{"id": "2507.09028", "pdf": "https://arxiv.org/pdf/2507.09028", "abs": "https://arxiv.org/abs/2507.09028", "authors": ["Amgad Muneer", "Muhammad Waqas", "Maliazurina B Saad", "Eman Showkatian", "Rukhmini Bandyopadhyay", "Hui Xu", "Wentao Li", "Joe Y Chang", "Zhongxing Liao", "Cara Haymaker", "Luisa Solis Soto", "Carol C Wu", "Natalie I Vokes", "Xiuning Le", "Lauren A Byers", "Don L Gibbons", "John V Heymach", "Jianjun Zhang", "Jia Wu"], "title": "From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research", "categories": ["q-bio.QM", "cs.AI"], "comment": "6 figures, 3 tables", "summary": "Cancer research is increasingly driven by the integration of diverse data\nmodalities, spanning from genomics and proteomics to imaging and clinical\nfactors. However, extracting actionable insights from these vast and\nheterogeneous datasets remains a key challenge. The rise of foundation models\n(FMs) -- large deep-learning models pretrained on extensive amounts of data\nserving as a backbone for a wide range of downstream tasks -- offers new\navenues for discovering biomarkers, improving diagnosis, and personalizing\ntreatment. This paper presents a comprehensive review of widely adopted\nintegration strategies of multimodal data to assist advance the computational\napproaches for data-driven discoveries in oncology. We examine emerging trends\nin machine learning (ML) and deep learning (DL), including methodological\nframeworks, validation protocols, and open-source resources targeting cancer\nsubtype classification, biomarker discovery, treatment guidance, and outcome\nprediction. This study also comprehensively covers the shift from traditional\nML to FMs for multimodal integration. We present a holistic view of recent FMs\nadvancements and challenges faced during the integration of multi-omics with\nadvanced imaging data. We identify the state-of-the-art FMs, publicly available\nmulti-modal repositories, and advanced tools and methods for data integration.\nWe argue that current state-of-the-art integrative methods provide the\nessential groundwork for developing the next generation of large-scale,\npre-trained models poised to further revolutionize oncology. To the best of our\nknowledge, this is the first review to systematically map the transition from\nconventional ML to advanced FM for multimodal data integration in oncology,\nwhile also framing these developments as foundational for the forthcoming era\nof large-scale AI models in cancer research.", "AI": {"tldr": "This paper reviews the integration of multimodal datasets in cancer research using foundation models (FMs), exploring advancements, challenges, and tools for data-driven oncology.", "motivation": "The motivation is to address the challenge of extracting actionable insights from the increasingly vast and heterogeneous multimodal datasets in cancer research.", "method": "The study reviews existing literature and methodologies, focusing on the transition from traditional machine learning to foundation models for multimodal data integration in oncology.", "result": "The review identifies state-of-the-art foundation models, publicly available multimodal repositories, advanced integration tools, and highlights the emerging trends and challenges.", "conclusion": "The paper concludes that current integrative methods lay groundwork for future large-scale, pretrained models, emphasizing their transformative potential for oncology advancements."}}
{"id": "2507.09766", "pdf": "https://arxiv.org/pdf/2507.09766", "abs": "https://arxiv.org/abs/2507.09766", "authors": ["Mohamadreza Akbari Pour", "Ali Ghasemzadeh", "MohamadAli Bijarchi", "Mohammad Behshad Shafii"], "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)\nis essential for Prognostics and Health Management (PHM) across a wide range of\nindustrial applications. We propose a novel framework -- Reinforced Graph-Based\nPhysics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that\ncombines physics-based supervision with advanced spatio-temporal learning.\nGraph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional\nfilters within recurrent units to capture how node representations evolve over\ntime. Graph Attention Convolution (GATConv) leverages a self-attention\nmechanism to compute learnable, edge-wise attention coefficients, dynamically\nweighting neighbor contributions for adaptive spatial aggregation. A Soft\nActor-Critic (SAC) module is positioned between the Temporal Attention Unit\n(TAU) and GCRN to further improve the spatio-temporal learning. This module\nimproves attention and prediction accuracy by dynamically scaling hidden\nrepresentations to minimize noise and highlight informative features. To\nidentify the most relevant physical constraints in each area, Q-learning agents\ndynamically assign weights to physics-informed loss terms, improving\ngeneralization across real-time industrial systems and reducing the need for\nmanual tuning. In both RUL and SOH estimation tasks, the proposed method\nconsistently outperforms state-of-the-art models, demonstrating strong\nrobustness and predictive accuracy across varied degradation patterns across\nthree diverse industrial benchmark datasets.", "AI": {"tldr": "The paper introduces RGPD, a novel machine learning framework combining physics-based supervision and spatio-temporal learning for accurate estimation of RUL and SOH.", "motivation": "Enhance accuracy and generalization in RUL and SOH predictions for industrial systems while reducing dependence on manual parameter tuning.", "method": "The RGPD framework integrates Graph Convolutional Recurrent Networks, self-attention mechanisms, SAC modules, and Q-learning agents to refine spatio-temporal learning and weight physical constraints.", "result": "RGPD demonstrates superior performance in predictive accuracy and robustness across diverse industrial benchmarks for RUL and SOH tasks.", "conclusion": "The proposed RGPD framework is effective in improving predictions for PHM applications, outperforming state-of-the-art models in robustness and accuracy."}}
{"id": "2507.10300", "pdf": "https://arxiv.org/pdf/2507.10300", "abs": "https://arxiv.org/abs/2507.10300", "authors": ["Hatef Otroshi Shahreza", "S\u00e9bastien Marcel"], "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "AI": {"tldr": "The paper introduces FaceLLM, a domain-specific multimodal large language model, trained for facial image understanding tasks. It leverages a novel weakly supervised pipeline using ChatGPT to introduce the FairFaceGPT dataset. The model achieves state-of-the-art performance.", "motivation": "Existing multimodal large language models fail to excel in domain-specific tasks, particularly facial image understanding, due to the lack of specialized datasets.", "method": "FaceLLM is trained using a weakly supervised pipeline that generates annotated data (FairFaceGPT dataset) using ChatGPT applied to the FairFace dataset with attribute-aware prompts.", "result": "FaceLLM outperforms other models in face-centric tasks, excels at recognizing facial features, and sets a new benchmark for these tasks.", "conclusion": "FaceLLM showcases the potential of synthetic supervision and opens avenues for domain-specialized, trustworthy multimodal AI systems. It also provides publicly accessible resources for further research."}}
{"id": "2507.09880", "pdf": "https://arxiv.org/pdf/2507.09880", "abs": "https://arxiv.org/abs/2507.09880", "authors": ["Keito Suzuki", "Bang Du", "Runfa Blark Li", "Kunyao Chen", "Lei Wang", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing", "categories": ["cs.CV"], "comment": null, "summary": "Understanding dynamic 3D human representation has become increasingly\ncritical in virtual and extended reality applications. However, existing human\npart segmentation methods are constrained by reliance on closed-set datasets\nand prolonged inference times, which significantly restrict their\napplicability. In this paper, we introduce the first 4D human parsing framework\nthat simultaneously addresses these challenges by reducing the inference time\nand introducing open-vocabulary capabilities. Building upon state-of-the-art\nopen-vocabulary 3D human parsing techniques, our approach extends the support\nto 4D human-centric video with three key innovations: 1) We adopt mask-based\nvideo object tracking to efficiently establish spatial and temporal\ncorrespondences, avoiding the necessity of segmenting all frames. 2) A novel\nMask Validation module is designed to manage new target identification and\nmitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating\nmemory-conditioned attention and logits equalization for robust embedding\nfusion. Extensive experiments demonstrate the effectiveness and flexibility of\nthe proposed method on 4D human-centric parsing tasks, achieving up to 93.3%\nacceleration compared to the previous state-of-the-art method, which was\nlimited to parsing fixed classes.", "AI": {"tldr": "The paper introduces an advanced 4D human parsing framework with open-vocabulary capabilities, drastically reducing inference time focused on 4D video content.", "motivation": "Current human part segmentation methods rely on closed-set datasets and are computationally expensive, limiting their usage in real-world extended reality applications.", "method": "The authors propose three innovations: (1) using mask-based video object tracking to reduce the need to segment all frames, (2) introducing a Mask Validation module for better tracking and identification, and (3) developing a 4D Mask Fusion module that enhances embedding through memory-conditioned attention and logits equalization.", "result": "The framework significantly accelerates 4D human-centric parsing tasks, achieving a 93.3% improvement in speed compared to the latest state-of-the-art.", "conclusion": "This method offers a powerful and adaptive solution for real-time dynamic human parsing in videos while supporting open-vocabulary parsing, addressing key limitations of prior work in this domain."}}
{"id": "2507.09768", "pdf": "https://arxiv.org/pdf/2507.09768", "abs": "https://arxiv.org/abs/2507.09768", "authors": ["Kenny Falk\u00e6r Olsen. Mads \u00d8stergaard", "Karl Ulb\u00e6k", "S\u00f8ren F\u00f8ns Nielsen", "Rasmus Malik H\u00f8egh Lindrup", "Bj\u00f8rn Sand Jensen", "Morten M\u00f8rup"], "title": "Knowing When to Quit: Probabilistic Early Exits for Speech Separation", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, deep learning-based single-channel speech separation has\nimproved considerably, in large part driven by increasingly compute- and\nparameter-efficient neural network architectures. Most such architectures are,\nhowever, designed with a fixed compute and parameter budget, and consequently\ncannot scale to varying compute demands or resources, which limits their use in\nembedded and heterogeneous devices such as mobile phones and hearables. To\nenable such use-cases we design a neural network architecture for speech\nseparation capable of early-exit, and we propose an uncertainty-aware\nprobabilistic framework to jointly model the clean speech signal and error\nvariance which we use to derive probabilistic early-exit conditions in terms of\ndesired signal-to-noise ratios. We evaluate our methods on both speech\nseparation and enhancement tasks, and we show that a single early-exit model\ncan be competitive with state-of-the-art models trained at many compute and\nparameter budgets. Our framework enables fine-grained dynamic compute-scaling\nof speech separation networks while achieving state-of-the-art performance and\ninterpretable exit conditions.", "AI": {"tldr": "The paper introduces a neural network architecture for single-channel speech separation that supports dynamic compute-scaling through early-exit mechanisms.", "motivation": "To address the limitations of fixed-budget neural networks in adapting to varying compute demands and resources, especially in embedded and heterogeneous devices.", "method": "The authors propose a probabilistic framework integrating uncertainty-aware modeling to derive early-exit conditions based on signal-to-noise ratio targets.", "result": "Experimental tests show the approach achieves state-of-the-art performance and interpretability, competing with fixed-budget models across varying compute and parameter settings.", "conclusion": "The framework provides an effective solution for dynamic compute-scaling in speech separation tasks, suitable for resource-constrained environments like mobile devices."}}
{"id": "2507.10398", "pdf": "https://arxiv.org/pdf/2507.10398", "abs": "https://arxiv.org/abs/2507.10398", "authors": ["Diksha Mehta", "Prateek Mehta"], "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "AI": {"tldr": "The paper focuses on using a convolutional neural network (CNN) to recognize handwritten Devanagari characters, achieving high accuracy rates.", "motivation": "The motivation of the paper stems from the lack of advanced digitization tools for the Devanagari script, highlighting the need for effective recognition methods.", "method": "The authors propose using two deep convolutional neural network layers trained on the Devanagari handwritten character dataset (DHCD), which contains 36 classes of characters and 1700 images per class.", "result": "The proposed model achieved 96.36% accuracy in testing and 99.55% accuracy during training.", "conclusion": "The study demonstrates the effectiveness of CNNs for handwritten Devanagari text recognition and provides a robust solution for digitizing this script."}}
{"id": "2507.09881", "pdf": "https://arxiv.org/pdf/2507.09881", "abs": "https://arxiv.org/abs/2507.09881", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "categories": ["cs.CV"], "comment": null, "summary": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism.", "AI": {"tldr": "CECAS is a novel framework for counterfactual visual explanations, addressing unintended alterations by integrating a causal perspective and outperforming state-of-the-art methods.", "motivation": "The paper aims to improve counterfactual visual explanations by addressing the lack of focus on causality and spurious correlations in current methods.", "method": "The framework, CECAS, uses a causally-guided adversarial approach to generate counterfactuals that minimize unwanted perturbations on irrelevant factors.", "result": "CECAS demonstrates superior performance compared to existing methods across multiple benchmarks in validity, sparsity, proximity, and realism.", "conclusion": "Integrating causal methods in counterfactual generation improves explanation quality and produces more meaningful visual explanations."}}
{"id": "2507.09785", "pdf": "https://arxiv.org/pdf/2507.09785", "abs": "https://arxiv.org/abs/2507.09785", "authors": ["Zhonglin Cao", "Mario Geiger", "Allan dos Santos Costa", "Danny Reidenbach", "Karsten Kreis", "Tomas Geffner", "Franco Pellegrini", "Guoqing Zhou", "Emine Kucukbenli"], "title": "Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow", "categories": ["cs.LG", "physics.chem-ph"], "comment": "ICML 2025 poster", "summary": "Fast and accurate generation of molecular conformers is desired for\ndownstream computational chemistry and drug discovery tasks. Currently,\ntraining and sampling state-of-the-art diffusion or flow-based models for\nconformer generation require significant computational resources. In this work,\nwe build upon flow-matching and propose two mechanisms for accelerating\ntraining and inference of generative models for 3D molecular conformer\ngeneration. For fast training, we introduce the SO(3)-Averaged Flow training\nobjective, which leads to faster convergence to better generation quality\ncompared to conditional optimal transport flow or Kabsch-aligned flow. We\ndemonstrate that models trained using SO(3)-Averaged Flow can reach\nstate-of-the-art conformer generation quality. For fast inference, we show that\nthe reflow and distillation methods of flow-based models enable few-steps or\neven one-step molecular conformer generation with high quality. The training\ntechniques proposed in this work show a path towards highly efficient molecular\nconformer generation with flow-based models.", "AI": {"tldr": "This paper proposes faster training and inference techniques for 3D molecular conformer generation using flow-based models, achieving state-of-the-art quality with reduced computational costs.", "motivation": "Efficient generation of molecular conformers is crucial for computational chemistry and drug discovery tasks, yet current methods require significant computational resources to train and sample state-of-the-art models.", "method": "The paper introduces SO(3)-Averaged Flow training for faster convergence and higher quality conformer generation, alongside reflow and distillation methods for efficient inference, enabling few-step or one-step generation.", "result": "Experiments show that SO(3)-Averaged Flow surpasses existing methods in generation quality. Reflow and distillation significantly reduce inference steps while maintaining high-quality conformer generation.", "conclusion": "The proposed approaches pave the way for computationally efficient, high-quality molecular conformer generation, improving state-of-the-art models in both training and inference."}}
{"id": "2507.10403", "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "CrisisLandMark introduces a large-scale corpus pairing SAR and optical data with textual annotations and presents CLOSP, a framework that aligns SAR and optical imagery for improved retrieval performance. GeoCLOSP further refines retrieval capabilities with geographic specificity.", "motivation": "The paper aims to address the limitations of text-to-image retrieval systems that primarily use RGB data by integrating diverse sensor information, such as SAR and optical multispectral data, for enhanced imagery retrieval.", "method": "It introduces the CrisisLandMark dataset and proposes CLOSP, a framework that aligns SAR and optical images using contrastive learning via text, alongside GeoCLOSP, which incorporates geographic coordinates.", "result": "CLOSP improves retrieval performance significantly, achieving a 54% enhancement in nDGC over prior models. GeoCLOSP enables specialized retrieval based on geographic-specific information.", "conclusion": "Integrating diverse sensor data with geographic context significantly enhances the capability of remote sensing archives, unlocking unique retrieval opportunities for both general tasks and specialized applications."}}
{"id": "2507.09885", "pdf": "https://arxiv.org/pdf/2507.09885", "abs": "https://arxiv.org/abs/2507.09885", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective\nsolution for various vision-based applications. However, most existing\nlearning-based hyperspectral reconstruction methods directly learn the\nRGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent\nchallenge of transitioning from low-dimensional to high-dimensional\ninformation. To address this limitation, we propose a two-stage approach, MCGA,\nwhich first learns spectral patterns before estimating the mapping. In the\nfirst stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI\ndatasets, extracting a Mixture of Codebooks (MoC). In the second stage, the\nRGB-to-HSI mapping is refined by querying features from the MoC to replace\nlatent HSI representations, incorporating prior knowledge rather than forcing a\ndirect high-dimensional transformation. To further enhance reconstruction\nquality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,\nwhich adaptively adjust feature map intensities to meet hyperspectral\nreconstruction requirements. This physically motivated attention mechanism\nensures lightweight and efficient HSI recovery. Moreover, we propose an\nentropy-based Test-Time Adaptation strategy to improve robustness in real-world\nscenarios. Extensive experiments demonstrate that our method, MCGA, achieves\nstate-of-the-art performance. The code and models will be released at\nhttps://github.com/Fibonaccirabbit/MCGA", "AI": {"tldr": "This paper introduces MCGA, a novel two-stage approach for reconstructing hyperspectral images (HSI) from RGB images, achieving state-of-the-art performance.", "motivation": "Current methods struggle with directly learning RGB-to-HSI mapping, leading to limitations in transitioning from low to high-dimensional data.", "method": "MCGA comprises two stages: (1) learning spectral patterns using multi-scale VQ-VAE and generating a Mixture of Codebooks (MoC), (2) refining the RGB-to-HSI mapping through querying features from MoC with attention mechanisms such as Grayscale-Aware Attention and Quantized Self-Attention.", "result": "MCGA improves the reconstruction quality with efficient performance and better adaptation to real-world scenarios through an entropy-based Test-Time Adaptation strategy.", "conclusion": "This approach effectively addresses the challenges in RGB-to-HSI transformation, demonstrating superior results and robustness, backed by extensive experiments."}}
{"id": "2507.09786", "pdf": "https://arxiv.org/pdf/2507.09786", "abs": "https://arxiv.org/abs/2507.09786", "authors": ["Junaid Iqbal Khan"], "title": "Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster", "categories": ["cs.LG"], "comment": "10 pages, 4 figures, 4 tables", "summary": "Approximate machine unlearning (AMU) enables models to `forget' specific\ntraining data through specialized fine-tuning on a retained dataset subset.\nHowever, processing this retained subset still dominates computational runtime,\nwhile reductions of epochs also remain a challenge. We propose two\ncomplementary methods to accelerate classification-oriented AMU. First,\n\\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges\nvisually similar images with shared blend-weights to significantly reduce the\nretained set size. It operates with minimal pre-processing overhead and is\norders of magnitude faster than state-of-the-art DC methods. Second, our\nloss-centric method, \\textbf{Accelerated-AMU (A-AMU)}, augments the unlearning\nobjective to quicken convergence. A-AMU achieves this by combining a steepened\nprimary loss to expedite forgetting with a novel, differentiable regularizer\nthat matches the loss distributions of forgotten and in-distribution unseen\ndata. Our extensive experiments demonstrate that this dual approach of data and\nloss-centric optimization dramatically reduces end-to-end unlearning latency\nacross both single and multi-round scenarios, all while preserving model\nutility and privacy. To our knowledge, this is the first work to systematically\ntackle unlearning efficiency by jointly designing a specialized dataset\ncondensation technique with a dedicated accelerated loss function. Code is\navailable at https://github.com/algebraicdianuj/DC_Unlearning.", "AI": {"tldr": "This paper introduces two methods, Blend and Accelerated-AMU (A-AMU), to enhance the efficiency of approximate machine unlearning (AMU) by reducing runtime and improving convergence during classification tasks.", "motivation": "The motivation behind this paper is the inefficiency in computational runtime of current AMU approaches, especially in handling the retained dataset subset and reducing epochs for processing.", "method": "The authors propose Blend, a dataset condensation method using visually similar image merging, and A-AMU, a loss-centric technique with steepened primary loss and novel regularization to expedite unlearning convergence.", "result": "Their experiments show significant reductions in unlearning latency across both single and multi-round scenarios, while maintaining model utility and privacy with improved computational efficiency.", "conclusion": "The study provides the first systematic approach combining dataset condensation and specialized loss functions to optimize unlearning efficiency, setting a new benchmark in the domain of AMU."}}
{"id": "2507.09055", "pdf": "https://arxiv.org/pdf/2507.09055", "abs": "https://arxiv.org/abs/2507.09055", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks", "categories": ["cs.SI", "cs.IR", "physics.soc-ph", "H.3.3"], "comment": "10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health\n  (2025)", "summary": "The rapid spread of health misinformation on online social networks (OSNs)\nduring global crises such as the COVID-19 pandemic poses challenges to public\nhealth, social stability, and institutional trust. Centrality metrics have long\nbeen pivotal in understanding the dynamics of information flow, particularly in\nthe context of health misinformation. However, the increasing complexity and\ndynamism of online networks, especially during crises, highlight the\nlimitations of these traditional approaches. This study introduces and compares\nthree novel centrality metrics: dynamic influence centrality (DIC), health\nmisinformation vulnerability centrality (MVC), and propagation centrality (PC).\nThese metrics incorporate temporal dynamics, susceptibility, and multilayered\nnetwork interactions. Using the FibVID dataset, we compared traditional and\nnovel metrics to identify influential nodes, propagation pathways, and\nmisinformation influencers. Traditional metrics identified 29 influential\nnodes, while the new metrics uncovered 24 unique nodes, resulting in 42\ncombined nodes, an increase of 44.83%. Baseline interventions reduced health\nmisinformation by 50%, while incorporating the new metrics increased this to\n62.5%, an improvement of 25%. To evaluate the broader applicability of the\nproposed metrics, we validated our framework on a second dataset, Monant\nMedical Misinformation, which covers a diverse range of health misinformation\ndiscussions beyond COVID-19. The results confirmed that the advanced metrics\ngeneralised successfully, identifying distinct influential actors not captured\nby traditional methods. In general, the findings suggest that a combination of\ntraditional and novel centrality measures offers a more robust and\ngeneralisable framework for understanding and mitigating the spread of health\nmisinformation in different online network contexts.", "AI": {"tldr": "The study introduces three novel centrality metrics that better identify misleading health information spreaders on online networks, achieving superior outcomes over traditional approaches.", "motivation": "To address the challenges posed by health misinformation on online social networks during crises, such as its impact on public health and trust, and to overcome limitations of traditional centrality metrics in understanding misinformation flow.", "method": "The research introduces and evaluates three novel centrality metrics\u2014dynamic influence centrality (DIC), health misinformation vulnerability centrality (MVC), and propagation centrality (PC)\u2014by comparing them with traditional metrics using datasets like FibVID and Monant Medical Misinformation.", "result": "The new metrics identified unique influential nodes missed by traditional methods, improved misinformation reduction by 25% during baseline interventions, and were successfully validated on a second dataset, indicating generalizability.", "conclusion": "A combination of traditional and novel centrality measures provides a robust framework for analyzing and mitigating health misinformation across diverse online social networks."}}
{"id": "2507.09896", "pdf": "https://arxiv.org/pdf/2507.09896", "abs": "https://arxiv.org/abs/2507.09896", "authors": ["Xiuyu Wu", "Xinhao Wang", "Xiubin Zhu", "Lan Yang", "Jiyuan Liu", "Xingchen Hu"], "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Due to the arbitrary orientation of objects in aerial images, rotation\nequivariance is a critical property for aerial object detectors. However,\nrecent studies on rotation-equivariant aerial object detection remain scarce.\nMost detectors rely on data augmentation to enable models to learn\napproximately rotation-equivariant features. A few detectors have constructed\nrotation-equivariant networks, but due to the breaking of strict rotation\nequivariance by typical downsampling processes, these networks only achieve\napproximately rotation-equivariant backbones. Whether strict rotation\nequivariance is necessary for aerial image object detection remains an open\nquestion. In this paper, we implement a strictly rotation-equivariant backbone\nand neck network with a more advanced network structure and compare it with\napproximately rotation-equivariant networks to quantitatively measure the\nimpact of rotation equivariance on the performance of aerial image detectors.\nAdditionally, leveraging the inherently grouped nature of rotation-equivariant\nfeatures, we propose a multi-branch head network that reduces the parameter\ncount while improving detection accuracy. Based on the aforementioned\nimprovements, this study proposes the Multi-branch head rotation-equivariant\nsingle-stage Detector (MessDet), which achieves state-of-the-art performance on\nthe challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an\nexceptionally low parameter count.", "AI": {"tldr": "This paper proposes a detector called MessDet that employs a strictly rotation-equivariant network and a novel multi-branch head design to achieve state-of-the-art performance in aerial image object detection.", "motivation": "The motivation is to address the limitations of current aerial object detection methods, which either rely on data augmentation or approximately rotation-equivariant networks. The study aims to investigate whether strict rotation equivariance can improve detection performance and efficiency.", "method": "The authors implement a strictly rotation-equivariant backbone and neck network and introduce a multi-branch head design leveraging rotation-equivariant features. They compare the proposed method against approximately rotation-equivariant networks using quantitative evaluations.", "result": "MessDet demonstrates state-of-the-art performance on aerial image datasets DOTA-v1.0, DOTA-v1.5, and DIOR-R, achieving high detection accuracy and efficiency with a low parameter count.", "conclusion": "Strict rotation equivariance and a multi-branch head design significantly improve aerial image detection performance while reducing computational complexity, demonstrating the value of these contributions."}}
{"id": "2507.09797", "pdf": "https://arxiv.org/pdf/2507.09797", "abs": "https://arxiv.org/abs/2507.09797", "authors": ["Ping Liu", "Rajat Arora", "Xiao Shi", "Benjamin Le", "Qianqi Shen", "Jianqiang Shen", "Chengming Jiang", "Nikita Zhiltsov", "Priya Bannur", "Yidan Zhu", "Liming Dong", "Haichao Wei", "Qi Guo", "Luke Simon", "Liangjie Hong", "Wenjing Zhang"], "title": "A Scalable and Efficient Signal Integration System for Job Matching", "categories": ["cs.LG"], "comment": "KDD2025", "summary": "LinkedIn, one of the world's largest platforms for professional networking\nand job seeking, encounters various modeling challenges in building\nrecommendation systems for its job matching product, including cold-start,\nfilter bubbles, and biases affecting candidate-job matching. To address these,\nwe developed the STAR (Signal Integration for Talent And Recruiters) system,\nleveraging the combined strengths of Large Language Models (LLMs) and Graph\nNeural Networks (GNNs). LLMs excel at understanding textual data, such as\nmember profiles and job postings, while GNNs capture intricate relationships\nand mitigate cold-start issues through network effects. STAR integrates diverse\nsignals by uniting LLM and GNN capabilities with industrial-scale paradigms\nincluding adaptive sampling and version management. It provides an end-to-end\nsolution for developing and deploying embeddings in large-scale recommender\nsystems. Our key contributions include a robust methodology for building\nembeddings in industrial applications, a scalable GNN-LLM integration for\nhigh-performing recommendations, and practical insights for real-world model\ndeployment.", "AI": {"tldr": "The paper introduces STAR, a system combining Large Language Models (LLMs) and Graph Neural Networks (GNNs) to improve job matching on LinkedIn by addressing challenges like cold-start and biases.", "motivation": "Overcoming challenges such as cold-start, filter bubbles, and biases in LinkedIn\u2019s job recommendation system while enhancing candidate-job matching accuracy.", "method": "Integrates Large Language Models for text understanding with Graph Neural Networks for relationship modeling, enhanced by adaptive sampling and version management.", "result": "Developed STAR, a scalable, high-performing system for embedding generation and integration into industrial recommendation setups.", "conclusion": "STAR successfully addresses existing challenges in job matching systems, offering practical insights for large-scale embedding development and recommendation systems deployment."}}
{"id": "2507.09910", "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "AI": {"tldr": "The paper introduces Instructional Graphic Designer (IGD), a system that uses natural language instructions to generate editable multimodal graphic design layers.", "motivation": "Address limitations in current graphic design automation methods, including poor legibility, lack of creativity, and the inability to produce editable files.", "method": "IGD combines parametric rendering, image asset generation using diffusion models, multimodal understanding for layer prediction and layout, and a standardized platform for multi-scenario design scalability.", "result": "IGD achieves effective attribute prediction, layer sequencing, and asset generation, resulting in superior graphic design outputs compared to traditional methods.", "conclusion": "IGD represents a scalable and extensible solution for automated graphic design, improving efficiency and flexibility while preserving design quality."}}
{"id": "2507.09805", "pdf": "https://arxiv.org/pdf/2507.09805", "abs": "https://arxiv.org/abs/2507.09805", "authors": ["Audri Banik", "Glaucio Haroldo Silva de Carvalho", "Renata Dividino"], "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at FedKDD 2025: International Joint Workshop on Federated\n  Learning for Data Mining and Graph Analytics. 6 pages, 1 figure", "summary": "In traffic prediction, the goal is to estimate traffic speed or flow in\nspecific regions or road segments using historical data collected by devices\ndeployed in each area. Each region or road segment can be viewed as an\nindividual client that measures local traffic flow, making Federated Learning\n(FL) a suitable approach for collaboratively training models without sharing\nraw data. In centralized FL, a central server collects and aggregates model\nupdates from multiple clients to build a shared model while preserving each\nclient's data privacy. Standard FL methods, such as Federated Averaging\n(FedAvg), assume that clients are independent, which can limit performance in\ntraffic prediction tasks where spatial relationships between clients are\nimportant. Federated Graph Learning methods can capture these dependencies\nduring server-side aggregation, but they often introduce significant\ncomputational overhead. In this paper, we propose a lightweight graph-aware FL\napproach that blends the simplicity of FedAvg with key ideas from graph\nlearning. Rather than training full models, our method applies basic\nneighbourhood aggregation principles to guide parameter updates, weighting\nclient models based on graph connectivity. This approach captures spatial\nrelationships effectively while remaining computationally efficient. We\nevaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,\nand show that it achieves competitive performance compared to standard\nbaselines and recent graph-based federated learning techniques.", "AI": {"tldr": "The paper proposes a lightweight graph-aware federated learning approach for traffic prediction that integrates graph connectivity principles into client model aggregation, achieving competitive performance with high efficiency.", "motivation": "Traditional FedAvg methods assume client independence which is suboptimal for traffic prediction tasks where spatial relationships are significant.", "method": "The paper introduces a lightweight federated learning method that combines FedAvg's simplicity with neighborhood aggregation based on graph connectivity.", "result": "The approach was tested on METR-LA and PEMS-BAY datasets, demonstrating competitive performance compared to existing methods.", "conclusion": "This method captures spatial relationships effectively while being computationally efficient, making it suitable for traffic prediction tasks."}}
{"id": "2507.10532", "pdf": "https://arxiv.org/pdf/2507.10532", "abs": "https://arxiv.org/abs/2507.10532", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.", "AI": {"tldr": "This paper evaluates reasoning performance in Large Language Models (LLMs) enhanced with Reinforcement Learning (RL). It highlights issues with benchmark contamination and introduces a reliable dataset, \"RandomCalculation.\" Their findings suggest that only accurate rewards consistently boost performance.", "motivation": "The study was motivated by unverified claims that reinforcement learning can improve reasoning in LLMs even with noisy rewards, and concerns about benchmark reliability due to data contamination.", "method": "The researchers analyzed reasoning performance on different LLMs, highlighted issues with benchmark contamination, and created a synthetic dataset called \"RandomCalculation\" to conduct controlled evaluations.", "result": "The findings show that accurate reward signals improve reasoning, while noisy or random signals fail to consistently enhance performance. Additionally, benchmark contamination in models like Qwen2.5 was identified as problematic.", "conclusion": "The paper argues that evaluating models on uncontaminated datasets and diverse model families is essential for reliable conclusions, emphasizing the importance of accuracy in reward signals."}}
{"id": "2507.09915", "pdf": "https://arxiv.org/pdf/2507.09915", "abs": "https://arxiv.org/abs/2507.09915", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "The scarcity of data in various scenarios, such as medical, industry and\nautonomous driving, leads to model overfitting and dataset imbalance, thus\nhindering effective detection and segmentation performance. Existing studies\nemploy the generative models to synthesize more training samples to mitigate\ndata scarcity. However, these synthetic samples are repetitive or simplistic\nand fail to provide \"crucial information\" that targets the downstream model's\nweaknesses. Additionally, these methods typically require separate training for\ndifferent objects, leading to computational inefficiencies. To address these\nissues, we propose Crucial-Diff, a domain-agnostic framework designed to\nsynthesize crucial samples. Our method integrates two key modules. The Scene\nAgnostic Feature Extractor (SAFE) utilizes a unified feature extractor to\ncapture target information. The Weakness Aware Sample Miner (WASM) generates\nhard-to-detect samples using feedback from the detection results of downstream\nmodel, which is then fused with the output of SAFE module. Together, our\nCrucial-Diff framework generates diverse, high-quality training data, achieving\na pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,\nCrucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be\nreleased after acceptance.", "AI": {"tldr": "The paper introduces Crucial-Diff, a framework to generate impactful synthetic data for improving detection and segmentation models.", "motivation": "Address data scarcity in fields like medical imaging and autonomous driving, which leads to overfitting and imbalance in datasets for model training.", "method": "Utilizes a unified feature extractor (SAFE) and a weakness-aware sample miner (WASM) to generate synthetic samples tailored to improve the performance of downstream models.", "result": "Achieved significant metrics improvement including pixel-level AP of 83.63% and F1-MAX of 78.12% on MVTec, and mIoU of 81.64% and mDice of 87.69% on the polyp dataset.", "conclusion": "The framework successfully synthesizes diverse and high-quality samples addressing model weaknesses, outperforming prior generative methods in targeted applications."}}
{"id": "2507.09816", "pdf": "https://arxiv.org/pdf/2507.09816", "abs": "https://arxiv.org/abs/2507.09816", "authors": ["Adam Newgas"], "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 9 figures", "summary": "Neural networks are capable of superposition -- representing more features\nthan there are dimensions. Recent work considers the analogous concept for\ncomputation instead of storage, proposing theoretical constructions. But there\nhas been little investigation into whether these circuits can be learned in\npractice. In this work, we investigate a toy model for the Universal-AND\nproblem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs.\nThe hidden dimension that determines the number of non-linear activations is\nrestricted to pressure the model to find a compute-efficient circuit, called\ncompressed computation. We find that the training process finds a simple\nsolution that does not correspond to theoretical constructions. It is fully\ndense -- every neuron contributes to every output. The solution circuit\nnaturally scales with dimension, trading off error rates for neuron efficiency.\nIt is similarly robust to changes in sparsity and other key parameters, and\nextends naturally to other boolean operations and boolean circuits. We explain\nthe found solution in detail and compute why it is more efficient than the\ntheoretical constructions at low sparsity. Our findings shed light on the types\nof circuits that models like to form and the flexibility of the superposition\nrepresentation. This contributes to a broader understanding of network\ncircuitry and interpretability.", "AI": {"tldr": "This paper investigates the learned solutions of neural networks constrained to compute-efficient circuits for a Universal-AND problem, finding robust and flexible dense circuit solutions.", "motivation": "The paper aims to explore whether neural networks can learn compute-efficient circuits that exploit superposition in practical scenarios, rather than just theoretical constructs.", "method": "The study uses a toy Universal-AND problem with restricted hidden dimensions to enforce compute-efficient circuits and examines how training influences the design of learned circuits.", "result": "The network learns a fully dense circuit, where each neuron contributes to every output. The solution scales efficiently with dimension, maintains robustness across key parameters, and extends naturally to various boolean tasks.", "conclusion": "The results reveal the neural networks' preference for dense and efficient circuit solutions that deviate from theoretical constructions, enriching understanding of model circuitry, superposition, and interpretability."}}
{"id": "2507.10548", "pdf": "https://arxiv.org/pdf/2507.10548", "abs": "https://arxiv.org/abs/2507.10548", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "AI": {"tldr": "This paper presents EmRACE-3K, a dataset for testing the embodied reasoning capabilities of vision-language models (VLMs) in complex, interactive tasks involving navigation, object manipulation, and multi-stage goals within photo-realistic environments. Benchmarking with this dataset reveals the limitations of existing advanced VLMs in such settings, with less than 20% success rates in zero-shot scenarios.", "motivation": "Existing advanced vision-language models struggle in embodied settings requiring spatial reasoning, first-person perspectives, and real-time interaction. The motivation is to address this gap by developing tools for testing and improving their embodied reasoning capabilities.", "method": "The authors introduce EmRACE-3K, a dataset of 3,000 tasks in virtual environments developed using Unreal Engine. Tasks consist of multi-step trajectories integrating visual observations, instructions, actions, and rationales. They benchmark VLMs using this dataset and fine-tune one model using supervised and reinforcement learning for performance improvement.", "result": "State-of-the-art VLMs achieve less than 20% success on EmRACE-3K tasks in zero-shot scenarios, highlighting the challenges in embodied interactions. Fine-tuning one model significantly improves its performance across exploration, reasoning, and multi-stage goal execution categories.", "conclusion": "EmRACE-3K exposes key limitations of current VLMs in interactive, embodied settings but offers a platform for advancing their capabilities. The dataset's utility is demonstrated through significant performance gains following targeted fine-tuning of a VLM model."}}
{"id": "2507.09950", "pdf": "https://arxiv.org/pdf/2507.09950", "abs": "https://arxiv.org/abs/2507.09950", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.", "AI": {"tldr": "This paper evaluates the use of large language models (LLMs) like GPT-4o-mini and Gemini 2.0 Flash for zero-shot fashion attribute recognition using the DeepFashion-MultiModal dataset.", "motivation": "Understanding fashion product attributes improves the customer experience and organizes product catalogs, ultimately affecting the 'discovery experience' of retail customers. However, LLMs' capabilities in fine-grained fashion attribute recognition are not yet well-explored.", "method": "The paper evaluates the zero-shot performance of GPT-4o-mini and Gemini 2.0 Flash on fashion attribution tasks using only images as input. Performance is measured across 18 categories of fashion attributes using metrics like the macro F1 score.", "result": "Gemini 2.0 Flash demonstrates the best overall performance with a macro F1 score of 56.79%, while GPT-4o-mini achieves 43.28%. Detailed error analysis highlights opportunities for improvement.", "conclusion": "The study shows that LLMs can be effective in fashion attribute recognition with room for domain-specific fine-tuning. It also sets a foundation for future work in fashion AI and multimodal attribute extraction."}}
{"id": "2507.09826", "pdf": "https://arxiv.org/pdf/2507.09826", "abs": "https://arxiv.org/abs/2507.09826", "authors": ["Jintao Qu", "Zichong Wang", "Chenhao Wu", "Wenbin Zhang"], "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural networks have achieved remarkable success in time series\nclassification, but their reliance on large amounts of labeled data for\ntraining limits their applicability in cold-start scenarios. Moreover, they\nlack interpretability, reducing transparency in decision-making. In contrast,\ndynamic time warping (DTW) combined with a nearest neighbor classifier is\nwidely used for its effectiveness in limited-data settings and its inherent\ninterpretability. However, as a non-parametric method, it is not trainable and\ncannot leverage large amounts of labeled data, making it less effective than\nneural networks in rich-resource scenarios. In this work, we aim to develop a\nversatile model that adapts to cold-start conditions and becomes trainable with\nlabeled data, while maintaining interpretability. We propose a dynamic\nlength-shortening algorithm that transforms time series into prototypes while\npreserving key structural patterns, thereby enabling the reformulation of the\nDTW recurrence relation into an equivalent recurrent neural network. Based on\nthis, we construct a trainable model that mimics DTW's alignment behavior. As a\nneural network, it becomes trainable when sufficient labeled data is available,\nwhile still retaining DTW's inherent interpretability. We apply the model to\nseveral benchmark time series classification tasks and observe that it\nsignificantly outperforms previous approaches in low-resource settings and\nremains competitive in rich-resource settings.", "AI": {"tldr": "The paper introduces a model combining the interpretability of dynamic time warping (DTW) with the trainability of neural networks, outperforming traditional methods in time series classification, especially in low-resource settings.", "motivation": "Neural networks excel in time series classification but require large labeled datasets and lack interpretability, making them less suitable for cold-start scenarios. DTW is effective in limited-data settings and interpretable but is non-trainable and less effective with abundant labeled data.", "method": "The authors propose a dynamic length-shortening algorithm to generate prototypes of time series, enabling the reformulation of the DTW recurrence relation into a trainable recurrent neural network while preserving DTW's alignment characteristics.", "result": "The proposed model significantly outperformed existing approaches in limited-data scenarios for time series classification and demonstrated competitive performance in settings with ample labeled data.", "conclusion": "The study achieves a balance between trainability, effectiveness in cold-start conditions, and interpretability by combining elements of DTW and neural networks, offering a versatile solution for time series classification."}}
{"id": "2507.09083", "pdf": "https://arxiv.org/pdf/2507.09083", "abs": "https://arxiv.org/abs/2507.09083", "authors": ["Anand Shah", "Kehang Zhu", "Yanchen Jiang", "Jeffrey G. Wang", "Arif K. Dayi", "John J. Horton", "David C. Parkes"], "title": "Learning from Synthetic Labs: Language Models as Auction Participants", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "This paper investigates the behavior of simulated AI agents (large language\nmodels, or LLMs) in auctions, introducing a novel synthetic data-generating\nprocess to help facilitate the study and design of auctions. We find that LLMs\n-- when endowed with chain of thought reasoning capacity -- agree with the\nexperimental literature in auctions across a variety of classic auction\nformats. In particular, we find that LLM bidders produce results consistent\nwith risk-averse human bidders; that they perform closer to theoretical\npredictions in obviously strategy-proof auctions; and, that they succumb to the\nwinner's curse in common value settings. On prompting, we find that LLMs are\nnot very sensitive to naive changes in prompts (e.g., language, currency) but\ncan improve dramatically towards theoretical predictions with the right mental\nmodel (i.e., the language of Nash deviations). We run 1,000$+$ auctions for\nless than $\\$$400 with GPT-4 models (three orders of magnitude cheaper than\nmodern auction experiments) and develop a framework flexible enough to run\nauction experiments with any LLM model and a wide range of auction design\nspecifications, facilitating further experimental study by decreasing costs and\nserving as a proof-of-concept for the use of LLM proxies.", "AI": {"tldr": "This paper studies AI agents (LLMs) in auctions using a new synthetic data approach, showing their behaviors align with known human patterns and theoretical predictions while introducing a cost-effective experimental framework.", "motivation": "To explore the performance of AI agents (LLMs) as proxies for human behavior in auctions, and to provide a cost-effective method for carrying out auction experiments.", "method": "Simulated auction experiments with GPT-4 models were conducted, using a novel synthetic data-generating framework and chain of thought reasoning. Prompts were tailored to analyze responsiveness to different factors.", "result": "LLMs replicated known behaviors of risk-averse human bidders, aligned closer to theoretical predictions in strategy-proof auctions, and exhibited the winner's curse in common value auctions. Performance improved with prompts using mental models like Nash deviations.", "conclusion": "The study demonstrates the feasibility and cost-efficiency of using LLMs as proxies in auction research, validating their adherence to known patterns and theoretical predictions and paving the way for further experimental studies."}}
{"id": "2507.09953", "pdf": "https://arxiv.org/pdf/2507.09953", "abs": "https://arxiv.org/abs/2507.09953", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "categories": ["cs.CV"], "comment": null, "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "AI": {"tldr": "This paper introduces a method to achieve atomic-scale super-resolution imaging for beam-sensitive materials using ultra-low electron doses.", "motivation": "Radiation damage limits the use of electron microscopy for analyzing beam-sensitive materials like proteins and 2D materials.", "method": "The authors employ multi-image super-resolution principles and a convolutional neural network to fuse low-resolution images and reconstruct atomic-scale structures.", "result": "Their approach, featuring a dual-path attention-guided network, achieves super-resolution comparable to conventional techniques but under significantly lower electron doses.", "conclusion": "The study extends the capabilities of 4D-STEM for structural analysis of materials prone to radiation damage, offering a more generalizable imaging solution."}}
{"id": "2507.09831", "pdf": "https://arxiv.org/pdf/2507.09831", "abs": "https://arxiv.org/abs/2507.09831", "authors": ["Jiatong Li", "Qi Liu", "Mengxiao Zhu"], "title": "Generative Cognitive Diagnosis", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "Preprint; 15 pages, 12 figures", "summary": "Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.", "AI": {"tldr": "This paper introduces a generative cognitive diagnosis approach that shifts from traditional predictive modeling to generative frameworks, enabling effective and scalable learner assessments.", "motivation": "Traditional cognitive diagnosis models struggle with limitations such as computational inefficiency during diagnosis of new learners and reliability issues in outputs.", "method": "Two generative models\u2014Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM)\u2014are proposed to infer cognitive states through a disentangled generative process.", "result": "Experiments demonstrate a 100x speedup for diagnosing new learners and improved scalability and reliability compared to traditional cognitive diagnosis methods.", "conclusion": "The proposed generative framework enhances cognitive diagnosis applications and supports advancements in intelligent education systems and AI model evaluation."}}
{"id": "2507.09980", "pdf": "https://arxiv.org/pdf/2507.09980", "abs": "https://arxiv.org/abs/2507.09980", "authors": ["Zhipeng Xue", "Yan Zhang", "Ming Li", "Chun Li", "Yue Liu", "Fei Yu"], "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures", "categories": ["cs.CV"], "comment": null, "summary": "Existing multi-view classification and clustering methods typically improve\ntask accuracy by leveraging and fusing information from different views.\nHowever, ensuring the reliability of multi-view integration and final decisions\nis crucial, particularly when dealing with noisy or corrupted data. Current\nmethods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty\nof network predictions, ignoring domain gaps between different modalities. To\naddress this issue, KPHD-Net, based on H\\\"older divergence, is proposed for\nmulti-view classification and clustering tasks. Generally, our KPHD-Net employs\na variational Dirichlet distribution to represent class probability\ndistributions, models evidences from different views, and then integrates it\nwith Dempster-Shafer evidence theory (DST) to improve uncertainty estimation\neffects. Our theoretical analysis demonstrates that Proper H\\\"older divergence\noffers a more effective measure of distribution discrepancies, ensuring\nenhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence\ntheory, recognized for its superior performance in multi-view fusion tasks, is\nintroduced and combined with the Kalman filter to provide future state\nestimations. This integration further enhances the reliability of the final\nfusion results. Extensive experiments show that the proposed KPHD-Net\noutperforms the current state-of-the-art methods in both classification and\nclustering tasks regarding accuracy, robustness, and reliability, with\ntheoretical guarantees.", "AI": {"tldr": "The paper presents KPHD-Net, which utilizes H\"older divergence and Dempster-Shafer evidence theory to improve multi-view classification and clustering accuracy, robustness, and reliability.", "motivation": "Current multi-view methods struggle with data noise and inter-modality domain gaps, often relying on KL divergence, which is limited for uncertainty estimation.", "method": "KPHD-Net adopts a variational Dirichlet distribution for class probabilities, employs H\"older divergence for measuring distribution discrepancies, integrates evidence from views using Dempster-Shafer theory, and combines this with Kalman filtering for future state estimations.", "result": "KPHD-Net is shown to outperform state-of-the-art methods in accuracy, robustness, and reliability for classification and clustering tasks through extensive experiments.", "conclusion": "The proposed KPHD-Net provides a theoretically robust and practically reliable solution for uncertainty estimation and multi-view fusion, addressing shortcomings of existing methods."}}
{"id": "2507.09837", "pdf": "https://arxiv.org/pdf/2507.09837", "abs": "https://arxiv.org/abs/2507.09837", "authors": ["Quang Truong", "Zhikai Chen", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Relational databases underpin critical infrastructure across a wide range of\ndomains, yet the design of generalizable pre-training strategies for learning\nfrom relational databases remains an open challenge due to task heterogeneity.\nSpecifically, there exist infinitely many possible downstream tasks, as tasks\nare defined based on relational schema graphs, temporal dependencies, and\nSQL-defined label logics. An effective pre-training framework is desired to\ntake these factors into account in order to obtain task-aware representations.\nBy incorporating knowledge of the underlying distribution that drives label\ngeneration, downstream tasks can benefit from relevant side-channel\ninformation. To bridge this gap, we introduce Task Vector Estimation (TVE), a\nnovel pre-training framework that constructs predictive supervisory signals via\nset-based aggregation over schema traversal graphs, explicitly modeling\nnext-window relational dynamics. We formalize our approach through an\ninformation-theoretic lens, demonstrating that task-informed representations\nretain more relevant signals than those obtained without task priors. Extensive\nexperiments on the RelBench benchmark show that TVE consistently outperforms\ntraditional pre-training baselines. Our findings advocate for pre-training\nobjectives that encode task heterogeneity and temporal structure as design\nprinciples for predictive modeling on relational databases.", "AI": {"tldr": "The paper introduces a novel pre-training framework named Task Vector Estimation (TVE) to enhance task-aware representations for relational databases, outperforming existing baselines.", "motivation": "Relational databases support critical infrastructure but face challenges due to task heterogeneity, making effective and generalizable pre-training strategies difficult to design.", "method": "TVE constructs supervisory signals through set-based aggregation of schema graphs and explicitly models temporal relational dynamics for pre-training.", "result": "Experimental results on the RelBench benchmark reveal that TVE consistently surpasses traditional pre-training methods.", "conclusion": "Incorporating task heterogeneity and temporal structures significantly improves predictive modeling in relational databases, as shown by TVE's superior performance."}}
{"id": "2507.09984", "pdf": "https://arxiv.org/pdf/2507.09984", "abs": "https://arxiv.org/abs/2507.09984", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "title": "Latent Diffusion Models with Masked AutoEncoders", "categories": ["cs.CV"], "comment": null, "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoder. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs). Through comprehensive experiments, we demonstrate significantly\nenhanced image generation quality and computational efficiency.", "AI": {"tldr": "This paper identifies weaknesses in current autoencoders within latent diffusion models (LDMs) and proposes Variational Masked AutoEncoders (VMAEs) to improve them. Integrating VMAEs with LDMs enhances image generation and computational performance.", "motivation": "Latent Diffusion Models (LDMs) excel in image generation, but the optimal autoencoder design has not been fully studied, leading to a gap in understanding their desired properties.", "method": "The authors propose Variational Masked AutoEncoders (VMAEs) that leverage hierarchical features from Masked AutoEncoders, then integrate them into the LDM framework, forming Latent Diffusion Models with Masked AutoEncoders (LDMAEs).", "result": "Experiments show that LDMAEs significantly improve image generation quality and computational efficiency compared to existing LDMs.", "conclusion": "LDMAEs address the limitations of current autoencoders in LDMs, delivering better performance in image synthesis and efficiency, establishing a promising new approach."}}
{"id": "2507.09839", "pdf": "https://arxiv.org/pdf/2507.09839", "abs": "https://arxiv.org/abs/2507.09839", "authors": ["MohammadReza Davari", "Utkarsh Garg", "Weixin Cai", "Eugene Belilovsky"], "title": "Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs", "categories": ["cs.LG"], "comment": null, "summary": "An increasing number of NLP applications interact with large language models\n(LLMs) through black-box APIs, making prompt engineering critical for\ncontrolling model outputs. While recent Automatic Prompt Optimization (APO)\nmethods iteratively refine prompts using model-generated feedback, textual\ngradients, they primarily focus on error correction and neglect valuable\ninsights from correct predictions. This limits both their effectiveness and\nefficiency. In this paper, we propose a novel APO framework centered on\nenhancing the feedback mechanism. We reinterpret the textual gradient as a form\nof negative reinforcement and introduce the complementary positive\nreinforcement to explicitly preserve beneficial prompt components identified\nthrough successful predictions. To mitigate the noise inherent in LLM-generated\nfeedback, we introduce a technique called feedback diversification, which\naggregates multiple feedback signals, emphasizing consistent, actionable advice\nwhile filtering out outliers. Motivated by the rapid evolution and diversity of\navailable LLMs, we also formalize Continual Prompt Optimization (CPO),\naddressing the practical challenge of efficiently migrating optimized prompts\nbetween different model versions or API providers. Our experiments reveal that\nnaive prompt migration often degrades performance due to loss of critical\ninstructions. In contrast, our approach consistently outperforms strong\nbaselines, achieving significant accuracy improvements, faster convergence, and\nlower computational costs in both standard and migration scenarios.", "AI": {"tldr": "The paper improves Automatic Prompt Optimization (APO) frameworks for better interaction with large language models (LLMs) by enhancing feedback mechanisms with positive reinforcement and feedback diversification, along with a new formalization for Continual Prompt Optimization (CPO).", "motivation": "Prompt engineering is increasingly critical in NLP applications interacting with LLMs via black-box APIs, yet existing APO methods are limited in handling valuable insights from correct predictions and managing prompt migration effectively.", "method": "The paper interprets textual gradients as negative reinforcement and introduces positive reinforcement to preserve beneficial prompt components. It also proposes feedback diversification to filter actionable advice and noise, formalizing Continual Prompt Optimization (CPO) for prompt migration between LLMs.", "result": "The authors demonstrate that their novel APO framework consistently outperforms baselines, showing significant accuracy improvements, faster convergence, and lower computational costs in both standard and migration scenarios.", "conclusion": "Enhancing the APO framework with improved feedback mechanisms and formalizing CPO leads to better prompt optimization, capable of addressing challenges posed by diverse and evolving LLMs, reducing performance degradation during migrations."}}
{"id": "2507.09993", "pdf": "https://arxiv.org/pdf/2507.09993", "abs": "https://arxiv.org/abs/2507.09993", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "categories": ["cs.CV"], "comment": "Submitted to WACV 2026", "summary": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving.", "AI": {"tldr": "The paper introduces 3D Gaussian-based Adversarial Attack (3DGAA), leveraging 3D Gaussian Splatting to create highly effective adversarial objects that substantially reduce detection performance in autonomous driving systems.", "motivation": "To address vulnerabilities in camera-based object detection systems in autonomous driving, and improve the balance between physical realism and attack robustness in adversarial attacks.", "method": "The method employs 3D Gaussian Splatting to jointly optimize geometry (shape, scale, rotation) and appearance (color, opacity), while incorporating physical filtering and augmentation modules to ensure physical realism and generalization.", "result": "3DGAA reduces detection mAP from 87.21% to 7.38% and showcases superior performance and transferability compared to prior 3D physical attacks.", "conclusion": "3DGAA provides a robust, physically realistic, and transferable adversarial attack framework, establishing a new benchmark for testing perception systems' safety in autonomous driving."}}
{"id": "2507.09871", "pdf": "https://arxiv.org/pdf/2507.09871", "abs": "https://arxiv.org/abs/2507.09871", "authors": ["Niket Patel", "Randall Balestriero"], "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.", "AI": {"tldr": "The paper critiques fixed downstream evaluation methods in AI and proposes a probabilistic framework using task priors to evaluate performance across all possible tasks.", "motivation": "Current AI evaluation methods are limited to fixed and hand-picked downstream benchmarks, creating bottlenecks in achieving the grand goal of solving any possible task.", "method": "The paper introduces a probabilistic space of downstream tasks by adopting a distribution of tasks and defining Task Priors to evaluate models comprehensively.", "result": "The framework answers key questions regarding average performance and variance of models across all possible downstream tasks based on task probabilities.", "conclusion": "Task Priors provide a new evaluation standard and could accelerate self-supervised learning research by improving the quality of downstream task evaluation signals."}}
{"id": "2507.10006", "pdf": "https://arxiv.org/pdf/2507.10006", "abs": "https://arxiv.org/abs/2507.10006", "authors": ["Guanghai Ding", "Yihua Ren", "Yuting Liu", "Qijun Zhao", "Shuiwang Li"], "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of UAV technology and its extensive application in\nvarious fields such as military reconnaissance, environmental monitoring, and\nlogistics, achieving efficient and accurate Anti-UAV tracking has become\nessential. The importance of Anti-UAV tracking is increasingly prominent,\nespecially in scenarios such as public safety, border patrol, search and\nrescue, and agricultural monitoring, where operations in complex environments\ncan provide enhanced security. Current mainstream Anti-UAV tracking\ntechnologies are primarily centered around computer vision techniques,\nparticularly those that integrate multi-sensor data fusion with advanced\ndetection and tracking algorithms. This paper first reviews the characteristics\nand current challenges of Anti-UAV detection and tracking technologies. Next,\nit investigates and compiles several publicly available datasets, providing\naccessible links to support researchers in efficiently addressing related\nchallenges. Furthermore, the paper analyzes the major vision-based and\nvision-fusion-based Anti-UAV detection and tracking algorithms proposed in\nrecent years. Finally, based on the above research, this paper outlines future\nresearch directions, aiming to provide valuable insights for advancing the\nfield.", "AI": {"tldr": "This paper discusses the advancements, challenges, and datasets of Anti-UAV tracking technologies, primarily focused on computer vision techniques and proposes future research directions.", "motivation": "The need for improved Anti-UAV tracking technologies has grown due to the increasing use of UAVs in fields like public safety, border patrol, and environmental monitoring, necessitating efficient and accurate solutions for complex environments.", "method": "The paper reviews current Anti-UAV detection and tracking technologies, compiles publicly available datasets, analyzes vision-based and vision-fusion-based algorithms, and identifies future research directions.", "result": "A comprehensive synthesis of current Anti-UAV technologies, accessible datasets, and in-depth analysis of detection and tracking algorithms.", "conclusion": "The paper provides an overview of existing challenges and progress in Anti-UAV tracking, offering actionable datasets and insights for future innovation in this critical field."}}
{"id": "2507.09882", "pdf": "https://arxiv.org/pdf/2507.09882", "abs": "https://arxiv.org/abs/2507.09882", "authors": ["Jiamin Wu", "Zichen Ren", "Junyu Wang", "Pengyu Zhu", "Yonghao Song", "Mianxin Liu", "Qihao Zheng", "Lei Bai", "Wanli Ouyang", "Chunfeng Song"], "title": "AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications", "categories": ["cs.LG"], "comment": null, "summary": "Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible\nmeans of connecting the human brain to external devices, with broad\napplications in home and clinical settings to enhance human capabilities.\nHowever, the high noise level and limited task-specific data in non-invasive\nsignals constrain decoding capabilities. Recently, the adoption of\nself-supervised pre-training is transforming the landscape of non-invasive BCI\nresearch, enabling the development of brain foundation models to capture\ngeneric neural representations from large-scale unlabeled\nelectroencephalography (EEG) signals with substantial noises. However, despite\nthese advances, the field currently lacks comprehensive, practical and\nextensible benchmarks to assess the utility of the public foundation models\nacross diverse BCI tasks, hindering their widespread adoption. To address this\nchallenge, we present AdaBrain-Bench, a large-scale standardized benchmark to\nsystematically evaluate brain foundation models in widespread non-invasive BCI\ntasks. AdaBrain-Bench encompasses a diverse collection of representative BCI\ndecoding datasets spanning 7 key applications. It introduces a streamlined task\nadaptation pipeline integrated with multi-dimensional evaluation metrics and a\nset of adaptation tools. The benchmark delivers an inclusive framework for\nassessing generalizability of brain foundation models across key transfer\nsettings, including cross-subject, multi-subject, and few-shot scenarios. We\nleverage AdaBrain-Bench to evaluate a suite of publicly available brain\nfoundation models and offer insights into practices for selecting appropriate\nmodels in various scenarios. We make our benchmark pipeline available to enable\nreproducible research and external use, offering a continuously evolving\nplatform to foster progress toward robust and generalized neural decoding\nsolutions.", "AI": {"tldr": "The paper introduces AdaBrain-Bench, a benchmark aiming to systematically evaluate brain foundation models in non-invasive BCI tasks, addressing gaps in practical and extensible assessment frameworks.", "motivation": "Non-invasive Brain-Computer Interfaces face challenges like high noise and limited task-specific data, limiting decoding capabilities, and the adoption of self-supervised pre-training lacks comprehensive benchmarks.", "method": "The authors present AdaBrain-Bench, a benchmark incorporating standardized datasets spanning 7 applications, streamlined task adaptation pipelines, multidimensional evaluation metrics, and a suite of adaptation tools.", "result": "AdaBrain-Bench evaluates multiple brain foundation models and provides insights into model selection practices for adapting to diverse BCI tasks.", "conclusion": "AdaBrain-Bench delivers a reproducible, dynamic framework to advance research in robust and generalized neural decoding solutions by fostering widespread adoption and progress."}}
{"id": "2507.10009", "pdf": "https://arxiv.org/pdf/2507.10009", "abs": "https://arxiv.org/abs/2507.10009", "authors": ["Geyou Zhang", "Kai Liu", "Ce Zhu"], "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry", "categories": ["cs.CV"], "comment": null, "summary": "Phase shifting profilometry (PSP) is widely used in high-precision 3D\nscanning due to its high accuracy, robustness, and pixel-wise handling.\nHowever, a fundamental assumption of PSP that the object should remain static\ndoes not hold in dynamic measurement, making PSP susceptible to object motion.\nTo address this challenge, our proposed solution, phase-sequential binomial\nself-compensation (P-BSC), sums successive motion-affected phase frames\nweighted by binomial coefficients. This approach exponentially reduces the\nmotion error in a pixel-wise and frame-wise loopable manner. Despite its\nefficacy, P-BSC suffers from high computational overhead and error accumulation\ndue to its reliance on multi-frame phase calculations and weighted summations.\nInspired by P-BSC, we propose an image-sequential binomial self-compensation\n(I-BSC) to weight sum the homogeneous fringe images instead of successive phase\nframes, which generalizes the BSC concept from phase sequences to image\nsequences. I-BSC computes the arctangent function only once, resolving both\nlimitations in P-BSC. Extensive analysis, simulations, and experiments show\nthat 1) the proposed BSC outperforms existing methods in reducing motion error\nwhile achieving a quasi-single-shot frame rate, i.e., depth map frame rate\nequals to the camera's acquisition rate, enabling 3D reconstruction with high\npixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the\ncomputational complexity by one polynomial order, thereby accelerating the\ncomputational frame rate by several to dozen times, while also reaching faster\nmotion error convergence.", "AI": {"tldr": "The paper addresses the challenge of object motion in phase shifting profilometry (PSP) with a new method, I-BSC, which improves motion error reduction and computational efficiency.", "motivation": "PSP, a high-precision 3D scanning technique, struggles with motion susceptibility in dynamic environments due to its assumption of static objects. Addressing this limitation enhances its applicability to dynamic measurements.", "method": "The authors propose extending the binomial self-compensation (BSC) concept from phase sequences to image sequences (I-BSC), which weights and sums homogeneous fringe images, computing the arctangent function once to improve efficiency and reduce errors.", "result": "The proposed I-BSC method outperforms other methods in reducing motion errors, achieves a quasi-single-shot frame rate for 3D reconstruction, accelerates the computational rate significantly compared to P-BSC, and enables faster motion error convergence.", "conclusion": "I-BSC presents a computationally efficient and effective solution to PSP's motion susceptibility, improving error reduction and enabling high-resolution, real-time 3D scanning in dynamic scenarios."}}
{"id": "2507.09887", "pdf": "https://arxiv.org/pdf/2507.09887", "abs": "https://arxiv.org/abs/2507.09887", "authors": ["Huynh Nguyen Dang", "Thang Pham", "Ngan Le", "Van Nguyen"], "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "10 pages, 6 figures. Accepted to ACM Multimedia 2025", "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.", "AI": {"tldr": "The paper introduces TolerantECG, a noise-resistant and flexible foundation model for ECG analysis, utilizing contrastive and self-supervised learning frameworks.", "motivation": "To overcome issues in heart disease diagnosis caused by noise or unavailable leads in standard 12-lead ECG recordings.", "method": "TolerantECG is trained using contrastive and self-supervised learning to unify ECG signal representation and retrieval-based text report descriptions, accommodating corrupted or incomplete signals.", "result": "The model excels in benchmarking tests, ranking as one of the top performers across various conditions in the PTB-XL dataset and demonstrating superior performance in the MIT-BIH Arrhythmia Database.", "conclusion": "TolerantECG is a robust tool for accurate ECG analysis under challenging signal conditions, minimizing diagnostic errors and uncertainty."}}
{"id": "2507.10015", "pdf": "https://arxiv.org/pdf/2507.10015", "abs": "https://arxiv.org/abs/2507.10015", "authors": ["Jaisidh Singh", "Diganta Misra", "Boris Knyazev", "Antonio Orvieto"], "title": "(Almost) Free Modality Stitching of Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Pre-print", "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.", "AI": {"tldr": "This paper addresses the computational challenge in multi-modal model creation by proposing Hyma, a hypernetwork-based framework.", "motivation": "The paper aims to simplify the selection and training of connectors between uni-modal models for multi-modal applications, tackling the complexity due to the growing availability of pretrained models and large-scale datasets.", "method": "It introduces Hyma, leveraging hypernetworks for simultaneous uni-modal model selection and connector training, enabling quicker identification of optimal combinations.", "result": "Hyma reduced the uni-modal model pair search cost by 10x and achieved competitive performance in connector training compared to traditional methods.", "conclusion": "Hyma offers an efficient and effective solution for multi-modal model design, streamlining the process and reducing resource demands."}}
{"id": "2507.09890", "pdf": "https://arxiv.org/pdf/2507.09890", "abs": "https://arxiv.org/abs/2507.09890", "authors": ["Ping Xu", "Pengfei Wang", "Zhiyuan Ning", "Meng Xiao", "Min Wu", "Yuanchun Zhou"], "title": "Soft Graph Clustering for single-cell RNA Sequencing Data", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)\ndata analysis for elucidating cellular heterogeneity and diversity. Recent\ngraph-based scRNA-seq clustering methods, particularly graph neural networks\n(GNNs), have significantly improved in tackling the challenges of\nhigh-dimension, high-sparsity, and frequent dropout events that lead to\nambiguous cell population boundaries. However, their reliance on hard graph\nconstructions derived from thresholded similarity matrices presents\nchallenges:(i) The simplification of intercellular relationships into binary\nedges (0 or 1) by applying thresholds, which restricts the capture of\ncontinuous similarity features among cells and leads to significant information\nloss.(ii) The presence of significant inter-cluster connections within hard\ngraphs, which can confuse GNN methods that rely heavily on graph structures,\npotentially causing erroneous message propagation and biased clustering\noutcomes. To tackle these challenges, we introduce scSGC, a Soft Graph\nClustering for single-cell RNA sequencing data, which aims to more accurately\ncharacterize continuous similarities among cells through non-binary edge\nweights, thereby mitigating the limitations of rigid data structures. The scSGC\nframework comprises three core components: (i) a zero-inflated negative\nbinomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed\nsoft graph embedding module; and (iii) an optimal transport-based clustering\noptimization module. Extensive experiments across ten datasets demonstrate that\nscSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,\ncell type annotation, and computational efficiency. These results highlight its\nsubstantial potential to advance scRNA-seq data analysis and deepen our\nunderstanding of cellular heterogeneity.", "AI": {"tldr": "The paper introduces scSGC, a novel clustering method for single-cell RNA sequencing (scRNA-seq) data that utilizes soft graph structures to overcome traditional limitations of graph-based methods.", "motivation": "Existing graph-based clustering methods in scRNA-seq analysis, especially graph neural networks (GNNs), face challenges due to their reliance on hard graph constructions, which oversimplify cellular relationships and lead to information loss and errors in clustering.", "method": "The scSGC framework includes three major modules: a zero-inflated negative binomial (ZINB)-based feature autoencoder, a dual-channel cut-informed soft graph embedding module, and an optimal transport-based clustering optimization module.", "result": "Across ten scRNA-seq datasets, scSGC outperformed 13 state-of-the-art models in clustering accuracy, cell type annotation, and computational efficiency.", "conclusion": "scSGC significantly improves scRNA-seq clustering by mitigating limitations of hard graph constructions, potentially paving the way for more accurate analysis of cellular heterogeneity and diversity."}}
{"id": "2507.09149", "pdf": "https://arxiv.org/pdf/2507.09149", "abs": "https://arxiv.org/abs/2507.09149", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.LG", "I.2.7; J.4"], "comment": "11 Pages, 2 Figures, 3 Tables conference paper to appear in\n  proceedings of International Conference on Artificial Intelligence, Computer,\n  Data Sciences and Applications (ACDSA'25)", "summary": "Health misinformation during the COVID-19 pandemic has significantly\nchallenged public health efforts globally. This study applies the Elaboration\nLikelihood Model (ELM) to enhance misinformation detection on social media\nusing a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory\n(LSTM) model. The model aims to enhance the detection accuracy and reliability\nof misinformation classification by integrating ELM-based features such as text\nreadability, sentiment polarity, and heuristic cues (e.g., punctuation\nfrequency). The enhanced model achieved an accuracy of 97.37%, precision of\n96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined\nmodel incorporating feature engineering further improved performance, achieving\na precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of\n99.80%. These findings highlight the value of ELM features in improving\ndetection performance, offering valuable contextual information. This study\ndemonstrates the practical application of psychological theories in developing\nadvanced machine learning algorithms to address health misinformation\neffectively.", "AI": {"tldr": "The study significantly enhances COVID-19 health misinformation detection on social media by integrating ELM-based features into a hybrid CNN-LSTM model, achieving exceptional accuracy and reliability.", "motivation": "To combat the challenge of health misinformation during the COVID-19 pandemic and improve the accuracy of misinformation detection methods on social media.", "method": "The study utilizes the Elaboration Likelihood Model (ELM) to incorporate features like text readability, sentiment polarity, and heuristic cues into a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model.", "result": "The enhanced model reached high performance metrics including 97.37% accuracy, 99.80% ROC-AUC, and strong precision, recall, and F1-scores. A combined approach further boosted effectiveness.", "conclusion": "ELM-based features offer significant improvements in misinformation detection, showcasing the applicability of psychological theories in machine learning algorithms to tackle health misinformation challenges."}}
{"id": "2507.10029", "pdf": "https://arxiv.org/pdf/2507.10029", "abs": "https://arxiv.org/abs/2507.10029", "authors": ["Seokeon Choi", "Sunghyun Park", "Hyoungwoo Park", "Jeongho Kim", "Sungrack Yun"], "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.", "AI": {"tldr": "The paper introduces a personalized text-to-image diffusion model method combining backpropagation and zeroth-order optimization to achieve memory-efficient and high-quality fine-tuning.", "motivation": "To enable memory-efficient personalization of text-to-image diffusion models suitable for edge devices while preserving user privacy and maintaining high-quality results.", "method": "The framework alternates between backpropagation on low-resolution images (BP-low) and zeroth-order optimization on high-resolution images (ZO-high), guided dynamically by a timestep-aware probabilistic function.", "result": "The approach achieves competitive performance with reduced memory usage, enabling high-quality on-device personalization without increasing inference latency.", "conclusion": "By integrating BP-low and ZO-high methods, the framework provides a practical solution for scalable, efficient, and privacy-preserving fine-tuning of diffusion models for edge applications."}}
{"id": "2507.09925", "pdf": "https://arxiv.org/pdf/2507.09925", "abs": "https://arxiv.org/abs/2507.09925", "authors": ["Md Ahsanul Kabir", "Abrar Jahin", "Mohammad Al Hasan"], "title": "Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model", "categories": ["cs.LG"], "comment": null, "summary": "Extracting cause and effect phrases from a sentence is an important NLP task,\nwith numerous applications in various domains, including legal, medical,\neducation, and scientific research. There are many unsupervised and supervised\nmethods proposed for solving this task. Among these, unsupervised methods\nutilize various linguistic tools, including syntactic patterns, dependency\ntree, dependency relations, etc. among different sentential units for\nextracting the cause and effect phrases. On the other hand, the contemporary\nsupervised methods use various deep learning based mask language models\nequipped with a token classification layer for extracting cause and effect\nphrases. Linguistic tools, specifically, dependency tree, which organizes a\nsentence into different semantic units have been shown to be very effective for\nextracting semantic pairs from a sentence, but existing supervised methods do\nnot have any provision for utilizing such tools within their model framework.\nIn this work, we propose DepBERT, which extends a transformer-based model by\nincorporating dependency tree of a sentence within the model framework.\nExtensive experiments over three datasets show that DepBERT is better than\nvarious state-of-the art supervised causality extraction methods.", "AI": {"tldr": "The paper introduces DepBERT, a transformer-based model integrating dependency tree structures to enhance cause-effect phrase extraction, outperforming existing supervised methods.", "motivation": "Existing supervised methods for extracting cause and effect phrases fail to integrate dependency tree structures, which are proven effective for semantic extraction.", "method": "DepBERT incorporates dependency trees into a transformer-based model framework, aiming to leverage syntactic organization for improved semantic pair extraction.", "result": "Experiments across three datasets reveal that DepBERT surpasses state-of-the-art supervised causality extraction methods in performance.", "conclusion": "The integration of dependency trees in supervised deep learning models like DepBERT can significantly enhance causality extraction tasks."}}
{"id": "2507.09158", "pdf": "https://arxiv.org/pdf/2507.09158", "abs": "https://arxiv.org/abs/2507.09158", "authors": ["Sunil Munthumoduku Krishna Murthy", "Kumar Rajamani", "Srividya Tirunellai Rajamani", "Yupei Li", "Qiyang Sun", "Bjoern W. Schuller"], "title": "Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In spinal vertebral mobility disease, accurately extracting and contouring\nvertebrae is essential for assessing mobility impairments and monitoring\nvariations during flexion-extension movements. Precise vertebral contouring\nplays a crucial role in surgical planning; however, this process is\ntraditionally performed manually by radiologists or surgeons, making it\nlabour-intensive, time-consuming, and prone to human error. In particular,\nmobility disease analysis requires the individual contouring of each vertebra,\nwhich is both tedious and susceptible to inconsistencies. Automated methods\nprovide a more efficient alternative, enabling vertebra identification,\nsegmentation, and contouring with greater accuracy and reduced time\nconsumption. In this study, we propose a novel U-Net variation designed to\naccurately segment thoracic vertebrae from anteroposterior view on X-Ray\nimages. Our proposed approach, incorporating a ``sandwich\" U-Net structure with\ndual activation functions, achieves a 4.1\\% improvement in Dice score compared\nto the baseline U-Net model, enhancing segmentation accuracy while ensuring\nreliable vertebral contour extraction.", "AI": {"tldr": "The study introduces a novel U-Net variation to enhance thoracic vertebrae segmentation in X-Ray images, achieving a 4.1% improvement in Dice score over traditional methods.", "motivation": "Traditional vertebrae contouring for mobility disease analysis is manual, time-consuming, and error-prone. Automation is needed for efficiency and consistency.", "method": "A novel 'sandwich' U-Net structure with dual activation functions is used to segment thoracic vertebrae from anteroposterior X-Ray images.", "result": "The proposed model improved segmentation accuracy with a 4.1% higher Dice score compared to the baseline U-Net model.", "conclusion": "The new U-Net variation enhances segmentation accuracy, offering a reliable and efficient solution for vertebral contouring in mobility disease analysis."}}
{"id": "2507.10053", "pdf": "https://arxiv.org/pdf/2507.10053", "abs": "https://arxiv.org/abs/2507.10053", "authors": ["Marc Serra Ortega", "Emanuele Vivoli", "Artemis Llabr\u00e9s", "Dimosthenis Karatzas"], "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream\nSegmentation (PSS) in comic books, a critical task for automated content\nunderstanding, as it is a necessary first stage for many downstream tasks like\ncharacter analysis, story indexing, or metadata enrichment. We formalize PSS\nfor this unique medium and curate a new 20,800-page annotated dataset. CoSMo,\ndeveloped in vision-only and multimodal variants, consistently outperforms\ntraditional baselines and significantly larger general-purpose vision-language\nmodels across F1-Macro, Panoptic Quality, and stream-level metrics. Our\nfindings highlight the dominance of visual features for comic PSS\nmacro-structure, yet demonstrate multimodal benefits in resolving challenging\nambiguities. CoSMo establishes a new state-of-the-art, paving the way for\nscalable comic book analysis.", "AI": {"tldr": "The paper introduces CoSMo, a multimodal Transformer for Page Stream Segmentation (PSS) in comic books, which outperforms existing models and sets new benchmarks.", "motivation": "To address the challenge of automated Page Stream Segmentation (PSS) in comic books, a prerequisite for advanced downstream content analysis.", "method": "They developed CoSMo in both vision-only and multimodal variants, leveraging annotated data to outperform baseline and general-purpose models across several metrics.", "result": "CoSMo performed better than traditional baselines and larger models in F1-Macro, Panoptic Quality, and stream-level metrics, with additional benefits from multimodal features.", "conclusion": "CoSMo sets a new state-of-the-art for comic book PSS, emphasizing the value of visual features with added multimodal benefits for ambiguous cases."}}
{"id": "2507.09931", "pdf": "https://arxiv.org/pdf/2507.09931", "abs": "https://arxiv.org/abs/2507.09931", "authors": ["Yoon Pyo Lee"], "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures", "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.", "AI": {"tldr": "This paper develops a methodology to interpret how Large Language Models (LLMs), specifically fine-tuned for nuclear engineering, utilize domain-specific knowledge by analyzing neuron activation patterns.", "motivation": "The motivation is to enhance the transparency and trust in LLMs to enable their application in safety-critical fields like nuclear engineering, addressing verification and validation requirements.", "method": "The authors fine-tuned an LLM (Gemma-3-1b-it) using Low-Rank Adaptation and analyzed neuron activation patterns pre- and post-fine-tuning. They probed specialized neurons through silencing techniques to assess their causal roles.", "result": "The study found that while silencing individual specialized neurons had minimal effect, deactivating the group collectively caused significant task performance degradation. The silencing also impaired the accuracy of technical information generation.", "conclusion": "The paper demonstrates a practical methodology to interpret black-box LLMs in safety-critical domains and proposes a pathway for achieving compliance with nuclear regulatory requirements by linking domain expertise to neural circuits for improved assurance."}}
{"id": "2507.10056", "pdf": "https://arxiv.org/pdf/2507.10056", "abs": "https://arxiv.org/abs/2507.10056", "authors": ["A. K. M. Shoriful Islam", "Md. Rakib Hassan", "Macbah Uddin", "Md. Shahidur Rahman"], "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.", "AI": {"tldr": "The paper introduces a lightweight machine learning model for detecting poultry diseases through fecal image analysis, achieving high accuracy with minimal computational resources, particularly suitable for low-resource settings.", "motivation": "Address the vulnerability of poultry farming to infectious diseases by developing an efficient diagnostic tool suitable for low-resource environments.", "method": "The authors utilize multi-color space feature extraction and descriptors, implement PCA and XGBoost for feature selection, and train an ANN classifier for disease detection.", "result": "The ANN classifier achieved 95.85% accuracy in disease detection, outperforming deep learning models in terms of resource efficiency.", "conclusion": "The proposed approach provides a cost-effective, interpretable, and scalable solution for real-time poultry disease detection, making it ideal for low-resource agricultural settings."}}
{"id": "2507.09937", "pdf": "https://arxiv.org/pdf/2507.09937", "abs": "https://arxiv.org/abs/2507.09937", "authors": ["Gaurav R. Ghosal", "Pratyush Maini", "Aditi Raghunathan"], "title": "Memorization Sinks: Isolating Memorization during LLM Training", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the 2025 International Conference of Machine Learning", "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.", "AI": {"tldr": "This paper proposes a new method called MemSinks to effectively separate memorization from general language capabilities in large language models, addressing privacy and copyright concerns caused by model memorization.", "motivation": "Language models often memorize repeated sequences, raising issues related to privacy and intellectual property. Current post-hoc techniques to address this memorization have been largely ineffective, particularly for natural and linguistically plausible text.", "method": "The authors introduce MemSinks, a technique that uses sequence identifiers to activate a distinct set of memorization neurons for each sequence. This promotes isolation of memorization during training and makes it easier to identify and remove memorized content without affecting general language abilities.", "result": "The implementation of MemSinks at billion-parameter and billion-token scale shows effective isolation of memorized content while maintaining strong language generalization capabilities. The results provide the first real-world demonstration of concurrent generalization and memorization isolation.", "conclusion": "MemSinks offers a promising approach to mitigating memorization in large language models by isolating memorized sequences without degrading their ability to generalize. This work lays the groundwork for further exploration and practical adoption in addressing memorization-related challenges."}}
{"id": "2507.10065", "pdf": "https://arxiv.org/pdf/2507.10065", "abs": "https://arxiv.org/abs/2507.10065", "authors": ["Chenguo Lin", "Yuchen Lin", "Panwang Pan", "Yifan Yu", "Honglei Yan", "Katerina Fragkiadaki", "Yadong Mu"], "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "categories": ["cs.CV"], "comment": "Project page: https://chenguolin.github.io/projects/MoVieS", "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.", "AI": {"tldr": "MoVieS is a fast feed-forward model capable of synthesizing dynamic 4D novel views from monocular videos in one second, combining appearance, geometry, and motion representation.", "motivation": "To unify appearance, geometry, and motion representation in dynamic 3D scenes for efficient view synthesis, reconstruction, and related tasks in a single framework.", "method": "MoVieS uses pixel-aligned grids of Gaussian primitives with explicit supervision of their time-varying motion, enabling large-scale training with minimal task-specific supervision.", "result": "The model achieves competitive performance on various tasks, such as scene flow estimation and moving object segmentation, while being extremely fast.", "conclusion": "MoVieS bridges dynamic geometry reconstruction with novel view synthesis, supporting diverse zero-shot applications and offering significant efficiency gains."}}
{"id": "2507.09940", "pdf": "https://arxiv.org/pdf/2507.09940", "abs": "https://arxiv.org/abs/2507.09940", "authors": ["Taigo Sakai", "Kazuhiro Hotta"], "title": "Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In conventional deep learning, the number of neurons typically remains fixed\nduring training. However, insights from biology suggest that the human\nhippocampus undergoes continuous neuron generation and pruning of neurons over\nthe course of learning, implying that a flexible allocation of capacity can\ncontribute to enhance performance. Real-world datasets often exhibit class\nimbalance situations where certain classes have far fewer samples than others,\nleading to significantly reduce recognition accuracy for minority classes when\nrelying on fixed size networks.To address the challenge, we propose a method\nthat periodically adds and removes neurons during training, thereby boosting\nrepresentational power for minority classes. By retaining critical features\nlearned from majority classes while selectively increasing neurons for\nunderrepresented classes, our approach dynamically adjusts capacity during\ntraining. Importantly, while the number of neurons changes throughout training,\nthe final network size and structure remain unchanged, ensuring efficiency and\ncompatibility with deployment.Furthermore, by experiments on three different\ndatasets and five representative models, we demonstrate that the proposed\nmethod outperforms fixed size networks and shows even greater accuracy when\ncombined with other imbalance-handling techniques. Our results underscore the\neffectiveness of dynamic, biologically inspired network designs in improving\nperformance on class-imbalanced data.", "AI": {"tldr": "The paper introduces a biologically-inspired method that dynamically adds and removes neurons during training to improve minority class recognition in class-imbalanced datasets.", "motivation": "The motivation is to address inefficiencies in handling imbalanced datasets with fixed-size neural networks and draw inspiration from how the human hippocampus flexibly adjusts capacity during learning.", "method": "The method proposed involves periodically adding and removing neurons for minority class representation while maintaining the final network size unchanged for deployment efficiency.", "result": "Experiments on three datasets and five models reveal enhanced performance on class-imbalanced data compared to fixed-size networks, with additional improvements when paired with imbalance-handling strategies.", "conclusion": "Dynamic allocation of neurons during training, inspired by biological processes, significantly improves class imbalance handling and recognition accuracy without compromising deployment efficiency."}}
{"id": "2507.10072", "pdf": "https://arxiv.org/pdf/2507.10072", "abs": "https://arxiv.org/abs/2507.10072", "authors": ["Meng Yu", "Kun Zhan"], "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models", "categories": ["cs.CV"], "comment": "ACM Multimedia 2025 accepted!", "summary": "Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of the predicted noisy images decreases during the\ndiffusion process. Building on this, we identify two important findings: 1) The\nreduction in energy follows distinct patterns in the low-frequency and\nhigh-frequency subbands; 2) This energy reduction results in amplitude\nvariations between the network-reconstructed clean data and the real clean\ndata. Based on the first finding, we introduce a frequency-domain regulation\nmechanism utilizing wavelet transforms, which separately adjusts the low- and\nhigh-frequency subbands. Leveraging the second insight, we provide a more\naccurate analysis of exposure bias in the two subbands. Our method is\ntraining-free and plug-and-play, significantly improving the generative quality\nof various diffusion models and providing a robust solution to exposure bias\nacross different model architectures. The source code is available at\nhttps://github.com/kunzhan/wpp.", "AI": {"tldr": "This paper addresses exposure bias in diffusion models by analyzing energy reduction patterns in noisy images and introducing a wavelet-based frequency-domain regulation mechanism.", "motivation": "Exposure bias in diffusion models hinders their generative performance. The study aims to understand and mitigate this bias by analyzing energy dynamics during the diffusion process.", "method": "The proposed method uses wavelet transforms to regulate high- and low-frequency subbands separately, based on observed energy patterns, providing a training-free, plug-and-play solution to improve generative quality.", "result": "The approach enhances the generative quality of various diffusion models and proves robust against exposure bias across different architectures.", "conclusion": "The frequency-domain regulation effectively addresses exposure bias in diffusion models, boosting generative capabilities and model robustness."}}
{"id": "2507.09227", "pdf": "https://arxiv.org/pdf/2507.09227", "abs": "https://arxiv.org/abs/2507.09227", "authors": ["Sanyam Jain", "Bruna Neves de Freitas", "Andreas Basse-OConnor", "Alexandros Iosifidis", "Ruben Pauwels"], "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "There has been increasing interest in the generation of high-quality,\nrealistic synthetic medical images in recent years. Such synthetic datasets can\nmitigate the scarcity of public datasets for artificial intelligence research,\nand can also be used for educational purposes. In this paper, we propose a\ncombination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)\nfor generating synthetic dental panoramic radiographs (PRs). The former\ngenerates a low-resolution (LR) seed of a PR (256 X 128) which is then\nprocessed by the SR model to yield a high-resolution (HR) PR of size 1024 X\n512. For SR, we propose a state-of-the-art transformer that learns local-global\nrelationships, resulting in sharper edges and textures. Experimental results\ndemonstrate a Frechet inception distance score of 40.69 between 7243 real and\nsynthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for\nreal HR, synthetic HR, real LR and synthetic LR images, respectively. Among a\ndiverse group of six clinical experts, all evaluating a mixture of 100\nsynthetic and 100 real PRs in a time-limited observation, the average accuracy\nin distinguishing real from synthetic images was 68.5% (with 50% corresponding\nto random guessing).", "AI": {"tldr": "This paper proposes a method combining diffusion-based generation (PanoDiff) and Super-Resolution (SR) to create high-resolution synthetic dental panoramic radiographs (PRs).", "motivation": "The motivation is to address the scarcity of publicly available medical datasets for AI research and education by generating high-quality, synthetic medical images.", "method": "The authors introduced a two-step method where a low-resolution PR is generated using PanoDiff and enhanced to high-resolution using a novel transformer-based SR model that learns local and global features.", "result": "The generated synthetic images achieved a Frechet inception distance score of 40.69, and clinical experts had an average accuracy of 68.5% in distinguishing real from synthetic images.", "conclusion": "The proposed approach successfully creates realistic synthetic medical images that are challenging to differentiate from actual ones, providing a promising avenue for dataset enhancement in medical imaging."}}
{"id": "2507.10084", "pdf": "https://arxiv.org/pdf/2507.10084", "abs": "https://arxiv.org/abs/2507.10084", "authors": ["Haonan Chen", "Xin Tong"], "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 6 figures, 2 tables", "summary": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.", "AI": {"tldr": "The study introduces a two-stage transfer learning strategy using the SegFormer model to improve water body segmentation in remote sensing images, enhancing IoU performance from 25.50% to 64.84%.", "motivation": "To address the challenges of domain shift and small sample sizes in remote sensing image segmentation for water bodies, particularly in complex and data-scarce environments.", "method": "A two-stage transfer learning strategy based on the SegFormer model: train on a diverse source domain and fine-tune on a target domain.", "result": "The strategy increased IoU for water body segmentation in the Zhada Tulin area from 25.50% to 64.84%.", "conclusion": "The proposed method effectively addresses domain discrepancies, offering a robust paradigm for remote sensing segmentation in unique and data-scarce regions."}}
{"id": "2507.09949", "pdf": "https://arxiv.org/pdf/2507.09949", "abs": "https://arxiv.org/abs/2507.09949", "authors": ["Md Ahsanul Kabir", "Kareem Abdelfatah", "Mohammed Korayem", "Mohammad Al Hasan"], "title": "Hierarchical Job Classification with Similarity Graph Integration", "categories": ["cs.LG"], "comment": null, "summary": "In the dynamic realm of online recruitment, accurate job classification is\nparamount for optimizing job recommendation systems, search rankings, and labor\nmarket analyses. As job markets evolve, the increasing complexity of job titles\nand descriptions necessitates sophisticated models that can effectively\nleverage intricate relationships within job data. Traditional text\nclassification methods often fall short, particularly due to their inability to\nfully utilize the hierarchical nature of industry categories. To address these\nlimitations, we propose a novel representation learning and classification\nmodel that embeds jobs and hierarchical industry categories into a latent\nembedding space. Our model integrates the Standard Occupational Classification\n(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both\ngraph and hierarchical relationships, thereby improving classification\naccuracy. By embedding hierarchical industry categories into a shared latent\nspace, we tackle cold start issues and enhance the dynamic matching of\ncandidates to job opportunities. Extensive experimentation on a large-scale\ndataset of job postings demonstrates the model's superior ability to leverage\nhierarchical structures and rich semantic features, significantly outperforming\nexisting methods. This research provides a robust framework for improving job\nclassification accuracy, supporting more informed decision-making in the\nrecruitment industry.", "AI": {"tldr": "This paper proposes a novel job classification model using hierarchical embeddings to improve recruitment processes.", "motivation": "Traditional text classification methods struggle to fully utilize the hierarchical nature of industry categories, which impacts job recommendation systems and labor market analysis.", "method": "A representation learning and classification model embedding jobs and industry hierarchies into a shared latent space using SOC and Carotene taxonomies.", "result": "Extensive tests on a large job dataset demonstrate the model's enhanced classification accuracy and ability to leverage hierarchical and semantic data.", "conclusion": "The study provides a robust framework for optimizing job classification, supporting better decision-making in the recruitment industry."}}
{"id": "2507.10095", "pdf": "https://arxiv.org/pdf/2507.10095", "abs": "https://arxiv.org/abs/2507.10095", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "categories": ["cs.CV"], "comment": null, "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To\nremedy this issue, we propose FIX-CLIP which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input.", "AI": {"tldr": "FIX-CLIP is introduced to address CLIP's limitations with long-text inputs by using a dual-branch pipeline, regional prompts, and feature alignment. It achieves state-of-the-art results on text retrieval benchmarks and enhances diffusion model performance.", "motivation": "CLIP faces performance challenges on tasks involving long-text inputs due to its limited text encoder input length.", "method": "FIX-CLIP employs three novel modules: a dual-branch training pipeline for aligning short and long texts, learnable regional prompts for information extraction, and hierarchical feature alignment in intermediate encoder layers. Additionally, it uses 30M images and MLLMs for generating long-text captions for training.", "result": "FIX-CLIP achieves state-of-the-art results on long-text and short-text retrieval benchmarks, demonstrating superior performance.", "conclusion": "FIX-CLIP significantly improves the ability of CLIP to handle long-text inputs while preserving performance on short-text cases, and it shows promising application potential for other downstream tasks like diffusion models."}}
{"id": "2507.09952", "pdf": "https://arxiv.org/pdf/2507.09952", "abs": "https://arxiv.org/abs/2507.09952", "authors": ["Zerui Zhang", "Yumou Qiu"], "title": "Radial Neighborhood Smoothing Recommender System", "categories": ["cs.LG", "stat.AP", "stat.ME", "68T01(General topics in artificial intelligence),\n  62G05(Nonparametric estimation)"], "comment": "34 pages, 2 figures. Submitted to NeurIPS 2025", "summary": "Recommender systems inherently exhibit a low-rank structure in latent space.\nA key challenge is to define meaningful and measurable distances in the latent\nspace to capture user-user, item-item, user-item relationships effectively. In\nthis work, we establish that distances in the latent space can be\nsystematically approximated using row-wise and column-wise distances in the\nobserved matrix, providing a novel perspective on distance estimation. To\nrefine the distance estimation, we introduce the correction based on empirical\nvariance estimator to account for noise-induced non-centrality. The novel\ndistance estimation enables a more structured approach to constructing\nneighborhoods, leading to the Radial Neighborhood Estimator (RNE), which\nconstructs neighborhoods by including both overlapped and partially overlapped\nuser-item pairs and employs neighborhood smoothing via localized kernel\nregression to improve imputation accuracy. We provide the theoretical\nasymptotic analysis for the proposed estimator. We perform evaluations on both\nsimulated and real-world datasets, demonstrating that RNE achieves superior\nperformance compared to existing collaborative filtering and matrix\nfactorization methods. While our primary focus is on distance estimation in\nlatent space, we find that RNE also mitigates the ``cold-start'' problem.", "AI": {"tldr": "The paper introduces a method for enhancing distance estimation in the latent space of recommender systems and proposes the Radial Neighborhood Estimator (RNE) to improve prediction accuracy and address the cold-start problem.", "motivation": "Existing recommender systems struggle to define accurate and meaningful distances in the latent space for capturing user-user, item-item, and user-item relationships.", "method": "The authors propose estimating latent space distances through row- and column-wise matrix distances, refining these estimates with a noise-correcting empirical variance estimator. RNE is introduced to construct neighborhoods and improve imputation accuracy using localized kernel regression.", "result": "Simulated and real-world dataset evaluations indicate that the RNE method outperforms traditional collaborative filtering and matrix factorization models.", "conclusion": "The novel distance estimation and RNE approach enhance overall recommendation accuracy and address challenges like the cold-start problem in recommender systems."}}
{"id": "2507.10115", "pdf": "https://arxiv.org/pdf/2507.10115", "abs": "https://arxiv.org/abs/2507.10115", "authors": ["Hamidreza Hashempoor"], "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association", "categories": ["cs.CV"], "comment": null, "summary": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.", "AI": {"tldr": "A framework for MCMT tracking maintaining consistent global ID assignment across camera views, leveraging trajectory and appearance data.", "motivation": "To address the challenge of maintaining consistent identity tracking across multiple cameras for multi-target scenarios.", "method": "Single-camera tracking via BoT-SORT, initialization with trajectory-feature matching, global matching strategy for IDs, and spatial validation using 3D positions derived from depth maps and calibration.", "result": "Reliable identity assignment across cameras and frames, ensuring seamless tracking consistency.", "conclusion": "The proposed system effectively matches tracklets to identities using prioritized strategies and spatial validation, minimizing ID errors across views."}}
{"id": "2507.09958", "pdf": "https://arxiv.org/pdf/2507.09958", "abs": "https://arxiv.org/abs/2507.09958", "authors": ["Zhenyuan Chen"], "title": "Rethinking Inductive Bias in Geographically Neural Network Weighted Regression", "categories": ["cs.LG"], "comment": null, "summary": "Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.", "AI": {"tldr": "The study revisits inductive biases in Geographically Neural Network Weighted Regression (GNNWR) and proposes enhancements to improve spatial regression performance using concepts from CNNs, RNNs, and transformers.", "motivation": "Understanding inductive bias is essential for improving spatial regression models and capturing complex patterns in limited data.", "method": "The paper generalizes GNNWR by integrating local receptive fields (via CNNs), sequential context (via RNNs), and self-attention mechanisms (via transformers) into spatial regression modeling.", "result": "GNNWR surpasses traditional methods in capturing nonlinear relationships, showing distinct advantages depending on data characteristics such as heterogeneity and sample size.", "conclusion": "Inductive biases significantly impact spatial modeling; leveraging novel neural architectures can improve performance and interpretability in non-stationary spatial data handling."}}
{"id": "2507.10118", "pdf": "https://arxiv.org/pdf/2507.10118", "abs": "https://arxiv.org/abs/2507.10118", "authors": ["Ivan Martinovi\u0107", "Josip \u0160ari\u0107", "Marin Or\u0161i\u0107", "Matej Kristan", "Sini\u0161a \u0160egvi\u0107"], "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation", "categories": ["cs.CV"], "comment": "ICCV 2025 Findings Workshop", "summary": "Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.", "AI": {"tldr": "The paper introduces a novel semi-supervised segmentation approach called DEARLi, leveraging foundation models to enhance both recognition and localization, outperforming state-of-the-art methods with fewer resources.", "motivation": "Pixel-level annotation is costly and time-intensive, and semi-supervised methods offer a solution by utilizing both labeled and unlabeled data. The paper aims to explore the integration of foundation models to address label scarcity and enhance efficiency in segmentation tasks.", "method": "The method involves unsupervised mask-transformer consistency combined with zero-shot classification of CLIP features to improve recognition. Localization is improved by warming up a class-agnostic decoder using SAM pseudo-labels. This decoupled strategy optimizes recognition and localization separately.", "result": "DEARLi achieves 29.9 PQ and 38.9 mIoU on the ADE20K dataset with only 158 labeled images. It outperforms current methods in semi-supervised segmentation by a large margin while using 8x less GPU memory.", "conclusion": "The proposed DEARLi model excels in handling large taxonomies and limited labeled datasets in semi-supervised scenarios, offering significant improvements in segmentation performance and resource efficiency."}}
{"id": "2507.09961", "pdf": "https://arxiv.org/pdf/2507.09961", "abs": "https://arxiv.org/abs/2507.09961", "authors": ["Lihua Zhou", "Mao Ye", "Nianxin Li", "Shuaifeng Li", "Jinlin Wu", "Xiatian Zhu", "Lei Deng", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "Text-Driven Causal Representation Learning for Source-Free Domain Generalization", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Deep learning often struggles when training and test data distributions\ndiffer. Traditional domain generalization (DG) tackles this by including data\nfrom multiple source domains, which is impractical due to expensive data\ncollection and annotation. Recent vision-language models like CLIP enable\nsource-free domain generalization (SFDG) by using text prompts to simulate\nvisual representations, reducing data demands. However, existing SFDG methods\nstruggle with domain-specific confounders, limiting their generalization\ncapabilities. To address this issue, we propose TDCRL\n(\\textbf{T}ext-\\textbf{D}riven \\textbf{C}ausal \\textbf{R}epresentation\n\\textbf{L}earning), the first method to integrate causal inference into the\nSFDG setting. TDCRL operates in two steps: first, it employs data augmentation\nto generate style word vectors, combining them with class information to\ngenerate text embeddings to simulate visual representations; second, it trains\na causal intervention network with a confounder dictionary to extract\ndomain-invariant features. Grounded in causal learning, our approach offers a\nclear and effective mechanism to achieve robust, domain-invariant features,\nensuring robust generalization. Extensive experiments on PACS, VLCS,\nOfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL\neffectiveness in SFDG.", "AI": {"tldr": "Traditional domain generalization methods are impractical due to data requirements, and source-free approaches using vision-language models face challenges with domain-specific confounders. The TDCRL method addresses these limitations using causal representation learning.", "motivation": "Deep learning's performance drops when training and testing data distributions differ, and traditional solutions require extensive data collection.", "method": "TDCRL combines style word vectors and text embeddings to simulate visual representations and uses causal intervention networks to extract domain-invariant features.", "result": "TDCRL achieves state-of-the-art results in source-free domain generalization tasks on datasets like PACS, VLCS, OfficeHome, and DomainNet.", "conclusion": "Integrating causal inference into source-free domain generalization ensures the extraction of domain-invariant features and leads to robust generalization capabilities."}}
{"id": "2507.10127", "pdf": "https://arxiv.org/pdf/2507.10127", "abs": "https://arxiv.org/abs/2507.10127", "authors": ["Md Abulkalam Azad", "John Nyberg", "H\u00e5vard Dalen", "Bj\u00f8rnar Grenne", "Lasse Lovstakken", "Andreas \u00d8stvik"], "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVAMD workshop at ICCV 2025", "summary": "Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.", "AI": {"tldr": "The paper enhances precision in echocardiographic tissue tracking by addressing motion bias and proposing improved training strategies and a lightweight network, achieving superior accuracy and reproducibility.", "motivation": "Existing motion estimation methods struggle with complex cardiac motion, and point tracking approaches remain underexplored in echocardiography.", "method": "Refining training strategies with tailored augmentations and developing a lightweight model leveraging multi-scale cost volumes from spatial context.", "result": "EchoTracker improves position accuracy by 60.7% and reduces trajectory error by 61.5% across heart cycle phases, outperforming many advanced models.", "conclusion": "The refined methods and proposed lightweight network show enhanced accuracy and generalization for echocardiographic tracking, supporting clinical application improvements."}}
{"id": "2507.09968", "pdf": "https://arxiv.org/pdf/2507.09968", "abs": "https://arxiv.org/abs/2507.09968", "authors": ["Xiangyu Sun", "Amin Yousefpour", "Shirin Hosseinmardi", "Ramin Bostanabad"], "title": "Compliance Minimization via Physics-Informed Gaussian Processes", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning (ML) techniques have recently gained significant attention\nfor solving compliance minimization (CM) problems. However, these methods\ntypically provide poor feature boundaries, are very expensive, and lack a\nsystematic mechanism to control the design complexity. Herein, we address these\nlimitations by proposing a mesh-free and simultaneous framework based on\nphysics-informed Gaussian processes (GPs). In our approach, we parameterize the\ndesign and state variables with GP priors which have independent kernels but\nshare a multi-output neural network (NN) as their mean function. The\narchitecture of this NN is based on Parametric Grid Convolutional Attention\nNetworks (PGCANs) which not only mitigate spectral bias issues, but also\nprovide an interpretable mechanism to control design complexity. We estimate\nall the parameters of our GP-based representations by simultaneously minimizing\nthe compliance, total potential energy, and residual of volume fraction\nconstraint. Importantly, our loss function exclude all data-based residuals as\nGPs automatically satisfy them. We also develop computational schemes based on\ncurriculum training and numerical integration to increase the efficiency and\nrobustness of our approach which is shown to (1) produce super-resolution\ntopologies with fast convergence, (2) achieve smaller compliance and less gray\narea fraction compared to traditional numerical methods, (3) provide control\nover fine-scale features, and (4) outperform competing ML-based methods.", "AI": {"tldr": "The paper presents a novel framework using physics-informed Gaussian processes to address limitations in compliance minimization problems, achieving improved design complexity control and computational efficiency.", "motivation": "Solve issues in machine learning methods for compliance minimization, including poor feature boundaries, high computational cost, and lack of design complexity control.", "method": "Design and state variables are parameterized using physics-informed Gaussian processes with neural networks as mean functions. These leverage Parametric Grid Convolutional Attention Networks (PGCANs).", "result": "The method produces super-resolution topologies with fast convergence, reduced compliance, less gray area fraction, and fine-scale design control while outperforming traditional and ML-based methods.", "conclusion": "The proposed framework effectively addresses limitations in CM problems and improves both performance and design interpretability through innovative use of Gaussian processes and advanced neural network architectures."}}
{"id": "2507.10143", "pdf": "https://arxiv.org/pdf/2507.10143", "abs": "https://arxiv.org/abs/2507.10143", "authors": ["David Calhas", "Arlindo L. Oliveira"], "title": "Deep Recurrence for Dynamical Segmentation Models", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages", "summary": "While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.", "AI": {"tldr": "This paper introduces a biologically inspired feedback mechanism in neural networks that enables iterative perception refinement, implemented in U-Net. Experiments show improved performance and generalization, especially in limited and noisy data settings.", "motivation": "The paper aims to address the limitation of most artificial neural networks that operate in a purely feedforward manner, lacking the feedback connections vital to biological vision systems.", "method": "The authors propose a predictive coding inspired feedback mechanism with recurrent loops from output to input, integrated into a U-Net architecture. Biologically motivated operations like softmax projection and exponential decay are used to stabilize the feedback loop.", "result": "The feedback model significantly outperformed the feedforward counterpart in synthetic segmentation tasks under noisy and limited data conditions. It performed above random with only two training examples, whereas the feedforward model needed at least four.", "conclusion": "The study demonstrates that feedback mechanisms enhance robustness and data efficiency in neural networks, showing potential for creating more adaptive architectures inspired by biological processes."}}
{"id": "2507.10171", "pdf": "https://arxiv.org/pdf/2507.10171", "abs": "https://arxiv.org/abs/2507.10171", "authors": ["Youngmin Kim", "Giyeong Oh", "Kwangsoo Youm", "Youngjae Yu"], "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.", "AI": {"tldr": "SlumpGuard is an AI-powered, video-based system for real-time concrete workability assessment, addressing limitations of traditional manual slump testing.", "motivation": "Traditional slump testing is manual, time-intensive, and inconsistent, hindering real-time concrete workability assessment during construction.", "method": "SlumpGuard analyzes video footage of concrete flow from truck chutes using AI to perform automated, real-time workability assessments.", "result": "SlumpGuard demonstrated improved efficiency and accuracy in full-batch concrete quality control during real-world deployments.", "conclusion": "SlumpGuard provides a practical and modern solution to enhance concrete workability monitoring, improving quality assurance in construction."}}
{"id": "2507.10014", "pdf": "https://arxiv.org/pdf/2507.10014", "abs": "https://arxiv.org/abs/2507.10014", "authors": ["Ali Sarabi", "Arash Sarabi", "Hao Yan", "Beckett Sterner", "Petar Jevti\u0107"], "title": "Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach", "categories": ["cs.LG", "92D30, 62M10, 68T07", "G.3; I.6.3; I.2.6; I.5.1"], "comment": null, "summary": "Coccidioidomycosis, commonly known as Valley Fever, remains a significant\npublic health concern in endemic regions of the southwestern United States.\nThis study develops the first graph neural network (GNN) model for forecasting\nValley Fever incidence in Arizona. The model integrates surveillance case data\nwith environmental predictors using graph structures, including soil\nconditions, atmospheric variables, agricultural indicators, and air quality\nmetrics. Our approach explores correlation-based relationships among variables\ninfluencing disease transmission. The model captures critical delays in disease\nprogression through lagged effects, enhancing its capacity to reflect complex\ntemporal dependencies in disease ecology. Results demonstrate that the GNN\narchitecture effectively models Valley Fever trends and provides insights into\nkey environmental drivers of disease incidence. These findings can inform early\nwarning systems and guide resource allocation for disease prevention efforts in\nhigh-risk areas.", "AI": {"tldr": "A graph neural network was developed to forecast Valley Fever incidence in Arizona, integrating case data and environmental predictors.", "motivation": "Addressing the public health concern of Valley Fever by creating accurate forecasting methods.", "method": "Designed a graph neural network incorporating surveillance case data and environmental factors, including lagged effects.", "result": "The GNN successfully modeled Valley Fever trends and identified critical environmental drivers.", "conclusion": "This work supports early warning systems and resource allocation for prevention in endemic areas."}}
{"id": "2507.10195", "pdf": "https://arxiv.org/pdf/2507.10195", "abs": "https://arxiv.org/abs/2507.10195", "authors": ["Shuyu Yang", "Yaxiong Wang", "Yongrui Li", "Li Zhu", "Zhedong Zheng"], "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.", "AI": {"tldr": "The paper proposes a dual-level domain adaptation pipeline for text-based person retrieval using synthetic data. It achieves state-of-the-art results by addressing domain gaps at the image and region levels.", "motivation": "Text-based person retrieval gains importance due to privacy concerns and the high costs of manual annotation. However, domain gaps between synthetic and real-world datasets hinder model performance.", "method": "The paper introduces a unified pipeline with two components: Domain-aware Diffusion for adapting images from synthetic to real-world datasets and Multi-granularity Relation Alignment for region-level visual-textual correspondence.", "result": "The proposed method achieves state-of-the-art results on popular text-based person retrieval datasets such as CUHK-PEDES, ICFG-PEDES, and RSTPReid.", "conclusion": "A dual-level adaptation approach effectively narrows domain gaps, improving performance in text-based person retrieval tasks. The dataset, model, and code are shared publicly."}}
{"id": "2507.10039", "pdf": "https://arxiv.org/pdf/2507.10039", "abs": "https://arxiv.org/abs/2507.10039", "authors": ["Steven Palayew", "Bo Wang", "Gary Bader"], "title": "Towards Applying Large Language Models to Complement Single-Cell Foundation Models", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.", "AI": {"tldr": "The study introduces scMPT, which combines single-cell foundation models like scGPT with insights from large language models (LLMs) to enhance single-cell biological data analysis, achieving stronger and more consistent performance than either model alone.", "motivation": "Single-cell foundation models are highly effective for biological tasks but lack the ability to utilize valuable text-based biological information; LLMs have shown promise as alternatives but lack complementary integration with these models.", "method": "The researchers developed scMPT, a model combining single-cell foundation models like scGPT with biological insights extracted by LLMs, and explored alternative fusion methods to refine their combined performance.", "result": "scMPT outperformed its individual component models across datasets, achieving stronger and more consistent performance with reduced performance disparities.", "conclusion": "The study highlights the importance of integrating LLMs with single-cell models, showcasing their complementary capabilities in advancing single-cell biological data analysis."}}
{"id": "2507.10202", "pdf": "https://arxiv.org/pdf/2507.10202", "abs": "https://arxiv.org/abs/2507.10202", "authors": ["Jaeseong Lee", "Yeeun Choi", "Heechan Choi", "Hanjung Kim", "Seonjoo Kim"], "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.", "AI": {"tldr": "This paper proposes the Extract Candidate then Predict (ECP) framework, addressing Multimodal Large Language Models' (MLLMs) limitations in high-resolution image tasks due to resolution constraints during training.", "motivation": "MLLMs perform poorly on tasks involving high-resolution images, due to a mismatch in image resolution during training and testing.", "method": "ECP employs a two-stage training-free framework: it identifies candidate regions from coarse predictions on downsampled images and refines outputs based on these regions.", "result": "ECP achieved significant improvements, including a +21.3% accuracy gain in 4K GUI grounding and minor gains (around +5%) for 4K/8K MLLM perception tasks.", "conclusion": "The ECP framework enhances MLLM performance on high-resolution images without compromising fine details and resolves resolution mismatch challenges effectively."}}
{"id": "2507.10048", "pdf": "https://arxiv.org/pdf/2507.10048", "abs": "https://arxiv.org/abs/2507.10048", "authors": ["Benedict Gerlach", "Marie Anastacio", "Holger H. Hoos"], "title": "On the Efficiency of Training Robust Decision Trees", "categories": ["cs.LG"], "comment": "Presented as a poster at SAIV 2025", "summary": "As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time.", "AI": {"tldr": "This paper presents a simple three-stage pipeline for training adversarially robust decision trees, focusing on improving efficiency and robustness verification.", "motivation": "With the rapid adoption of machine learning in the industry, ensuring the trustworthiness and efficiency of adversarially robust models has become increasingly essential.", "method": "The pipeline has three stages: (1) automatically determining perturbation size using a novel, simpler algorithm, scalable with smaller models; (2) applying state-of-the-art adversarial training methods while evaluating efficiency and adversarial accuracy; (3) certifying and analyzing model robustness, including verification efficiency.", "result": "Verification time, critical to the full pipeline's efficiency, is found to be independent of the training time. Perturbation size estimation and training evaluations yield useful insights for optimization.", "conclusion": "The study identifies ways to improve training and verification pipelines for adversarially robust decision trees, emphasizing efficiency without compromising robustness."}}
{"id": "2507.10203", "pdf": "https://arxiv.org/pdf/2507.10203", "abs": "https://arxiv.org/abs/2507.10203", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Improving Multimodal Learning via Imbalanced Learning", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}", "AI": {"tldr": "The paper introduces a novel strategy, ARL, for enhancing multimodal learning by leveraging imbalanced optimization based on modality-specific variances.", "motivation": "Multimodal learning often performs worse than unimodal learning due to under-optimization caused by modality imbalance. The paper suggests that balanced learning is not optimal and presents an alternative grounded in bias-variance analysis.", "method": "The proposed ARL strategy utilizes auxiliary regularizers to measure the prediction variance of each modality encoder, calculates re-weighting coefficients based on these variances, and jointly optimizes biases and multimodal losses without introducing extra parameters.", "result": "Experiments on diverse datasets confirm that ARL improves performance and is versatile across various multimodal learning structures and fusion methods.", "conclusion": "ARL presents an effective solution for multimodal learning by aligning modality dependency ratios to variance ratios, optimizing bias, and minimizing generalization error, all while maintaining model flexibility."}}
{"id": "2507.10078", "pdf": "https://arxiv.org/pdf/2507.10078", "abs": "https://arxiv.org/abs/2507.10078", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to IEEE Control Systems Letters", "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.", "AI": {"tldr": "The paper introduces a novel parameter reduction method for deep learning models using $H^2$ model order reduction to improve deployment efficiency on resource-constrained devices.", "motivation": "Deploying deep learning models with linear sequential state-space models (SSMs) is challenging on resource-constrained devices due to their large parameter sizes.", "method": "The authors use $H^2$ model order reduction techniques from control theory to compress the linear SSM components of these models.", "result": "The experiments on the LRA benchmark show superior model compression compared to a prior Balanced Truncation method, achieving $1/32$ the parameters without performance degradation.", "conclusion": "This approach effectively addresses deployment challenges of deep learning models by drastically reducing parameter size while maintaining performance."}}
{"id": "2507.09378", "pdf": "https://arxiv.org/pdf/2507.09378", "abs": "https://arxiv.org/abs/2507.09378", "authors": ["Mohammadsaleh Refahi", "Mahdi Abavisani", "Bahrad A. Sokhansanj", "James R. Brown", "Gail Rosen"], "title": "Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis", "categories": ["q-bio.GN", "cs.AI"], "comment": null, "summary": "Transformers have revolutionized nucleotide sequence analysis, yet capturing\nlong-range dependencies remains challenging. Recent studies show that\nautoregressive transformers often exhibit Markovian behavior by relying on\nfixed-length context windows for next-token prediction. However, standard\nself-attention mechanisms are computationally inefficient for long sequences\ndue to their quadratic complexity and do not explicitly enforce global\ntransition consistency.\n  We introduce CARMANIA (Context-Aware Regularization with Markovian\nIntegration for Attention-Based Nucleotide Analysis), a self-supervised\npretraining framework that augments next-token (NT) prediction with a\ntransition-matrix (TM) loss. The TM loss aligns predicted token transitions\nwith empirically derived n-gram statistics from each input sequence,\nencouraging the model to capture higher-order dependencies beyond local\ncontext. This integration enables CARMANIA to learn organism-specific sequence\nstructures that reflect both evolutionary constraints and functional\norganization.\n  We evaluate CARMANIA across diverse genomic tasks, including regulatory\nelement prediction, functional gene classification, taxonomic inference,\nantimicrobial resistance detection, and biosynthetic gene cluster\nclassification. CARMANIA outperforms the previous best long-context model by at\nleast 7 percent, matches state-of-the-art on shorter sequences (exceeding prior\nresults on 20 out of 40 tasks while running approximately 2.5 times faster),\nand shows particularly strong improvements on enhancer and housekeeping gene\nclassification tasks, including up to a 34 percent absolute gain in Matthews\ncorrelation coefficient (MCC) for enhancer prediction. The TM loss boosts\naccuracy in 33 of 40 tasks, especially where local motifs or regulatory\npatterns drive prediction.", "AI": {"tldr": "CARMANIA, a new framework, improves long-range and higher-order dependency modeling in nucleotide sequence analysis using a novel transition-matrix (TM) loss to enhance predictions based on evolutionary constraints and functional structures.", "motivation": "To address the inefficiencies of standard self-attention in transformer models for long nucleotide sequences and their tendency to rely on local context windows instead of global structural dependencies.", "method": "CARMANIA combines next-token prediction with a transition-matrix loss derived from n-gram statistics, aligning token transitions with sequence-specific structures via self-supervised pretraining.", "result": "It outperforms prior long-context models by at least 7%, matches the state-of-the-art for short sequences, and significantly improves functional classification tasks, achieving up to a 34% absolute gain in Matthews correlation coefficient for enhancer prediction.", "conclusion": "CARMANIA demonstrates the effectiveness of integrating context-aware regularization for capturing long-range and organism-specific dependencies, paving the way for improved genomic analysis across diverse tasks."}}
{"id": "2507.10209", "pdf": "https://arxiv.org/pdf/2507.10209", "abs": "https://arxiv.org/abs/2507.10209", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Is Micro-expression Ethnic Leaning?", "categories": ["cs.CV"], "comment": null, "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "AI": {"tldr": "This paper investigates the role of ethnicity in emotional and micro-expression recognition, challenging the universality of emotional expressions across cultures.", "motivation": "To challenge and revisit Ekman's universality hypothesis regarding emotional expression and explore the role ethnicity plays in micro-expression recognition.", "method": "The authors create a cross-cultural micro-expression database annotated with ethnic labels and conduct experimental comparisons between mono-ethnicity and stereo-ethnicity in controlled settings. They propose an ethnicity-sensitive framework for emotional feature learning and provide qualitative analyses.", "result": "The findings indicate that ethnic background influences micro-expression recognition, revealing ethnic biases in the emotional universality hypothesis.", "conclusion": "The study establishes the need to consider ethnic diversity in micro-expression research and provides an ethnically-aware framework to improve recognition systems."}}
{"id": "2507.10213", "pdf": "https://arxiv.org/pdf/2507.10213", "abs": "https://arxiv.org/abs/2507.10213", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}", "AI": {"tldr": "This paper addresses the under-optimization issue in multimodal learning by introducing a Disentangled Gradient Learning (DGL) framework to decouple the gradient flows between modality encoders and the fusion module.", "motivation": "The motivation is to resolve the underperformance of multimodal learning, even when compared to unimodal learning, which arises due to the optimization conflict between modality encoders and the modality fusion module.", "method": "A Disentangled Gradient Learning (DGL) framework is proposed. It truncates cross-modal gradients from the fusion module to the encoder and replaces them with gradients from unimodal loss to isolate and optimize distinct components independently.", "result": "The experiments conducted across various modalities, tasks, and frameworks with dense cross-modal interaction confirm the effectiveness and versatility of the DGL framework.", "conclusion": "DGL successfully mitigates the optimization conflict, improving the performance of multimodal learning while addressing the underperformance of its components when compared to unimodal learning."}}
{"id": "2507.10120", "pdf": "https://arxiv.org/pdf/2507.10120", "abs": "https://arxiv.org/abs/2507.10120", "authors": ["Cheng Sun", "Zhen Zhang", "Shaofu Yang"], "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 1 figure", "summary": "In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.", "AI": {"tldr": "This paper introduces VR-CR-PN, a variance-reduced cubic-regularized policy Newton algorithm, addressing the distribution shift problem in second-order reinforcement learning optimization with horizon-independent sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$.", "motivation": "Improve second-order policy optimization in reinforcement learning by overcoming limitations such as suboptimal sample complexity and dependence on unrealistic importance sampling assumptions.", "method": "Developed VR-CR-PN, combining Hessian-aided variance reduction with cubic regularization, and designed a novel Hessian estimator with uniform bounds independent of the horizon.", "result": "VR-CR-PN achieves a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$, significantly better than the previous $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$, under general nonconvex conditions without the need for importance sampling.", "conclusion": "The proposed algorithm provides efficient second-order reinforcement learning optimization while addressing critical challenges. It sets a new benchmark in sample complexity and eliminates dependency on horizon length."}}
{"id": "2507.10217", "pdf": "https://arxiv.org/pdf/2507.10217", "abs": "https://arxiv.org/abs/2507.10217", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.", "AI": {"tldr": "The paper introduces Wardrobe Polyptych LoRA, a model designed for personalized human image generation, achieving precise and consistent identity and clothing detail preservation without computational-heavy approaches.", "motivation": "Personalized human image generation struggles with maintaining precise subject attributes while limiting computational costs in real-time scenarios.", "method": "The model focuses on training LoRA layers and employs conditioning on wardrobe details and spatial references, alongside a selective subject region loss to boost fidelity and alignment.", "result": "The Wardrobe Polyptych LoRA consistently generates high-fidelity identity-preserving human images, requiring significantly less computational effort compared to existing methods.", "conclusion": "Wardrobe Polyptych LoRA is highly effective for personalized human image generation due to its innovative techniques, achieving realistic synthesis with minimal computational demands."}}
{"id": "2507.10218", "pdf": "https://arxiv.org/pdf/2507.10218", "abs": "https://arxiv.org/abs/2507.10218", "authors": ["Jimin Dai", "Jiexi Yan", "Jian Yang", "Lei Luo"], "title": "Straighten Viscous Rectified Flow via Noise Optimization", "categories": ["cs.CV"], "comment": null, "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.", "AI": {"tldr": "The paper presents VRFNO, an improvement to Reflow for generating high-quality images rapidly by addressing distribution gaps in coupling between noise and images.", "motivation": "The paper aims to overcome the identified limitations of Reflow, particularly the gap in image quality caused by inaccurate deterministic coupling between noise and real images during generation.", "method": "VRFNO integrates an encoder and neural velocity field, with innovations like historical velocity term and noise optimization via reparameterization to create optimized couplings for training.", "result": "VRFNO significantly improves image quality and generation speed, achieving state-of-the-art performance in both one-step and few-step generation across synthetic and real datasets with varying resolutions.", "conclusion": "The proposed VRFNO framework effectively addresses Reflow's shortcomings, offering higher-quality image generation with reduced errors and improved predictive accuracy of trajectory velocities."}}
{"id": "2507.10222", "pdf": "https://arxiv.org/pdf/2507.10222", "abs": "https://arxiv.org/abs/2507.10222", "authors": ["Mingzhi Xu", "Yizhe Zhang"], "title": "Spatial Lifting for Dense Prediction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Preprint. Under review", "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.", "AI": {"tldr": "The paper presents Spatial Lifting (SL), a method to lift 2D inputs to higher-dimensional spaces for dense prediction, achieving competitive performance with fewer parameters and reduced inference costs.", "motivation": "To create a more efficient, accurate, and reliable methodology for dense prediction tasks, addressing challenges in model complexity and resource consumption.", "method": "SL lifts 2D inputs, like images, to a higher-dimensional space, processed by networks like 3D U-Net, enabling dense supervision and prediction quality assessment during training and test time.", "result": "SL shows competitive performance across 19 benchmarks in semantic segmentation and depth estimation, reducing model parameters by over 98% and lowering inference costs.", "conclusion": "Spatial Lifting offers a novel, efficient paradigm for dense prediction tasks, paving the way for compact and reliable vision models."}}
{"id": "2507.10223", "pdf": "https://arxiv.org/pdf/2507.10223", "abs": "https://arxiv.org/abs/2507.10223", "authors": ["Xiangyu Yin", "Boyuan Yang", "Weichen Liu", "Qiyao Xue", "Abrar Alamri", "Goeran Fiedler", "Wei Gao"], "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV'25", "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.", "AI": {"tldr": "The paper introduces a specialized dataset called ProGait for improving vision-based machine learning methods in analyzing gait patterns for individuals with transfemoral prosthetic legs. It also provides benchmark tasks and baseline models, showing improved performance compared to existing pre-trained models.", "motivation": "To address the gap in vision-based machine learning methods which struggle to effectively detect and analyze unique gait patterns and appearances of transfemoral prosthetic legs.", "method": "The authors created ProGait, a dataset with 412 video clips of gait analysis trials featuring four above-knee amputees testing prosthetic legs. They also developed benchmark tasks and fine-tuned baseline models tailored for this dataset.", "result": "The study demonstrated that models trained using the ProGait dataset showed better performance and generalizability for prosthesis-specific analysis compared to existing pre-trained models.", "conclusion": "ProGait represents a significant resource for advancing vision-based machine learning in gait analysis for prosthetic leg users, contributing to improved mobility and life quality for these individuals."}}
{"id": "2507.10170", "pdf": "https://arxiv.org/pdf/2507.10170", "abs": "https://arxiv.org/abs/2507.10170", "authors": ["Wuyang Zhou", "Giorgos Iacovides", "Kriton Konstantinidis", "Ilya Kisil", "Danilo Mandic"], "title": "Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach", "categories": ["cs.LG"], "comment": null, "summary": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.", "AI": {"tldr": "This paper addresses the concept of TN ranks, aiming to clarify their role and utility through examples and intuitive methods for selecting these ranks in tensor networks.", "motivation": "To demystify the foundational yet misunderstood concept of TN (Tensor Network) ranks and provide tools for their informed selection based on domain knowledge.", "method": "The study employs real-life examples and visualizations, along with a self-explanatory graphical approach, to illustrate how TN ranks can be selected and generalized to tensors of arbitrary order by considering tensor unfoldings.", "result": "The paper presents a unified understanding of TN ranks, their relationship with tensor mathematics, and their practical selection and deployment, avoiding the complexity of multi-index tensor algebra.", "conclusion": "The insights provided help readers select and utilize TN ranks effectively in practical applications and educational settings, leveraging domain knowledge for better Tensor Network designs."}}
{"id": "2507.10225", "pdf": "https://arxiv.org/pdf/2507.10225", "abs": "https://arxiv.org/abs/2507.10225", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by\n11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.", "AI": {"tldr": "SynOOD uses foundation models to generate synthetic OOD samples for fine-tuning CLIP models, achieving state-of-the-art OOD detection performance on ImageNet benchmarks.", "motivation": "Despite advancements in vision-language models, differentiating challenging OOD samples close to InD data remains problematic.", "method": "SynOOD employs iterative in-painting with MLLMs to refine OOD data near the InD boundary, utilizing gradient-based noise adjustments for sample generation.", "result": "SynOOD significantly outperforms previous methods on ImageNet benchmarks, improving AUROC by 2.80% and reducing FPR95 by 11.13%.", "conclusion": "SynOOD enhances boundary discrimination between InD and OOD samples with minimal computational overhead, advancing the field of OOD detection using foundation models."}}
{"id": "2507.10172", "pdf": "https://arxiv.org/pdf/2507.10172", "abs": "https://arxiv.org/abs/2507.10172", "authors": ["Ruizhe Yu Xia", "Jeremy Gow", "Simon Lucas"], "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as Short Paper for IEEE CoG", "summary": "Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.", "AI": {"tldr": "The study develops an unsupervised CNN-LSTM autoencoder for identifying play styles using low-level game data without relying on domain expertise.", "motivation": "To reduce reliance on domain knowledge and biases when analyzing play styles, while improving adaptive gaming and game-playing agents.", "method": "The authors use CNN-LSTM autoencoder models to extract latent representations from raw play-trace data in the MicroRTS game environment.", "result": "The approach creates a clear differentiation of game-playing agents in latent space, showing effectiveness in characterizing diverse play styles.", "conclusion": "This method provides a domain-agnostic way to analyze play styles, enabling unbiased and meaningful insights into game-playing behaviors."}}
{"id": "2507.10236", "pdf": "https://arxiv.org/pdf/2507.10236", "abs": "https://arxiv.org/abs/2507.10236", "authors": ["Despina Konstantinidou", "Dimitrios Karageorgiou", "Christos Koutlis", "Olga Papadopoulou", "Emmanouil Schinas", "Symeon Papadopoulos"], "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?", "categories": ["cs.CV"], "comment": "35 pages, 4 figures", "summary": "The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.", "AI": {"tldr": "The study reveals that AI-Generated Image Detection (AID) models perform well on controlled datasets but struggle in real-world scenarios. Introducing a new dataset (ITW-SM), the authors improve AID performance through systematic modifications.", "motivation": "To address the critical need for detecting AI-generated images as they reach a realism level capable of deceiving even experts.", "method": "The authors analyzed backbone architecture, training data composition, pre-processing strategies, and data augmentation techniques. They introduced ITW-SM, a dataset sourced from social media, and systematically evaluated AID models.", "result": "The proposed modifications led to an average AUC improvement of 26.87% in performance under real-world conditions.", "conclusion": "The paper highlights the limitations of existing AID models and demonstrates how strategic modifications can significantly enhance detection efficacy in real-world settings."}}
{"id": "2507.10183", "pdf": "https://arxiv.org/pdf/2507.10183", "abs": "https://arxiv.org/abs/2507.10183", "authors": ["Alireza Dizaji", "Benedict Aaron Tjandra", "Mehrab Hamidi", "Shenyang Huang", "Guillaume Rabusseau"], "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "categories": ["cs.LG"], "comment": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)", "summary": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.", "AI": {"tldr": "This paper introduces T-GRAB, a benchmark for testing Temporal Graph Neural Networks (TGNNs) on key temporal reasoning tasks.", "motivation": "There is a lack of clarity on whether Temporal Graph Neural Networks effectively capture core temporal patterns like periodicity, causality, and long-range dependencies.", "method": "The authors propose the Temporal Graph Reasoning Benchmark (T-GRAB), which comprises synthetic tasks targeting specific temporal skills: periodic repetition, causal effects, and long-range dependencies. They evaluate 11 temporal graph learning methods on these tasks.", "result": "The evaluation revealed significant limitations in existing TGNNs' ability to generalize temporal patterns, highlighting the need for improved models.", "conclusion": "Current TGNNs struggle with core temporal reasoning skills, necessitating advancements in their architectures. T-GRAB serves as a diagnostic tool for these advancements."}}
{"id": "2507.10239", "pdf": "https://arxiv.org/pdf/2507.10239", "abs": "https://arxiv.org/abs/2507.10239", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika M\u00fctze", "Kira Maag", "Matthias Rottmann"], "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "categories": ["cs.CV"], "comment": "accepted at ECAI 2025", "summary": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method.", "AI": {"tldr": "The paper explores style transfer as a data augmentation method to reduce texture bias and increase robustness in semantic segmentation deep neural networks (DNNs).", "motivation": "Traditional DNNs have biases toward texture in image classification, which can negatively affect robustness and generalization. While style transfer has improved texture bias in classification, it remains unknown whether it has similar benefits for semantic segmentation.", "method": "The authors applied style transfer to random Voronoi-cell-based image regions, creating stylized datasets to train semantic segmentation DNNs. Experiments were conducted on convolutional neural networks and transformer architectures using datasets like Cityscapes and PASCAL Context.", "result": "Style transfer reduced texture dependency, promoted shape-based feature reliance, and significantly improved robustness against image corruptions and adversarial attacks.", "conclusion": "Style transfer can effectively enhance the robustness and generalization of semantic segmentation DNNs across different architectures and datasets, suggesting it as a promising augmentation approach."}}
{"id": "2507.10194", "pdf": "https://arxiv.org/pdf/2507.10194", "abs": "https://arxiv.org/abs/2507.10194", "authors": ["Tassilo Klein", "Moin Nabi"], "title": "Learning Private Representations through Entropy-based Adversarial Training", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage.", "AI": {"tldr": "The paper introduces an adversarial representation learning method using focal entropy to sanitize sensitive data while maintaining high predictive performance.", "motivation": "Address the challenge of balancing predictive performance with user privacy in representation learning.", "method": "Proposes a novel adversarial learning method incorporating focal entropy to reduce information leakage from learned representations.", "result": "Method demonstrated effectiveness across multiple benchmarks with high utility and moderate privacy risks.", "conclusion": "The approach achieves a practical balance between predictive power and privacy preservation."}}
{"id": "2507.10265", "pdf": "https://arxiv.org/pdf/2507.10265", "abs": "https://arxiv.org/abs/2507.10265", "authors": ["Xinlong Ding", "Hongwei Yu", "Jiawei Li", "Feifan Li", "Yu Shang", "Bochao Zou", "Huimin Ma", "Jiansheng Chen"], "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025. Project page is available at\n  https://wakuwu.github.io/KBA", "summary": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.", "AI": {"tldr": "The paper introduces a novel attack method called Kaleidoscopic Background Attack (KBA) to reduce the robustness of camera pose estimation models by leveraging symmetrical background patterns.", "motivation": "Camera pose estimation for object-centric scenes with sparse inputs can suffer substantial inaccuracies due to background textures. Exploring how these textures can be optimized to disrupt pose estimation models is a vital task.", "method": "The authors propose the Kaleidoscopic Background Attack (KBA), which uses multi-fold symmetrical textures to create adversarial disc patterns. They further introduce a projected orientation consistency loss to enhance the attack's effectiveness.", "result": "Tests demonstrated that the adversarial kaleidoscopic backgrounds can successfully degrade the performance of various camera pose estimation models.", "conclusion": "Adversarial backgrounds generated using symmetrical textures can significantly impact pose estimation models, highlighting a vulnerability in their design and prompting methods to counter such attacks."}}
{"id": "2507.10215", "pdf": "https://arxiv.org/pdf/2507.10215", "abs": "https://arxiv.org/abs/2507.10215", "authors": ["Cencheng Shen", "Yuexiao Dong"], "title": "A Graph Sufficiency Perspective for Neural Networks", "categories": ["cs.LG", "stat.AP"], "comment": "23 pages", "summary": "This paper analyzes neural networks through graph variables and statistical\nsufficiency. We interpret neural network layers as graph-based transformations,\nwhere neurons act as pairwise functions between inputs and learned anchor\npoints. Within this formulation, we establish conditions under which layer\noutputs are sufficient for the layer inputs, that is, each layer preserves the\nconditional distribution of the target variable given the input variable. Under\ndense anchor point assumptions, we prove that asymptotic sufficiency holds in\nthe infinite-width limit and is preserved throughout training. To align more\nclosely with practical architectures, we further show that sufficiency can be\nachieved with finite-width networks by assuming region-separated input\ndistributions and constructing appropriate anchor points. Our framework covers\nfully connected layers, general pairwise functions, ReLU and sigmoid\nactivations, and convolutional neural networks. This work bridges statistical\nsufficiency, graph-theoretic representations, and deep learning, providing a\nnew statistical understanding of neural networks.", "AI": {"tldr": "Analyzes neural networks using graph variables and statistical sufficiency, proposing that layers transform inputs into sufficient outputs under certain conditions.", "motivation": "To provide a new statistical understanding of neural networks by bridging statistical sufficiency, graph-theoretic representations, and deep learning.", "method": "Models neural network layers as graph-based transformations and proves sufficiency under dense anchor point assumptions for infinite-width networks and structured inputs for finite-width networks.", "result": "Demonstrates asymptotic sufficiency for infinite-width networks, sufficiency in practical cases with region-separated inputs, and covers diverse architectures including fully connected and convolutional layers.", "conclusion": "Establishes statistical sufficiency conditions for neural networks, offering insights into their fundamental mathematical behavior and interpretations."}}
{"id": "2507.10283", "pdf": "https://arxiv.org/pdf/2507.10283", "abs": "https://arxiv.org/abs/2507.10283", "authors": ["Muyi Bao", "Changyu Zeng", "Yifan Wang", "Zhengni Yang", "Zimu Wang", "Guangliang Cheng", "Jun Qi", "Wei Wang"], "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.", "AI": {"tldr": "FTCFormer introduces a novel clustering-based module to enhance semantic-based tokenization in vision transformers, improving classification performance across diverse datasets.", "motivation": "Transformer-based models often use grid-based tokens that lack semantic awareness, leading to suboptimal representations of image regions.", "method": "The paper proposes mechanisms like DPC-FKNN for clustering centers, SCS for token assignment, and Cmerge for merging, ensuring semantically relevant token allocation.", "result": "FTCFormer demonstrates consistent improvements in classification accuracy across 32 datasets with gains ranging from 0.55% to 1.43%.", "conclusion": "Clustering-based tokenization enhances vision transformers' effectiveness by prioritizing semantically important image regions and optimizing their representation."}}
{"id": "2507.10241", "pdf": "https://arxiv.org/pdf/2507.10241", "abs": "https://arxiv.org/abs/2507.10241", "authors": ["Vikas Dwivedi", "Balaji Srinivasan", "Monica Sigovan", "Bruno Sixou"], "title": "Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning\nMachine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of\nPI-ELM designed to solve both forward and inverse Partial Differential Equation\n(PDE) problems involving localized sharp gradients. While PI-ELMs outperform\nthe traditional Physics-Informed Neural Networks (PINNs) in speed due to their\nsingle-shot, least square optimization, this advantage comes at a cost: their\nfixed, randomly initialized input layer limits their ability to capture sharp\ngradients. To overcome this limitation, we introduce a lightweight Bayesian\nOptimization (BO) framework that, instead of adjusting each input layer\nparameter individually as in traditional backpropagation, learns a small set of\nhyperparameters defining the statistical distribution from which the input\nweights are drawn. This novel distributional optimization strategy -- combining\nBO for input layer distributional parameters with least-squares optimization\nfor output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's\nspeed while matching or exceeding the expressiveness of PINNs. We validate the\nproposed methodology on several challenging forward and inverse PDE benchmarks,\nincluding a 1D singularly perturbed convection-diffusion equation, a 2D Poisson\nequation with sharp localized sources, and a time-dependent advection equation.\nNotably, KAPI-ELM achieves state-of-the-art accuracy in both forward and\ninverse settings. In stiff PDE regimes, it matches or even outperforms advanced\nmethods such as the Extended Theory of Functional Connections (XTFC), while\nrequiring nearly an order of magnitude fewer tunable parameters. These results\nestablish the potential of KAPI-ELM as a scalable, interpretable, and\ngeneralizable physics-informed learning framework, especially in stiff PDE\nregimes.", "AI": {"tldr": "This paper introduces KAPI-ELM, a physics-informed learning framework designed to solve PDE problems, combining Bayesian optimization for input layer distributions with least-squares output layer optimization for enhanced speed and accuracy.", "motivation": "To address the limitation of PI-ELMs in capturing sharp gradients in PDE problems while maintaining their speed advantage over traditional PINNs.", "method": "The authors propose KAPI-ELM, which integrates Bayesian Optimization (BO) for tuning input layer distributional parameters and least-squares optimization for the output layer. This allows better handling of sharp gradients using an adaptive RBF-based structure.", "result": "KAPI-ELM demonstrates state-of-the-art accuracy across several challenging PDE benchmarks, including singularly perturbed convection-diffusion and time-dependent advection equations. It surpasses advanced methods like XTFC in stiff PDE regimes while using fewer tunable parameters.", "conclusion": "KAPI-ELM is a scalable and interpretable framework that matches or outperforms state-of-the-art methods in solving stiff PDE problems, making it a promising tool for various physics-informed learning applications."}}
{"id": "2507.10293", "pdf": "https://arxiv.org/pdf/2507.10293", "abs": "https://arxiv.org/abs/2507.10293", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "categories": ["cs.CV"], "comment": "Accepted by MM 2025", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.", "AI": {"tldr": "IP-FVR is a novel method for Face Video Restoration that uses reference images and advanced attention mechanisms to improve quality and preserve identity.", "motivation": "Current Face Video Restoration techniques struggle with severe degradation and often fail to maintain fine-grained, identity-specific features, resulting in generic or average-looking outputs.", "method": "IP-FVR uses a reference face image for identity conditioning during denoising, employs cross-attention mechanisms for detailed restoration, and addresses identity drift both intra-clip and inter-clip through feedback learning and exponential blending strategies. It also uses a multi-stream negative prompt to enhance restoration accuracy.", "result": "Experiments on both synthetic and real-world datasets show that IP-FVR delivers superior quality and maintains identities more effectively compared to existing methods.", "conclusion": "IP-FVR demonstrates strong potential for practical applications, achieving state-of-the-art results in face video restoration with consistent identity preservation and refined detail handling."}}
{"id": "2507.10273", "pdf": "https://arxiv.org/pdf/2507.10273", "abs": "https://arxiv.org/abs/2507.10273", "authors": ["Lu Zhu", "Emmanuel Noutahi"], "title": "Conditional Chemical Language Models are Versatile Tools in Drug Discovery", "categories": ["cs.LG", "q-bio.BM"], "comment": "12 pages, extra 13 pages of appendix", "summary": "Generative chemical language models (CLMs) have demonstrated strong\ncapabilities in molecular design, yet their impact in drug discovery remains\nlimited by the absence of reliable reward signals and the lack of\ninterpretability in their outputs. We present SAFE-T, a generalist chemical\nmodeling framework that conditions on biological context -- such as protein\ntargets or mechanisms of action -- to prioritize and design molecules without\nrelying on structural information or engineered scoring functions. SAFE-T\nmodels the conditional likelihood of fragment-based molecular sequences given a\nbiological prompt, enabling principled scoring of molecules across tasks such\nas virtual screening, drug-target interaction prediction, and activity cliff\ndetection. Moreover, it supports goal-directed generation by sampling from this\nlearned distribution, aligning molecular design with biological objectives. In\ncomprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,\nACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves\nperformance comparable to or better than existing approaches while being\nsignificantly faster. Fragment-level attribution further reveals that SAFE-T\ncaptures known structure-activity relationships, supporting interpretable and\nbiologically grounded design. Together with its computational efficiency, these\nresults demonstrate that conditional generative CLMs can unify scoring and\ngeneration to accelerate early-stage drug discovery.", "AI": {"tldr": "SAFE-T is a chemical modeling framework that leverages conditional generative models tailored for biological contexts to enable scoring and design of molecules, excelling in both predictive and generative tasks.", "motivation": "Existing chemical language models for molecular design lack reliable reward signals and interpretability, limiting their utility in real-world drug discovery.", "method": "SAFE-T conditions molecular designs on biological contexts, such as protein targets or mechanisms of action, without using structural data or engineered scoring. It models fragment-based molecular sequences to achieve tasks like virtual screening and drug-target interaction prediction while supporting generative goals.", "result": "SAFE-T delivers competitive or superior performance versus alternatives in predictive and generative benchmarks. It also offers significantly faster outcomes and fragment-level interpretability, revealing structure-activity relationships.", "conclusion": "SAFE-T showcases how conditional generative chemical language models can unify molecular scoring and generation, addressing both computational efficiency and biological interpretability to enhance early-stage drug discovery efforts."}}
{"id": "2507.10296", "pdf": "https://arxiv.org/pdf/2507.10296", "abs": "https://arxiv.org/abs/2507.10296", "authors": ["Shijie Li", "Weiqiang He", "Ruobing Bai", "Pan Peng"], "title": "Average Sensitivity of Hierarchical $k$-Median Clustering", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "Hierarchical clustering is a widely used method for unsupervised learning\nwith numerous applications. However, in the application of modern algorithms,\nthe datasets studied are usually large and dynamic. If the hierarchical\nclustering is sensitive to small perturbations of the dataset, the usability of\nthe algorithm will be greatly reduced. In this paper, we focus on the\nhierarchical $k$ -median clustering problem, which bridges hierarchical and\ncentroid-based clustering while offering theoretical appeal, practical utility,\nand improved interpretability. We analyze the average sensitivity of algorithms\nfor this problem by measuring the expected change in the output when a random\ndata point is deleted. We propose an efficient algorithm for hierarchical\n$k$-median clustering and theoretically prove its low average sensitivity and\nhigh clustering quality. Additionally, we show that single linkage clustering\nand a deterministic variant of the CLNSS algorithm exhibit high average\nsensitivity, making them less stable. Finally, we validate the robustness and\neffectiveness of our algorithm through experiments.", "AI": {"tldr": "This paper focuses on an improved hierarchical $k$-median clustering algorithm with low average sensitivity and high clustering quality. The study addresses stability issues in existing methods through theoretical and experimental validations.", "motivation": "Modern datasets are often large and dynamic, making the stability and robustness of hierarchical clustering crucial. The paper aims to address the sensitivity and usability challenges in hierarchical $k$-median clustering.", "method": "The paper analyzes average sensitivity by measuring the expected change in clustering results upon the deletion of random data points. Based on this, it proposes an efficient algorithm with provable theoretical guarantees, compared to existing unstable methods.", "result": "The proposed hierarchical $k$-median clustering algorithm demonstrates low average sensitivity and high clustering quality. It outperforms existing methods like single linkage clustering and a deterministic CLNSS variant in stability.", "conclusion": "The proposed algorithm improves robustness and effectiveness in hierarchical $k$-median clustering, offering both theoretical and practical advantages. Experimental validation further underscores its stability and utility."}}
{"id": "2507.10302", "pdf": "https://arxiv.org/pdf/2507.10302", "abs": "https://arxiv.org/abs/2507.10302", "authors": ["Jiahe Zhao", "Rongkun Zheng", "Yi Wang", "Helin Wang", "Hengshuang Zhao"], "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.", "AI": {"tldr": "DisCo improves video token representation through semantic clarity and temporal coherence, surpassing previous methods in video understanding tasks.", "motivation": "To address issues of semantic indistinctness and temporal incoherence in video token extraction for Video MLLMs.", "method": "DisCo combines a Visual Concept Discriminator (VCD) and a Temporal Focus Calibrator (TFC) for enhanced video token encapsulation.", "result": "DisCo significantly surpasses prior state-of-the-art performance across video understanding tasks, offering improved token efficiency.", "conclusion": "DisCo effectively resolves central challenges in video tokenization, enhancing video representation for multimodal language models."}}
{"id": "2507.10311", "pdf": "https://arxiv.org/pdf/2507.10311", "abs": "https://arxiv.org/abs/2507.10311", "authors": ["Liming Wang", "Saurabhchand Bhati", "Cody Karjadi", "Rhoda Au", "James Glass"], "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861", "AI": {"tldr": "The paper introduces Demenba, an automatic dementia classification framework using state space models, outperforming prior methods by 21% in classification accuracy while being efficient.", "motivation": "To improve early detection of dementia, as traditional neuropsychological tests rely on manual scoring which is less scalable.", "method": "The authors utilize state space models for developing Demenba, leveraging over 1,000 hours of cognitive assessment data and fusing the model with large language models for added improvement.", "result": "Demenba achieves a 21% improvement in dementia classification accuracy compared to prior approaches, with fewer parameters and efficient scaling.", "conclusion": "Demenba offers a more transparent, scalable, and accurate framework for automatic dementia classification, paving the way for enhanced cognitive assessment methodologies."}}
{"id": "2507.09508", "pdf": "https://arxiv.org/pdf/2507.09508", "abs": "https://arxiv.org/abs/2507.09508", "authors": ["Weichen Yu", "Ravi Mangal", "Terry Zhuo", "Matt Fredrikson", "Corina S. Pasareanu"], "title": "A Mixture of Linear Corrections Generates Secure Code", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%.", "AI": {"tldr": "The paper explores how large language models (LLMs) internally encode security-related knowledge and presents a method to improve the security and functionality of code generated by LLMs leveraging those representations.", "motivation": "Large language models excel at code generation but struggle with identifying and mitigating code vulnerabilities, posing risks for secure software development.", "method": "The study investigates LLMs' internal representations regarding code vulnerabilities and introduces a token-generation steering method, called a mixture of corrections (MoC), to guide LLM outputs toward more secure coding practices.", "result": "The MoC technique improves both code security and functionality in model outputs, notably enhancing Qwen2.5-Coder-7B's security by 8.9% and code functionality by 2.1%.", "conclusion": "LLMs do encode concepts pertinent to security, and harnessing these representations at inference time offers a practical solution to reducing vulnerabilities in AI-generated code without sacrificing code quality."}}
{"id": "2507.10306", "pdf": "https://arxiv.org/pdf/2507.10306", "abs": "https://arxiv.org/abs/2507.10306", "authors": ["Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation", "categories": ["cs.CV"], "comment": "Accepted at 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT), will be held in conjunction with IVA'25", "summary": "Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.", "AI": {"tldr": "This paper presents a new gloss-free method for Sign Language Translation (SLT) using a dual visual encoder framework, achieving state-of-the-art results on a benchmark.", "motivation": "Gloss annotations in SLT are costly and inadequate for capturing continuous signing complexity, necessitating a gloss-free alternative.", "method": "A dual visual encoder framework is used, leveraging contrastive visual-language pretraining with two visual backbones and an encoder-decoder model for SLT.", "result": "The proposed method outperforms single-stream variants and achieves the highest BLEU-4 score on the Phoenix-2014T benchmark for gloss-free SLT.", "conclusion": "The dual visual encoder framework mitigates reliance on gloss annotations and sets a new performance standard for gloss-free SLT methods."}}
{"id": "2507.10318", "pdf": "https://arxiv.org/pdf/2507.10318", "abs": "https://arxiv.org/abs/2507.10318", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.", "AI": {"tldr": "The paper presents IMD, a framework that uses pre-trained diffusion models to resolve misalignment issues in feature matching caused by foundation models. It achieves state-of-the-art performance and significant improvements on multi-instance benchmarks.", "motivation": "The motivation is to address the misalignment issue arising when using vision foundation models for feature matching, especially for multi-instance scenarios. This misalignment stems from the difference between single-image understanding and the cross-image understanding needed for feature matching.", "method": "The authors propose IMD, a framework that incorporates pre-trained generative-based diffusion models to better capture instance-level details. It introduces a cross-image interaction prompting module facilitated by generative model prompts, enabling bidirectional communication between image pairs. Additionally, a new benchmark, IMIM, is proposed to measure multi-instance performance.", "result": "IMD achieves state-of-the-art results on commonly used benchmarks and demonstrates a significant 12% improvement on the IMIM benchmark, showing effective mitigation of the misalignment issue.", "conclusion": "The study demonstrates that using diffusion models and interactive prompting mechanisms effectively aligns vision foundation models for feature matching, providing substantial performance gains in multi-instance scenarios."}}
{"id": "2507.10334", "pdf": "https://arxiv.org/pdf/2507.10334", "abs": "https://arxiv.org/abs/2507.10334", "authors": ["Mahmoud Bekhit", "Ahmad Salah", "Ahmed Salim Alrawahi", "Tarek Attia", "Ahmed Ali", "Esraa Eldesokey", "Ahmed Fathalla"], "title": "MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data", "categories": ["cs.LG"], "comment": "22 pages, 7 figures, 3 algorithms, 2 tables", "summary": "Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)\nis vital for applications in sports science, but its utility is often\ncompromised by missing data. Despite numerous imputation techniques, a\nsystematic performance evaluation for IMU-derived MoCap time-series data is\nlacking. We address this gap by conducting a comprehensive comparative analysis\nof statistical, machine learning, and deep learning imputation methods. Our\nevaluation considers three distinct contexts: univariate time-series,\nmultivariate across subjects, and multivariate across kinematic angles. To\nfacilitate this benchmark, we introduce the first publicly available MoCap\ndataset designed specifically for imputation, featuring data from 53 karate\npractitioners. We simulate three controlled missingness mechanisms: missing\ncompletely at random (MCAR), block missingness, and a novel value-dependent\npattern at signal transition points. Our experiments, conducted on 39 kinematic\nvariables across all subjects, reveal that multivariate imputation frameworks\nconsistently outperform univariate approaches, particularly for complex\nmissingness. For instance, multivariate methods achieve up to a 50% mean\nabsolute error reduction (MAE from 10.8 to 5.8) compared to univariate\ntechniques for transition point missingness. Advanced models like Generative\nAdversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the\nhighest accuracy in these challenging scenarios. This work provides a critical\nbaseline for future research and offers practical recommendations for improving\nthe integrity and robustness of Mo-Cap data analysis.", "AI": {"tldr": "The paper presents a systematic evaluation of statistical, machine learning, and deep learning methods for imputing missing Motion Capture (MoCap) data from wearable IMUs. A novel dataset with controlled missingness mechanisms is introduced.", "motivation": "Missing data in MoCap applications hinders its utility in sports science, and comprehensive benchmarks for various imputation methods are absent.", "method": "The authors evaluate imputation methods across univariate and multivariate contexts using a novel dataset from 53 karate practitioners, simulating controlled missingness mechanisms.", "result": "Multivariate imputation methods outperform univariate approaches, reducing mean absolute error by up to 50%. Advanced methods like GAIN and Iterative Imputers yield the highest accuracy under challenging scenarios.", "conclusion": "The study establishes a benchmark for effective imputation techniques, aiding future research and improving the robustness of MoCap data utilization."}}
{"id": "2507.10340", "pdf": "https://arxiv.org/pdf/2507.10340", "abs": "https://arxiv.org/abs/2507.10340", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.", "AI": {"tldr": "This paper introduces QLIP, a method for efficient diffusion model quantization using text prompts to optimize layer precision, reducing computational complexity while maintaining image generation quality.", "motivation": "The high computational cost of diffusion models limits their applicability in resource-constrained environments, necessitating a more efficient quantization approach that incorporates text prompts.", "method": "The proposed method, QLIP, uses text prompts to dynamically guide bit precision selection for model layers at each time step, integrating easily with existing quantization strategies.", "result": "QLIP reduces computational overhead and enhances the quality of generated images, as confirmed by experiments on multiple datasets.", "conclusion": "QLIP effectively improves the efficiency of diffusion models in generating high-quality images, especially in constrained environments, through a novel prompt-guided quantization methodology."}}
{"id": "2507.10345", "pdf": "https://arxiv.org/pdf/2507.10345", "abs": "https://arxiv.org/abs/2507.10345", "authors": ["Yuwen Li", "Guozhi Zhang"], "title": "Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions", "categories": ["cs.LG"], "comment": null, "summary": "This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU\nneural networks for Korobov functions. In terms of network width and depth, we\nderive nearly optimal super-approximation error bounds of order $2m$ in the\n$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with\n$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages\nsparse grid finite elements and the bit extraction technique. Our results\nimprove upon classical lowest order $L_\\infty$ and $H^1$ norm error bounds and\ndemonstrate that the expressivity of neural networks is largely unaffected by\nthe curse of dimensionality.", "AI": {"tldr": "The paper provides advanced error bounds for ReLU neural networks when approximating Korobov functions, reflecting improvements over classical metrics.", "motivation": "To address limitations in classical error bounds for neural network expressivity and mitigate the curse of dimensionality.", "method": "The researchers employ sparse grid finite elements and bit extraction techniques to analyze approximation errors in $L_p$ and $W^1_p$ norms.", "result": "The study achieved nearly optimal error bounds in $L_p$ (order $2m$) and $W^1_p$ norms (order $2m-2$) for target functions.", "conclusion": "ReLU neural networks demonstrate strong approximation abilities without being significantly impacted by the curse of dimensionality."}}
{"id": "2507.10343", "pdf": "https://arxiv.org/pdf/2507.10343", "abs": "https://arxiv.org/abs/2507.10343", "authors": ["Hugo Norrby", "Gabriel F\u00e4rm", "Kevin Hernandez-Diaz", "Fernando Alonso-Fernandez"], "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "categories": ["cs.CV"], "comment": "Accepted at International Workshop on Artificial Intelligence and\n  Pattern Recognition, IWAIPR 2025", "summary": "We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.", "AI": {"tldr": "FGSSNet is a novel architecture designed for enhancing wall segmentation in floorplans using domain-specific feature maps.", "motivation": "To improve the generalization ability of wall segmentation in floorplans.", "method": "FGSSNet utilizes a U-Net backbone and a multi-headed feature extractor trained to encode texture and width features of wall patches, injecting these features into U-Net's latent space.", "result": "Experiments demonstrate superior performance in wall segmentation compared to a vanilla U-Net.", "conclusion": "FGSSNet validates the effectiveness of using guided features for improving semantic segmentation tasks, specifically wall segmentation in floorplans."}}
{"id": "2507.10347", "pdf": "https://arxiv.org/pdf/2507.10347", "abs": "https://arxiv.org/abs/2507.10347", "authors": ["Yan-Ting Chen", "Hao-Wei Chen", "Tsu-Ching Hsiao", "Chun-Yi Lee"], "title": "Parallel Sampling of Diffusion Models on $SO(3)$", "categories": ["cs.LG"], "comment": "MVA2025", "summary": "In this paper, we design an algorithm to accelerate the diffusion process on\nthe $SO(3)$ manifold. The inherently sequential nature of diffusion models\nnecessitates substantial time for denoising perturbed data. To overcome this\nlimitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$\nspace. We demonstrate our algorithm on an existing method that employs\ndiffusion models to address the pose ambiguity problem. Moreover, we show that\nthis acceleration advantage occurs without any measurable degradation in task\nreward. The experiments reveal that our algorithm achieves a speed-up of up to\n4.9$\\times$, significantly reducing the latency for generating a single sample.", "AI": {"tldr": "The paper introduces a faster diffusion algorithm on SO(3) manifold, achieving up to 4.9x speed-up without degrading performance.", "motivation": "Diffusion models are sequential and time-intensive, posing a limitation for tasks requiring quick processing.", "method": "The authors adapt the numerical Picard iteration technique to accelerate the diffusion process on the SO(3) manifold.", "result": "Up to 4.9x speed-up in generating samples without measurable degradation in task reward during experimentation.", "conclusion": "The proposed adaptation effectively reduces latency in diffusion models on SO(3) manifold while maintaining performance."}}
{"id": "2507.10355", "pdf": "https://arxiv.org/pdf/2507.10355", "abs": "https://arxiv.org/abs/2507.10355", "authors": ["Bo Jiang", "Xueyang Ze", "Beibei Wang", "Xixi Wang", "Xixi Wan", "Bin Luo"], "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter", "categories": ["cs.CV"], "comment": null, "summary": "Textual adapter-based tuning methods have shown significant potential in\ntransferring knowledge from pre-trained Vision-Language Models (VLMs) to\ndownstream tasks. Existing works generally employ the deterministic textual\nfeature adapter to refine each category textual representation. However, due to\ninherent factors such as different attributes and contexts, there exists\nsignificant diversity in textual descriptions for each category. Such\ndescription diversity offers rich discriminative semantic knowledge that can\nbenefit downstream visual learning tasks. Obviously, traditional deterministic\nadapter model cannot adequately capture this varied semantic information. Also,\nit is desirable to exploit the inter-class relationships in VLM adapter. To\naddress these issues, we propose to exploit random graph model into VLM adapter\nand develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first\nmodels the inherent diverse descriptions of each category and inter-class\nrelationships of different categories simultaneously by leveraging a Vertex\nRandom Knowledge Graph (VRKG) model. Then, it employs probabilistic message\npropagation on VRKG to learn context-aware distribution representation for each\nclass node. Finally, it adopts a reparameterized sampling function to achieve\ntextual adapter learning. Note that, VRGAdapter provides a more general adapter\nsolution that encompasses traditional graph-based adapter as a special case. In\naddition, to enable more robust performance for downstream tasks, we also\nintroduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that\ndynamically integrates multiple pre-trained models for ensemble prediction.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "The paper proposes VRGAdapter, a novel textual adapter leveraging random graph models to represent diverse semantic knowledge and inter-class relationships for vision-language tasks, backed by an uncertainty-guided fusion scheme.", "motivation": "Challenges exist in capturing varied semantic information and inter-class relationships from textual descriptions using existing deterministic adapters.", "method": "The proposed VRGAdapter uses a Vertex Random Knowledge Graph (VRKG) along with probabilistic message propagation and reparameterized sampling for textual adapter learning, complemented by an uncertainty-guided multi-model fusion scheme.", "result": "Experiments on benchmark datasets reveal that VRGAdapter significantly improves performance across various vision-language tasks.", "conclusion": "VRGAdapter is a general solution that effectively utilizes diverse semantic knowledge and inter-class dynamics, outperforming traditional deterministic adapters."}}
{"id": "2507.10348", "pdf": "https://arxiv.org/pdf/2507.10348", "abs": "https://arxiv.org/abs/2507.10348", "authors": ["Yichen Li"], "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing\nattention for its ability to aggregate knowledge from heterogeneous models\nwhile keeping private data locally. To better aggregate knowledge from clients,\nensemble distillation, as a widely used and effective technique, is often\nemployed after global aggregation to enhance the performance of the global\nmodel. However, simply combining Hetero-FL and ensemble distillation does not\nalways yield promising results and can make the training process unstable. The\nreason is that existing methods primarily focus on logit distillation, which,\nwhile being model-agnostic with softmax predictions, fails to compensate for\nthe knowledge bias arising from heterogeneous models. To tackle this challenge,\nwe propose a stable and efficient Feature Distillation for model-heterogeneous\nFederated learning, dubbed FedFD, that can incorporate aligned feature\ninformation via orthogonal projection to integrate knowledge from heterogeneous\nmodels better. Specifically, a new feature-based ensemble federated knowledge\ndistillation paradigm is proposed. The global model on the server needs to\nmaintain a projection layer for each client-side model architecture to align\nthe features separately. Orthogonal techniques are employed to re-parameterize\nthe projection layer to mitigate knowledge bias from heterogeneous models and\nthus maximize the distilled knowledge. Extensive experiments show that FedFD\nachieves superior performance compared to state-of-the-art methods.", "AI": {"tldr": "This paper introduces FedFD, a federated learning method that employs feature distillation with orthogonal projection for better aggregation of knowledge from heterogeneous models.", "motivation": "To address the instability and inefficiency in aggregating knowledge from heterogeneous models during federated learning, particularly problems caused by logit-based distillation methods.", "method": "FedFD uses a feature-based ensemble federated knowledge distillation paradigm. The server maintains a projection layer for every client's model architecture, employing orthogonal techniques to align features and mitigate knowledge bias.", "result": "Experiments demonstrate that FedFD outperforms existing state-of-the-art methods in federated learning with heterogeneous models.", "conclusion": "FedFD provides a stable and efficient solution to tackle knowledge bias in model-heterogeneous federated learning, significantly enhancing performance."}}
{"id": "2507.10358", "pdf": "https://arxiv.org/pdf/2507.10358", "abs": "https://arxiv.org/abs/2507.10358", "authors": ["Hongxu Ma", "Chenbo Zhang", "Lu Zhang", "Jiaogen Zhou", "Jihong Guan", "Shuigeng Zhou"], "title": "Fine-Grained Zero-Shot Object Detection", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to\nlocalize and recognize objects of both seen and unseen classes. Existing ZSD\nworks are mainly coarse-grained object detection, where the classes are\nvisually quite different, thus are relatively easy to distinguish. However, in\nreal life we often have to face fine-grained object detection scenarios, where\nthe classes are too similar to be easily distinguished. For example, detecting\ndifferent kinds of birds, fishes, and flowers.\n  In this paper, we propose and solve a new problem called Fine-Grained\nZero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of\ndifferent classes with minute differences in details under the ZSD paradigm. We\ndevelop an effective method called MSHC for the FG-ZSD task, which is based on\nan improved two-stage detector and employs a multi-level semantics-aware\nembedding alignment loss, ensuring tight coupling between the visual and\nsemantic spaces. Considering that existing ZSD datasets are not suitable for\nthe new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,\nwhich contains 148,820 images falling into 36 orders, 140 families, 579 genera\nand 1432 species. Extensive experiments on FGZSD-Birds show that our method\noutperforms existing ZSD models.", "AI": {"tldr": "The paper introduces the Fine-Grained Zero-Shot Object Detection (FG-ZSD) problem, proposes a novel method called MSHC, and evaluates it on a newly created FGZSD-Birds dataset, achieving superior results.", "motivation": "The motivation is to address the challenge of detecting fine-grained object classes with subtle differences, which current Zero-Shot Detection (ZSD) models fail to handle effectively, especially in cases involving similar-looking objects like bird species.", "method": "The proposed method, MSHC, incorporates an improved two-stage detector with a multi-level semantics-aware embedding alignment loss. This ensures a tight alignment between visual and semantic spaces for better fine-grained object detection.", "result": "The authors built the FGZSD-Birds dataset specifically for this task and demonstrated through extensive experiments that their MSHC method outperforms existing ZSD models on fine-grained object detection.", "conclusion": "The study successfully addresses the limitations of existing ZSD approaches in fine-grained scenarios by proposing a new benchmark and an improved detection method, setting the stage for further research in FG-ZSD."}}
{"id": "2507.10349", "pdf": "https://arxiv.org/pdf/2507.10349", "abs": "https://arxiv.org/abs/2507.10349", "authors": ["Zhiyuan Zhao", "Sitan Yang", "Kin G. Olivares", "Boris N. Oreshkin", "Stan Vitebsky", "Michael W. Mahoney", "B. Aditya Prakash", "Dmitry Efimov"], "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future", "summary": "Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.", "AI": {"tldr": "This paper proposes Temporal-Aligned Transformer (TAT), a forecasting model that improves peak demand prediction using temporal alignment and context data, achieving up to 30% accuracy improvement.", "motivation": "Accurate demand prediction is critical for supply chain management, especially during high-stake sales events with challenging peak demand forecasting.", "method": "The model, TAT, uses an encoder-decoder structure with Temporal Alignment Attention (TAA) to leverage context variables like holidays and promotions, improving predictive accuracy.", "result": "TAT shows up to 30% accuracy improvement in peak demand forecasting based on evaluations with large-scale e-commerce datasets.", "conclusion": "The proposed TAT model effectively improves peak demand predictions, ensuring better decision-making for supply chains during critical events, and performs competitively overall."}}
{"id": "2507.09566", "pdf": "https://arxiv.org/pdf/2507.09566", "abs": "https://arxiv.org/abs/2507.09566", "authors": ["Timo Wilm", "Philipp Normann"], "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library", "summary": "A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.", "AI": {"tldr": "The paper proposes a method to identify offline metrics aligned with online impact in recommender systems, evaluated via a large-scale experiment.", "motivation": "The motivation is to address the gap in understanding the relationship between offline and online metrics in recommender systems, ensuring offline tests better predict real-world performance.", "method": "The method involves leveraging Pareto front approximation and deploying a single model to analyze multiple offline metrics across test groups in online experiments. The approach is model-agnostic for neural network-based systems.", "result": "The approach is validated through an online experiment on the OTTO e-commerce platform, demonstrating significant alignment between offline metrics and key online indicators like click-through rate, conversion rate, and units sold.", "conclusion": "The strategy offers a practical tool for aligning offline metrics with online performance, aiding industry practitioners in making data-driven decisions."}}
{"id": "2507.10375", "pdf": "https://arxiv.org/pdf/2507.10375", "abs": "https://arxiv.org/abs/2507.10375", "authors": ["Utkarsh Singhal", "Ryan Feng", "Stella X. Yu", "Atul Prakash"], "title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "categories": ["cs.CV", "cs.LG"], "comment": "Published at ICML 2025", "summary": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.", "AI": {"tldr": "The paper introduces FOCAL, a test-time framework enhancing visual perception robustness via internet-scale visual priors, without retraining or architectural changes.", "motivation": "Current methods for visual perception invariance often depend on specialized architectures or predefined augmentations, which limit generalization across diverse transformations.", "method": "FOCAL generates and optimizes visual transformations to align with visually typical or 'canonical' views, leveraging visual priors from foundation models like CLIP and SAM.", "result": "FOCAL improves robustness against challenging transformations, such as rotations, lighting changes, and day-night variability, without requiring retraining.", "conclusion": "Transform-specific training is not always necessary; FOCAL provides a scalable alternative to achieve perceptual invariance using online visual priors."}}
{"id": "2507.10368", "pdf": "https://arxiv.org/pdf/2507.10368", "abs": "https://arxiv.org/abs/2507.10368", "authors": ["Yongjin Choi", "Chenying Liu", "Jorge Macedo"], "title": "Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation", "categories": ["cs.LG", "physics.geo-ph"], "comment": null, "summary": "Deep Operator Networks (DeepONets) have emerged as a powerful surrogate\nmodeling framework for learning solution operators in PDE-governed systems.\nWhile their use is expanding across engineering disciplines, applications in\ngeotechnical engineering remain limited. This study systematically evaluates\nseveral DeepONet architectures for the one-dimensional consolidation problem.\nWe initially consider three architectures: a standard DeepONet with the\ncoefficient of consolidation embedded in the branch net (Models 1 and 2), and a\nphysics-inspired architecture with the coefficient embedded in the trunk net\n(Model 3). Results show that Model 3 outperforms the standard configurations\n(Models 1 and 2) but still has limitations when the target solution (excess\npore pressures) exhibits significant variation. To overcome this limitation, we\npropose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses\nthe identified limitations by capturing rapidly varying functions. All proposed\narchitectures achieve speedups ranging from 1.5 to 100 times over traditional\nexplicit and implicit solvers, with Model 4 being the most efficient. Larger\ncomputational savings are expected for more complex systems than the explored\n1D case, which is promising. Overall, the study highlights the potential of\nDeepONets to enable efficient, generalizable surrogate modeling in geotechnical\napplications, advancing the integration of scientific machine learning in\ngeotechnics, which is at an early stage.", "AI": {"tldr": "DeepONets are used for modeling PDE-governed systems, and this paper evaluates different architectures for geotechnical engineering applications, specifically 1D consolidation problems, with a focus on computational efficiency.", "motivation": "There is a limited application of DeepONets in geotechnical engineering, despite its growing use in other disciplines. The study aims to explore and enhance its capabilities for PDE-governed problems like the 1D consolidation problem.", "method": "The study compares several DeepONet architectures (standard and physics-inspired) and introduces a Trunknet Fourier feature-enhanced DeepONet (Model 4) to overcome limitations in capturing rapidly varying solutions.", "result": "Model 3 outperforms standard configurations (Models 1 and 2), but Model 4 offers the best results by addressing variations effectively. Speedups of 1.5 to 100 times are achieved compared to traditional solvers.", "conclusion": "DeepONets hold promise for efficient and scalable surrogate modeling in geotechnical engineering, with Model 4 being the most efficient option studied. The findings emphasize the potential for broader applications in the field."}}
{"id": "2507.10381", "pdf": "https://arxiv.org/pdf/2507.10381", "abs": "https://arxiv.org/abs/2507.10381", "authors": ["Aaryam Sharma"], "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 8 figures", "summary": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.", "AI": {"tldr": "This paper integrates Topological Data Analysis (TDA) with deep learning models for remote sensing classification, achieving state-of-the-art accuracy.", "motivation": "Topological Data Analysis (TDA) effectively describes complex datasets, but its integration with CNNs, which are biased towards local features, remains unexplored in remote sensing applications.", "method": "The authors propose a TDA-based feature engineering pipeline to combine TDA features with deep learning models, applying the approach to the EuroSAT and RESISC45 datasets.", "result": "The proposed method improves ResNet18's performance, achieving 99.33% accuracy on EuroSAT (1.44% improvement) and 1.82% improvement on RESISC45, surpassing stronger and larger models like ResNet50 and XL Vision Transformers.", "conclusion": "Integrating TDA features with deep learning enhances performance even for datasets without explicit topological structures, proving TDA's versatility and establishing a new benchmark for satellite scene classification."}}
{"id": "2507.10382", "pdf": "https://arxiv.org/pdf/2507.10382", "abs": "https://arxiv.org/abs/2507.10382", "authors": ["Yue Ding", "Conor McCarthy", "Kevin O'Shea", "Mingming Liu"], "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis", "categories": ["cs.LG"], "comment": null, "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.", "AI": {"tldr": "This paper introduces a cloud-based shared e-mobility platform, powered by LLMs, featuring personalized route recommendations and schema-level evaluation of its optimization and query functionalities.", "motivation": "The paper aims to address the growing demand for comprehensive e-mobility solutions by leveraging advanced technologies like LLMs and cloud-based simulation systems.", "method": "The study integrates LLM-powered frameworks into a shared e-mobility platform, evaluates travel optimization in traffic scenarios, and tests schema-level query accuracy using methods like XiYanSQL.", "result": "Schema-level RAG with XiYanSQL achieved execution accuracies of 0.81 for system operators and 0.98 for users, demonstrating high accuracy in query handling.", "conclusion": "The proposed platform successfully combines cloud technology and LLMs to offer effective e-mobility solutions, promising enhanced user engagement and operational efficiency."}}
{"id": "2507.10385", "pdf": "https://arxiv.org/pdf/2507.10385", "abs": "https://arxiv.org/abs/2507.10385", "authors": ["Md. Ahsanul Kabir", "Mohammad Al Hasan", "Aritra Mandal", "Liyang Hao", "Ishita Khan", "Daniel Tunkelang", "Zhe Wu"], "title": "Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model", "categories": ["cs.LG"], "comment": null, "summary": "The major task of any e-commerce search engine is to retrieve the most\nrelevant inventory items, which best match the user intent reflected in a\nquery. This task is non-trivial due to many reasons, including ambiguous\nqueries, misaligned vocabulary between buyers, and sellers, over- or\nunder-constrained queries by the presence of too many or too few tokens. To\naddress these challenges, query reformulation is used, which modifies a user\nquery through token dropping, replacement or expansion, with the objective to\nbridge semantic gap between query tokens and users' search intent. Early\nmethods of query reformulation mostly used statistical measures derived from\ntoken co-occurrence frequencies from selective user sessions having clicks or\npurchases. In recent years, supervised deep learning approaches, specifically\ntransformer-based neural language models, or sequence-to-sequence models are\nbeing used for query reformulation task. However, these models do not utilize\nthe semantic tags of a query token, which are significant for capturing user\nintent of an e-commerce query. In this work, we pose query reformulation as a\ntoken classification task, and solve this task by designing a dependency-aware\ntransformer-based language model, TagBERT, which makes use of semantic tags of\na token for learning superior query phrase embedding. Experiments on large,\nreal-life e-commerce datasets show that TagBERT exhibits superior performance\nthan plethora of competing models, including BERT, eBERT, and\nSequence-to-Sequence transformer model for important token classification task.", "AI": {"tldr": "The paper introduces a novel transformer-based language model called TagBERT for e-commerce query reformulation, which outperforms competing models.", "motivation": "The paper addresses the challenges in e-commerce search engines to better align user queries with inventory items by bridging semantic gaps and improving query reformulation.", "method": "A dependency-aware transformer-based model (TagBERT) is introduced, leveraging semantic tags for enhanced query embedding and token classification.", "result": "Experiments demonstrate that TagBERT surpasses models like BERT, eBERT, and sequence-to-sequence transformers in token classification for query reformulation.", "conclusion": "TagBERT effectively improves query reformulation in e-commerce search engines by incorporating dependency-aware semantics and demonstrates superior performance compared to existing models."}}
{"id": "2507.09592", "pdf": "https://arxiv.org/pdf/2507.09592", "abs": "https://arxiv.org/abs/2507.09592", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "THOR: Transformer Heuristics for On-Demand Retrieval", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.", "AI": {"tldr": "The THOR Module is a Text-to-SQL tool that converts natural language into secure SQL for enterprise use, integrating schema awareness, fault-tolerance, and user-friendly insights.", "motivation": "To simplify and democratize access to enterprise databases for non-technical users while ensuring security, reliability, and compliance.", "method": "The module uses a decoupled architecture with a Supervisor Agent to route queries, schema retrieval for metadata injection, a SQL generation agent for query creation, and a self-correction loop for improving outputs. The result interpretation agent enables human-readable insights.", "result": "Smoke tests across various scenarios like finance and sales confirmed that the tool provides reliable querying and automated reporting.", "conclusion": "The THOR Module enables non-technical users to perform secure and efficient querying of enterprise databases with ease, ensuring scalability and safety."}}
{"id": "2507.10400", "pdf": "https://arxiv.org/pdf/2507.10400", "abs": "https://arxiv.org/abs/2507.10400", "authors": ["Nicholas Casetti", "Dylan Anstine", "Olexandr Isayev", "Connor W. Coley"], "title": "Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials", "categories": ["cs.LG", "q-bio.QM"], "comment": "32 pages, 5 figures", "summary": "Reaction mechanism search tools have demonstrated the ability to provide\ninsights into likely products and rate-limiting steps of reacting systems.\nHowever, reactions involving several concerted bond changes - as can be found\nin many key steps of natural product synthesis - can complicate the search\nprocess. To mitigate these complications, we present a mechanism search\nstrategy particularly suited to help expedite exploration of an exemplary\nfamily of such complex reactions, cyclizations. We provide a cost-effective\nstrategy for identifying relevant elementary reaction steps by combining\ngraph-based enumeration schemes and machine learning techniques for\nintermediate filtering. Key to this approach is our use of a neural network\npotential (NNP), AIMNet2-rxn, for computational evaluation of each candidate\nreaction pathway. In this article, we evaluate the NNP's ability to estimate\nactivation energies, demonstrate the correct anticipation of stereoselectivity,\nand recapitulate complex enabling steps in natural product synthesis.", "AI": {"tldr": "The paper introduces a computational strategy combining graph-based methods and machine learning to search reaction mechanisms, particularly for complex cyclization reactions.", "motivation": "Complex reactions involving concerted bond changes, such as those seen in cyclizations during natural product synthesis, pose challenges in mechanism exploration.", "method": "The strategy combines graph-based enumeration to identify reaction steps and machine learning using AIMNet2-rxn neural network potentials for filtering and evaluating pathways.", "result": "The neural network potential successfully estimates activation energies, anticipates stereoselectivity, and reconstructs enabling steps in natural product synthesis.", "conclusion": "This approach provides a cost-effective and accurate methodology to expedite exploration of complex reaction pathways, advancing understanding in reaction mechanism searches."}}
{"id": "2507.10407", "pdf": "https://arxiv.org/pdf/2507.10407", "abs": "https://arxiv.org/abs/2507.10407", "authors": ["Timothy Duff"], "title": "Numerically Computing Galois Groups of Minimal Problems", "categories": ["cs.CV", "cs.SC", "math.AG", "68W30"], "comment": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/\n  references", "summary": "I discuss a seemingly unlikely confluence of topics in algebra, numerical\ncomputation, and computer vision. The motivating problem is that of solving\nmultiples instances of a parametric family of systems of algebraic (polynomial\nor rational function) equations. No doubt already of interest to ISSAC\nattendees, this problem arises in the context of robust model-fitting paradigms\ncurrently utilized by the computer vision community (namely \"Random Sampling\nand Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the\nlast 5+ years that aspires to measure the intrinsic difficulty of solving such\nparametric systems, and makes strides towards practical solutions.", "AI": {"tldr": "The paper connects algebra, numerical computation, and computer vision by examining solution methods for parametric systems of algebraic equations, relevant to robust model-fitting in computer vision.", "motivation": "Understanding the difficulty and solutions of parametric systems of polynomial or rational functions for applications like RanSaC in computer vision.", "method": "Analysis of parametric systems to measure complexity and develop practical problem-solving methods.", "result": "Steps have been taken over the last 5+ years to determine intrinsic difficulty and improve solutions.", "conclusion": "The study blends theoretical and practical advancements, contributing to robust model-fitting and algebraic problem-solving."}}
{"id": "2507.10401", "pdf": "https://arxiv.org/pdf/2507.10401", "abs": "https://arxiv.org/abs/2507.10401", "authors": ["Ryan Bausback", "Jingqiao Tang", "Lu Lu", "Feng Bao", "Toan Huynh"], "title": "Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning", "categories": ["cs.LG", "math.PR"], "comment": null, "summary": "We develop a novel framework for uncertainty quantification in operator\nlearning, the Stochastic Operator Network (SON). SON combines the stochastic\noptimal control concepts of the Stochastic Neural Network (SNN) with the\nDeepONet. By formulating the branch net as an SDE and backpropagating through\nthe adjoint BSDE, we replace the gradient of the loss function with the\ngradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.\nThis allows SON to learn the uncertainty present in operators through its\ndiffusion parameters. We then demonstrate the effectiveness of SON when\nreplicating several noisy operators in 2D and 3D.", "AI": {"tldr": "SON (Stochastic Operator Network) introduces uncertainty quantification in operator learning by leveraging Stochastic Optimal Control principles, integrating with DeepONet.", "motivation": "To address the challenge of incorporating uncertainty quantification into operator learning frameworks, often needed for noisy systems.", "method": "SON constructs the branch network as a Stochastic Differential Equation (SDE) and employs adjoint Backward Stochastic Differential Equation (BSDE) for optimization, integrating Stochastic Maximum Principle concepts into the SGD update.", "result": "SON successfully demonstrates its capability to replicate noisy operators in 2D and 3D environments, effectively quantifying uncertainties.", "conclusion": "The introduced SON framework enhances operator learning by integrating uncertainty quantification, potentially aiding complex systems with stochastic processes."}}
{"id": "2507.10432", "pdf": "https://arxiv.org/pdf/2507.10432", "abs": "https://arxiv.org/abs/2507.10432", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "categories": ["cs.CV", "I.4.7"], "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "AI": {"tldr": "The paper presents SC-AGIQA, a framework for evaluating text-image alignment and visual quality in AI-generated images, addressing semantic misalignment and subtle detail perception issues.", "motivation": "The motivation is to improve the accuracy of evaluating AGI quality by overcoming limitations in existing methods, particularly semantic misalignment and missing detail perception.", "method": "SC-AGIQA uses a Text-assisted Semantic Alignment Module (TSAM) leveraging MLLMs for semantic checks, and a Frequency-domain Fine-Grained Degradation Perception Module (FFDPM) to analyze fine visual details in the frequency domain.", "result": "Experiments on benchmark datasets show SC-AGIQA outperforms SOTA methods in evaluating text-image consistency and perceptual quality.", "conclusion": "The approach effectively advances AGI quality assessment by addressing key challenges and improving evaluation metrics for both alignment and detail perception."}}
{"id": "2507.10409", "pdf": "https://arxiv.org/pdf/2507.10409", "abs": "https://arxiv.org/abs/2507.10409", "authors": ["Amine Lbath", "Ibtissam Labriji"], "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX\n\\textit{student} model that emulates the performance of the \\textit{teacher}\nmodel but with reduced energy consumption. We experiment with different student\nmodel sizes, optimal teacher sizes, and KD hyperparameters. Performance is\nmeasured by comparing the Bit Error Rate (BER) performance versus\nSignal-to-Interference \\& Noise Ratio (SINR) values of the distilled model and\na model trained from scratch. The distilled models demonstrate a lower error\nfloor across SINR levels, highlighting the effectiveness of KD in achieving\nenergy-efficient AI solutions.", "AI": {"tldr": "This paper analyzes energy efficiency in AI/ML models, focusing on DeepRX with ResNet architecture, evaluates energy consumption, and leverages knowledge distillation to create energy-efficient models.", "motivation": "To address the challenge of balancing energy efficiency with performance in AI/ML models, particularly for DeepRX, a machine learning receiver.", "method": "The paper examines energy metrics during training and inference, applies knowledge distillation to create compact student models, and evaluates performance using parameters such as BER and SINR.", "result": "Compact DeepRX student models trained via knowledge distillation achieved reduced energy consumption and better BER performance across SINR levels compared to models trained from scratch.", "conclusion": "Knowledge distillation effectively reduces the energy consumption of AI models while maintaining performance, offering a viable solution for energy-efficient AI implementations."}}
{"id": "2507.10437", "pdf": "https://arxiv.org/pdf/2507.10437", "abs": "https://arxiv.org/abs/2507.10437", "authors": ["Shanshan Zhong", "Jiawei Peng", "Zehan Zheng", "Zhongzhan Huang", "Wufei Ma", "Guofeng Zhang", "Qihao Liu", "Alan Yuille", "Jieneng Chen"], "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "categories": ["cs.CV"], "comment": null, "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.", "AI": {"tldr": "4D-Animal is a novel framework for reconstructing animatable 3D animals from videos without relying on sparse keypoint annotations.", "motivation": "Existing methods require laborious keypoint annotations and rely on unreliable keypoint detectors trained on limited data, which motivates the need for a more efficient and less annotation-dependent solution.", "method": "4D-Animal employs a dense feature network to map 2D data to SMAL parameters and uses hierarchical alignment incorporating silhouette, part-level, pixel-level, and temporal cues to improve quality and coherence.", "result": "4D-Animal surpasses existing model-based and model-free approaches in terms of accuracy and temporal coherence.", "conclusion": "This framework advances 3D animal reconstruction by eliminating dependency on keypoints and creating high-quality, reusable 3D assets for broader applications."}}
{"id": "2507.09647", "pdf": "https://arxiv.org/pdf/2507.09647", "abs": "https://arxiv.org/abs/2507.09647", "authors": ["Peican Zhu", "Yubo Jing", "Le Cheng", "Keke Tang", "Yangming Guo"], "title": "KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection", "categories": ["cs.MM", "cs.AI"], "comment": "Accepted by ACM MM 2025", "summary": "In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.", "AI": {"tldr": "The paper proposes KEN, a novel model leveraging knowledge augmentation and emotion guidance to improve multimodal fake news detection by enhancing semantics understanding and considering emotional types.", "motivation": "To address the challenges of understanding image semantics and tailored emotional analysis in fake news detection, which are overlooked by prior research.", "method": "Introduced KEN, which utilizes LVLM for semantic enhancement and world knowledge integration, along with balanced learning for modeling emotional type differences.", "result": "KEN demonstrated superior performance in multimodal fake news detection through experiments on two real-world datasets.", "conclusion": "The proposed KEN model effectively improves fake news detection by addressing semantic gaps and emotional type diversity, proving its superiority in experiments."}}
{"id": "2507.10449", "pdf": "https://arxiv.org/pdf/2507.10449", "abs": "https://arxiv.org/abs/2507.10449", "authors": ["Hongyong Han", "Wei Wang", "Gaowei Zhang", "Mingjie Li", "Yi Wang"], "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.", "AI": {"tldr": "This work introduces CoralVQA, the first large-scale Visual Question Answering (VQA) dataset focused on coral reef images, aimed at supporting marine conservation through improved vision-language reasoning.", "motivation": "The motivation is to aid coral reef conservation by addressing the challenges of interpreting coral reef images, specifically through the development of a domain-specific VQA dataset for better understanding and analysis.", "method": "The researchers developed CoralVQA, consisting of 12,805 coral images and 277,653 question-answer pairs, using a semi-automatic data pipeline created in collaboration with marine biologists for scalability and high-quality data.", "result": "CoralVQA highlights novel challenges in vision-language reasoning and serves as a benchmark for studying coral reef imagery; evaluations of current LVLMs reveal both their limitations and potential in this domain.", "conclusion": "CoralVQA sets the stage for future advancements in LVLMs, specifically tailored toward coral conservation and improving ecological image analysis."}}
{"id": "2507.09664", "pdf": "https://arxiv.org/pdf/2507.09664", "abs": "https://arxiv.org/abs/2507.09664", "authors": ["Zoe Kaputa", "Anika Rajaram", "Vryan Almanon Feliciano", "Zhuoyue Lyu", "Maneesh Agrawala", "Hari Subramonyam"], "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Programming-by-prompting with generative AI offers a new paradigm for\nend-user programming, shifting the focus from syntactic fluency to semantic\nintent. This shift holds particular promise for non-programmers such as\neducators, who can describe instructional goals in natural language to generate\ninteractive learning content. Yet in bypassing direct code authoring, many of\nprogramming's core affordances - such as traceability, stepwise refinement, and\nbehavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)\nframework as a way to recover these affordances while preserving the expressive\nflexibility of natural language. CoA decomposes the synthesis process into a\nsequence of cognitively meaningful, task-aligned representations that function\nas checkpoints for specification, inspection, and refinement. We instantiate\nthis approach in SimStep, an authoring environment for teachers that scaffolds\nsimulation creation through four intermediate abstractions: Concept Graph,\nScenario Graph, Learning Goal Graph, and UI Interaction Graph. To address\nambiguities and misalignments, SimStep includes an inverse correction process\nthat surfaces in-filled model assumptions and enables targeted revision without\nrequiring users to manipulate code. Evaluations with educators show that CoA\nenables greater authoring control and interpretability in\nprogramming-by-prompting workflows.", "AI": {"tldr": "The paper introduces a framework called Chain-of-Abstractions (CoA) that enhances programming-by-prompting with generative AI, aimed particularly at helping educators create learning content using natural language.", "motivation": "To enable non-programmers, particularly educators, to create interactive learning content using natural language while retaining programming's core affordances such as traceability and refinement.", "method": "The method involves decomposing the synthesis process into cognitively meaningful representations, instantiated as the SimStep authoring environment with intermediate abstractions. It uses an inverse correction mechanism to address model assumptions and ambiguities.", "result": "Evaluations with educators show that the CoA framework provides better authoring control and interpretability in programming-by-prompting workflows.", "conclusion": "CoA and SimStep enhance the flexibility and usability of programming-by-prompting, making it practical and beneficial for educators to create simulations effectively without prior programming knowledge."}}
{"id": "2507.10461", "pdf": "https://arxiv.org/pdf/2507.10461", "abs": "https://arxiv.org/abs/2507.10461", "authors": ["Tao Tang", "Chengxu Yang"], "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "comment": "To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures", "summary": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.", "AI": {"tldr": "The paper presents RAPNet, a novel pansharpening technique using content-adaptive convolution for higher performance.", "motivation": "Overcome the limitations of CNNs in pansharpening due to their uniform application of convolution, neglecting spatial content variations.", "method": "Introduced RAPNet with RAPConv for spatially adaptive kernels and PAN-DFF module for balancing spatial detail and spectral fidelity.", "result": "RAPNet showed superior performance against prior methods using quantitative and qualitative measures on public datasets.", "conclusion": "RAPNet's adaptive components demonstrably enhance pansharpening via evaluations and ablation studies."}}
{"id": "2507.10434", "pdf": "https://arxiv.org/pdf/2507.10434", "abs": "https://arxiv.org/abs/2507.10434", "authors": ["Giacomo Cignoni", "Andrea Cossu", "Alexandra Gomez-Villa", "Joost van de Weijer", "Antonio Carta"], "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at CoLLAs 2025 conference", "summary": "Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.", "AI": {"tldr": "The paper introduces CLA, a novel self-supervised learning approach for online continual learning that mitigates forgetting and enhances performance.", "motivation": "To address the lack of effective self-supervised learning techniques for online continual learning, where data is processed sequentially, constrained by computational budgets, and comes without clear task boundaries.", "method": "Proposed Continual Latent Alignment (CLA), which aligns current model representations with past ones to reduce forgetting and improve training convergence.", "result": "CLA sped up convergence in the online continual learning scenario and outperformed existing approaches under identical computational constraints. Early-stage CLA pretraining also enhanced final performance compared to traditional i.i.d. pretraining.", "conclusion": "CLA is both a superior method for online continual learning and a beneficial pretraining protocol, proving its efficacy and flexibility."}}
{"id": "2507.10470", "pdf": "https://arxiv.org/pdf/2507.10470", "abs": "https://arxiv.org/abs/2507.10470", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.", "AI": {"tldr": "This paper introduces RefSTAR, a method addressing identity preservation in blind facial image restoration by using effective feature integration from high-quality references.", "motivation": "Existing facial restoration methods struggle with restoring identity features mainly due to improper handling of detailed textures.", "method": "RefSTAR involves a reference selection module (RefSel), feature fusion for transferring information from references, and a reconstruction mechanism with redesigned cycle consistency loss.", "result": "Experiments on multiple models demonstrate superior identity preservation and reference feature transfer quality using the proposed approach.", "conclusion": "The RefSTAR method effectively enhances blind facial restoration by improving the integration of features from high-quality reference images, resulting in better identity preservation and restoration quality."}}
{"id": "2507.10442", "pdf": "https://arxiv.org/pdf/2507.10442", "abs": "https://arxiv.org/abs/2507.10442", "authors": ["Shivam Chandhok", "Wan-Cyuan Fan", "Vered Shwartz", "Vineeth N Balasubramanian", "Leonid Sigal"], "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Vision-language Models (VLMs) have emerged as general-purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, lacking some basic visual\nunderstanding skills. In this paper, we set out to understand the limitations\nof SoTA VLMs on fundamental visual tasks by constructing a series of tests that\nprobe which components of design, specifically, may be lacking. Importantly, we\ngo significantly beyond the current benchmarks, which simply measure the final\nperformance of VLM response, by also comparing and contrasting it to the\nperformance of probes trained directly on features obtained from the visual\nencoder, intermediate vision-language projection and LLM-decoder output. In\ndoing so, we uncover shortcomings in VLMs and make a number of important\nobservations about their capabilities, robustness and how they process visual\ninformation. We hope our insights will guide progress in further improving\nVLMs.", "AI": {"tldr": "The paper investigates limitations in Vision-language Models (VLMs) on fundamental visual tasks through detailed probes and evaluations beyond current benchmarks.", "motivation": "To identify shortcomings and enhance the understanding of Vision-language Models (VLMs) regarding their performance on fundamental visual tasks.", "method": "Develop tests to probe VLMs at different stages\u2014features from visual encoders, intermediate vision-language projection, and LLM-decoder outputs\u2014to uncover weaknesses and analyze performance.", "result": "The study identifies specific limitations in VLMs' visual understanding capabilities and provides insights into their robustness, processing methods, and areas for improvement.", "conclusion": "The observations aim to contribute to improving VLMs and provide guidance for future development of these models."}}
{"id": "2507.10473", "pdf": "https://arxiv.org/pdf/2507.10473", "abs": "https://arxiv.org/abs/2507.10473", "authors": ["David G. Shatwell", "Ishan Rajendrakumar Dave", "Sirnam Swetha", "Mubarak Shah"], "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space", "categories": ["cs.CV"], "comment": "Accepted in ICCV2025", "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.", "AI": {"tldr": "The paper introduces GT-Loc, a method for predicting image timestamps and geo-locations using a shared embedding space and novel temporal metric-learning.", "motivation": "Accurately predicting image timestamps and geo-locations is crucial for applications like metadata correction, retrieval, and digital forensics, yet current methods do not adequately integrate these tasks and their interdependencies.", "method": "GT-Loc employs separate encoders for images, time, and location, aligning embeddings in a shared space. It introduces a temporal metric-learning model to account for cyclic time differences over a toroidal surface.", "result": "GT-Loc outperforms prior timestamp prediction methods and demonstrates competitive performance in geo-localization tasks. The unified embedding space also enables advanced uses like compositional and text-based image retrieval.", "conclusion": "The proposed joint optimization approach enhances both timestamp and geo-location predictions, showing its utility across different applications."}}
{"id": "2507.10452", "pdf": "https://arxiv.org/pdf/2507.10452", "abs": "https://arxiv.org/abs/2507.10452", "authors": ["Eduardo D. Sontag"], "title": "Some remarks on gradient dominance and LQR policy optimization", "categories": ["cs.LG"], "comment": "This is a short paper summarizing the first part of the slides\n  presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control\n  Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has\n  been added. A second part on neural net feedback controllers is to be added", "summary": "Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.", "AI": {"tldr": "The paper discusses optimization via gradient descent, focusing on the Polyak-\u0141ojasiewicz Inequality (PLI) and its generalized versions to address global exponential convergence in reinforcement learning and control. Input-to-state stability is explored, alongside challenges like gradient estimation errors.", "motivation": "The authors aim to generalize PLI-like conditions to address the gap between global exponential convergence in discrete-time LQR problems versus mixed locally and globally linear convergence in continuous-time LQR problems.", "method": "The authors approach the issue by exploring generalized PLI-like conditions and conducting an 'input-to-state stability' (ISS) analysis to assess transient and asymptotic effects of gradient estimation errors.", "result": "The paper identifies generalized PLI-like frameworks that can enhance understanding of optimization behavior under gradient flow and account for errors arising from various practical challenges.", "conclusion": "The generalized PLI conditions and ISS analysis provide insights into optimization convergence properties and resilience to gradient estimation errors, advancing knowledge in machine learning, control, and optimization."}}
{"id": "2507.10484", "pdf": "https://arxiv.org/pdf/2507.10484", "abs": "https://arxiv.org/abs/2507.10484", "authors": ["Paul Fogel", "Christophe Geissler", "George Luta"], "title": "The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization", "categories": ["cs.LG"], "comment": "6 pages, 4 figures, International Conference on Robust Statistics\n  2025, Stresa, Italy", "summary": "This paper introduces the \"Target Polish,\" a robust and computationally\nefficient framework for nonnegative matrix and tensor factorization. Although\nconventional weighted NMF approaches are resistant to outliers, they converge\nslowly due to the use of multiplicative updates to minimize the objective\ncriterion. In contrast, the Target Polish approach remains compatible with the\nFast-HALS algorithm, which is renowned for its speed, by adaptively smoothing\nthe data with a weighted median-based transformation. This innovation provides\noutlier resistance while maintaining the highly efficient additive update\nstructure of Fast-HALS. Empirical evaluations using image datasets corrupted\nwith structured (block) and unstructured (salt) noise demonstrate that the\nTarget Polish approach matches or exceeds the accuracy of state-of-the-art\nrobust NMF methods and reduces computational time by an order of magnitude in\nthe studied scenarios.", "AI": {"tldr": "The paper introduces 'Target Polish,' a fast, robust framework for nonnegative matrix and tensor factorization that improves efficiency and outlier resistance, tested on image data with noise.", "motivation": "Existing weighted NMF approaches are slow due to multiplicative updates, necessitating a faster and robust method for handling outliers.", "method": "The Target Polish framework applies an adaptive weighted median-based transformation that complements the Fast-HALS algorithm's speed while enhancing outlier resistance.", "result": "Empirical tests on noisy image datasets show that Target Polish matches or surpasses state-of-the-art robust NMF methods in accuracy and significantly reduces computation time.", "conclusion": "Target Polish offers a robust and efficient solution for nonnegative matrix and tensor factorization, overcoming limitations of existing methods."}}
{"id": "2507.10490", "pdf": "https://arxiv.org/pdf/2507.10490", "abs": "https://arxiv.org/abs/2507.10490", "authors": ["Tugberk Erol", "Tuba Caglikantar", "Duygu Sarikaya"], "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation", "categories": ["cs.CV"], "comment": "9 pages, 3 figures", "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.", "AI": {"tldr": "The paper introduces a confidence-based self-distillation method for polyp segmentation that achieves superior results without requiring additional computation during testing.", "motivation": "State-of-the-art deep learning models for polyp detection often demand significant resources and struggle with overfitting and generalization across diverse datasets.", "method": "The authors propose a confidence-based self-distillation strategy that leverages previous iteration data within a batch using a dynamic confidence coefficient, avoiding extra computation or memory at test time.", "result": "The proposed approach achieves better performance than state-of-the-art methods and demonstrates strong generalization across datasets from various clinical centers.", "conclusion": "The novel self-distillation method offers a resource-efficient solution for polyp segmentation, overcoming limitations of existing models and addressing generalization challenges."}}
{"id": "2507.10485", "pdf": "https://arxiv.org/pdf/2507.10485", "abs": "https://arxiv.org/abs/2507.10485", "authors": ["Brandon Shuen Yi Loke", "Filippo Quadri", "Gabriel Vivanco", "Maximilian Casagrande", "Sa\u00fal Fenollosa"], "title": "Overcoming catastrophic forgetting in neural networks", "categories": ["cs.LG", "cs.IR"], "comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project", "summary": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.", "AI": {"tldr": "The paper focuses on mitigating catastrophic forgetting in continual learning through Elastic Weight Consolidation (EWC), showcasing its effectiveness in retaining prior knowledge while slightly reducing adaptability.", "motivation": "To address the challenge of catastrophic forgetting in neural networks learning sequential tasks, inspired by biological synaptic consolidation.", "method": "The study evaluates EWC in supervised learning using PermutedMNIST and RotatedMNIST benchmarks and compares it to L2 regularization and SGD. It also explores dropout effects and hyperparameter variations.", "result": "EWC is confirmed to reduce forgetting significantly compared to naive training, albeit at the cost of slightly lowered learning adaptability for new tasks.", "conclusion": "EWC is validated as an effective method for lifelong learning, balancing knowledge retention and adaptability across diverse scenarios."}}
{"id": "2507.10492", "pdf": "https://arxiv.org/pdf/2507.10492", "abs": "https://arxiv.org/abs/2507.10492", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "title": "BenchReAD: A systematic benchmark for retinal anomaly detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "MICCAI 2025", "summary": "Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.", "AI": {"tldr": "A new benchmark for retinal anomaly detection is introduced to address existing limitations in evaluating methods and datasets. The proposed approach, NFM-DRA, sets a new state-of-the-art.", "motivation": "The study aims to overcome shortcomings in retinal anomaly detection research, such as limited datasets, inadequate test setups, and the underutilization of labeled abnormal and unlabeled data.", "method": "A benchmark for retinal anomaly detection was developed, categorizing methods and introducing NFM-DRA\u2014a fully supervised approach integrated with Normal Feature Memory.", "result": "The NFM-DRA approach enhances performance, addresses unseen anomaly challenges, and sets a new state-of-the-art in retinal anomaly detection.", "conclusion": "The research fills a critical gap in the field by creating a systematic benchmark and advancing methodology, providing opportunities for more robust evaluations and improved detection techniques."}}
{"id": "2507.10494", "pdf": "https://arxiv.org/pdf/2507.10494", "abs": "https://arxiv.org/abs/2507.10494", "authors": ["Tanveer Khan", "Mindaugas Budzys", "Antonis Michalas"], "title": "Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.", "AI": {"tldr": "SplitHappens enhances Split Learning's (SL) security by combining it with Function Secret Sharing (FSS), addressing a wider range of attacks, and improving efficiency.", "motivation": "Addressing vulnerabilities in Split Learning, particularly its susceptibility to attacks while preserving data privacy and optimizing communication and computation costs.", "method": "Use U-shaped Split Learning combined with Function Secret Sharing to improve security and solve limitations tied to label sharing with servers.", "result": "The approach improves training efficiency, reduces communication costs, and successfully counters model inversion and label inference attacks, achieving comparable accuracy levels.", "conclusion": "By expanding security analyses and optimizing computational demands, SplitHappens offers a practical advancement in protecting SL workflows from emerging attacks."}}
{"id": "2507.10496", "pdf": "https://arxiv.org/pdf/2507.10496", "abs": "https://arxiv.org/abs/2507.10496", "authors": ["Ruilong Li", "Brent Yi", "Junchen Liu", "Hang Gao", "Yi Ma", "Angjoo Kanazawa"], "title": "Cameras as Relative Positional Encoding", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://www.liruilong.cn/prope/", "summary": "Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.", "AI": {"tldr": "The paper evaluates and compares methods for integrating camera geometry into multi-view transformers for 3D computer vision tasks, introducing Projective Positional Encoding (PRoPE) for enhanced performance.", "motivation": "There is a need to effectively integrate geometric relationships between viewpoints into multi-view transformers for better 3D perception in computer vision tasks.", "method": "The study compares techniques like raymap encodings, relative pose encodings, and a novel encoding method (PRoPE) that incorporates complete camera frustums as relative positional encodings.", "result": "PRoPE outperforms other methods in various tasks, including novel view synthesis, stereo depth estimation, and spatial cognition, while also generalizing well to diverse scenarios.", "conclusion": "Conditioning multi-view transformers on camera geometry, especially using PRoPE, significantly improves their performance and generalization capabilities across different computer vision tasks."}}
{"id": "2507.10502", "pdf": "https://arxiv.org/pdf/2507.10502", "abs": "https://arxiv.org/abs/2507.10502", "authors": ["Elizabeth Fahsbender", "Alma Andersson", "Jeremy Ash", "Polina Binder", "Daniel Burkhardt", "Benjamin Chang", "Georg K. Gerber", "Anthony Gitter", "Patrick Godau", "Ankit Gupta", "Genevieve Haliburton", "Siyu He", "Trey Ideker", "Ivana Jelic", "Aly Khan", "Yang-Joon Kim", "Aditi Krishnapriyan", "Jon M. Laurent", "Tianyu Liu 28", "Emma Lundberg", "Shalin B. Mehta", "Rob Moccia", "Angela Oliveira Pisco", "Katherine S. Pollard", "Suresh Ramani", "Julio Saez-Rodriguez", "Yasin Senbabaoglu", "Elana Simon", "Srinivasan Sivanandan", "Gustavo Stolovitzky", "Marc Valer", "Bo Wang", "Xikun Zhang", "James Zou", "Katrina Kalantar"], "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.", "AI": {"tldr": "This paper highlights the need for standardized benchmarks to evaluate AI-driven biological models across domains and modalities, addressing challenges such as data heterogeneity and reproducibility.", "motivation": "The motivation lies in the current lack of cross-domain benchmarks, which hampers the development of reliable AI models in biological fields like genomics and proteomics.", "method": "The authors convened a workshop with experts to identify bottlenecks and proposed recommendations for building effective benchmarking frameworks.", "result": "The paper identifies key bottlenecks like data heterogeneity and biases and proposes solutions like standardized tooling and collaborative platforms.", "conclusion": "Standardized benchmarks are essential for ensuring the rigor and trustworthiness of AI-driven Virtual Cell models, advancing biological research and therapeutic insights."}}
{"id": "2507.10499", "pdf": "https://arxiv.org/pdf/2507.10499", "abs": "https://arxiv.org/abs/2507.10499", "authors": ["Philippe Rufin", "Pauline Lucie Hammer", "Leon-Friedrich Thomas", "S\u00e1 Nogueira Lisboa", "Natasha Ribeiro", "Almeida Sitoe", "Patrick Hostert", "Patrick Meyfroidt"], "title": "National level satellite-based crop field inventories in smallholder landscapes", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.", "AI": {"tldr": "This paper uses high-resolution satellite imagery and deep transfer learning to map 21 million crop fields in Mozambique, achieving a high level of accuracy and providing valuable insights into agricultural patterns.", "motivation": "The study aims to address the challenge of designing sustainable policies for smallholder agriculture by improving the understanding of cropland distribution and field sizes, especially in complex systems.", "method": "The researchers employed very-high-resolution Earth observation data (1.5 m) and deep transfer learning techniques to accurately delineate crop fields across Mozambique, requiring minimal reference data and ensuring transferability.", "result": "The study produced the first national dataset of 21 million individual crop fields in Mozambique with 93% accuracy, revealing details about fragmented agricultural regions and variations in field size.", "conclusion": "Field size is a crucial indicator of socio-economic and environmental agricultural outcomes, highlighting trade-offs in livelihoods, food production, and biodiversity, especially in diverse farming ecosystems."}}
{"id": "2507.09759", "pdf": "https://arxiv.org/pdf/2507.09759", "abs": "https://arxiv.org/abs/2507.09759", "authors": ["Abdul Manaf", "Nimra Mughal"], "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification", "AI": {"tldr": "This study created a CNN-based system for accurate pediatric pneumonia diagnosis using chest X-rays, leveraging data augmentation and GANs for enhanced results.", "motivation": "The study aims to address the critical need for accurate and efficient pneumonia diagnosis in children under five, as it is a leading cause of mortality.", "method": "A CNN model was trained on 5,863 pediatric X-ray images. Augmentation techniques and GANs were used to enhance data diversity and address class imbalance. The system was deployed via a Flask web app for real-time use.", "result": "The model, combining original, augmented, and GAN-generated data, achieved high diagnostic accuracy and efficiency, validated by metrics like accuracy and F1 score.", "conclusion": "The system demonstrates the efficacy of deep learning and GANs in pediatric pneumonia diagnosis, with significant potential for application in resource-limited clinical settings."}}
{"id": "2507.10547", "pdf": "https://arxiv.org/pdf/2507.10547", "abs": "https://arxiv.org/abs/2507.10547", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "AI": {"tldr": "The paper introduces the ReVQ framework, transforming pre-trained VAEs into VQ-VAEs efficiently, drastically cutting computational costs while maintaining quality.", "motivation": "To address the computational inefficiency of training high-compression-rate VQ-VAEs, making development less GPU-intensive for multimodal large models.", "method": "ReVQ leverages pre-trained VAEs, introducing channel multi-group quantization and post rectifiers to enhance codebook capacity and minimize quantization errors.", "result": "ReVQ compresses ImageNet images into 512 tokens with competitive reconstruction quality (rFID = 1.06), reducing GPU training requirements significantly.", "conclusion": "ReVQ offers an efficient and cost-effective approach for training VQ-VAEs, setting a new benchmark in efficiency and reconstruction quality trade-offs."}}
{"id": "2507.10536", "pdf": "https://arxiv.org/pdf/2507.10536", "abs": "https://arxiv.org/abs/2507.10536", "authors": ["Qiaoyue Tang", "Alain Zhiyanov", "Mathias L\u00e9cuyer"], "title": "On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we analyze the optimization behaviour of common private\nlearning optimization algorithms under heavy-tail class imbalanced\ndistribution. We show that, in a stylized model, optimizing with Gradient\nDescent with differential privacy (DP-GD) suffers when learning low-frequency\nclasses, whereas optimization algorithms that estimate second-order information\ndo not. In particular, DP-AdamBC that removes the DP bias from estimating loss\ncurvature is a crucial component to avoid the ill-condition caused by\nheavy-tail class imbalance, and empirically fits the data better with\n$\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the\nleast frequent classes on both controlled experiments and real data\nrespectively.", "AI": {"tldr": "This paper studies how differential privacy optimization algorithms perform under class imbalance with heavy-tail distributions. Gradient Descent struggles with low-frequency classes, but second-order methods like DP-AdamBC improve training accuracy.", "motivation": "To understand and improve the optimization behavior of private learning algorithms under challenging class imbalance and heavy-tail distribution scenarios.", "method": "The authors analyze optimization behavior using a stylized model and compare optimization mechanisms such as DP-GD and DP-AdamBC. DP-AdamBC adjusts second-order information to mitigate differential privacy bias.", "result": "Second-order optimization algorithms outperform DP-GD in learning low-frequency classes of imbalanced distributions. DP-AdamBC shows increases in training accuracy by 8% and 5% for least frequent classes in both controlled setups and real-world data.", "conclusion": "Second-order methods (e.g., DP-AdamBC) are crucial for optimizing private learning models in class imbalanced and heavy-tail contexts. These methods address issues in first-order approaches and enhance model performance."}}
{"id": "2507.10539", "pdf": "https://arxiv.org/pdf/2507.10539", "abs": "https://arxiv.org/abs/2507.10539", "authors": ["Tao Feng", "Yexin Wu", "Guanyu Lin", "Jiaxuan You"], "title": "Graph World Model", "categories": ["cs.LG"], "comment": null, "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.", "AI": {"tldr": "This paper proposes Graph World Model (GWM), a framework combining unstructured and graph-structured data to handle multi-modal tasks using unified token or embedding spaces, outperforming baselines in six diverse domains.", "motivation": "Existing World Models struggle with incorporating structured data often represented as graphs, while graph foundation models lack versatility in dealing with diverse multi-modal data.", "method": "The paper introduces GWM, employing a generic message-passing algorithm over multi-modal token or embedding spaces, and incorporates action nodes for diverse tasks via direct reference or similarity computation.", "result": "Experiments in six areas, including recommendation, graph prediction, and planning, show GWM's enhanced performance, adaptability to unseen tasks, and benefits from multi-hop structures.", "conclusion": "GWM successfully integrates unstructured and structured data in a unified framework, demonstrating versatility and superior performance across multiple domains and tasks."}}
{"id": "2507.10552", "pdf": "https://arxiv.org/pdf/2507.10552", "abs": "https://arxiv.org/abs/2507.10552", "authors": ["Vladimir Iashin", "Horace Lee", "Dan Schofield", "Andrew Zisserman"], "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/", "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.", "AI": {"tldr": "This paper proposes a self-supervised method for identifying chimpanzee faces from camera-trap footage without requiring labeled data, showing strong performance compared to supervised approaches.", "motivation": "The manual identification of individual animals from camera-trap footage is a bottleneck in wildlife monitoring due to the time and effort required.", "method": "The study employs the DINOv2 framework with Vision Transformers to learn face embeddings from automatically cropped and unlabelled chimpanzee face images, relying entirely on self-supervised learning.", "result": "The new method achieves superior open-set re-identification performance compared to supervised baselines on datasets like Bossou.", "conclusion": "The approach highlights the potential of self-supervised learning for scalable biodiversity monitoring and supports non-invasive population study techniques."}}
{"id": "2507.10540", "pdf": "https://arxiv.org/pdf/2507.10540", "abs": "https://arxiv.org/abs/2507.10540", "authors": ["Tao Feng", "Haozhen Zhang", "Zijie Lei", "Pengrui Han", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Jiaxuan You"], "title": "Fusing LLM Capabilities with Routing Data", "categories": ["cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.", "AI": {"tldr": "The paper addresses the inefficiency of relying on a single LLM by introducing FusionBench, a routing benchmark, and FusionFactory, a systematic fusion framework that integrates strengths of multiple models to enhance performance across diverse tasks.", "motivation": "Although a wide variety of LLMs exist with unique capabilities, reliance on a single model leads to inefficiencies and underutilizes the strengths of the broader ecosystem in tasks requiring diverse abilities.", "method": "The authors propose FusionBench, a benchmark to assess LLMs across tasks, and FusionFactory, a fusion framework with three levels: query-level routing, thought-level aggregation, and model-level distillation to systematically leverage multiple LLMs.", "result": "FusionFactory showed consistent performance improvements over individual LLMs across 14 benchmarks, with the optimal configurations depending on the benchmark.", "conclusion": "Systematically combining multiple LLMs via routing, aggregation, and distillation enhances task coverage, performance, and highlights the potential of collaborative use of diverse models."}}
{"id": "2507.10546", "pdf": "https://arxiv.org/pdf/2507.10546", "abs": "https://arxiv.org/abs/2507.10546", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "title": "Disentangling Neural Disjunctive Normal Form Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at NeSy 2025", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "AI": {"tldr": "The paper improves neural-disjunctive normal form (DNF) models by addressing performance losses during post-training symbolic translation via a disentanglement method. This approach delivers more compact and interpretable logical representations.", "motivation": "There is a need to address performance degradation of neural DNF-based models caused by failures in disentangling learned knowledge during the symbolic translation process.", "method": "The paper introduces a disentanglement method that splits nodes encoding nested rules into smaller, independent nodes. This ensures better preservation of the pre-translation model's performance.", "result": "Experiments on binary, multiclass, and multilabel classification tasks show that the new method leads to compact and interpretable symbolic models with performance closer to the pre-translation models.", "conclusion": "The proposed disentanglement approach enhances the preservation of model performance, enabling neural DNF models to maintain their classification abilities while yielding more interpretable logical representations."}}
{"id": "2507.08855", "pdf": "https://arxiv.org/pdf/2507.08855", "abs": "https://arxiv.org/abs/2507.08855", "authors": ["Yang Ming", "Jiang Shi Zhong", "Zhou Su Juan"], "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\ncharacterized by progressive cognitive decline as its main symptom. In the\nresearch field of deep learning-assisted diagnosis of AD, traditional\nconvolutional neural networks and simple feature concatenation methods fail to\neffectively utilize the complementary information between multimodal data, and\nthe simple feature concatenation approach is prone to cause the loss of key\ninformation during the process of modal fusion. In recent years, the\ndevelopment of deep learning technology has brought new possibilities for\nsolving the problem of how to effectively fuse multimodal features. This paper\nproposes a novel deep learning algorithm framework to assist medical\nprofessionals in AD diagnosis. By fusing medical multi-view information such as\nbrain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance\nimaging (MRI), genetic data, and clinical data, it can accurately detect the\npresence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).\nThe innovation of the algorithm lies in the use of an asymmetric cross-modal\ncross-attention mechanism, which can effectively capture the key information\nfeatures of the interactions between different data modal features. This paper\ncompares the asymmetric cross-modal cross-attention mechanism with the\ntraditional algorithm frameworks of unimodal and multimodal deep learning\nmodels for AD diagnosis, and evaluates the importance of the asymmetric\ncross-modal cross-attention mechanism. The algorithm model achieves an accuracy\nof 94.88% on the test set.", "AI": {"tldr": "The paper introduces an advanced deep learning algorithm with an asymmetric cross-modal cross-attention mechanism to accurately diagnose Alzheimer's Disease using multimodal data, achieving 94.88% accuracy.", "motivation": "Traditional deep learning methods struggle to effectively utilize multimodal data and often lose key information due to simple feature concatenation. This paper aims to enhance the fusion of complementary multimodal data for improved Alzheimer's Disease diagnosis.", "method": "The authors propose a novel deep learning framework employing an asymmetric cross-modal cross-attention mechanism to fuse multimodal data from brain PET, MRI, genetic, and clinical sources, which enhances key information detection during inter-modal interactions.", "result": "The proposed algorithm achieved an impressive accuracy of 94.88% on the test set, demonstrating its significant improvement over traditional unimodal and multimodal approaches in Alzheimer's Disease diagnosis.", "conclusion": "The study highlights the effectiveness of the asymmetric cross-modal cross-attention mechanism for multimodal data fusion, offering a promising tool for assisting medical professionals in detecting Alzheimer's Disease and related conditions."}}
{"id": "2507.09792", "pdf": "https://arxiv.org/pdf/2507.09792", "abs": "https://arxiv.org/abs/2507.09792", "authors": ["Prashant Govindarajan", "Davide Baldelli", "Jay Pathak", "Quentin Fournier", "Sarath Chandar"], "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.", "AI": {"tldr": "Introducing a large-scale dataset and fine-tuned large language models for automating CAD design using natural language descriptions.", "motivation": "To address the time-intensive, manual nature of CAD modeling, and to explore the potential of large language models (LLMs) for efficient CAD sequence automation.", "method": "A dataset of 170k CAD models with human-like descriptions generated via GPT-4 is introduced. Fine-tuned code-LLMs use JSON-based CAD sequence representations for text-conditioned CAD generation. Geometric and topological metrics are proposed to evaluate quality.", "result": "Fine-tuned LLMs successfully automate CAD design, drastically accelerating the process and exhibiting effectiveness in both synthetic and human-annotated cases. New evaluation metrics provide richer structural insights.", "conclusion": "The work demonstrates the viability of LLMs for automating CAD design, offering a significant improvement in efficiency. The dataset and models are released for community use."}}
{"id": "2507.08952", "pdf": "https://arxiv.org/pdf/2507.08952", "abs": "https://arxiv.org/abs/2507.08952", "authors": ["Silas Nyboe \u00d8rting", "Kristina Miger", "Anne Sophie Overgaard Olesen", "Mikael Ploug Boesen", "Michael Brun Andersen", "Jens Petersen", "Olav W. Nielsen", "Marleen de Bruijne"], "title": "Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": "34 pages, 11 figures, Submitted to \"Radiology AI\"", "summary": "Introduction: Chest CT scans are increasingly used in dyspneic patients where\nacute heart failure (AHF) is a key differential diagnosis. Interpretation\nremains challenging and radiology reports are frequently delayed due to a\nradiologist shortage, although flagging such information for emergency\nphysicians would have therapeutic implication. Artificial intelligence (AI) can\nbe a complementary tool to enhance the diagnostic precision. We aim to develop\nan explainable AI model to detect radiological signs of AHF in chest CT with an\naccuracy comparable to thoracic radiologists.\n  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen\nUniversity Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees\nmodel was trained to predict AHF based on measurements of segmented cardiac and\npulmonary structures from acute thoracic CT scans. Diagnostic labels for\ntraining and testing were extracted from radiology reports. Structures were\nsegmented with TotalSegmentator. Shapley Additive explanations values were used\nto explain the impact of each measurement on the final prediction.\n  Results: Of the 4,672 subjects, 49% were female. The final model incorporated\ntwelve key features of AHF and achieved an area under the ROC of 0.87 on the\nindependent test set. Expert radiologist review of model misclassifications\nfound that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false\nnegatives were actually correct model predictions, with the errors originating\nfrom inaccuracies in the initial radiology reports.\n  Conclusion: We developed an explainable AI model with strong discriminatory\nperformance, comparable to thoracic radiologists. The AI model's stepwise,\ntransparent predictions may support decision-making.", "AI": {"tldr": "An AI model was developed to detect acute heart failure indicators in chest CT scans with high accuracy comparable to expert radiologists.", "motivation": "The paper aims to address delays and challenges in radiological interpretations of chest CT scans for diagnosing acute heart failure, utilizing AI as a complementary diagnostic tool.", "method": "The study involved training a Boosted Trees model using segmented cardiac and pulmonary CT measurements, with diagnostic labels derived from radiology reports. Shapley Additive values explained feature importance.", "result": "The model achieved a ROC-AUC of 0.87, with expert review indicating errors were often due to inaccuracies in radiology reports rather than model predictions.", "conclusion": "The AI model demonstrates potential for enhancing diagnostic precision by providing explainable and accurate predictions comparable to radiologist evaluations."}}
{"id": "2507.08821", "pdf": "https://arxiv.org/pdf/2507.08821", "abs": "https://arxiv.org/abs/2507.08821", "authors": ["Pedro D. Alvim", "Hugerles S. Silva", "Ugo S. Dias", "Osamah S. Badarneh", "Felipe A. P. Figueiredo", "Rausley A. A. de Souza"], "title": "LNN-powered Fluid Antenna Multiple Access", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Fluid antenna systems represent an innovative approach in wireless\ncommunication, recently applied in multiple access to optimize the\nsignal-to-interference-plus-noise ratio through port selection. This letter\nframes the port selection problem as a multi-label classification task for the\nfirst time, improving best-port selection with limited port observations. We\naddress this challenge by leveraging liquid neural networks (LNNs) to predict\nthe optimal port under emerging fluid antenna multiple access scenarios\nalongside a more general $\\alpha$-$\\mu$ fading model. We also apply\nhyperparameter optimization to refine LNN architectures for different\nobservation scenarios. Our approach yields lower outage probability values than\nexisting methods.", "AI": {"tldr": "The paper applies liquid neural networks (LNNs) to optimize port selection in fluid antenna systems, achieving better performance with lower outage probabilities.", "motivation": "To enhance the efficiency of best-port selection in fluid antenna systems under limited port observations, particularly in the context of multiple-access scenarios.", "method": "The researchers use LNNs for optimal port prediction in fluid antenna systems and optimize LNN architectures for various observation scenarios through hyperparameter tuning.", "result": "The proposed approach achieves lower outage probability compared to existing methods, improving system performance.", "conclusion": "Liquid neural networks are effective in addressing port selection challenges in fluid antenna systems, demonstrating significant improvements in reliability and efficiency."}}
{"id": "2507.08982", "pdf": "https://arxiv.org/pdf/2507.08982", "abs": "https://arxiv.org/abs/2507.08982", "authors": ["Hanene F. Z. Brachemi Meftah", "Wassim Hamidouche", "Sid Ahmed Fezza", "Olivier D\u00e9forges"], "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack.", "AI": {"tldr": "The paper proposes a novel adversarial attack technique for Vision-Language Models (VLMs) to preserve user privacy by concealing sensitive regions in images while retaining the semantic integrity of the rest of the image.", "motivation": "The increasing adoption of Vision-Language Models has raised concerns about user privacy, particularly regarding the inadvertent exposure of private visual information.", "method": "The proposed approach involves selectively concealing information in specific Regions of Interest (ROIs) within images to prevent VLMs from accessing sensitive content. The technique ensures coherence in unmasked areas while disrupting targeted regions.", "result": "The approach achieved up to 98% reduction in detecting sensitive ROIs in three state-of-the-art VLMs\u2014LLaVA, Instruct-BLIP, and BLIP2-T5\u2014while maintaining high semantic similarity between clean and adversarial outputs.", "conclusion": "This work offers a method for privacy-conscious use of VLMs by masking sensitive content effectively and ensures its practical utility for future research by making the source code publicly available."}}
{"id": "2507.08849", "pdf": "https://arxiv.org/pdf/2507.08849", "abs": "https://arxiv.org/abs/2507.08849", "authors": ["Emilio Carrizosa", "Martina Fischetti", "Roshell Haaker", "Juan Miguel Morales"], "title": "Counterfactual optimization for fault prevention in complex wind energy systems", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "Machine Learning models are increasingly used in businesses to detect faults\nand anomalies in complex systems. In this work, we take this approach a step\nfurther: beyond merely detecting anomalies, we aim to identify the optimal\ncontrol strategy that restores the system to a safe state with minimal\ndisruption. We frame this challenge as a counterfactual problem: given a\nMachine Learning model that classifies system states as either good or\nanomalous, our goal is to determine the minimal adjustment to the system's\ncontrol variables (i.e., its current status) that is necessary to return it to\nthe good state. To achieve this, we leverage a mathematical model that finds\nthe optimal counterfactual solution while respecting system specific\nconstraints. Notably, most counterfactual analysis in the literature focuses on\nindividual cases where a person seeks to alter their status relative to a\ndecision made by a classifier, such as for loan approval or medical diagnosis.\nOur work addresses a fundamentally different challenge: optimizing\ncounterfactuals for a complex energy system, specifically an offshore wind\nturbine oil type transformer. This application not only advances counterfactual\noptimization in a new domain but also opens avenues for broader research in\nthis area. Our tests on real world data provided by our industrial partner show\nthat our methodology easily adapts to user preferences and brings savings in\nthe order of 3 million euros per year in a typical farm.", "AI": {"tldr": "The paper proposes a methodology to optimize control strategies for complex systems by solving counterfactual problems and demonstrates its effectiveness in managing offshore wind turbine systems.", "motivation": "To advance beyond detecting anomalies in complex systems and identify optimal control strategies for restoring these systems to a safe state with minimal disruption.", "method": "The authors use a mathematical model to compute optimal counterfactual adjustments to system control variables, while respecting system-specific constraints.", "result": "The approach was tested on real-world data from offshore wind turbine systems, achieving significant savings of approximately 3 million euros per year in a typical farm.", "conclusion": "This work successfully applies counterfactual optimization to a novel domain and provides industrial-scale savings, opening the door to broader research opportunities in complex energy systems."}}
{"id": "2507.08854", "pdf": "https://arxiv.org/pdf/2507.08854", "abs": "https://arxiv.org/abs/2507.08854", "authors": ["Qingsong Yang", "Binglan Wu", "Xuwei Liu", "Bo Chen", "Wei Li", "Gen Long", "Xin Chen", "Mingjun Xiao"], "title": "DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation", "categories": ["physics.chem-ph", "cs.LG"], "comment": null, "summary": "Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization\nmethod for molecular structure elucidation, yet interpreting NMR spectra to\ndeduce molecular structures remains challenging due to the complexity of\nspectral data and the vastness of the chemical space. In this work, we\nintroduce DiffNMR, a novel end-to-end framework that leverages a conditional\ndiscrete diffusion model for de novo molecular structure elucidation from NMR\nspectra. DiffNMR refines molecular graphs iteratively through a diffusion-based\ngenerative process, ensuring global consistency and mitigating error\naccumulation inherent in autoregressive methods. The framework integrates a\ntwo-stage pretraining strategy that aligns spectral and molecular\nrepresentations via diffusion autoencoder (Diff-AE) and contrastive learning,\nthe incorporation of retrieval initialization and similarity filtering during\ninference, and a specialized NMR encoder with radial basis function (RBF)\nencoding for chemical shifts, preserving continuity and chemical correlation.\nExperimental results demonstrate that DiffNMR achieves competitive performance\nfor NMR-based structure elucidation, offering an efficient and robust solution\nfor automated molecular analysis.", "AI": {"tldr": "The paper introduces DiffNMR, a novel framework using diffusion models for molecular structure interpretation from NMR spectra.", "motivation": "Interpreting NMR spectra to deduce molecular structures remains challenging due to data complexity and large chemical space.", "method": "DiffNMR employs a conditional discrete diffusion model, integrates pretraining strategies (Diff-AE and contrastive learning), retrieval initialization, NMR encoders, and RBF encoding for enhanced spectral-molecular alignment.", "result": "DiffNMR delivers competitive performance in NMR-based molecular structure elucidation, ensuring global consistency and reducing errors.", "conclusion": "DiffNMR provides an efficient and robust automated molecular analysis framework, addressing key challenges in NMR spectral interpretation."}}
{"id": "2507.09860", "pdf": "https://arxiv.org/pdf/2507.09860", "abs": "https://arxiv.org/abs/2507.09860", "authors": ["Nguyen Van Duc", "Bui Duc Manh", "Quang-Trung Luu", "Dinh Thai Hoang", "Van-Linh Nguyen", "Diep N. Nguyen"], "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper aims to propose a novel machine learning (ML) approach\nincorporating Homomorphic Encryption (HE) to address privacy limitations in\nUnmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related\nto distance, altitude, and face orientation, high-resolution imagery and\nsophisticated neural networks enable accurate face recognition in dynamic\nenvironments. However, privacy concerns arise from the extensive surveillance\ncapabilities of UAVs. To resolve this issue, we propose a novel framework that\nintegrates HE with advanced neural networks to secure facial data throughout\nthe inference phase. This method ensures that facial data remains secure with\nminimal impact on detection accuracy. Specifically, the proposed system\nleverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly\non encrypted data, optimizing computational efficiency and security.\nFurthermore, we develop an effective data encoding method specifically designed\nto preprocess the raw facial data into CKKS form in a\nSingle-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a\nsecure inference algorithm to compute on ciphertext without needing decryption.\nThis approach not only protects data privacy during the processing of facial\ndata but also enhances the efficiency of UAV-based face detection systems.\nExperimental results demonstrate that our method effectively balances privacy\nprotection and detection performance, making it a viable solution for UAV-based\nsecure face detection. Significantly, our approach (while maintaining data\nconfidentially with HE encryption) can still achieve an accuracy of less than\n1% compared to the benchmark without using encryption.", "AI": {"tldr": "This paper integrates Homomorphic Encryption (HE) with advanced neural networks for UAV-based face detection to secure facial data during processing without significantly affecting accuracy.", "motivation": "The paper is motivated by privacy concerns related to UAVs' ability to perform extensive surveillance through high-resolution imagery and sophisticated face detection.", "method": "The proposed method incorporates the Cheon-Kim-Kim-Song (CKKS) encryption scheme to enable computation on encrypted data, utilizes a specialized data encoding technique, and constructs secure inference algorithms.", "result": "Experimental findings show the approach effectively ensures privacy while maintaining detection accuracy within 1% of unencrypted systems.", "conclusion": "The framework optimizes privacy protection and efficiency, offering a practical solution for secure UAV-based facial recognition systems."}}
{"id": "2507.09441", "pdf": "https://arxiv.org/pdf/2507.09441", "abs": "https://arxiv.org/abs/2507.09441", "authors": ["Ankit Sanjyal"], "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "categories": ["cs.GR", "cs.CV"], "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:\n  https://github.com/ANKITSANJYAL/RectifiedHR", "summary": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior.", "AI": {"tldr": "The paper addresses quality degradation issues in high-resolution image synthesis with diffusion models by proposing adaptive guidance schedules, improving stability and image fidelity.", "motivation": "To address energy instabilities and guidance artifacts in high-resolution image generation using diffusion models, which affect visual quality.", "method": "The authors analyze the latent energy landscape during sampling and develop energy-aware adaptive classifier-free guidance (CFG) schedules to stabilize energy trajectories over time.", "result": "The proposed approach achieves superior stability scores (0.9998) and consistency metrics (0.9873), producing sharper and more accurate images compared to fixed-guidance methods.", "conclusion": "Energy-aware CFG scheduling improves the performance of diffusion models by reducing artifacts and enhancing image quality, with a profiling framework as a diagnostic tool for optimization."}}
{"id": "2507.08908", "pdf": "https://arxiv.org/pdf/2507.08908", "abs": "https://arxiv.org/abs/2507.08908", "authors": ["M. Z. Naser"], "title": "The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations", "categories": ["cs.CY", "cs.ET", "cs.LG"], "comment": null, "summary": "Despite the widespread interest in machine learning (ML), the engineering\nindustry has not yet fully adopted ML-based methods, which has left engineers\nand stakeholders uncertain about the legal and regulatory frameworks that\ngovern their decisions. This gap remains unaddressed as an engineer's\ndecision-making process, typically governed by professional ethics and\npractical guidelines, now intersects with complex algorithmic outputs. To\nbridge this gap, this paper explores how engineers can navigate legal\nprinciples and legislative justifications that support and/or contest the\ndeployment of ML technologies. Drawing on recent precedents and experiences\ngained from other fields, this paper argues that analogical reasoning can\nprovide a basis for embedding ML within existing engineering codes while\nmaintaining professional accountability and meeting safety requirements. In\nexploring these issues, the discussion focuses on established liability\ndoctrines, such as negligence and product liability, and highlights how courts\nhave evaluated the use of predictive models. We further analyze how legislative\nbodies and standard-setting organizations can furnish explicit guidance\nequivalent to prior endorsements of emergent technologies. This exploration\nstresses the vitality of understanding the interplay between technical\njustifications and legal precedents for shaping an informed stance on ML's\nlegitimacy in engineering practice. Finally, our analysis catalyzes a legal\nframework for integrating ML through which stakeholders can critically assess\nthe responsibilities, liabilities, and benefits inherent in ML-driven\nengineering solutions.", "AI": {"tldr": "The paper investigates how engineers can navigate legal and ethical considerations when using machine learning in their practice, proposing a framework based on analogical reasoning and established liability doctrines.", "motivation": "The adoption of machine learning in engineering is hindered by uncertainty regarding legal and regulatory frameworks governing its use.", "method": "The authors explore analogical reasoning and analyze legal precedents, liability doctrines, and legislative guidance to align ML with professional engineering standards.", "result": "The analysis identifies established liability doctrines like negligence and product liability, alongside insights from legislative bodies, to furnish guidance on the use of ML in engineering.", "conclusion": "Integrating machine learning into engineering practice requires a legal framework that aligns ethical guidance and professional accountability with regulatory standards regarding liabilities and benefits."}}
{"id": "2507.09448", "pdf": "https://arxiv.org/pdf/2507.09448", "abs": "https://arxiv.org/abs/2507.09448", "authors": ["Pramod Chunduri", "Yao Lu", "Joy Arulraj"], "title": "TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing", "categories": ["cs.DB", "cs.CV"], "comment": null, "summary": "Efficiently re-identifying and tracking objects across a network of cameras\nis crucial for applications like traffic surveillance. Spatula is the\nstate-of-the-art video database management system (VDBMS) for processing Re-ID\nqueries. However, it suffers from two limitations. Its spatio-temporal\nfiltering scheme has limited accuracy on large camera networks due to localized\ncamera history. It is not suitable for critical video analytics applications\nthat require high recall due to a lack of support for adaptive query\nprocessing.\n  In this paper, we present Tracer, a novel VDBMS for efficiently processing\nRe-ID queries using an adaptive query processing framework. Tracer selects the\noptimal camera to process at each time step by training a recurrent network to\nmodel long-term historical correlations. To accelerate queries under a high\nrecall constraint, Tracer incorporates a probabilistic adaptive search model\nthat processes camera feeds in incremental search windows and dynamically\nupdates the sampling probabilities using an exploration-exploitation strategy.\nTo address the paucity of benchmarks for the Re-ID task due to privacy\nconcerns, we present a novel synthetic benchmark for generating multi-camera\nRe-ID datasets based on real-world traffic distribution. Our evaluation shows\nthat Tracer outperforms the state-of-the-art cross-camera analytics system by\n3.9x on average across diverse datasets.", "AI": {"tldr": "Tracer is a novel Video Database Management System (VDBMS) that improves object re-identification and tracking across large camera networks using adaptive query processing.", "motivation": "There is a need to improve object re-identification in large camera networks due to the limitations of existing systems like Spatula in terms of accuracy and recall.", "method": "Tracer employs a recurrent neural network to select optimal cameras for processing while optimizing query speed and recall through a probabilistic adaptive search model. It also introduces a synthetic benchmark for generating realistic Re-ID datasets.", "result": "Tracer outperforms the state-of-the-art system by achieving a 3.9x improvement in performance across diverse datasets.", "conclusion": "Tracer addresses key challenges in object re-identification and tracking by providing a scalable and efficient solution for large camera networks while meeting high recall requirements."}}
{"id": "2507.09864", "pdf": "https://arxiv.org/pdf/2507.09864", "abs": "https://arxiv.org/abs/2507.09864", "authors": ["Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a\nstructured and interpretable alternative to Deep Neural Network (DNN)-based RL\nmethods, with lower computational complexity and greater transparency. However,\nstandard MPC-RL approaches often suffer from slow convergence, suboptimal\npolicy learning due to limited parameterization, and safety issues during\nonline adaptation. To address these challenges, we propose a novel framework\nthat integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The\nproposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its\ngradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)\napproach, and incorporates them into a MOBO algorithm using the Expected\nHypervolume Improvement (EHVI) acquisition function. This fusion enables\nefficient and safe tuning of the MPC parameters to achieve improved closed-loop\nperformance, even under model imperfections. A numerical example demonstrates\nthe effectiveness of the proposed approach in achieving sample-efficient,\nstable, and high-performance learning for control systems.", "AI": {"tldr": "This paper integrates MPC-based reinforcement learning with multi-objective Bayesian optimization to enhance convergence, parameter tuning, and safety in control systems.", "motivation": "Address the challenges of slow convergence, limited parameterization, and safety issues in MPC-based RL methods.", "method": "Combines MPC-RL with MOBO using noisy evaluations and the Expected Hypervolume Improvement acquisition function.", "result": "Demonstrated sample-efficient, stable, and high-performance learning through numerical experiments.", "conclusion": "The novel framework improves closed-loop performance efficiently and safely, even with model imperfections."}}
{"id": "2507.09608", "pdf": "https://arxiv.org/pdf/2507.09608", "abs": "https://arxiv.org/abs/2507.09608", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a novel framework for phase retrieval that leverages Langevin\ndynamics to enable efficient posterior sampling, yielding reconstructions that\nexplicitly balance distortion and perceptual quality. Unlike conventional\napproaches that prioritize pixel-wise accuracy, our method navigates the\nperception-distortion tradeoff through a principled combination of stochastic\nsampling, learned denoising, and model-based updates. The framework comprises\nthree variants of increasing complexity, integrating theoretically grounded\nLangevin inference, adaptive noise schedule learning, parallel reconstruction\nsampling, and warm-start initialization from classical solvers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple benchmarks, both in terms of fidelity and perceptual quality.", "AI": {"tldr": "The paper introduces a novel phase retrieval framework using Langevin dynamics for efficient posterior sampling, achieving balanced reconstructions in distortion and perceptual quality.", "motivation": "Existing phase retrieval methods often overemphasize pixel-wise accuracy, leaving room for improvement in balancing distortion and perceptual quality.", "method": "The approach combines Langevin inference, adaptive noise scheduling, parallel reconstruction sampling, and warm-start initialization.", "result": "Experiments confirm state-of-the-art performance in fidelity and perceptual quality across various benchmarks.", "conclusion": "This framework provides a principled tradeoff between distortion and perception while leveraging advanced computational techniques like Langevin dynamics."}}
{"id": "2507.09609", "pdf": "https://arxiv.org/pdf/2507.09609", "abs": "https://arxiv.org/abs/2507.09609", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "title": "I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Phase retrieval involves recovering a signal from intensity-only\nmeasurements, crucial in many fields such as imaging, holography, optical\ncomputing, crystallography, and microscopy. Although there are several\nwell-known phase retrieval algorithms, including classical iterative solvers,\nthe reconstruction performance often remains sensitive to initialization and\nmeasurement noise. Recently, image-to-image diffusion models have gained\ntraction in various image reconstruction tasks, yielding significant\ntheoretical insights and practical breakthroughs. In this work, we introduce a\nnovel phase retrieval approach based on an image-to-image diffusion framework\ncalled Inversion by Direct Iteration. Our method begins with an enhanced\ninitialization stage that leverages a hybrid iterative technique, combining the\nHybrid Input-Output and Error Reduction methods and incorporating a novel\nacceleration mechanism to obtain a robust crude estimate. Then, it iteratively\nrefines this initial crude estimate using the learned image-to-image pipeline.\nOur method achieves substantial improvements in both training efficiency and\nreconstruction quality. Furthermore, our approach utilizes aggregation\ntechniques to refine quality metrics and demonstrates superior results compared\nto both classical and contemporary techniques. This highlights its potential\nfor effective and efficient phase retrieval across various applications.", "AI": {"tldr": "The paper introduces a novel phase retrieval method leveraging image-to-image diffusion models for enhanced initialization and iterative refinement, surpassing classical techniques.", "motivation": "The need to improve phase retrieval performance, especially under conditions sensitive to initialization and measurement noise.", "method": "A hybrid iterative technique for initialization coupled with an image-to-image diffusion-based refinement pipeline.", "result": "Substantial improvements in training efficiency and reconstruction quality compared to existing techniques.", "conclusion": "The proposed method delivers superior phase retrieval results, showcasing its potential for diverse applications."}}
{"id": "2507.09627", "pdf": "https://arxiv.org/pdf/2507.09627", "abs": "https://arxiv.org/abs/2507.09627", "authors": ["Muhammad Kamran Saeed", "Ashfaq Khokhar", "Shakil Ahmed"], "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices", "categories": ["cs.IT", "cs.CV", "cs.LG", "cs.NI", "math.IT"], "comment": null, "summary": "Next-generation wireless technologies such as 6G aim to meet demanding\nrequirements such as ultra-high data rates, low latency, and enhanced\nconnectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable\nIntelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and\nenergy efficiency through numerous antennas, and RIS offering dynamic control\nover the wireless environment via passive reflective elements. However,\nrealizing their full potential depends on accurate Channel State Information\n(CSI). Recent advances in deep learning have facilitated efficient cascaded\nchannel estimation. However, the scalability and practical deployment of\nexisting estimation models in XL-MIMO systems remain limited. The growing\nnumber of antennas and RIS elements introduces a significant barrier to\nreal-time and efficient channel estimation, drastically increasing data volume,\nescalating computational complexity, requiring advanced hardware, and resulting\nin substantial energy consumption. To address these challenges, we propose a\nlightweight deep learning framework for efficient cascaded channel estimation\nin XL-MIMO systems, designed to minimize computational complexity and make it\nsuitable for deployment on resource-constrained edge devices. Using spatial\ncorrelations in the channel, we introduce a patch-based training mechanism that\nreduces the dimensionality of input to patch-level representations while\npreserving essential information, allowing scalable training for large-scale\nsystems. Simulation results under diverse conditions demonstrate that our\nframework significantly improves estimation accuracy and reduces computational\ncomplexity, regardless of the increasing number of antennas and RIS elements in\nXL-MIMO systems.", "AI": {"tldr": "This paper proposes a lightweight deep learning framework for efficient cascaded channel estimation in XL-MIMO and RIS-enabled 6G systems, significantly improving scalability and reducing computational complexity.", "motivation": "The motivation behind this work is to address the challenges in channel estimation for XL-MIMO and RIS systems in 6G wireless technologies, where scalability, computational complexity, and energy efficiency are critical issues.", "method": "The authors developed a lightweight deep learning framework using spatial correlations in the wireless channel. It introduces a patch-based training mechanism to reduce the dimensionality of the inputs while retaining crucial information, making the model scalable and efficient.", "result": "The proposed framework achieved improved channel estimation accuracy across various conditions and significantly reduced computational complexity, making it suitable for resource-constrained edge devices.", "conclusion": "The research provides a practical and efficient solution for channel estimation in large-scale 6G systems, overcoming scalability and deployment challenges, and paving the way for real-world implementation."}}
{"id": "2507.08986", "pdf": "https://arxiv.org/pdf/2507.08986", "abs": "https://arxiv.org/abs/2507.08986", "authors": ["Ashish S. Nair", "Narendra Singh", "Marco Panesi", "Justin Sirignano", "Jonathan F. MacArt"], "title": "Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "Modeling rarefied hypersonic flows remains a fundamental challenge due to the\nbreakdown of classical continuum assumptions in the transition-continuum\nregime, where the Knudsen number ranges from approximately 0.1 to 10.\nConventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall\nboundary conditions fail to accurately predict nonequilibrium effects such as\nvelocity slip, temperature jump, and shock structure deviations. We develop a\nphysics-constrained machine learning framework that augments transport models\nand boundary conditions to extend the applicability of continuum solvers in\nnonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs)\nfor the viscous stress and heat flux embedded in the governing PDEs and trained\nvia adjoint-based optimization. We evaluate these for two-dimensional\nsupersonic flat-plate flows across a range of Mach and Knudsen numbers.\nAdditionally, we introduce a wall model based on a mixture of skewed Gaussian\napproximations of the particle velocity distribution function. This wall model\nreplaces empirical slip conditions with physically informed, data-driven\nboundary conditions for the streamwise velocity and wall temperature. Our\nresults show that a trace-free anisotropic viscosity model, paired with the\nskewed-Gaussian distribution function wall model, achieves significantly\nimproved accuracy, particularly at high-Mach and high-Knudsen number regimes.\nStrategies such as parallel training across multiple Knudsen numbers and\ninclusion of high-Mach data during training are shown to enhance model\ngeneralization. Increasing model complexity yields diminishing returns for\nout-of-sample cases, underscoring the need to balance degrees of freedom and\noverfitting. This work establishes data-driven, physics-consistent strategies\nfor improving hypersonic flow modeling for regimes in which conventional\ncontinuum approaches are invalid.", "AI": {"tldr": "The paper introduces a physics-constrained machine learning framework to improve hypersonic flow modeling in transition-continuum regimes (Knudsen number 0.1\u201310) where conventional continuum approaches fail.", "motivation": "Classical Navier-Stokes-Fourier models cannot accurately predict nonequilibrium effects in rarefied hypersonic flows, necessitating advanced methods for modeling such regimes.", "method": "The study proposes deep learning PDE models for stress and heat flux coupled with a skewed-Gaussian based wall model to replace empirical boundary conditions. These models are trained with adjoint-based optimization.", "result": "Enhanced accuracy is achieved using anisotropic viscosity and skewed-Gaussian wall models, particularly for high-Mach and high-Knudsen number flows. Parallel training and high-Mach data inclusion further boost generalization.", "conclusion": "The approach provides a data-driven, physics-informed strategy to extend continuum solver applicability in nonequilibrium hypersonic regimes, showing promise for scenarios beyond conventional models."}}
{"id": "2507.09879", "pdf": "https://arxiv.org/pdf/2507.09879", "abs": "https://arxiv.org/abs/2507.09879", "authors": ["Tanvi Bajpai", "Chandra Chekuri", "Pooja Kulkarni"], "title": "Covering a Few Submodular Constraints and Applications", "categories": ["cs.DS", "cs.AI", "cs.GT"], "comment": "34 pages. Accepted to APPROX 2025", "summary": "We consider the problem of covering multiple submodular constraints. Given a\nfinite ground set $N$, a cost function $c: N \\rightarrow \\mathbb{R}_+$, $r$\nmonotone submodular functions $f_1,f_2,\\ldots,f_r$ over $N$ and requirements\n$b_1,b_2,\\ldots,b_r$ the goal is to find a minimum cost subset $S \\subseteq N$\nsuch that $f_i(S) \\ge b_i$ for $1 \\le i \\le r$. When $r=1$ this is the\nwell-known Submodular Set Cover problem. Previous work\n\\cite{chekuri2022covering} considered the setting when $r$ is large and\ndeveloped bi-criteria approximation algorithms, and approximation algorithms\nfor the important special case when each $f_i$ is a weighted coverage function.\nThese are fairly general models and capture several concrete and interesting\nproblems as special cases. The approximation ratios for these problem are at\nleast $\\Omega(\\log r)$ which is unavoidable when $r$ is part of the input. In\nthis paper, motivated by some recent applications, we consider the problem when\n$r$ is a \\emph{fixed constant} and obtain two main results. For covering\nmultiple submodular constraints we obtain a randomized bi-criteria\napproximation algorithm that for any given integer $\\alpha \\ge 1$ outputs a set\n$S$ such that $f_i(S) \\ge$ $(1-1/e^\\alpha -\\epsilon)b_i$ for each $i \\in [r]$\nand $\\mathbb{E}[c(S)] \\le (1+\\epsilon)\\alpha \\cdot \\sf{OPT}$. Second, when the\n$f_i$ are weighted coverage functions from a deletion-closed set system we\nobtain a $(1+\\epsilon)$ $(\\frac{e}{e-1})$ $(1+\\beta)$-approximation where\n$\\beta$ is the approximation ratio for the underlying set cover instances via\nthe natural LP. These results show that one can obtain nearly as good an\napproximation for any fixed $r$ as what one would achieve for $r=1$. We mention\nsome applications that follow easily from these general results and anticipate\nmore in the future.", "AI": {"tldr": "The paper focuses on solving the problem of covering multiple submodular constraints when the number of constraints ($r$) is fixed, proposing efficient approximation algorithms and achieving results comparable to single constraint cases.", "motivation": "The motivation stems from the need to address computational challenges in covering multiple submodular constraints, particularly in practical applications where $r$ (number of constraints) is a fixed constant rather than a variable.", "method": "The authors developed a randomized bi-criteria approximation algorithm for general submodular functions with guarantees on coverage and cost, and a specialized approximation for weighted coverage functions in deletion-closed set systems.", "result": "The paper presents a near-optimal randomized approximation algorithm for fixed $r$, achieving coverage guarantees while keeping costs controlled. Additionally, it addresses specialized cases with weighted coverage functions for improved approximation ratios.", "conclusion": "The results demonstrate that for fixed $r$, approximation performance approaches that of the simpler single-constraint ($r=1$) case, making the approach highly effective for such scenarios. Potential applications are highlighted and future work is anticipated."}}
{"id": "2507.09731", "pdf": "https://arxiv.org/pdf/2507.09731", "abs": "https://arxiv.org/abs/2507.09731", "authors": ["Robby Hoover", "Nelly Elsayed", "Zag ElSayed", "Chengcheng Li"], "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, under review", "summary": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts.", "AI": {"tldr": "This paper evaluates the robustness of pre-trained deep learning models (ResNet50, VGG16, EfficientNetv2) in classifying bone fractures from X-ray images under varying noise conditions.", "motivation": "To address global healthcare disparities by assessing how the performance of pre-trained deep learning models is affected by noise in X-ray images, simulating real-world diagnostic challenges.", "method": "Three pre-trained deep learning models (ResNet50, VGG16, EfficientNetv2) were tested on X-ray images with progressive noise degradation to examine their robustness in bone fracture classification.", "result": "The study provides empirical evidence on how noise impacts the performance and robustness of the models, highlighting variations in their ability to classify bone fractures under degraded conditions.", "conclusion": "This research establishes a practical framework for evaluating AI model degradation due to noise, offering insights into their generalizability and robustness for global healthcare applications."}}
{"id": "2507.09001", "pdf": "https://arxiv.org/pdf/2507.09001", "abs": "https://arxiv.org/abs/2507.09001", "authors": ["Sazzad Hossain", "Ponkrshnan Thiagarajan", "Shashank Pathrudkar", "Stephanie Taylor", "Abhijeet S. Gangan", "Amartya S. Banerjee", "Susanta Ghosh"], "title": "Surprisingly High Redundancy in Electronic Structure Data", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "cs.LG", "physics.comp-ph", "quant-ph"], "comment": null, "summary": "Machine Learning (ML) models for electronic structure rely on large datasets\ngenerated through expensive Kohn-Sham Density Functional Theory simulations.\nThis study reveals a surprisingly high level of redundancy in such datasets\nacross various material systems, including molecules, simple metals, and\ncomplex alloys. Our findings challenge the prevailing assumption that large,\nexhaustive datasets are necessary for accurate ML predictions of electronic\nstructure. We demonstrate that even random pruning can substantially reduce\ndataset size with minimal loss in predictive accuracy, while a state-of-the-art\ncoverage-based pruning strategy retains chemical accuracy and model\ngeneralizability using up to 100-fold less data and reducing training time by\nthreefold or more. By contrast, widely used importance-based pruning methods,\nwhich eliminate seemingly redundant data, can catastrophically fail at higher\npruning factors, possibly due to the significant reduction in data coverage.\nThis heretofore unexplored high degree of redundancy in electronic structure\ndata holds the potential to identify a minimal, essential dataset\nrepresentative of each material class.", "AI": {"tldr": "The paper reveals high redundancy in electronic structure datasets, showing that data can be pruned significantly with minimal loss to ML performance.", "motivation": "To challenge the belief that large exhaustive datasets are required for accurate ML predictions of electronic structure.", "method": "The authors analyzed redundancy in datasets and compared random, coverage-based, and importance-based pruning strategies to assess their impact on model accuracy and generalizability.", "result": "Coverage-based pruning retained chemical accuracy and used 100x less data with reduced training time, while importance-based pruning performed poorly at higher pruning levels.", "conclusion": "Electronic structure datasets have high redundancy, enabling the creation of minimal, efficient datasets for ML without compromising accuracy."}}
{"id": "2507.09026", "pdf": "https://arxiv.org/pdf/2507.09026", "abs": "https://arxiv.org/abs/2507.09026", "authors": ["Kasra Fallah", "Leonardo F. Toso", "James Anderson"], "title": "On the Gradient Domination of the LQG Problem", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "We consider solutions to the linear quadratic Gaussian (LQG) regulator\nproblem via policy gradient (PG) methods. Although PG methods have demonstrated\nstrong theoretical guarantees in solving the linear quadratic regulator (LQR)\nproblem, despite its nonconvex landscape, their theoretical understanding in\nthe LQG setting remains limited. Notably, the LQG problem lacks gradient\ndominance in the classical parameterization, i.e., with a dynamic controller,\nwhich hinders global convergence guarantees. In this work, we study PG for the\nLQG problem by adopting an alternative parameterization of the set of\nstabilizing controllers and employing a lifting argument. We refer to this\nparameterization as a history representation of the control input as it is\nparameterized by past input and output data from the previous p time-steps.\nThis representation enables us to establish gradient dominance and approximate\nsmoothness for the LQG cost. We prove global convergence and per-iteration\nstability guarantees for policy gradient LQG in model-based and model-free\nsettings. Numerical experiments on an open-loop unstable system are provided to\nsupport the global convergence guarantees and to illustrate convergence under\ndifferent history lengths of the history representation.", "AI": {"tldr": "This paper studies linear quadratic Gaussian (LQG) regulators with policy gradient methods and proves global convergence by introducing an alternative parameterization based on history representation.", "motivation": "To address the lack of global convergence guarantees for policy gradient methods in the LQG problem due to the absence of gradient dominance in the classical parameterization.", "method": "The authors propose a history-based parameterization, utilizing past input and output data from previous steps, which enables gradient dominance and approximate smoothness for the LQG cost. They analyze both model-based and model-free settings.", "result": "The paper establishes global convergence and per-iteration stability guarantees for policy gradient methods in the LQG setting, supported by numerical experiments on an unstable system.", "conclusion": "The history representation of controllers resolves the nonconvexity barrier in LQG problems, enabling robust global optimization with policy gradient methods."}}
{"id": "2507.09891", "pdf": "https://arxiv.org/pdf/2507.09891", "abs": "https://arxiv.org/abs/2507.09891", "authors": ["Jiaxin Huang", "Yan Zhu", "Giulio Chiribella", "Ya-Dong Wu"], "title": "Sequence-Model-Guided Measurement Selection for Quantum State Learning", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Characterization of quantum systems from experimental data is a central\nproblem in quantum science and technology. But which measurements should be\nused to gather data in the first place? While optimal measurement choices can\nbe worked out for small quantum systems, the optimization becomes intractable\nas the system size grows large. To address this problem, we introduce a deep\nneural network with a sequence model architecture that searches for efficient\nmeasurement choices in a data-driven, adaptive manner. The model can be applied\nto a variety of tasks, including the prediction of linear and nonlinear\nproperties of quantum states, as well as state clustering and state tomography\ntasks. In all these tasks, we find that the measurement choices identified by\nour neural network consistently outperform the uniformly random choice.\nIntriguingly, for topological quantum systems, our model tends to recommend\nmeasurements at the system's boundaries, even when the task is to predict bulk\nproperties. This behavior suggests that the neural network may have\nindependently discovered a connection between boundaries and bulk, without\nhaving been provided any built-in knowledge of quantum physics.", "AI": {"tldr": "The paper introduces a deep neural network model for optimizing measurement choices in quantum system characterization, leading to better data-driven predictions and insights.", "motivation": "Characterizing quantum systems requires effective measurement strategies, but optimization becomes infeasible for large systems. The paper seeks to address this limitation.", "method": "A deep neural network with a sequence model architecture is developed to identify efficient measurement strategies for various quantum state tasks, including prediction, clustering, and tomography.", "result": "The neural network's measurement choices consistently outperform random selections, revealing insights like correlations between system boundaries and bulk properties in topological systems.", "conclusion": "The approach provides a data-driven way to optimize measurements for quantum systems, uncovering patterns and outperforming traditional random strategies even without embedded quantum knowledge."}}
{"id": "2507.09898", "pdf": "https://arxiv.org/pdf/2507.09898", "abs": "https://arxiv.org/abs/2507.09898", "authors": ["Alireza Golkarieha", "Kiana Kiashemshakib", "Sajjad Rezvani Boroujenic", "Nasibeh Asadi Isakand"], "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "This manuscript has 20 pages and 10 figures. It is submitted to the\n  Journal 'Scientific Reports'", "summary": "This study investigates the effectiveness of U-Net architectures integrated\nwith various convolutional neural network (CNN) backbones for automated lung\ncancer detection and segmentation in chest CT images, addressing the critical\nneed for accurate diagnostic tools in clinical settings. A balanced dataset of\n832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed\nusing Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to\n128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,\nVGG16, and Xception, to segment lung regions. After segmentation, CNN-based\nclassifiers and hybrid models combining CNN feature extraction with traditional\nmachine learning classifiers (Support Vector Machine, Random Forest, and\nGradient Boosting) were evaluated using 5-fold cross-validation. Metrics\nincluded accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.\nU-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:\n0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for\nnon-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For\nclassification, the CNN model using U-Net with Xception achieved 99.1 percent\naccuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid\nCNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent\nF1-score. Compared to prior methods, our framework consistently outperformed\nexisting models. In conclusion, combining U-Net with advanced CNN backbones\nprovides a powerful method for both segmentation and classification of lung\ncancer in CT scans, supporting early diagnosis and clinical decision-making.", "AI": {"tldr": "The study explores U-Net models with ResNet50, VGG16, and Xception CNN backbones for lung cancer detection and segmentation in CT images, achieving high diagnostic accuracy and outperforming previous models.", "motivation": "Address the need for accurate and automated diagnostic tools for lung cancer detection through effective segmentation and classification of chest CT images.", "method": "The research employed U-Net architectures with different CNN backbones (ResNet50, VGG16, Xception) for segmentation, preprocessing images with CLAHE and resizing, and evaluated CNN-based and hybrid classification models using machine learning approaches. Performance was measured with metrics such as dice coefficient, accuracy, and F1-score.", "result": "U-Net with ResNet50 achieved high performance in cancerous segmentation (Dice: 0.9495, Accuracy: 0.9735), U-Net with VGG16 was best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513), and Xception-based models achieved the highest classification accuracy (99.1 percent). Hybrid models also performed well but slightly underperformed compared to pure CNNs.", "conclusion": "Integrating U-Net with advanced CNN backbones is a robust method for lung cancer segmentation and classification, advancing early diagnosis capabilities in clinical settings."}}
{"id": "2507.09834", "pdf": "https://arxiv.org/pdf/2507.09834", "abs": "https://arxiv.org/abs/2507.09834", "authors": ["Shu-wen Yang", "Byeonggeun Kim", "Kuan-Po Huang", "Qingming Tang", "Huy Phan", "Bo-Ru Lu", "Harsha Sundar", "Shalini Ghosh", "Hung-yi Lee", "Chieh-Chi Kao", "Chao Wang"], "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": "Accepted by ICML 2025. Project website: https://audiomntp.github.io/", "summary": "Autoregressive next-token prediction with the Transformer decoder has become\na de facto standard in large language models (LLMs), achieving remarkable\nsuccess in Natural Language Processing (NLP) at scale. Extending this paradigm\nto audio poses unique challenges due to its inherently continuous nature. We\nresearch audio generation with a causal language model (LM) without discrete\ntokens. We leverage token-wise diffusion to model the continuous distribution\nof the next continuous-valued token. Our approach delivers significant\nimprovements over previous discrete solution, AudioGen, achieving 20% and 40%\nrelative gains on AudioCaps in Frechet Audio Distance (FAD) and\nKullback-Leibler (KL) divergence, respectively. Additionally, we propose a\nnovel masked next-token prediction task that incorporates masked prediction\ninto the causal LM framework. On AudioCaps, the innovation yields 41% and 33%\nrelative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)\nmodels, respectively, and is on par with the state-of-the-art (SOTA) diffusion\nmodels. Furthermore, we achieve these results with significantly fewer\nparameters -- 193M for our Base and 462M for our Large models.", "AI": {"tldr": "This paper proposes a novel approach for audio generation by using a token-wise diffusion method instead of discrete tokens for autoregressive modeling, showing significant improvements over previous methods.", "motivation": "To extend the success of autoregressive next-token prediction (used in large language models) to audio, addressing challenges posed by its continuous nature.", "method": "The authors use token-wise diffusion to model the continuous distribution for the next audio token and introduce a masked next-token prediction task within the causal language model framework.", "result": "The proposed method achieves 20-40% relative gains in Frechet Audio Distance (FAD) and Kullback-Leibler (KL) divergence over AudioGen, and delivers SOTA-level performance with fewer parameters.", "conclusion": "The proposed continuous token modeling and masked prediction task present a significant advancement in efficient and high-performing audio generation techniques."}}
{"id": "2507.09050", "pdf": "https://arxiv.org/pdf/2507.09050", "abs": "https://arxiv.org/abs/2507.09050", "authors": ["James Kotary", "Himanshu Sharma", "Ethan King", "Draguna Vrabie", "Ferdinando Fioretto", "Jan Drgona"], "title": "A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML\nmodels are trained to solve parametric optimization problems. The general goal\nis to learn a fast approximator of solutions to constrained optimization\nproblems, as a function of their defining parameters. Prior L2O methods focus\nalmost entirely on single-level programs, in contrast to the bilevel programs,\nwhose constraints are themselves expressed in terms of optimization\nsubproblems. Bilevel programs have numerous important use cases but are\nnotoriously difficult to solve, particularly under stringent time demands. This\npaper proposes a framework for learning to solve a broad class of challenging\nbilevel optimization problems, by leveraging modern techniques for\ndifferentiation through optimization problems. The framework is illustrated on\nan array of synthetic bilevel programs, as well as challenging control system\nco-design problems, showing how neural networks can be trained as efficient\napproximators of parametric bilevel optimization.", "AI": {"tldr": "The paper develops a framework allowing neural networks to efficiently approximate solutions to complex bilevel optimization problems.", "motivation": "Bilevel optimization problems are tough but critical, and existing learning-to-optimize work focuses only on simpler, single-level cases.", "method": "The authors leverage differentiation through optimization to train neural networks to approximate solutions for various bilevel problems.", "result": "Their framework performs effectively on both synthetic bilevel optimization problems and real-life control system co-design problems.", "conclusion": "The approach demonstrates that neural networks can serve as practical tools to address challenging bilevel optimization problems efficiently."}}
{"id": "2507.09901", "pdf": "https://arxiv.org/pdf/2507.09901", "abs": "https://arxiv.org/abs/2507.09901", "authors": ["Ayush Chopra"], "title": "Large Population Models", "categories": ["cs.MA", "cs.AI"], "comment": "Aggregation of Several Papers from MIT PhD Research.\n  github.com/AgentTorch/AgentTorch", "summary": "Many of society's most pressing challenges, from pandemic response to supply\nchain disruptions to climate adaptation, emerge from the collective behavior of\nmillions of autonomous agents making decisions over time. Large Population\nModels (LPMs) offer an approach to understand these complex systems by\nsimulating entire populations with realistic behaviors and interactions at\nunprecedented scale. LPMs extend traditional modeling approaches through three\nkey innovations: computational methods that efficiently simulate millions of\nagents simultaneously, mathematical frameworks that learn from diverse\nreal-world data streams, and privacy-preserving communication protocols that\nbridge virtual and physical environments. This allows researchers to observe\nhow agent behavior aggregates into system-level outcomes and test interventions\nbefore real-world implementation. While current AI advances primarily focus on\ncreating \"digital humans\" with sophisticated individual capabilities, LPMs\ndevelop \"digital societies\" where the richness of interactions reveals emergent\nphenomena. By bridging individual agent behavior and population-scale dynamics,\nLPMs offer a complementary path in AI research illuminating collective\nintelligence and providing testing grounds for policies and social innovations\nbefore real-world deployment. We discuss the technical foundations and some\nopen problems here. LPMs are implemented by the AgentTorch framework\n(github.com/AgentTorch/AgentTorch)", "AI": {"tldr": "The paper introduces Large Population Models (LPMs) for simulating societies at scale using advanced computational, mathematical, and privacy-preserving methods.", "motivation": "To address societal challenges arising from collective behavior, such as pandemics, climate change, and supply chain issues.", "method": "LPMs utilize efficient computational simulations for millions of agents, real-world data learning frameworks, and privacy-preserving communication protocols.", "result": "LPMs enable researchers to analyze system-level outcomes from agent interactions and test interventions virtually before physical implementation.", "conclusion": "LPMs provide a new AI approach focusing on \"digital societies\" to better understand collective phenomena, complementing current advances in individual agent capabilities."}}
{"id": "2507.09872", "pdf": "https://arxiv.org/pdf/2507.09872", "abs": "https://arxiv.org/abs/2507.09872", "authors": ["Shengjie Liu", "Lu Zhang", "Siqin Wang"], "title": "Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "ICCV 2025 Workshop SEA -- International Conference on Computer Vision\n  2025 Workshop on Sustainability with Earth Observation and AI", "summary": "Central to Earth observation is the trade-off between spatial and temporal\nresolution. For temperature, this is especially critical because real-world\napplications require high spatiotemporal resolution data. Current technology\nallows for hourly temperature observations at 2 km, but only every 16 days at\n100 m, a gap further exacerbated by cloud cover. Earth system models offer\ncontinuous hourly temperature data, but at a much coarser spatial resolution\n(9-31 km). Here, we present a physics-guided deep learning framework for\ntemperature data reconstruction that integrates these two data sources. The\nproposed framework uses a convolutional neural network that incorporates the\nannual temperature cycle and includes a linear term to amplify the coarse Earth\nsystem model output into fine-scale temperature values observed from\nsatellites. We evaluated this framework using data from two satellites, GOES-16\n(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective\ntemperature reconstruction with hold-out and in situ data across four datasets.\nThis physics-guided deep learning framework opens new possibilities for\ngenerating high-resolution temperature data across spatial and temporal scales,\nunder all weather conditions and globally.", "AI": {"tldr": "The paper introduces a physics-guided deep learning framework to reconstruct high-resolution temperature data by integrating coarse Earth system models and satellite observations.", "motivation": "To address the trade-off between spatial and temporal resolution in temperature data, especially under limitations influenced by current satellite technology and weather conditions.", "method": "A convolutional neural network that incorporates the annual temperature cycle and uses a linear amplification term to refine coarse data from Earth system models into fine-scale temperature values.", "result": "The framework demonstrated effective temperature reconstruction using two satellites (GOES-16 and Landsat) and was validated with hold-out and in situ data across four datasets.", "conclusion": "This approach enables global, all-weather generation of high-resolution temperature data, improving spatiotemporal coverage beyond current constraints."}}
{"id": "2507.09054", "pdf": "https://arxiv.org/pdf/2507.09054", "abs": "https://arxiv.org/abs/2507.09054", "authors": ["Fr\u00e9d\u00e9ric A. Dreyer", "Jan Ludwiczak", "Karolis Martinkus", "Brennan Abanades", "Robert G. Alberstein", "Pan Kessel", "Pranav Rao", "Jae Hyeon Lee", "Richard Bonneau", "Andrew M. Watkins", "Franziska Seeger"], "title": "Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins", "categories": ["q-bio.BM", "cs.LG"], "comment": "17 pages, 12 figures, 2 tables, code at\n  https://github.com/prescient-design/ibex, model weights at\n  https://doi.org/10.5281/zenodo.15866555", "summary": "We introduce Ibex, a pan-immunoglobulin structure prediction model that\nachieves state-of-the-art accuracy in modeling the variable domains of\nantibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex\nexplicitly distinguishes between bound and unbound protein conformations by\ntraining on labeled apo and holo structural pairs, enabling accurate prediction\nof both states at inference time. Using a comprehensive private dataset of\nhigh-resolution antibody structures, we demonstrate superior\nout-of-distribution performance compared to existing specialized and general\nprotein structure prediction tools. Ibex combines the accuracy of cutting-edge\nmodels with significantly reduced computational requirements, providing a\nrobust foundation for accelerating large molecule design and therapeutic\ndevelopment.", "AI": {"tldr": "Ibex is a novel structure prediction model for antibodies, nanobodies, and T-cell receptors, delivering high accuracy with computational efficiency.", "motivation": "Existing methods for antibody and other immunoglobulin structure prediction fail to distinguish bound and unbound conformations, limiting their accuracy in diverse states.", "method": "Ibex uses apo and holo structural pairs for training, enabling separate modeling for bound and unbound protein states. It is powered by a private high-resolution dataset for enhanced learning.", "result": "Ibex outperforms specialized and general protein prediction tools on accuracy, especially in out-of-distribution scenarios, while also being computationally efficient.", "conclusion": "Ibex provides a powerful, accurate, and efficient tool for accelerating molecular design and therapeutic developments in immunology."}}
{"id": "2507.09929", "pdf": "https://arxiv.org/pdf/2507.09929", "abs": "https://arxiv.org/abs/2507.09929", "authors": ["Haoyang Li", "Nana Hou", "Yuchen Hu", "Jixun Yao", "Sabato Marco Siniscalchi", "Eng Siong Chng"], "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": null, "summary": "This work investigates speech enhancement (SE) from the perspective of\nlanguage models (LMs). We propose a novel method that leverages Direct\nPreference Optimization (DPO) to improve the perceptual quality of enhanced\nspeech. Using UTMOS, a neural MOS prediction model, as a proxy for human\nratings, our approach guides optimization toward perceptually preferred\noutputs. This differs from existing LM-based SE methods that focus on\nmaximizing the likelihood of clean speech tokens, which may misalign with human\nperception and degrade quality despite low prediction error. Experiments on the\n2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO\nto a pretrained LM-based SE model yields consistent improvements across various\nspeech quality metrics, with relative gains of up to 56%. To our knowledge,\nthis is the first application of DPO to SE and the first to incorporate proxy\nperceptual feedback into LM-based SE training, pointing to a promising\ndirection for perceptually aligned SE.", "AI": {"tldr": "This paper introduces a novel speech enhancement method using Direct Preference Optimization (DPO) with perceptual feedback metrics.", "motivation": "Existing speech enhancement methods using language models often misalign with human perception, compromising speech quality despite low prediction error.", "method": "The method employs UTMOS, a neural MOS prediction model, as a proxy for human ratings to guide optimization towards perceptually preferred outputs through DPO.", "result": "The proposed approach yields consistent improvements across various speech quality metrics with relative gains of up to 56% on 2020 Deep Noise Suppression Challenge test sets.", "conclusion": "DPO applied to SE offers a perceptually aligned training mechanism, representing a promising advancement for LM-based speech enhancement techniques."}}
{"id": "2507.09923", "pdf": "https://arxiv.org/pdf/2507.09923", "abs": "https://arxiv.org/abs/2507.09923", "authors": ["Sejin Park", "Sangmin Lee", "Kyong Hwan Jin", "Seung-Won Jung"], "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "ICCV 2025", "summary": "Super-resolution (SR) has been a pivotal task in image processing, aimed at\nenhancing image resolution across various applications. Recently, look-up table\n(LUT)-based approaches have attracted interest due to their efficiency and\nperformance. However, these methods are typically designed for fixed scale\nfactors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing\nASISR techniques often employ implicit neural representations, which come with\nconsiderable computational cost and memory demands. To address these\nlimitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework\nthat operates ASISR by learning to blend multiple interpolation functions to\nmaximize their representational capacity. Specifically, we introduce IM-Net, a\nnetwork trained to predict mixing weights for interpolation functions based on\nlocal image patterns and the target scale factor. To enhance efficiency of\ninterpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are\nemployed to replace computationally expensive operations, enabling lightweight\nand fast inference on CPUs while preserving reconstruction quality.\nExperimental results on several benchmark datasets demonstrate that IM-LUT\nconsistently achieves a superior balance between image quality and efficiency\ncompared to existing methods, highlighting its potential as a promising\nsolution for resource-constrained applications.", "AI": {"tldr": "IM-LUT is introduced as an efficient solution for arbitrary-scale image super-resolution by leveraging interpolation blending and look-up tables to reduce computational costs.", "motivation": "Address the high computational demand and memory usage of current methods for arbitrary-scale image super-resolution (ASISR), while overcoming limitations of fixed-scale LUT-based approaches.", "method": "Propose a framework, IM-LUT, that uses IM-Net to predict interpolation mixing weights and then converts the weights into lightweight LUTs for efficient image reconstruction.", "result": "IM-LUT demonstrates a superior balance of image quality and efficiency compared to existing ASISR techniques, validated on benchmark datasets.", "conclusion": "IM-LUT is a promising approach for ASISR, especially in resource-limited settings, due to its efficient use of interpolation blending and LUTs while maintaining high reconstruction quality."}}
{"id": "2507.09945", "pdf": "https://arxiv.org/pdf/2507.09945", "abs": "https://arxiv.org/abs/2507.09945", "authors": ["Huilai Li", "Yonghao Dang", "Ying Xing", "Yiming Wang", "Jianqin Yin"], "title": "ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Dense audio-visual event localization (DAVE) aims to identify event\ncategories and locate the temporal boundaries in untrimmed videos. Most studies\nonly employ event-related semantic constraints on the final outputs, lacking\ncross-modal semantic bridging in intermediate layers. This causes modality\nsemantic gap for further fusion, making it difficult to distinguish between\nevent-related content and irrelevant background content. Moreover, they rarely\nconsider the correlations between events, which limits the model to infer\nconcurrent events among complex scenarios. In this paper, we incorporate\nmulti-stage semantic guidance and multi-event relationship modeling, which\nrespectively enable hierarchical semantic understanding of audio-visual events\nand adaptive extraction of event dependencies, thereby better focusing on\nevent-related information. Specifically, our eventaware semantic guided network\n(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of\ndependency experts (MoDE) module. ESI applys multi-stage semantic guidance to\nexplicitly constrain the model in learning semantic information through\nmulti-modal early fusion and several classification loss functions, ensuring\nhierarchical understanding of event-related content. MoDE promotes the\nextraction of multi-event dependencies through multiple serial mixture of\nexperts with adaptive weight allocation. Extensive experiments demonstrate that\nour method significantly surpasses the state-of-the-art methods, while greatly\nreducing parameters and computational load. Our code will be released on\nhttps://github.com/uchiha99999/ESG-Net.", "AI": {"tldr": "This research introduces ESG-Net, a framework addressing dense audio-visual event localization (DAVE). It improves event distinction by tackling semantic gaps and capturing event relationships.", "motivation": "The authors aim to close the semantic gap between audio and visual modalities during the event localization process and to enable the system to model complex event relationships in untrimmed videos.", "method": "The ESG-Net combines a multi-stage semantic guidance system (ESI) for hierarchical understanding and a MoDE module for learning dependencies, through early fusion and adaptive weight allocation mechanisms.", "result": "The proposed ESG-Net method outperforms state-of-the-art approaches in terms of accuracy, while simultaneously reducing parameters and computational costs.", "conclusion": "By addressing cross-modal semantic gaps and integrating multi-event dependency modeling, ESG-Net offers an efficient and accurate solution for audio-visual event localization."}}
{"id": "2507.09966", "pdf": "https://arxiv.org/pdf/2507.09966", "abs": "https://arxiv.org/abs/2507.09966", "authors": ["Mingda Zhang"], "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages,6 figures", "summary": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is\nessential for neuro-oncology diagnosis and treatment planning. Despite advances\nin deep learning methods, automatic segmentation remains challenging due to\ntumor morphological heterogeneity and complex three-dimensional spatial\nrelationships. Current techniques primarily rely on visual features extracted\nfrom MRI sequences while underutilizing semantic knowledge embedded in medical\nreports. This research presents a multi-level fusion architecture that\nintegrates pixel-level, feature-level, and semantic-level information,\nfacilitating comprehensive processing from low-level data to high-level\nconcepts. The semantic-level fusion pathway combines the semantic understanding\ncapabilities of Contrastive Language-Image Pre-training (CLIP) models with the\nspatial feature extraction advantages of 3D U-Net through three mechanisms:\n3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based\nattention mechanisms. Experimental validation on the BraTS 2020 dataset\ndemonstrates that the proposed model achieves an overall Dice coefficient of\n0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with\na 7.3% Dice coefficient increase in the clinically important enhancing tumor\n(ET) region.", "AI": {"tldr": "The paper introduces a multi-level fusion model integrating pixel, feature, and semantic information for accurate brain tumor segmentation using MRI, outperforming traditional methods.", "motivation": "Current brain tumor segmentation techniques rely heavily on visual MRI features, overlooking semantic knowledge embedded in medical reports. This limits performance due to tumor complexity and variability.", "method": "The method combines 3D U-Net spatial feature extraction with semantic understanding from CLIP models using three innovative mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention.", "result": "Testing on the BraTS 2020 dataset shows the model achieved a Dice coefficient of 0.8567, a 4.8% overall improvement and a 7.3% improvement in enhancing tumor segmentation compared to traditional 3D U-Net.", "conclusion": "The proposed multi-level fusion architecture effectively integrates semantic knowledge and visual features, showing significant segmentation improvements, especially for clinically critical tumor regions."}}
{"id": "2507.09995", "pdf": "https://arxiv.org/pdf/2507.09995", "abs": "https://arxiv.org/abs/2507.09995", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Brain tumor segmentation plays a critical role in clinical diagnosis and\ntreatment planning, yet the variability in imaging quality across different MRI\nscanners presents significant challenges to model generalization. To address\nthis, we propose the Edge Iterative MRI Lesion Localization System\n(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to\nadaptively fine-tune segmentation models based on clinician feedback, thereby\nenhancing robustness to scanner-specific imaging characteristics. Central to\nthis system is the Graph-based Multi-Modal Interaction Lightweight Network for\nBrain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive\nEncoder (M2AE) to extract multi-scale semantic features efficiently, and a\nGraph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model\ncomplementary cross-modal relationships via graph structures. Additionally, we\nintroduce a novel Voxel Refinement UpSampling Module (VRUM) that\nsynergistically combines linear interpolation and multi-scale transposed\nconvolutions to suppress artifacts while preserving high-frequency details,\nimproving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves\na Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million\nparameters, representing a 98% reduction compared to mainstream 3D Transformer\nmodels, and significantly outperforms existing lightweight approaches. This\nwork demonstrates a synergistic breakthrough in achieving high-accuracy,\nresource-efficient brain tumor segmentation suitable for deployment in\nresource-constrained clinical environments.", "AI": {"tldr": "The paper introduces EdgeIMLocSys, a system using human feedback and the GMLN-BTS model to improve brain tumor segmentation under varying MRI scanner conditions. It achieves high accuracy (Dice score 85.1%) with a lightweight, resource-efficient design.", "motivation": "The motivation is to address the challenge of varying MRI imaging quality across scanners that affects the generalization ability of brain tumor segmentation models.", "method": "The proposed EdgeIMLocSys system incorporates Continuous Learning from Human Feedback with the GMLN-BTS model, which integrates a Modality-Aware Adaptive Encoder (M2AE), a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM), and a Voxel Refinement UpSampling Module (VRUM).", "result": "The GMLN-BTS model achieved a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, making it more efficient and accurate compared to existing methods.", "conclusion": "EdgeIMLocSys demonstrates that lightweight, accurate, and resource-efficient models can significantly enhance brain tumor segmentation, making them suitable for real-world clinical deployment."}}
{"id": "2507.10066", "pdf": "https://arxiv.org/pdf/2507.10066", "abs": "https://arxiv.org/abs/2507.10066", "authors": ["Abhijeet Narang", "Parul Gupta", "Liuyijia Su", "Abhinav Dhall"], "title": "LayLens: Improving Deepfake Understanding through Simplified Explanations", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "This demonstration paper presents $\\mathbf{LayLens}$, a tool aimed to make\ndeepfake understanding easier for users of all educational backgrounds. While\nprior works often rely on outputs containing technical jargon, LayLens bridges\nthe gap between model reasoning and human understanding through a three-stage\npipeline: (1) explainable deepfake detection using a state-of-the-art forgery\nlocalization model, (2) natural language simplification of technical\nexplanations using a vision-language model, and (3) visual reconstruction of a\nplausible original image via guided image editing. The interface presents both\ntechnical and layperson-friendly explanations in addition to a side-by-side\ncomparison of the uploaded and reconstructed images. A user study with 15\nparticipants shows that simplified explanations significantly improve clarity\nand reduce cognitive load, with most users expressing increased confidence in\nidentifying deepfakes. LayLens offers a step toward transparent, trustworthy,\nand user-centric deepfake forensics.", "AI": {"tldr": "LayLens is a tool designed to improve deepfake understanding by simplifying technical explanations and providing visual aids, making it accessible to users of all educational backgrounds.", "motivation": "Deepfake forensics often present information in technical jargon, making it challenging for non-expert users to understand. LayLens aims to bridge this gap by presenting simplified explanations and visual comparisons.", "method": "The tool uses a three-stage pipeline: (1) explainable deepfake detection via forgery localization models, (2) technical explanation simplification using vision-language models, and (3) visual reconstruction of the original image by guided editing.", "result": "A user study involving 15 participants demonstrated that LayLens significantly enhances clarity and reduces cognitive load. Most users reported improved confidence in identifying deepfakes.", "conclusion": "LayLens represents progress toward transparent and user-friendly deepfake detection tools, promoting trust and understanding in forensic applications."}}
{"id": "2507.10250", "pdf": "https://arxiv.org/pdf/2507.10250", "abs": "https://arxiv.org/abs/2507.10250", "authors": ["Ashkan Shakarami", "Lorenzo Nicole", "Rocco Cappellesso", "Angelo Paolo Dei Tos", "Stefano Ghidoni"], "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "25 pages, 15 figures", "summary": "Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.", "AI": {"tldr": "This paper introduces DepViT-CAD, an AI system for diagnosing multiple cancers from histopathological slides, leveraging a novel Multi-Attention Vision Transformer (MAViT).", "motivation": "The paper aims to improve accuracy and reliability in cancer diagnosis through advanced AI models, addressing the challenge of interpreting diverse tumor types in histopathology.", "method": "The authors developed MAViT, a Multi-Attention Vision Transformer, trained on 1008 whole-slide images to diagnose 11 categories of cancer. They tested the system on independent datasets for real-world validation.", "result": "DepViT-CAD demonstrated high diagnostic sensitivity: 94.11% on The Cancer Genome Atlas data and 92% on routine clinical samples.", "conclusion": "The study proposes a robust, scalable, and transparent AI solution for cancer diagnostics, making the code and software publicly available to ensure reproducibility."}}
{"id": "2507.09138", "pdf": "https://arxiv.org/pdf/2507.09138", "abs": "https://arxiv.org/abs/2507.09138", "authors": ["Zhengding Hu", "Vibha Murthy", "Zaifeng Pan", "Wanlu Li", "Xiaoyi Fang", "Yufei Ding", "Yuke Wang"], "title": "HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving", "categories": ["cs.DB", "cs.LG"], "comment": "Accepted by SOSP 2025", "summary": "This paper addresses emerging system-level challenges in heterogeneous\nretrieval-augmented generation (RAG) serving, where complex multi-stage\nworkflows and diverse request patterns complicate efficient execution. We\npresent HedraRAG, a runtime system built on a graph-based abstraction that\nexposes optimization opportunities across stage-level parallelism,\nintra-request similarity, and inter-request skewness. These opportunities are\nrealized through dynamic graph transformations, such as node splitting,\nreordering, edge addition, and dependency rewiring, applied to wavefronts of\nsubgraphs spanning concurrent requests. The resulting execution plans are\nmapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce\nlatency. Evaluations across a wide range of RAG workflows demonstrate speedups\nexceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the\neffectiveness of coordinated generation and retrieval in serving environments.", "AI": {"tldr": "HedraRAG optimizes heterogeneous RAG serving with a graph-based runtime system, achieving up to 5x speedups.", "motivation": "Efficient execution in heterogeneous RAG systems is challenging due to complex workflows and varied requests.", "method": "Proposes HedraRAG, utilizing graph-based abstractions and dynamic graph transformations for optimization.", "result": "Demonstrated speedups of 1.5x to 5x across diverse RAG workflows.", "conclusion": "HedraRAG successfully improves resource utilization and latency in serving environments through coordinated optimization strategies."}}
{"id": "2507.09990", "pdf": "https://arxiv.org/pdf/2507.09990", "abs": "https://arxiv.org/abs/2507.09990", "authors": ["Ming Wen", "Jiaqi Zhu", "Yuedong Xu", "Yipeng Zhou", "Dingding Han"], "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix", "categories": ["cs.CR", "cs.AI"], "comment": "23 pages, NeurIPS 2025 under review", "summary": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions.", "AI": {"tldr": "FedASK addresses the privacy challenges in federated fine-tuning of LLMs using LoRA by introducing a novel sketching pipeline for robust differential privacy.", "motivation": "To address privacy leakage issues and efficiency challenges in federated LoRA fine-tuning for domain-specific tasks while ensuring differential privacy.", "method": "FedASK uses a two-stage sketching pipeline inspired by randomized SVD, enabling privacy-preserving local update aggregation and effective global adapter reconstruction.", "result": "Experimental evaluations show that FedASK outperforms baseline methods consistently in privacy settings and various data distributions.", "conclusion": "FedASK provides a practical and theoretically robust solution to secure federated LLM adaptation without compromising learnability or privacy."}}
{"id": "2507.10542", "pdf": "https://arxiv.org/pdf/2507.10542", "abs": "https://arxiv.org/abs/2507.10542", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nie\u00dfner", "Derek Bradley"], "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.", "AI": {"tldr": "This paper proposes ScaffoldAvatar, a method for generating ultra-high fidelity, expressive, and photorealistic 3D head avatars using a patch-based approach with 3D Gaussian splatting.", "motivation": "The paper addresses the challenge of creating realistic and expressive 3D head avatars for applications like telepresence and movies, particularly focusing on capturing facial microfeatures and detailed motions.", "method": "The authors use a patch-based geometric 3D face model combined with 3D Gaussian splatting, extracting patch-level local expressions and linking them to hierarchical scene anchor points for realistic dynamic rendering.", "result": "ScaffoldAvatar achieves high resolution training (3K images) and state-of-the-art performance, producing visually natural facial motions and diverse expressions in real-time.", "conclusion": "The proposed method successfully enables photorealistic, expressive 3D avatar generation, improving fidelity and motion realism while achieving efficient performance in diverse scenarios."}}
{"id": "2507.09166", "pdf": "https://arxiv.org/pdf/2507.09166", "abs": "https://arxiv.org/abs/2507.09166", "authors": ["Louise Largeau", "Erwan Koch", "David Leutwyler", "Gregoire Mariethoz", "Valerie Chavez-Demoulin", "Tom Beucler"], "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates", "categories": ["physics.ao-ph", "cs.LG"], "comment": "31 pages, 9 figures, 1 table, submitted to AGU JAMES", "summary": "The coarse spatial resolution of gridded climate models, such as general\ncirculation models, limits their direct use in projecting socially relevant\nvariables like extreme precipitation. Most downscaling methods estimate the\nconditional distributions of extremes by generating large ensembles,\ncomplicating the assessment of robustness under distributional shifts, such as\nthose induced by climate change. To better understand and potentially improve\nrobustness, we propose super-resolving the parameters of the target variable's\nprobability distribution directly using analytically tractable mappings. Within\na perfect-model framework over Switzerland, we demonstrate that vector\ngeneralized linear and additive models can super-resolve the generalized\nextreme value distribution of summer hourly precipitation extremes from coarse\nprecipitation fields and topography. We introduce the notion of a \"robustness\ngap\", defined as the difference in predictive error between present-trained and\nfuture-trained models, and use it to diagnose how model structure affects the\ngeneralization of each quantile to a pseudo-global warming scenario. By\nevaluating multiple model configurations, we also identify an upper limit on\nthe super-resolution factor based on the spatial auto- and cross-correlation of\nprecipitation and elevation, beyond which coarse precipitation loses predictive\nvalue. Our framework is broadly applicable to variables governed by parametric\ndistributions and offers a model-agnostic diagnostic for understanding when and\nwhy empirical downscaling generalizes to climate change and extremes.", "AI": {"tldr": "This paper addresses the limitations of coarse-resolution climate models in predicting extreme precipitation by introducing direct super-resolution methods for analyzing probability distributions.", "motivation": "To overcome challenges in predicting socially significant variables like extreme precipitation and improve robustness amidst climate change-induced distributional shifts.", "method": "The authors propose directly super-resolving the parameters of probability distributions using vector generalized linear and additive models, within a perfect-model framework focused on Switzerland.", "result": "The study successfully super-resolves generalized extreme value distributions of summer hourly precipitation, identifies a robustness gap, and establishes a super-resolution limit based on spatial correlations.", "conclusion": "The proposed framework enhances the understanding of empirical downscaling methods and their generalization to climate extremes, offering model-agnostic diagnostics for broader applications."}}
{"id": "2507.10133", "pdf": "https://arxiv.org/pdf/2507.10133", "abs": "https://arxiv.org/abs/2507.10133", "authors": ["Nicholas Leisegang", "Thomas Meyer", "Ivan Varzinczak"], "title": "Extending Defeasibility for Propositional Standpoint Logics", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a new defeasible version of propositional\nstandpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz\nand Varzinczak's notions of defeasible necessity and distinct possibility,\nalong with Leisegang et al.'s approach to defeasibility into the standpoint\nlogics of G\\'omez \\'Alvarez and Rudolph. The resulting logical framework allows\nfor the expression of defeasibility on the level of implications, standpoint\nmodal operators, and standpoint-sharpening statements. We provide a\npreferential semantics for this extended language and propose a tableaux\ncalculus, which is shown to be sound and complete with respect to preferential\nentailment. We also establish the computational complexity of the tableaux\nprocedure to be in PSpace.", "AI": {"tldr": "The paper introduces a new defeasible version of propositional standpoint logic, offering a robust framework to handle defeasibility across different logical constructs.", "motivation": "To enhance standpoint logic by incorporating defeasible reasoning, making it more flexible and applicable to situations involving conditional and modal implications.", "method": "The authors integrate defeasible conditionals, specific notions of necessity and possibility, and approaches to defeasibility into propositional standpoint logic, developing a preferential semantics and a sound and complete tableaux calculus.", "result": "A logical framework enabling defeasibility in implications, modal operators, and standpoint statements was developed, and the tableaux calculus was proven computationally efficient in PSpace complexity.", "conclusion": "The paper successfully expands standpoint logic through defeasible reasoning, providing useful tools and efficient computational methods for logical reasoning."}}
{"id": "2507.10136", "pdf": "https://arxiv.org/pdf/2507.10136", "abs": "https://arxiv.org/abs/2507.10136", "authors": ["Zhonglin Liu"], "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run'' Therapeutic Strategy in Melanoma", "categories": ["q-bio.QM", "cs.AI"], "comment": "9 pages, 5 figures. Submitted to the IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at\n  https://github.com/Liu-Zhonglin/pbn-melanoma-project", "summary": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ``hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.", "AI": {"tldr": "The paper investigates how to overcome resistance to anti-PD-1 therapy in metastatic melanoma using computational modeling and AI-driven intervention strategies.", "motivation": "Understanding and combating innate resistance to anti-PD-1 immunotherapy in metastatic melanoma.", "method": "Development of a Probabilistic Boolean Network using transcriptomic data; application of reinforcement learning and explainable AI to identify effective therapeutic strategies.", "result": "The most effective intervention was a 4-step, temporary inhibition of LOXL2, identified as erasing the molecular resistance signature.", "conclusion": "This computational approach offers novel, time-dependent intervention methods for overcoming immunotherapy resistance, applicable to complex biological systems."}}
{"id": "2507.09330", "pdf": "https://arxiv.org/pdf/2507.09330", "abs": "https://arxiv.org/abs/2507.09330", "authors": ["Linus Walter", "Qingkai Kong", "Sara Hanson-Hedgecock", "V\u00edctor Vilarrasa"], "title": "WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Accurate representation of wells is essential for reliable reservoir\ncharacterization and simulation of operational scenarios in subsurface flow\nmodels. Physics-informed neural networks (PINNs) have recently emerged as a\npromising method for reservoir modeling, offering seamless integration of\nmonitoring data and governing physical equations. However, existing PINN-based\nstudies face major challenges in capturing fluid pressure near wells,\nparticularly during the early stage after injection begins. To address this, we\npropose WellPINN, a modeling workflow that combines the outputs of multiple\nsequentially trained PINN models to accurately represent wells. This workflow\niteratively approximates the radius of the equivalent well to match the actual\nwell dimensions by decomposing the domain into stepwise shrinking subdomains\nwith a simultaneously reducing equivalent well radius. Our results demonstrate\nthat sequential training of superimposing networks around the pumping well is\nthe first workflow that focuses on accurate inference of fluid pressure from\npumping rates throughout the entire injection period, significantly advancing\nthe potential of PINNs for inverse modeling and operational scenario\nsimulations. All data and code for this paper will be made openly available at\nhttps://github.com/linuswalter/WellPINN.", "AI": {"tldr": "Introduces WellPINN, a workflow using sequentially trained Physics-Informed Neural Networks (PINNs) to improve well pressure modeling in reservoir simulations.", "motivation": "Existing PINN-based approaches struggle to capture fluid pressure near wells after injection begins, limiting their effectiveness in reservoir modeling.", "method": "WellPINN iteratively trains multiple PINNs across subdomains, shrinking the equivalent well radius stepwise, to approximate the real well dimensions more accurately.", "result": "WellPINN demonstrates improved accuracy in fluid pressure inference throughout injection periods, addressing limitations of current PINN-based models.", "conclusion": "WellPINN enhances PINN-based reservoir modeling, especially for operational scenarios and inverse modeling; data/code are made available for further research."}}
{"id": "2507.10179", "pdf": "https://arxiv.org/pdf/2507.10179", "abs": "https://arxiv.org/abs/2507.10179", "authors": ["Asvin G"], "title": "The Second Machine Turn: From Checking Proofs to Creating Concepts", "categories": ["math.HO", "cs.AI"], "comment": null, "summary": "We identify a second machine turn in the process of mathematical discovery:\nafter automating proof-checking, AI is now poised to automate the *creation* of\nmathematical concepts themselves. We discuss the current state of the art,\nobstacles and potential solutions as well as a preliminary attempt at\nmathematizing the creation of concepts itself. The paper ends with an\nassessment of how these capabilities could reshape mathematics and\nhuman-machine collaboration, and a few different futures we might find\nourselves in.", "AI": {"tldr": "This paper discusses AI's role in automating the creation of mathematical concepts and explores implications, methods, and future scenarios.", "motivation": "The paper aims to advance AI capabilities from proof-checking to the automation of creating mathematical concepts.", "method": "Exploration of current AI technologies, obstacles, proposed solutions, and an attempt to mathematize concept creation.", "result": "Preliminary demonstration of AI creating mathematical concepts and identification of challenges in the field.", "conclusion": "AI's capacity to create mathematical concepts could significantly impact mathematics and human-machine collaboration, with multiple potential future pathways."}}
{"id": "2507.09431", "pdf": "https://arxiv.org/pdf/2507.09431", "abs": "https://arxiv.org/abs/2507.09431", "authors": ["Zefang Liu", "Weston M. Stacey"], "title": "Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations", "categories": ["physics.plasm-ph", "cs.LG"], "comment": null, "summary": "Achieving controlled burning plasma in tokamaks requires precise regulation\nof external particle and energy sources to reach and maintain target core\ndensities and temperatures. This work presents an inverse modeling approach\nusing a multinodal plasma dynamics model based on neural ordinary differential\nequations (Neural ODEs). Given a desired time evolution of nodal quantities\nsuch as deuteron density or electron temperature, we compute the external\nsource profiles, such as neutral beam injection (NBI) power, that drive the\nplasma toward the specified behavior. The approach is implemented within the\nNeuralPlasmaODE framework, which models multi-region, multi-timescale transport\nand incorporates physical mechanisms including radiation, auxiliary heating,\nand internodal energy exchange. By formulating the control task as an\noptimization problem, we use automatic differentiation through the Neural ODE\nsolver to minimize the discrepancy between simulated and target trajectories.\nThis framework transforms the forward simulation tool into a control-oriented\nmodel and provides a practical method for computing external source profiles in\nboth current and future fusion devices.", "AI": {"tldr": "This paper introduces a neural ODE-based framework, NeuralPlasmaODE, to inversely model and regulate plasma dynamics in tokamaks by computing external inputs like neutral beam injection power to achieve specified plasma behaviors.", "motivation": "To address the challenge of controlling plasma conditions for burning states in tokamaks, precise regulation of particle and energy sources is needed, motivating the development of a computational framework for inverse modeling.", "method": "The paper employs Neural Ordinary Differential Equations (Neural ODEs) for modeling plasma dynamics and formulates the control problem as an optimization task using automatic differentiation through the Neural ODE solver.", "result": "The proposed NeuralPlasmaODE framework successfully computes external source profiles that drive plasma dynamics toward specified target behaviors, demonstrating practical applicability in controlling tokamak operations.", "conclusion": "NeuralPlasmaODE enables control-oriented modeling of plasma and offers an effective approach for regulating external energy sources in fusion devices, paving the way for advancements in both current and future tokamak systems."}}
{"id": "2507.09432", "pdf": "https://arxiv.org/pdf/2507.09432", "abs": "https://arxiv.org/abs/2507.09432", "authors": ["Zefang Liu", "Weston M. Stacey"], "title": "Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas", "categories": ["physics.plasm-ph", "cs.LG"], "comment": null, "summary": "Understanding how key physical parameters influence burning plasma behavior\nis critical for the reliable operation of ITER. In this work, we extend\nNeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary\ndifferential equations, to perform a sensitivity analysis of transport and\nradiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge\ntemperatures and densities are computed with respect to transport\ndiffusivities, electron cyclotron radiation (ECR) parameters, impurity\nfractions, and ion orbit loss (IOL) timescales. The analysis focuses on\nperturbations around a trained nominal model for the ITER inductive scenario.\nResults highlight the dominant influence of magnetic field strength, safety\nfactor, and impurity content on energy confinement, while also revealing how\ntemperature-dependent transport contributes to self-regulating behavior. These\nfindings demonstrate the utility of NeuralPlasmaODE for predictive modeling and\nscenario optimization in burning plasma environments.", "AI": {"tldr": "This paper uses NeuralPlasmaODE, a machine learning-based model, to analyze how physical parameters affect plasma behavior in ITER, focusing on energy confinement and predictive modeling.", "motivation": "The paper aims to improve understanding of key physical influences on burning plasma behavior to ensure the reliable operation of ITER.", "method": "The study extends a neural ordinary differential equation-based model, NeuralPlasmaODE, to perform sensitivity analyses of various plasma-related parameters.", "result": "Key factors such as magnetic field strength, safety factor, and impurity content are found to dominantly impact energy confinement, with temperature-dependent transport enabling self-regulating behavior.", "conclusion": "The findings confirm NeuralPlasmaODE's effectiveness in predictive modeling and optimizing scenarios within ITER burning plasma contexts."}}
{"id": "2507.10240", "pdf": "https://arxiv.org/pdf/2507.10240", "abs": "https://arxiv.org/abs/2507.10240", "authors": ["Angelos Chatzimparmpas"], "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Our society increasingly depends on intelligent systems to solve complex\nproblems, ranging from recommender systems suggesting the next movie to watch\nto AI models assisting in medical diagnoses for hospitalized patients. With the\niterative improvement of diagnostic accuracy and efficiency, AI holds\nsignificant potential to mitigate medical misdiagnoses by preventing numerous\ndeaths and reducing an economic burden of approximately 450 EUR billion\nannually. However, a key obstacle to AI adoption lies in the lack of\ntransparency: many automated systems function as \"black boxes,\" providing\npredictions without revealing the underlying processes. This opacity can hinder\nexperts' ability to trust and rely on AI systems. Visual analytics (VA)\nprovides a compelling solution by combining AI models with interactive\nvisualizations. These specialized charts and graphs empower users to\nincorporate their domain expertise to refine and improve the models, bridging\nthe gap between AI and human understanding. In this work, we define,\ncategorize, and explore how VA solutions can foster trust across the stages of\na typical AI pipeline. We propose a design space for innovative visualizations\nand present an overview of our previously developed VA dashboards, which\nsupport critical tasks within the various pipeline stages, including data\nprocessing, feature engineering, hyperparameter tuning, understanding,\ndebugging, refining, and comparing models.", "AI": {"tldr": "AI systems are advancing in accuracy and efficiency, but their lack of transparency undermines trust. Visual analytics (VA) seeks to address this by using interactive visualizations that make AI processes more interpretable and trustworthy.", "motivation": "Improve transparency and trust in AI systems essential in critical domains like medical diagnosis, which could save lives and reduce significant economic burdens.", "method": "Define and categorize how visual analytics can enhance trust at different stages of the AI pipeline. Provide a design space for visualization innovation and review previous VA tools developed.", "result": "Demonstrated how VA dashboards can assist in tasks ranging from data processing to comparing models, making AI systems more transparent and trustworthy.", "conclusion": "Visual analytics addresses critical transparency deficiencies of AI systems, enhancing trust and enabling better collaboration between AI and human expertise."}}
{"id": "2507.09503", "pdf": "https://arxiv.org/pdf/2507.09503", "abs": "https://arxiv.org/abs/2507.09503", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Submitted to IEEE Transactions on Power Systems", "summary": "This paper proposes a neural stochastic optimization method for efficiently\nsolving the two-stage stochastic unit commitment (2S-SUC) problem under\nhigh-dimensional uncertainty scenarios. The proposed method approximates the\nsecond-stage recourse problem using a deep neural network trained to map\ncommitment decisions and uncertainty features to recourse costs. The trained\nnetwork is subsequently embedded into the first-stage UC problem as a\nmixed-integer linear program (MILP), allowing for explicit enforcement of\noperational constraints while preserving the key uncertainty characteristics. A\nscenario-embedding network is employed to enable dimensionality reduction and\nfeature aggregation across arbitrary scenario sets, serving as a data-driven\nscenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and\n118-bus systems demonstrate that the proposed neural two-stage stochastic\noptimization method achieves solutions with an optimality gap of less than 1%,\nwhile enabling orders-of-magnitude speedup compared to conventional MILP\nsolvers and decomposition-based methods. Moreover, the model's size remains\nconstant regardless of the number of scenarios, offering significant\nscalability for large-scale stochastic unit commitment problems.", "AI": {"tldr": "The paper introduces a neural approach to solve a large-scale two-stage stochastic unit commitment (2S-SUC) problem efficiently by embedding a trained deep neural network into a mixed-integer linear program (MILP).", "motivation": "The paper addresses the computational inefficiency and scalability issues of conventional methods for solving 2S-SUC problems under high-dimensional uncertainty.", "method": "A deep neural network is trained to approximate the second-stage recourse costs and is embedded into the first-stage problem as a MILP. A scenario-embedding network is used for dimensionality reduction and scenario reduction.", "result": "The method achieves an optimality gap of less than 1% and significantly faster computation (orders-of-magnitude speedup) compared to traditional methods, while maintaining scalability as the number of scenarios grows.", "conclusion": "This neural optimization approach offers a computationally efficient, scalable, and accurate method for solving large-scale 2S-SUC problems."}}
{"id": "2507.09624", "pdf": "https://arxiv.org/pdf/2507.09624", "abs": "https://arxiv.org/abs/2507.09624", "authors": ["Xiaojie Lin", "Baihe Ma", "Xu Wang", "Guangsheng Yu", "Ying He", "Wei Ni", "Ren Ping Liu"], "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Driving trajectory data remains vulnerable to privacy breaches despite\nexisting mitigation measures. Traditional methods for detecting driving\ntrajectories typically rely on map-matching the path using Global Positioning\nSystem (GPS) data, which is susceptible to GPS data outage. This paper\nintroduces CAN-Trace, a novel privacy attack mechanism that leverages\nController Area Network (CAN) messages to uncover driving trajectories, posing\na significant risk to drivers' long-term privacy. A new trajectory\nreconstruction algorithm is proposed to transform the CAN messages,\nspecifically vehicle speed and accelerator pedal position, into weighted graphs\naccommodating various driving statuses. CAN-Trace identifies driving\ntrajectories using graph-matching algorithms applied to the created graphs in\ncomparison to road networks. We also design a new metric to evaluate matched\ncandidates, which allows for potential data gaps and matching inaccuracies.\nEmpirical validation under various real-world conditions, encompassing\ndifferent vehicles and driving regions, demonstrates the efficacy of CAN-Trace:\nit achieves an attack success rate of up to 90.59% in the urban region, and\n99.41% in the suburban region.", "AI": {"tldr": "The paper introduces CAN-Trace, a privacy attack mechanism that uses vehicle CAN messages to reconstruct driving trajectories, showing a high success rate in trajectory identification.", "motivation": "To address the vulnerability of driving trajectory data to privacy attacks and demonstrate how CAN messages can be exploited beyond traditional GPS methods.", "method": "The proposed method transforms vehicle speed and accelerator pedal position data from CAN messages into weighted graphs and uses graph-matching algorithms to uncover driving trajectories, with allowances for data gaps.", "result": "CAN-Trace achieves up to 90.59% attack success in urban areas and 99.41% in suburban areas, validated under diverse real-world conditions.", "conclusion": "CAN-Trace poses a significant privacy risk, demonstrating the effectiveness of leveraging vehicle CAN data to compromise driver privacy through trajectory reconstruction."}}
{"id": "2507.09652", "pdf": "https://arxiv.org/pdf/2507.09652", "abs": "https://arxiv.org/abs/2507.09652", "authors": ["Christof Sch\u00f6tz", "Niklas Boers"], "title": "Machine-Precision Prediction of Low-Dimensional Chaotic Systems", "categories": ["nlin.CD", "cs.LG", "math.DS"], "comment": null, "summary": "Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used\nto benchmark system-agnostic methods for learning dynamics from data. Here we\nshow that learning from noise-free observations in such systems can be achieved\nup to machine precision: using ordinary least squares regression on high-degree\npolynomial features with 512-bit arithmetic, our method exceeds the accuracy of\nstandard 64-bit numerical ODE solvers of the true underlying dynamical systems.\nDepending on the configuration, we obtain valid prediction times of 32 to 105\nLyapunov times for the Lorenz-63 system, dramatically outperforming prior work\nthat reaches 13 Lyapunov times at most. We further validate our results on\nThomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is\nconsiderably more complex than the Lorenz-63 model, and show that similar\nresults extend also to higher dimensions using the spatiotemporally chaotic\nLorenz-96 model. Our findings suggest that learning low-dimensional chaotic\nsystems from noise-free data is a solved problem.", "AI": {"tldr": "This paper presents a methodology to accurately learn dynamics from noise-free observations in low-dimensional chaotic systems, achieving unprecedented prediction times.", "motivation": "The study aims to evaluate the efficacy of system-agnostic methods for learning the dynamics of low-dimensional chaotic systems, such as the Lorenz-63 model, using noise-free data.", "method": "The paper employs ordinary least squares regression applied to high-degree polynomial features with 512-bit arithmetic, allowing for machine-precision accuracy.", "result": "The proposed method achieves valid prediction times of up to 105 Lyapunov times for the Lorenz-63 system, outperforming previous approaches which capped at 13 Lyapunov times. Results are validated on more complex chaotic systems like Thomas' Cyclically Symmetric Attractor and the higher-dimensional Lorenz-96 model.", "conclusion": "Learning dynamics from noise-free observations in low-dimensional chaotic systems has reached a highly accurate and reliable stage, indicating that such problems are effectively solved under these conditions."}}
{"id": "2507.09685", "pdf": "https://arxiv.org/pdf/2507.09685", "abs": "https://arxiv.org/abs/2507.09685", "authors": ["Yutong Li", "Ilya Kolmanovsky"], "title": "Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "6 pages, 5 figures", "summary": "Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid\ndisorders but carry significant risks when administered chronically at high\ndoses. Precise long-term control of gastric acidity is challenged by the\nimpracticality of invasive gastric acid monitoring beyond 72 hours and wide\ninter-patient variability. We propose a noninvasive, symptom-based framework\nthat tailors PPI dosing solely on patient-reported reflux and digestive symptom\npatterns. A Bayesian Neural Network prediction model learns to predict patient\nsymptoms and quantifies its uncertainty from historical symptom scores, meal,\nand PPIs intake data. These probabilistic forecasts feed a chance-constrained\nModel Predictive Control (MPC) algorithm that dynamically computes future PPI\ndoses to minimize drug usage while enforcing acid suppression with high\nconfidence - without any direct acid measurement. In silico studies over\ndiverse dietary schedules and virtual patient profiles demonstrate that our\nlearning-augmented MPC reduces total PPI consumption by 65 percent compared to\nstandard fixed regimens, while maintaining acid suppression with at least 95\npercent probability. The proposed approach offers a practical path to\npersonalized PPI therapy, minimizing treatment burden and overdose risk without\ninvasive sensors.", "AI": {"tldr": "The paper presents a noninvasive and symptom-based framework for optimizing Proton Pump Inhibitor (PPI) therapy, offering personalized dosing based on patient-reported symptoms and Bayesian Neural Network predictions, while reducing drug consumption.", "motivation": "Chronic high doses of PPIs pose significant risks, and the current approach lacks precision due to impractical gastric acid monitoring methods and inter-patient variability.", "method": "A Bayesian Neural Network generates symptom forecasts from historical data, which informs a chance-constrained Model Predictive Control algorithm to dynamically optimize PPI doses without invasive gastric acid measurements.", "result": "In silico studies show a 65% reduction in total PPI consumption compared to fixed regimens, while maintaining acid suppression with at least 95% probability.", "conclusion": "The approach demonstrates potential for personalized PPI therapy, enabling significant drug reduction and overdose risk minimization without sacrificing efficacy or requiring invasive monitoring tools."}}
{"id": "2507.10447", "pdf": "https://arxiv.org/pdf/2507.10447", "abs": "https://arxiv.org/abs/2507.10447", "authors": ["Tomasz Sroka", "Tomasz W\u0119\u017cowicz", "Dominik Sidorczuk", "Mateusz Modrzejewski"], "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure", "summary": "With the rapid advancement of generative audio models, distinguishing between\nhuman-composed and generated music is becoming increasingly challenging. As a\nresponse, models for detecting fake music have been proposed. In this work, we\nexplore the robustness of such systems under audio augmentations. To evaluate\nmodel generalization, we constructed a dataset consisting of both real and\nsynthetic music generated using several systems. We then apply a range of audio\ntransformations and analyze how they affect classification accuracy. We test\nthe performance of a recent state-of-the-art musical deepfake detection model\nin the presence of audio augmentations. The performance of the model decreases\nsignificantly even with the introduction of light augmentations.", "AI": {"tldr": "Generative audio models make detecting fake music challenging. This study tests the robustness of fake detection systems against audio modifications, revealing weaknesses in existing methods.", "motivation": "The paper aims to address the growing difficulty in identifying generated (fake) music due to the advancement in generative audio models.", "method": "A dataset of real and synthetic music, generated by multiple systems, was subjected to diverse audio transformations. A state-of-the-art detection model was then evaluated under these conditions.", "result": "The performance of the musical deepfake detection model drops substantially even with minor audio augmentations.", "conclusion": "Current detection models lack robustness against audio augmentations, indicating the need for improved systems to reliably classify fake and real music."}}
{"id": "2507.09750", "pdf": "https://arxiv.org/pdf/2507.09750", "abs": "https://arxiv.org/abs/2507.09750", "authors": ["Enric Gus\u00f3", "Joanna Luberadzka", "Umut Sayin", "Xavier Serra"], "title": "MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Accepted to WASPAA25", "summary": "We investigate the effects of four strategies for improving the ecological\nvalidity of synthetic room impulse response (RIR) datasets for monoaural Speech\nEnhancement (SE). We implement three features on top of the traditional image\nsource method-based (ISM) shoebox RIRs: multiband absorption coefficients,\nsource directivity and receiver directivity. We additionally consider\nmesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3\nmodel for each RIR dataset and evaluate the performance on a test set of real\nRIRs both objectively and subjectively. We find that RIRs which use\nfrequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain\n+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs\ndataset is publicly available for free download.", "AI": {"tldr": "This paper explores enhancing ecological validity in synthetic room impulse response datasets for monoaural speech enhancement. By examining strategies like multiband absorption coefficients, directional sources, and mesh-based responses, they assess improved performance using DeepFilternet3.", "motivation": "The authors aim to address the gap in ecological validity in synthetic RIR datasets for monoaural speech enhancement tasks, impacting performance evaluations on real-world scenarios.", "method": "They implement three features\u2014multiband absorption coefficients, source directivity, and receiver directivity\u2014on ISM-based RIRs and also explore mesh-based RIRs from SoundSpaces. Performance is tested using DeepFilternet3 on both objective and subjective metrics.", "result": "Using frequency-dependent absorption coefficients for RIRs resulted in better performance: +0.51dB SDR and +8.9 MUSHRA score compared to traditional methods.", "conclusion": "Frequency-dependent acoustic absorption improves alignment between synthetic and real RIR datasets. Their MB-RIR dataset achieves notable gains and is publicly available for future research."}}
{"id": "2507.09757", "pdf": "https://arxiv.org/pdf/2507.09757", "abs": "https://arxiv.org/abs/2507.09757", "authors": ["Chunyan Li", "Wenkai Yu", "Qi Wang"], "title": "Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems", "categories": ["math.NA", "cs.LG", "cs.NA", "35K57, 68T07"], "comment": "32 pages, 22 figures", "summary": "We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)\nstrategy, a novel method that substantially enhances the performance of\nPhysics-Informed Neural Networks (PINNs) in solving thermodynamically\nconsistent partial differential equations (PDEs) over arbitrary domains. EDRAS\nleverages the local energy dissipation rate density as a guiding metric to\nidentify and adaptively re-sample critical collocation points from both the\ninterior and boundary of the computational domain. This dynamical sampling\napproach improves the accuracy of residual-based PINNs by aligning the training\nprocess with the underlying physical structure of the system. In this study, we\ndemonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model\nin irregular geometries, achieving up to a sixfold reduction in the relative\nmean square error compared to traditional residual-based adaptive refinement\n(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive\nsampling approaches and show that EDRAS is not only computationally more\nefficient but also more likely to identify high-impact collocation points.\nThrough numerical solutions of the Allen-Cahn equation with both static\n(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped\ndomains solved using PINN coupled with EDRAS, we gain significant insights into\nhow dynamic boundary conditions influence bulk phase evolution and\nthermodynamic behavior. The proposed approach offers an effective, physically\ninformed enhancement to PINN frameworks for solving thermodynamically\nconsistent models, making PINN a robust and versatile computational tool for\ninvestigating complex thermodynamic processes in arbitrary geometries.", "AI": {"tldr": "This paper introduces EDRAS, an adaptive sampling method that improves PINNs for solving thermodynamically consistent PDEs, demonstrating significant error reduction using the Allen-Cahn model.", "motivation": "To improve the accuracy and efficiency of Physics-Informed Neural Networks (PINNs) in solving PDEs related to thermodynamic processes over complex geometries.", "method": "The authors developed EDRAS, leveraging the energy dissipation rate density to dynamically and adaptively re-sample collocation points during training.", "result": "EDRAS showed up to sixfold reduction in relative mean square error compared to traditional methods when applied to the Allen-Cahn model in irregular geometries.", "conclusion": "EDRAS enhances the physical accuracy, computational efficiency, and versatility of PINN frameworks, making them robust tools for solving complex thermodynamic equations over arbitrary domains."}}
{"id": "2507.10457", "pdf": "https://arxiv.org/pdf/2507.10457", "abs": "https://arxiv.org/abs/2507.10457", "authors": ["Hammad Atta", "Ken Huang", "Manish Bhatt", "Kamal Ahmed", "Muhammad Aziz Ul Haq", "Yasir Mehmood"], "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.", "AI": {"tldr": "The paper introduces Logic-Layer Prompt Control Injection (LPCI), a new attack on enterprise systems using LLMs by embedding delayed payloads in memory or tool outputs.", "motivation": "To address and highlight a critical security vulnerability in enterprise systems using LLMs, specifically regarding undetected logic-layer attacks.", "method": "The authors propose the concept of LPCI, detailing how encoded, delayed, and conditionally triggered payloads can subvert security by bypassing input filters.", "result": "LPCI payloads can remain hidden across sessions and trigger unauthorized behavior, bypassing standard defenses.", "conclusion": "Enterprise systems incorporating LLMs are vulnerable to sophisticated attacks like LPCI, necessitating better detection and defense mechanisms."}}
{"id": "2507.10464", "pdf": "https://arxiv.org/pdf/2507.10464", "abs": "https://arxiv.org/abs/2507.10464", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "title": "AudioMAE++: learning better masked audio representations with SwiGLU FFNs", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "TO APPEAR AT IEEE MLSP 2025", "summary": "Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged\nas a prominent approach for learning self-supervised audio representations.\nWhile several recent papers have evaluated key aspects of training MAEs on\naudio data, the majority of these approaches still leverage vanilla transformer\nbuilding blocks, whereas the transformer community has seen steady integration\nof newer architectural advancements. In this work, we propose AudioMAE++, a\nrevamped audio masked autoencoder with two such enhancements, namely\nmacaron-style transformer blocks with gated linear units. When pretrained on\nthe AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE\nbased approaches on 10 diverse downstream tasks, demonstrating excellent\nperformance on audio classification and speech-based benchmarks. The proposed\nAudioMAE++ models also demonstrate excellent scaling characteristics,\noutperforming directly comparable standard MAE baselines with up to 4x more\nparameters.", "AI": {"tldr": "AudioMAE++ is a revamped masked autoencoder for audio spectrograms, enhancing audio representation learning with architectural advancements, and demonstrating superior performance across diverse tasks compared to existing approaches.", "motivation": "To address the limitations of existing audio masked autoencoders that rely on traditional transformer architecture and to incorporate modern transformer advancements.", "method": "Proposes AudioMAE++ incorporating macaron-style transformer blocks and gated linear units, pretrained on the AudioSet dataset.", "result": "AudioMAE++ outperformed existing MAE approaches on 10 downstream tasks, excelling in both audio classification and speech-based benchmarks.", "conclusion": "AudioMAE++ leverages architectural enhancements to set new standards in self-supervised audio representation learning with better performance and scalability."}}
{"id": "2507.10469", "pdf": "https://arxiv.org/pdf/2507.10469", "abs": "https://arxiv.org/abs/2507.10469", "authors": ["Mikko Korkiakoski", "Saeid Sheikhi", "Jesper Nyman", "Jussi Saariniemi", "Kalle Tapio", "Panos Kostakos"], "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments", "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.", "AI": {"tldr": "The paper evaluates AI-driven NPCs using GPT-4 Turbo in a VR interrogation simulator, assessing realism, usability, and performance.", "motivation": "To explore the potential of large language models for improving realism and interactivity of NPCs in VR.", "method": "Conducted a user study with 18 participants using questionnaires to measure usability, believability, and latency performance.", "result": "Believability scored 6.67/10, SUS usability score was 79.44, and average latency was 7 seconds.", "conclusion": "Large language models enhance NPC realism but require optimization in latency and emotional depth."}}
{"id": "2507.09823", "pdf": "https://arxiv.org/pdf/2507.09823", "abs": "https://arxiv.org/abs/2507.09823", "authors": ["Ekaterina Borodich", "Dmitry Kovalev"], "title": "Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "In this paper, we focus on the problem of minimizing a continuously\ndifferentiable convex objective function $\\min_x f(x)$. Recently, several\nadaptive gradient methods, including GRAAL (Malitsky, 2020), have been\ndeveloped. These methods estimate the local curvature of the objective function\nto compute stepsizes, attain the standard convergence rate $\\mathcal{O}(1/k)$\nof fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not\nrequire any line search procedures or hyperparameter tuning. However, a natural\nquestion arises: is it possible to accelerate the convergence of these\nalgorithms to match the optimal rate $\\mathcal{O}(1/k^2)$ of the accelerated\ngradient descent of Nesterov (1983)? Although some attempts have been made (Li\nand Lan, 2023), the capabilities of the existing accelerated algorithms to\nadapt to the curvature of the objective function are highly limited.\nConsequently, we provide a positive answer to this question and develop GRAAL\nwith Nesterov acceleration. We prove that our algorithm achieves the desired\noptimal convergence rate for Lipschitz smooth functions. Moreover, in contrast\nto existing methods, it does so with an arbitrary, even excessively small,\ninitial stepsize at the cost of a logarithmic additive term in the iteration\ncomplexity.", "AI": {"tldr": "This paper improves adaptive gradient methods like GRAAL by combining them with Nesterov acceleration to achieve optimal convergence rates.", "motivation": "To address the question of whether adaptive gradient methods can be accelerated to match the optimal convergence rate of $\\\\mathcal{O}(1/k^2)$ for convex optimization problems.", "method": "The authors develop a new algorithm, GRAAL with Nesterov acceleration, which adapts to local curvature, computes stepsizes without hyperparameter tuning, and achieves accelerated rates.", "result": "The proposed algorithm achieves the optimal convergence rate of $\\\\mathcal{O}(1/k^2)$ for Lipschitz smooth functions, even with an excessively small initial stepsize, incurring only a logarithmic additive cost in iteration complexity.", "conclusion": "GRAAL with Nesterov acceleration successfully combines adaptability and acceleration for minimizing differentiable convex functions, overcoming limitations of prior methods."}}
{"id": "2507.10510", "pdf": "https://arxiv.org/pdf/2507.10510", "abs": "https://arxiv.org/abs/2507.10510", "authors": ["Jiangkai Wu", "Zhiyuan Ren", "Liming Liu", "Xinggong Zhang"], "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI", "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.MM"], "comment": null, "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.", "AI": {"tldr": "The paper introduces an AI-focused video communication framework called Artic, tailored to optimize real-time interactions between humans and AI models despite network challenges.", "motivation": "Address the latency and transmission inefficiencies in AI Video Chats where a Multimodal Large Language Model (MLLM) is involved.", "method": "Introduced Context-Aware Video Streaming to prioritize relevant video regions and Loss-Resilient Adaptive Frame Rate to handle packet loss and avoid waste. Developed a benchmark named DeViBench to analyze video streaming quality's impact on MLLM accuracy.", "result": "The framework effectively reduces bitrate while preserving MLLM interaction quality, ensuring better real-time communication performance.", "conclusion": "Artic represents a significant advancement in RTC for AI Video Chats, enabling more fluid and accurate interactions under network uncertainty, with potential for further solutions in the field."}}
{"id": "2507.10530", "pdf": "https://arxiv.org/pdf/2507.10530", "abs": "https://arxiv.org/abs/2507.10530", "authors": ["Ping Tuo", "Jiale Chen", "Ju Li"], "title": "Accurate generation of chemical reaction transition states by conditional flow matching", "categories": ["physics.chem-ph", "cs.AI"], "comment": null, "summary": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms.", "AI": {"tldr": "This paper introduces TS-GEN, a generative model for accurately and efficiently predicting chemical transition-state geometries, offering substantial improvements over existing methods.", "motivation": "Transition-state structures are critical to understanding chemical reactivity, but their experimental inaccessibility and the high computational cost of traditional methods create a need for more efficient and accurate solutions.", "method": "TS-GEN utilizes a conditional flow-matching generative model that maps latent Gaussian noise directly to transition-state geometries in a deterministic and non-iterative manner. The model leverages reactant and product conformations as conditioning data, employing optimal-transport principles.", "result": "The model achieves high accuracy with a root-mean-square deviation of 0.004 \u00c5, a barrier-height error of 1.019 kcal/mol, and a computation time of 0.06 seconds per inference on a GPU. Over 87% of generated transition states meet chemical-accuracy standards, outperforming existing methods significantly.", "conclusion": "TS-GEN provides a transformative approach to high-throughput exploration of reaction networks by combining exceptional speed, precision, and wide applicability, enabling the investigation of novel chemical mechanisms efficiently."}}
{"id": "2507.10534", "pdf": "https://arxiv.org/pdf/2507.10534", "abs": "https://arxiv.org/abs/2507.10534", "authors": ["Qihui Yang", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Zachary Novack"], "title": "WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Despite rapid progress in end-to-end AI music generation, AI-driven modeling\nof professional Digital Signal Processing (DSP) workflows remains challenging.\nIn particular, while there is growing interest in neural black-box modeling of\naudio effect graphs (e.g. reverb, compression, equalization), AI-based\napproaches struggle to replicate the nuanced signal flow and parameter\ninteractions used in professional workflows. Existing differentiable plugin\napproaches often diverge from real-world tools, exhibiting inferior performance\nrelative to simplified neural controllers under equivalent computational\nconstraints. We introduce WildFX, a pipeline containerized with Docker for\ngenerating multi-track audio mixing datasets with rich effect graphs, powered\nby a professional Digital Audio Workstation (DAW) backend. WildFX supports\nseamless integration of cross-platform commercial plugins or any plugins in the\nwild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,\nsidechains, crossovers) and achieving efficient parallelized processing. A\nminimalist metadata interface simplifies project/plugin configuration.\nExperiments demonstrate the pipeline's validity through blind estimation of\nmixing graphs, plugin/gain parameters, and its ability to bridge AI research\nwith practical DSP demands. The code is available on:\nhttps://github.com/IsaacYQH/WildFX.", "AI": {"tldr": "WildFX provides a solution for AI-driven modeling of professional Digital Signal Processing (DSP) workflows using a Docker pipeline with rich audio mixing datasets and enhanced integration capabilities.", "motivation": "The paper aims to address the challenges faced by AI approaches in replicating the complex signal flows and parameter interactions within professional DSP workflows.", "method": "The authors introduce a Docker-powered pipeline called WildFX, incorporating multi-track audio mixing datasets, a professional DAW backend, and support for standard plugin formats to enable detailed structural complexity and efficient parallelized processing.", "result": "Experiments show that WildFX can perform blind estimations of mixing graphs and gain parameters, and successfully bridge the gap between AI research and practical DSP needs.", "conclusion": "WildFX provides a robust framework that enhances AI-driven DSP workflows, making it comparable to professional mixing practices and accessible for further AI research and development."}}
{"id": "2507.10201", "pdf": "https://arxiv.org/pdf/2507.10201", "abs": "https://arxiv.org/abs/2507.10201", "authors": ["Gleb Shishaev", "Vasily Demyanov", "Daniel Arnold"], "title": "History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions", "categories": ["stat.AP", "cs.LG", "physics.data-an"], "comment": "Part of the completed PhD thesis\n  https://geodatascience.hw.ac.uk/theses/", "summary": "The graph-based variational autoencoder represents an architecture that can\nhandle the uncertainty of different geological scenarios, such as depositional\nor structural, through the concept of a lowerdimensional latent space. The main\ndifference from recent studies is utilisation of a graph-based approach in\nreservoir modelling instead of the more traditional lattice-based deep learning\nmethods. We provide a solution to implicitly control the geological realism\nthrough the latent variables of a generative model and Geodesic metrics. Our\nexperiments of AHM with synthetic dataset that consists of 3D realisations of\nchannelised geological representations with two distinct scenarios with one and\ntwo channels shows the viability of the approach. We offer in-depth analysis of\nthe latent space using tools such as PCA, t-SNE, and TDA to illustrate its\nstructure.", "AI": {"tldr": "The paper introduces a graph-based variational autoencoder for reservoir modeling that can handle geological uncertainty using lower-dimensional latent space.", "motivation": "To better manage uncertainty in geological scenarios by introducing a graph-based modeling method as an alternative to traditional lattice-based approaches.", "method": "Employing a graph-based generative model combined with Geodesic metrics to control geological realism and analyzing latent space using PCA, t-SNE, and TDA.", "result": "Demonstrated the methodology's viability through experiments with synthetic datasets representing two geological scenarios, successfully analyzing latent space structure.", "conclusion": "Graph-based variational autoencoder approach was effective in managing geological uncertainty and provided insightful latent space analysis."}}
{"id": "2507.10267", "pdf": "https://arxiv.org/pdf/2507.10267", "abs": "https://arxiv.org/abs/2507.10267", "authors": ["Novruz Amirov", "Baran Isik", "Bilal Ihsan Tuncer", "Serif Bahtiyar"], "title": "DNS Tunneling: Threat Landscape and Improved Detection Solutions", "categories": ["cs.CR", "cs.LG", "cs.NI"], "comment": null, "summary": "Detecting Domain Name System (DNS) tunneling is a significant challenge in\nsecurity due to its capacity to hide harmful actions within DNS traffic that\nappears to be normal and legitimate. Traditional detection methods are based on\nrule-based approaches or signature matching methods that are often insufficient\nto accurately identify such covert communication channels. This research is\nabout effectively detecting DNS tunneling. We propose a novel approach to\ndetect DNS tunneling with machine learning algorithms. We combine machine\nlearning algorithms to analyze the traffic by using features extracted from DNS\ntraffic. Analyses results show that the proposed approach is a good candidate\nto detect DNS tunneling accurately.", "AI": {"tldr": "The study proposes a machine learning-based approach for detecting DNS tunneling, which overcomes the limitations of traditional rule-based methods.", "motivation": "The need to address the limitation of rule-based or signature-matching methods in detecting covert DNS tunneling activities, which are often used to hide harmful actions.", "method": "A novel detection system using machine learning algorithms to analyze DNS traffic by extracting features from it.", "result": "The analysis demonstrates the effectiveness of the proposed machine learning approach in detecting DNS tunneling accurately.", "conclusion": "Machine learning offers an effective alternative to conventional methods for identifying DNS tunneling, showcasing its potential as a reliable detection mechanism."}}
{"id": "2507.10448", "pdf": "https://arxiv.org/pdf/2507.10448", "abs": "https://arxiv.org/abs/2507.10448", "authors": ["Yingqian Wu", "Qiushi Wang", "Zefei Long", "Rong Ye", "Zhongtian Lu", "Xianyin Zhang", "Bingxuan Li", "Wei Chen", "Liwen Zhang", "Zhongyu Wei"], "title": "FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios", "categories": ["cs.CE", "cs.LG"], "comment": "NLPCC 2025 Oral", "summary": "Financial report generation tasks range from macro- to micro-economics\nanalysis, also requiring extensive data analysis. Existing LLM models are\nusually fine-tuned on simple QA tasks and cannot comprehensively analyze real\nfinancial scenarios. Given the complexity, financial companies often distribute\ntasks among departments. Inspired by this, we propose FinTeam, a financial\nmulti-agent collaborative system, with a workflow with four LLM agents:\ndocument analyzer, analyst, accountant, and consultant. We train these agents\nwith specific financial expertise using constructed datasets. We evaluate\nFinTeam on comprehensive financial tasks constructed from real online\ninvestment forums, including macroeconomic, industry, and company analysis. The\nhuman evaluation shows that by combining agents, the financial reports generate\nfrom FinTeam achieved a 62.00% acceptance rate, outperforming baseline models\nlike GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%\naverage improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project\nis available at https://github.com/FudanDISC/DISC-FinLLM/.", "AI": {"tldr": "The paper introduces FinTeam, a multi-agent financial report generation framework, outperforming existing models in real financial analysis tasks.", "motivation": "Existing LLM models are inadequate for comprehensive financial analysis, prompting the need for specialized systems.", "method": "A four-agent workflow of LLMs (document analyzer, analyst, accountant, and consultant) trained on specific financial expertise datasets.", "result": "FinTeam achieved a 62.00% human acceptance rate on financial report generation and improved performance on FinCUGE and FinEval benchmarks.", "conclusion": "FinTeam demonstrates the potential of collaborative multi-agent systems for handling complex financial analyses effectively."}}
